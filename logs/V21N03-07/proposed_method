本研究では，ゼロ照応解析および著者・読者表現推定において，ランキング学習と呼ばれる手法を利用する．
ランキング学習は優先度学習とも呼ばれ，インスタンス間のランキングを学習するための機械学習手法である[CITE]．
ランキング学習では識別関数を[MATH]とし以下のように[MATH]を学習する．
ここで[MATH]は，入力インスタンスの素性表現であり，[MATH]は[MATH]に対応する，重みベクトルである．
まずランキングに含まれる各インスタンスの組み合わせを生成する．
ここでランキング[MATH]を考えると，生成される組み合わせは[MATH]，[MATH]，[MATH]となる．
そして各組み合わせにおいて識別関数の値がランキング上の順序と同じになるように[MATH]を学習する．
上述の例で，各インスタンスに対応する素性ベクトルが[MATH]，[MATH]，[MATH]だとすると，[MATH]などとなるように学習する．
なお，学習する順位内に同順位のものがあっても，「それらが同順位である」ということは学習されない．
例えば[MATH]という順位があった場合には生成される組み合わせは[MATH]，[MATH]だけであり[MATH]という関係が考慮されることはない．
また，同時に複数のランキングを学習することも可能である．
例えば，[MATH]と[MATH]という二つの独立したランキングがあった場合には[MATH]，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]のようにそれぞれ独立した組み合わせを生成し，これら全てを満たすように識別関数を学習する．
未知のインスタンス集合に対するランキング予測では，各インスタンスに対して学習された[MATH]を用いて[MATH]を計算し，その値の順が出力されるランキングとなる．
ランキング学習は二値分類に適用することが可能であり，正例と負例に対応関係がある場合には通常の二値分類よりも有効であると言われている[CITE]．
これは通常の二値分類器では，全ての正例と負例を同一の特徴空間に写像するが，ランキング学習では正例と負例の差を特徴空間に写像するためである．
例えば，入力[MATH]に対する出力候補が[MATH]，入力[MATH]に対する出力候補が[MATH]となるような学習事例があったとする．
この場合，通常の二値分類器では[MATH]のように事例をひとまとめにして扱うため，本来直接の比較の対象ではない[MATH]と[MATH]などが同一の特徴空間上で比較されることとなる．
一方，ランキング学習であれば，[MATH]と[MATH]のように，ランキングとして表現することで，[MATH]と[MATH]などが同一特徴空間上で比較されることはない．
このようにして学習された識別関数は二値分類問題における識別関数として利用することができ，二値分類の場合と同様に出力の信頼度としても利用できる．
そこで本研究では，入力毎の出力候補に対して正負の正解がラベル付けされた事例からランキング学習により識別関数を学習し，推定の際には識別関数の出力が最も高くなる（最尤）ものを出力する形でランキング学習を利用する．
本節では本研究でのベースラインとなる外界ゼロ照応を考慮しないゼロ照応解析モデルを説明する．
本研究ではゼロ照応解析を用言単位の述語項構造解析の一部として扱う．
用言単位の述語項構造解析では，用言と複数の項の間の関係を扱うことができる．
例えば「(不動産屋ガ)物件を紹介する」のガ格のゼロ照応解析ではヲ格の項が「物件」であることが大きな手掛かりとなる．
各述語項構造は格フレームと，その格フレームの格スロットとその格スロットを埋める項の対応付けとして表現される．
格フレームは用言の用法毎に構築されており，各格フレームはその用言が項を取る表層格（格スロット）とその格スロットの項として取られる語（用例）からなる．
本研究では，Webページから収集された69億文から[CITE]の手法で自動構築された格フレームを用いる．
構築された格フレームの例を図[REF_述語項構造解析によるゼロ照応解析の概要]に示す．
本研究では，ゼロ代名詞の照応先を談話要素という単位で扱う．
談話要素とは文中の表現のうち共参照関係にあるものをひとまとめにしたものである．
例えば図[REF_述語項構造解析によるゼロ照応解析の概要]の例では，「僕」と「自分」や「ラーメン屋[MATH]」，「その店」，「ラーメン屋[MATH]」と「お店」は共参照関係にあるので，それぞれ一つの談話要素として扱う．
そしてゼロ代名詞の照応先はこの談話要素から選択する．
例えば，「紹介したい」のガ格では「僕」に対応する(a)を照応先として選択することになる．
述語項構造解析の例を図[REF_述語項構造解析によるゼロ照応解析の概要]に示す．
なお，本研究ではゼロ照応解析の対象としてはガ，ヲ，ニ，ガ2格のみを扱うため，時間格などの他の格については省略することがある．
ここでガ2格とは京都大学テキストコーパスで定義されている，二重主格構文における主格にあたる格である．
この例では，「紹介する」に対応する格フレームから「紹介する(1)」を選択し，そのガ格に談話要素(a)，ヲ格に談話要素(c)，時間格に談話要素(d)を対応付け，それ以外の格には談話要素を対応付けない．
ゼロ照応解析の出力としては，ガ格の談話要素(a)のみが出力される．
ベースラインモデルでは，先行研究[CITE]と同様に以下の手順で解析を行う．
形態素解析，固有表現認識，構文解析を行う．
共参照解析を行いテキスト中に出現した談話要素を認識する．
各用言について以下の手順で述語項構造を決定する．
以下の手順で解析対象用言がとりえる述語項構造（格フレームと談話要素の対応付け）の組み合わせを列挙する．
解析対象用言の格フレームを1つ選ぶ．
解析対象用言と係り受け関係にある語と格スロットの対応付けを行う．
対応付けられなかったガ格，ヲ格，ニ格，ガ2格の格スロットと，対象用言の格スロットとまだ対応付けられていない談話要素の対応付けを行う．
学習されたランキングモデルによりもっとも高いスコアが与えられたものを述語項構造として出力する．
先行研究と本研究でのベースラインモデルとの違いは，([REF_手順:スコア関数])でのスコア付けの際の重みの学習方法の違いである．
先行研究では対数線形モデルを利用していたが，本研究ではランキング学習を用いた．
このランキング学習の詳細は[REF_素性の重みの学習]節で説明する．
手順([REF_述語項構造])の述語項構造解析について説明する．
まず，([REF_述語項構造列挙])の手順で候補となる述語項構造[MATH]を列挙する．
ここで[MATH]は選ばれた格フレーム，[MATH]は格スロットと談話要素の対応付けである．
ただし，同一用言の複数の格に同じ要素が入りにくいという経験則[CITE]から，手順([REF_ゼロ照応])では既に他の格に対応付けられた談話要素は，ゼロ代名詞には対応付けないこととする．
手順([REF_述語項構造列挙])で列挙される述語項構造の例を図[REF_述語項構造の候補の例]に示す．
【1-1】と【2-1】，【1-2】と【2-2】などは，格と談話要素の割り当ては同じであるが格フレームは異なるため別々の述語項構造候補として扱う．
【1-2】と【2-2】のどちらを述語項構造として選んでもゼロ照応解析としての出力は同じになる．
この列挙された述語項構造をそれぞれ[REF_述語項構造を表現する素性]節で説明する手法で素性として表現し，[REF_素性の重みの学習]節で説明する方法で学習された重みを利用してスコア付けを行い，最終的に最もスコアが高かった述語項構造を出力する．
本節では述語項構造を表現する素性について説明する．
入力テキスト[MATH]の解析対象用言[MATH]に格フレーム[MATH]を割り当て，その格フレームの格スロットと談話要素の対応付けを[MATH]とした述語項構造を表現する素性ベクトルを[MATH]とする．
[MATH]は直接係り受けがある述語項構造に関する素性ベクトル[MATH]とゼロ照応解析で対象となる各格[MATH]に談話要素[MATH]が割り当てられることに関する素性ベクトル[MATH]からなり，具体的には以下のような形とする．
ここで，[MATH]は用言[MATH]と直接係り受けのある談話要素と格スロットの対応付けである．
各格に対応する素性ベクトル[MATH]は格[MATH]に談話要素[MATH]が対応付けられた場合の素性ベクトル[MATH]と何も対応付けられなかった場合の素性ベクトル[MATH]からなる．
[MATH]は格[MATH]がゼロ照応として対応付けられた場合にのみ考慮し，直接係り受け関係にある談話要素に対応付けられた場合には0ベクトルとする．
例えば図[REF_述語項構造の候補の例]の【2-2】を表現する素性ベクトル[MATH]は以下のようになる．
(紹介する(2),{ガ:(a)僕，ヲ:(c)ラーメン屋，ニ:×，ガ2:×，時間:(d)今日})= \nonumber
[-2pt]
(\phi_{overt-PAS}(紹介する(2),{ガ:×，ヲ:(d)ラーメン屋，ニ:×，
ガ2:×，デ格:(c)ブログ}),
[-2pt]
&\phi_{A}(紹介する(2),ガ\leftarrow(a)僕),&0__\mathit{NA},\nonumber
[-2pt] &0__\mathit{A},&0__\mathit{NA},
[-2pt] &0__{A},&\phi_\mathit{NA}(紹介する(2),ニ\leftarrow×),\nonumber
[-2pt] &0__{A},&\phi_\mathit{NA}(紹介する(2),ガ2 \leftarrow×))\nonumber
述語項構造ベクトルを表現する各要素[MATH]，[MATH]，[MATH]の素性について説明する．
まず，[MATH]には確率的格解析モデル[CITE]から得られる表層の係り受けの確率を用いる．
[MATH]に用いる素性の一覧を表[REF_割り当ての素性一覧(1)]に示す．
格フレーム素性は，格フレームから得られる情報である．
[MATH]が複数回言及される場合には，各素性ごとにそれらの値で最も大きいものをその素性の値とする．
例えば，談話要素[MATH]が格フレーム[MATH]の格[MATH]に対応付く確率の素性を式([REF_素性ベクトル例])のガ格について考える．
上述の例ではガ格に対応付けられた談話要素(a)は「僕」「自分」と2回言及されている．
そこで「僕」「自分」が「紹介する(2)」のガ格に対応付く確率をそれぞれ計算し，最も値が高いものを(a)が「紹介する(2)」のガ格に対応付く確率とする．
用言素性における[MATH]の持つモダリティなどの情報は，用言の属する基本句に日本語構文・格解析システムKNP ver. 4.0により付与された情報を利用する．
文脈素性は[MATH]が前後の文脈でどのような表現で出現するかを扱う素性であり，[MATH]が複数回言及される場合には，その全てを素性として扱う．
[MATH]が割り当てられたことの素性は，その格にどの程度ゼロ代名詞が出現するかを調整するための素性となっている．
[MATH]に用いる素性を表[REF_割り当てないの素性一覧]に示す．
[MATH]では対応付けられる要素[MATH]がないため，格フレームに関する素性のみとなっている．
前節で入力テキスト[MATH]，解析対象用言[MATH]が与えれられたとき，格フレーム[MATH]，格スロットと談話要素の対応付け[MATH]からなる述語項構造を表現する素性を[MATH]としたが，それに対応する素性の重み[MATH]をランキング学習により学習する．
ランキング学習の学習データ作成は，対象用言ごとに順位データを作成し，全用言の順位データを集約したものとする．
もし，述語項構造の正解が一意に求められるなら，その述語項構造を上位とし，それ以外の述語項構造を下位とする順位データを作成すればよい．
しかし，実際には以下の2つの問題がある．
1つ目は正解コーパスには1つの格に対して複数の談話要素が対応付けられたものが含まれることである．
例えば図[REF_述語項構造の候補の例(2)]の「焼いている」では正解として{ガ:×，ヲ:(b)ケーキ+(c)クッキー，ニ:×，ガ2:×，時間:(d)毎週}のように，ヲ格に2つの談話要素が対応付けられる．
一方，提案手法では先に述べたように1つの格に対して1つの談話要素しか対応付けない．
そこで，1つの格に対して複数の談話要素が対応付けられている場合には，そのうちどれか1つの談話要素を割り当てていれば正解として扱うこととする．
例えば，図[REF_述語項構造の候補の例(2)]では{ガ:×，ヲ:(b)ケーキ，ニ:×，ガ2:×，時間:(d)毎週}と{ガ:×，ヲ:(c)クッキー，ニ:×，ガ2:×，時間:(d)毎週}を正解の談話要素対応付けとする．
また，この正解となる対応付けの集合を[MATH]とする．
2つ目はコーパスには格フレームの正解は付与されていないことである．
先に述べたように述語項構造は格フレームと格スロットと談話要素の対応付けからなる．
格フレームは用言の用法ごとに構築されており，述語項構造候補の格フレームには文脈で使用される用法と全く異なるものが含まれる．
格スロットと談話要素の対応付けは正しいが，文脈での使用とは異なる用法の格フレームを持つような述語項構造を正解として扱った場合，学習に悪影響を与えると考えられる．
そこで，確率的ゼロ照応解析[CITE]を利用することで，各文脈における用法の格フレームを推定する．
確率的ゼロ照応解析では各述語項構造[MATH]に対し格フレームの情報などを用いることで[MATH]を推定する．
ここで[MATH]は文章中に出現する談話要素[MATH]の集合である．
具体的には以下の手順で各対象用言[MATH]に対して学習データとなるランキングを生成する．
用言[MATH]に対して取り得る述語項構造[MATH]を訓練事例として列挙する．
正解となる対応付け[MATH]について
各[MATH]の確率的ゼロ照応解析確率を計算し，最も確率が高いものを[MATH]とする．
[MATH]のうち，[MATH]以外のものを訓練事例から取り除く．
各[MATH]が他の[MATH]より順位が高くなるようなランキングを用言[MATH]に対する学習データとする．
図[REF_述語項構造の候補の例(2)]の「焼いている」を例に説明する．
まず「焼いている」の述語項構造解析の候補として【1-1】，[MATH]，【2-1】，[MATH]を列挙する（手順([REF_190308_16Oct13])）．
このうち，格と談話要素の対応付けが正解となるもの[MATH]は，{ガ:×，ヲ:(b)ケーキ，ニ:×，ガ2:×，時間:(d)毎週}となっている【1-2】と【2-2】，{ガ:×，ヲ:(c)クッキー，ニ:×，ガ2:×，時間:(d)毎週}となっている【1-3】と【2-3】である．
【1-2】，【2-2】，【1-3】，【2-3】について確率的ゼロ照応解析スコアを計算した結果，【1-2】[MATH]【2-2】，【1-3】[MATH]【2-3】となったとする．
この場合【1-2】と【1-3】が[MATH]となる（手順([REF_190424_16Oct13])）．
そこで，訓練事例から【2-2】と【2-3】を取り除く（手順([REF_190439_16Oct13])）．
そして，【1-2】[MATH]【1-3】[MATH]【1-1】[MATH]【1-4】[MATH]【2-1】[MATH]【2-4】[MATH]というランキングを「焼いている」についての学習データとする（手順([REF_190449_16Oct13])）．
このように各対象用言に対するランキング学習データを生成し，それらを統合したものに対してランキング学習を行うことで[MATH]を学習する．
本研究では，Diverse Document Leads Corpus (DDLC) [CITE]を利用する．
DDLCはWebから収集されたテキストに対して，形態素情報，構文関係，固有表現，共参照関係，述語項構造，著者・読者表現が付与されている．
形態素情報，構文関係，固有表現，共参照関係は京都大学テキストコーパス[CITE]と同様の基準で付与されている．
述語項構造も京都大学テキストコーパスと同様の基準で付与されており，文章内ゼロ照応だけでなく外界ゼロ照応も付与されている．
外界ゼロ照応の照応先としては表[REF_外界ゼロ照応の照応先と例]の5種類が設定されている．
著者・読者表現は，「=:[著者]」「=:[読者]」というタグで基本句単位に付与されており，著者・読者表現が複合語の場合にはその主辞に対して付与されている．
DDLCでは，著者表現，読者表現は1文書中にそれぞれ最大でも1つの談話要素と仮定されており，著者・読者が複数回言及される場合には，そのうち1つに「=:[著者]」「=:[読者]」を付与し，それ以外のものは，著者・読者表現と共参照関係にある，という形で表現される．
下記の例[REF_こま]では，著者は「主婦」や「こま」，「母」など複数の表現で言及されているが，「=:[著者]」は「主婦」に対してだけ付与され，「こま」や「母」には「=:主婦」というタグにより，「主婦」と共参照関係にあるという情報が付与されている．
\ex .
東京都に住む「お気楽主婦」こまです。
０歳と６歳の男の子の母をしてます。
[MATH]\leftarrow[MATH]\leftarrow[MATH]\leftarrow[MATH]また，組織のウェブページなどの場合にはその組織名や組織を表す表現を著者表現としている．
\ex.ここでは弊社の商品及び事業を簡単にご説明します。
(弊社[MATH]=:[著者]) \ex.神戸徳洲会病院では地域の医療機関との連携を大切にしています。
(病院[MATH]=:[著者])ウェブページでは実際には不特定多数が閲覧できる状態であることが多いが，著者が特定の読者を想定していると考えられる場合には，その特定の読者を表す表現も読者表現として扱っている．
下記の例[REF_読者=人]では，想定している読者が「今後就職を迎える人」だと考えられるので，その主辞の「人」に「=:[読者]」が付与されている．
\ex.今後就職を迎える人に，就職活動をどのように考えれば良いのかをお知らせしてみましょう。
(人[MATH]=:[読者])一方，想定している読者のうち一部だけを対象とした表現は読者表現として扱っていない．
下記の例[REF_ローソン]では，想定される読者は「オーナーを希望する人」であり，「店舗運営の経験がない方」はそのうちの一部であると考えられるので，読者表現として扱われていない．
\ex.店舗運営の経験がない方でも、ご安心ください。
ローソンの研修制度なら、オーナーに必要とされるノウハウを段階的に修得することができます。
表[REF_著者・読者表現の例]にDDLC中の著者・読者表現の例を示す．
DDLC全体1,000文書のうち著者表現が付与された文書は271文書，読者表現が付与された文書は84文書であった．
DDLCにおけるゼロ照応の個数を表[REF_ゼロ照応の個数]に，文章内ゼロ照応，外界ゼロ照応における照応先の内訳を表[REF_本コーパスの文章内ゼロ照応の内訳]と表[REF_本コーパスの外界ゼロ照応の内訳]に示す．
表[REF_本コーパスの文章内ゼロ照応の内訳]において著者・読者とは照応先が著者・読者表現にあたることを示す．
DDLCにおいてはゼロ照応のうち54%が外界ゼロ照応であること，ゼロ照応はガ格で特に多く起こることが分かる．
また，著者や読者に関するゼロ照応はガ格，ニ格，ガ2格で多く出現し，ヲ格ではほとんど出現しないことが分かる．
日本語では様々な表現で著者や読者が文章中で言及され，[REF_130555_9May13]節で述べたように人称代名詞だけでなく，固有表現，役職など様々な表現で言及される．
一方，表[REF_著者・読者表現の例]に挙げたような表現でも文脈によっては著者・読者表現にならないこともある．
下記の例[REF_お客様not著者]では，「お客様」はこの文章の読者として想定している客とは別の客を指していると考えられるので，読者表現とはならない．
\ex.先月、お部屋のリフォームをされたお客様の例を紹介します。
このように，表記のみから著者・読者表現を同定することは困難である．
本研究では著者・読者表現候補自体の表現だけでなく，周辺文脈や文章全体に含まれる情報から文章の著者・読者表現を推定することとする．
[REF_130555_9May13]節で述べたように，DDLCでは基本句単位で著者・読者表現がアノテーションされており，共参照関係にある複数の表現が著者・読者表現である場合には，その内の1つに対して著者・読者表現を付与するとしている．
これは[REF_114838_18Jun13]節で述べた談話要素単位に著者・読者表現が付与されていると言え，本研究でも著者・読者表現は談話要素単位で扱う．
著者・読者表現の推定にはランキング学習を利用し，その素性には著者・読者表現自身および周辺文脈の語彙統語パターンを利用する．
著者・読者表現の推定は著者表現，読者表現それぞれ独立して行う．
著者・読者表現の推定にはランキング学習を利用し，著者・読者表現にあたる談話要素が他の談話要素より上位になるように学習する．
例えば図[REF_著者表現が出現する文章例]の著者表現では，談話要素(1)が他の談話要素より上位となる学習データを作成する．
そして学習された識別関数により最上位となった談話要素を著者・読者表現と推定する．
なお，著者・読者表現候補として扱う談話要素は下記の条件のうち最低1つは満たしているもののみとする．
自立語の形態素のJUMANカテゴリが「人」「組織・団体」「場所」
固有表現である
形態素に「方」「人」を含む
ここで，学習データ作成時および推定時に考慮しなければならないのは，著者・読者表現が出現しない文書が存在することである．
著者・読者表現が出現しない文書は大きく分けて2つの種類がある．
1つ目は図[REF_談話構造自体に著者が出現しない文章例]のように談話構造自体に著者・読者が出現しない文書である．
2つ目は図[REF_談話構造に著者が出現するが著者表現が出現しない文章例]のように談話構造には著者・読者が出現するが著者・読者表現として明示的に言及されない場合である．
これらに対応する仮想的なインスタンスとして，「著者・読者表現なし(談話構造)」と「著者・読者表現なし(省略)」を設定する．
「著者・読者表現なし(談話構造)」は談話構造自体に外界ゼロ照応としても著者・読者が出現しないことに対応するインスタンスであり，文書全体の語彙統語パターンを素性とした文書ベクトルで表現されるインスタンスである．
これは，著者・読者が談話構造自体に出現しない文書では，尊敬や謙譲表現が少ないなど文体的な特徴があると考えられ，文書全体の語彙統語パターンは文体を反映した素性といえるからである．
「著者・読者表現なし(省略)」は談話構造には外界ゼロ照応として著者・読者が出現するが，著者・読者表現として明示的に言及されないことに対応するインスタンスであり，ゼロベクトルとして表現される．
識別関数はゼロベクトルについて常に[MATH]を返すため，このインスタンスより下位に順位付けされた談話要素は二値分類における負例とみなすことができる．
学習された識別関数によるランキングの結果，これらのインスタンスが最上位となった文書については，著者・読者表現が出現しないものとする．
各文書に対する学習データの作成について説明する．
以下の手順で著者表現，読者表現に対して文書ごとにランキングデータ作成し，統合したものを最終的な学習データとする．
著者・読者表現が存在する文書については，著者・読者表現にあたる談話要素が他の談話要素および「著者・読者表現なし」より上位になるように学習データを作成する．
例えば，図[REF_著者表現が出現する文章例]の文書における著者表現推定では，
となるように学習データを作成する．
談話構造自体に著者・読者が出現しない場合には，「著者・読者表現なし(談話構造)」が文書中の談話要素および「著者・読者表現なし(省略)」より上位になるように学習データを作成する．
例えば，図[REF_談話構造自体に著者が出現しない文章例]の文書における著者表現推定では，
となるように学習データを作成する．
談話構造に著者・読者が出現するが著者・読者表現が出現しない場合には，「著者・読者表現なし(省略)」が文書中の談話要素および「著者・読者表現なし(談話構造)」より上位になるように学習データを作成する．
例えば，図[REF_談話構造に著者が出現するが著者表現が出現しない文章例]の文書における著者表現推定では，
となるように学習データを作成する．
学習時の談話構造に著者・読者が出現するかの判定には，コーパスに付与された外界ゼロ照応の情報を利用する．
外界ゼロ照応の照応先として著者・読者が出現する場合には談話構造に著者・読者が出現するとし，それ以外の場合には出現なしとする．
例えば図[REF_談話構造に著者が出現するが著者表現が出現しない文章例]では，「気がつけば」のガ2格，「ほっとしました」のガ格などの照応先で著者が出現するので，談話構造に著者が出現していると分かる．
なお，この情報はランキングの学習データを作成する際にのみ利用するので，テストデータに対する著者・読者表現推定時には利用しない．
談話要素に対しては，談話要素自身と係り先およびこれらの係り受けの語彙統語パターンを素性として扱う．
ここで，語彙統語パターンを扱う単位として，基本句と文節という2つの単位を考える．
談話要素は基本句単位であるが，その基本句が含まれる文節の情報も重要と考えられるからである．
談話要素を表現する語彙統語パターンとしては，談話要素が含まれる基本句・文節，談話要素の係り先の基本句・文節およびこれらの係り受け関係を後述する基準にて汎化したものとなる．
1つの談話要素が複数言及されている場合には，それらを合わせたものをその談話要素の素性として用いる．
また，1文目には自己紹介的な表現が用いられることが多く，以降の文と区別して扱うことが有効であると考えられる．
そこで，1文目で出現した語彙統語パターンは別の素性としても扱うこととする．
例えば，図[REF_著者表現が出現する文章例]の談話要素(1)に対応する素性として利用するものは以下のものを汎化したものとなる．
「基本句:ホテルは」「基本句係り先:ございます。
」「基本句係受け:ホテルは[MATH]ございます。
」「文節:米子タウンホテルは」「文節係り先:ございます。
」「文節係り受け:米子タウンホテルは[MATH]ございます。
」「基本句:ホテルです。
」「文節:ホテルです。
」「1文目基本句:ホテルは」「1文目基本句係り先:ございます。
」「1文目基本句係受け:ホテルは[MATH]ございます。
」「1文目文節:米子タウンホテルは」「1文目文節係り先:ございます。
」「1文目文節係り受け:米子タウンホテルは[MATH]ございます。
」
これらの要素を表[REF_汎化する種類と基準]の基準で汎化することで語彙統語パターンの素性として利用する．
「形態素単位A」の汎化は形態素単位に付与された情報を元に形態素毎に汎化を行う．
なお「品詞+活用」では内容語に対してのみ行い，機能語については汎化しない．
「形態素単位B」でも形態素単位での汎化を行うが，内容語に対する汎化のみを行う．
対象となる汎化表現を持たない場合（固有表現による汎化の際に固有表現を持たない場合など）には「品詞+活用」で汎化を行う．
「形態素単位C」では「形態素単位B」と同様に汎化を行うが，複数の形態素をまたいだ汎化を行う場合がある．
分類語彙表による汎化では，分類語彙表に複合語として登録されている場合には，その複合語の分類語彙表の内容を利用する．
例えば，「ゴルフ場」は2形態素であるが，分類語彙表には「ゴルフ場:土地利用」として登録されているので，「ゴルフ場」を「土地利用」と汎化する．
固有表現による汎化では，形態素に付与された固有表現は「固有表現名:head」「固有表現名:middle」「固有表現名:tail」「固有表現名:single」のように固有表現中での位置が付与されている．
文節による汎化の際には連続したこれらの表現をまとめて「固有表現」という形に汎化する．
例えば「ヤフージャパン株式会社」では「ORGANIZATION:head+ORGANIZATION:middle+ORGANIZATION:middle+[2]ORGANIZATION:tail」のように固有表現が付与されるが，「ORGANIZATION」として汎化する．
これらの形態素単位での汎化では，形態素ごとに汎化を行い，その後基本句内，文節内で汎化された情報を結合することでその基本句，文節の語彙統語パターンとする．
例えば，「基本句:ホテルは」をカテゴリ(CT)の基準で汎化する際には，「ホテル」を「CT-場所-施設」に汎化し，「は」は機能語なのでそのまま「は」とする．
そしてこれらを結合した「基本句:CT-場所-施設+は」がこの基本句のカテゴリによる汎化表現となる．
上述の形態素単位の汎化の場合には，基本句，文節内に含まれる個々の汎化表現も素性とする．
例えば，「基本句:ホテルは」のカテゴリによる汎化では上述の「基本句:CT-場所-施設+は」に加えて，「基本句内形態素:CT-場所-施設」も素性とする．
基本句・文節単位での汎化は基本句・文節に付与された情報を元に汎化を行う場合に利用する．
基本句・文節単位での汎化では基本句・文節に付与された情報そのものを基本句，文節の語彙統語パターンとして利用する．
このため，形態素単位による情報は素性としては利用しない．
本節では，外界ゼロ照応および著者・読者表現を考慮したゼロ照応解析モデルについて説明する．
提案モデルでは，ベースラインモデルと同様にゼロ照応解析を述語項構造解析の一部として解く．
提案モデルではゼロ照応解析は以下の手順で行う．
形態素解析，固有表現認識，構文解析を行う．
共参照解析を行いテキスト中に出現した談話要素を認識する．
著者・読者表現の推定を行い，どの談話要素が著者・読者表現にあたるのかを推定する．
推定された著者・読者表現から仮想的な談話要素を設定する（[REF_節:仮想的談話要素]節で説明）．
各用言について以下の手順で述語項構造を決定する．
以下の手順で解析対象用言がとりえる述語項構造（格フレームと談話要素の対応付け）の組み合わせを列挙する．
解析対象用言の格フレームを1つ選ぶ．
解析対象用言と係り受け関係にある語と格スロットの対応付けを行う．
対応付けられなかったガ格，ヲ格，ニ格，ガ2格の格スロットと，対象用言の格スロットとまだ対応付けられていない談話要素の対応付けを行う．
学習されたランキングモデルによりもっとも高いスコアが与えられたものを述語項構造として出力する．
ベースラインモデルと異なる点は，手順([REF_手順:著者・読者推定])で文章中の著者・読者表現を推定すること，手順([REF_手順:仮想的談話要素])で仮想的な談話要素を設定することである．
ベースラインモデルではゼロ代名詞の照応先を文章中の談話要素から選択することとした．
提案モデルでは，文章中の談話要素に加えて仮想的な談話要素として[著者]，[読者]，[不特定:人]，[不特定:その他]を設定し，解析の際の述語項構造の列挙においては，格に対応付ける談話要素の候補としてこれらの仮想的な談話要素も考えることとする．
そして，これらが対応付けられた格は外界ゼロ照応であるとする．
例[REF_仮想的談話要素例]では，「説明します」のガ格が外界ゼロ照応で著者を照応しており，ニ格は外界ゼロ照応で読者を照応しているため，ガ格に[著者]を，ニ格に[読者]を対応付けた述語項構造として表現される．
\ex.今日はお得なポイントカードについて説明します。
著者・読者表現が文章中に出現する場合には，[著者]と[読者]の仮想的談話要素の扱いが問題となる．
下記の例[REF_著者・読者表現あり]ではゼロ代名詞[MATH]の照応先は，著者表現である「私」とも[著者]とも考えられる．
\ex.肩こりや腰痛で来院された患者さんに対し、私[MATH]は脈を診ることにしています。
それは心臓の状態を([MATH]ガ)診ているだけではなく、身体全体のバランスを([MATH]ガ)診たいからです。
本研究では，このような曖昧性を取り除くため，照応先としては[著者]，[読者]より著者・読者表現を優先することする．
解析の際，文章中に著者・読者表現が存在する場合には，[著者]，[読者]の仮想的談話要素は照応先として対応付けないこととする．
図[REF_述語項構造解析によるゼロ照応解析の概要]の「紹介します」に対して列挙される述語項構造の例を図[REF_提案手法における述語項構造の候補の例]に示す．
この例では，【1-3】のガ格や【1-4】のガ格などに仮想的な談話要素が対応付けられている．
また，「(a)僕」が著者表現にあたるので[著者]はどの格にも対応付けを行わない．
一方，著者・読者表現は外界の[著者]や[読者]と似た振る舞いを取ると考えられる．
そこで，著者・読者表現は他の談話要素と区別し，素性表現の際に[著者]や[読者]の性質を持つように素性を与える．
詳細は[REF_素性による述語項構造の表現]節で示す．
ベースラインモデルと同様に提案モデルでも述語項構造単位を素性で表現し，その構成もベースラインモデルと同様に[MATH]は直接係り受けがある述語項構造に関する素性ベクトル[MATH]とゼロ照応解析で対象となる格[MATH]に談話要素[MATH]が割り当てられることに関する素性ベクトル[MATH]からなる．
ベースラインモデルと提案モデルの差は[MATH]のうち，[MATH]に談話要素[MATH]が割り当てられた場合の素性ベクトル[MATH]の構成である．
ここでベースラインモデルでの[MATH]を[MATH]とおく．
この[MATH]と同内容，同次元の素性ベクトルを文章中に出現した談話要素，外界の[著者]，[読者]などと対応する形で複数並べることで[MATH]を構成する．
具体的には以下のような形となる．
\phi_{A}(\mathit{cf},c \leftarrow e,p,t) = (&\phi_\mathit{mentioned}(\mathit{cf},c \leftarrow e,p,t),\phi_{[著者]}(\mathit{cf},c \leftarrow e,p,t),
& \phi_{[読者]}(\mathit{cf},c \leftarrow e,p,t),\phi_{[不特定:人]}(\mathit{cf},c \leftarrow e,p,t),
& \phi_{[不特定:その他]}(\mathit{cf},c \leftarrow e,p,t),\phi_\mathit{max}(\mathit{cf},c \leftarrow e,p,t) )
ここで，[MATH]は[MATH]が文章中に出現した談話要素の場合のみに発火し，内容は[MATH]と同内容とする．
[MATH]，[MATH]は[MATH]が外界の[著者]，[読者]の場合または著者・読者表現に対応する談話要素の場合にのみ発火する．
[MATH]，[MATH]は[MATH]が外界の[不特定:人]，[不特定:その他]の場合にのみ発火する．
[MATH]，[MATH]，[MATH]，[MATH]は外界の談話要素に対応するため，表記やJUMANカテゴリといった情報を持たない．
そこで，格フレーム素性で[MATH]の表記やJUMANカテゴリの情報を利用する場合には擬似的に表[REF_疑似表記，JUMANカテゴリ]の表記やJUMANカテゴリを利用する．
最後の[MATH]は各素性において[MATH]，[MATH]，[MATH]，[MATH]，[MATH]で対応する素性の最大値を持つ素性ベクトルとする．
このように素性を表現することで，著者・読者表現に対応する談話要素では[MATH]および[MATH]・[MATH]が発火することになり，通常の談話要素と著者・読者としての両方の性質を持つこととなる．
また，[MATH]は全ての要素に対して発火するため，[MATH]に対応する重みでは，ゼロ照応全体に影響する性質が学習されると考えられる．
ここで，図[REF_提案手法における述語項構造の候補の例]の述語項構造候補【1-5】ついて各格の[MATH]の例を示す．
\phi_{A}(\mathit{cf},ガ\leftarrow(a)僕,p,t) & = (\phi_\mathit{mentioned}(\mathit{cf},ガ\leftarrow(a)僕,p,t),\phi_{[著者]}(\mathit{cf},ガ\leftarrow(a)僕,p,t),
&　　0__{[読者]},0__{[不特定:人]},0__{[不特定:その他]},
&　　\mathit{max}(\phi_\mathit{mentioned}(\mathit{cf},ガ\leftarrow(a)僕,p,t),\phi_{[著者]}(\mathit{cf},ガ\leftarrow(a)僕,p,t)) )
まず，ガ格では「僕」は文章中に言及されているので，[MATH]が発火し，また著者表現に対応している談話要素なので[MATH]も発火する．
[MATH]，[MATH]，[MATH]は発火せず0ベクトルとなる．
[MATH]は各要素において[MATH]と[MATH]のうち大きい方の値が素性の値となる．
\phi_{A}(cf,ニ\leftarrow[読者],p,t) & = (0__\mathit{mentioned},0__{[著者]},
&　　\phi_{[読者]}(\mathit{cf},ニ\leftarrow[読者],p,t),0__{[不特定:人]},
&　　0__{[不特定:その他]},\mathit{max}(\phi_{[読者]}(\mathit{cf},ニ\leftarrow[読者],p,t)) )
ニ格では文章中に言及されていない読者が対応付けられているので，[MATH]のみが発火し，[MATH]は[MATH]と同じ値となる．
ヲ格は直接係り受けのある「ラーメン屋」と対応付けられているので，ベースラインモデルと同様に素性として考えず，[MATH]，[MATH]ともに0ベクトルとなる．
ガ2格は談話要素に対応付けられていないので，ベースラインモデルと同様に[MATH]が発火し，[MATH]は0ベクトルとなる．
提案モデルでは[REF_述語項構造を表現する素性]節で述べたものに加えて，著者表現，読者表現推定スコアを素性として利用する．
著者表現，読者表現推定スコアは[REF_135602_6May13]節で著者・読者表現を推定した際のランキング学習の識別関数のスコアである．
著者表現推定スコアは[MATH]が著者表現の場合に[MATH]と[MATH]に，読者表現推定スコアは[MATH]が読者表現の場合に[MATH]と[MATH]に素性として導入する．
