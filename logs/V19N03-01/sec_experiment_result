本章では，提案手法の有効性を調査するために行った実験と，その結果について報告する．
実験は，第[MATH]回NTCIRワークショップで構築されたウェブ検索評価用テストセット[CITE]を用いて，これを行った．
テストセットは，[MATH]ページの日本語ウェブ文書と，[MATH]個の検索課題から成る．
検索課題ごとに，約[MATH]文書に，その課題に対する適合度が付与されている．
ただし，適合度は「高適合」「適合」「部分適合」「不適合」のいずれかである．
これらの適合度が付与された文書を用いて，検索結果のランキング精度を測ることができる．
図[REF_fig:ntcir_subject]に検索課題の一例を示す．
各タグが表す意味内容は次の通りである．
\itemsep=-0.1zw
課題番号．
検索システムに入力するであろう単語．
課題作成者によって[MATH]〜[MATH]語がリストアップされている．
左から順に重要．
課題作成者の情報要求を一文で表したもの．
情報要求に適合する代表的な文書のID．
課題作成者によって[MATH]〜[MATH]個がリストアップされている．
実験では，[MATH]TITLE[MATH]タグの単語をクエリとして使用した．
ただし，提案手法では，検索の質を高めるため，クエリを含む文書（クエリを構成する各タームが最低でも[MATH]回以上出現する文書）のみをスコア付けの対象として収集する（[REF_ssec:initial_resuls_acquisition]節参照）．
そのため，[MATH]TITLE[MATH]タグの全ての単語を用いると，多くの検索課題において，検索される文書数が極端に少なくなってしまった．
例えば，課題番号0027，0047，0058などは，それぞれ[MATH]文書，[MATH]文書，[MATH]文書しか検索できなかった．
課題番号0061に至っては[MATH]文書も検索できなかった．
このように検索される文書が少ないと，適合性フィードバックの有効性が検証しにくい．
すなわち，実際に適合性フィードバックによって初期検索結果のランキングが改善されても，その結果がP@10などの評価尺度の値に反映されにくく，適合性フィードバックが有効に働いたかどうかが判断しづらい．
そこで，実験では，この問題を避けるため，十分な検索結果が得られるように，クエリとして使用する単語を[MATH]TITLE[MATH]タグの最初の[MATH]語のみとした．
ただし，「十分」の定義は「[MATH]文書以上」とした．
また，[MATH]RDOC[MATH]タグのIDが付与された文書を，ユーザのフィードバックとして使用した．
上で述べた通り，これらは課題作成者本人によって選択された代表的な適合文書であり，フィードバックとして使用するのに最適と考えられる．
これらの文書は，提案手法の初期検索結果に含まれるとは限らない．
初期検索結果に含まれない場合，これらをユーザのフィードバックとして使用するのは奇異に感じられるかもしれない．
しかし，これらの文書は，仮に初期検索結果に含まれていた場合も，リランキング前後のランキング精度を測定・比較する際，結局ランキングから取り除かれる（[REF_ssec:evaluation_method]節で後述）．
言い換えれば，これらは，初期検索結果に含まれていた場合も，初期検索結果に含まれない場合のように，検索結果中に存在していないものとして扱われる．
このように，どちらの場合でも存在していないものとして扱われることを考えると，これらの文書が初期検索結果に含まれているか含まれていないかは重要ではない．
以上を踏まえ，実験では，これらが初期検索結果に含まれているか含まれていないかは問題にしなかった．
[MATH]個の検索課題のうち，[MATH]個の検索課題（課題番号: 0011, 0018, 0032, 0040, 0044, 0047, 0061）については，実験で使用しなかった．
これは，上で述べたようにクエリとして使用する単語を[MATH]語にしても，十分な文書（i.e., [MATH]文書）が検索できなかったためである．
さらに，残った[MATH]課題を，開発データと評価データに分けて使用した．
開発データは，提案手法のパラメータを最適化するために使用した．
評価データは，提案手法のランキング精度を測定するために使用した．
開発データには[MATH]課題（課題番号：0008〜0017）を，評価データには[MATH]課題（課題番号：0018〜0063）を使用した．
実験を行うため，提案手法に従って適合性フィードバックを行う検索システムを作成した．
実装の詳細は以下の通りである．
検索対象とする文書セット(i.e., [MATH])には，テストセットの[MATH]文書を使用した．
また，文書セット中の各文書について，次の手順に従って文書モデルを構築した．
Shinzatoらの手法[CITE]を用いて本文を抽出し，JUMAN [CITE]を用いて各文を解析する．
解析結果及び式([REF_equ:dir])を用いて，DIRに基づく文書モデルを構築する．
ただし，先行研究[CITE]に倣って，[MATH]とした．
クエリが与えられたら，次の手順に従ってクエリモデルを構築した．
JUMANを用いてクエリを解析する．
解析結果及び式([REF_equ:mle])を用いて，MLEに基づくクエリモデルを構築する．
LDAの実装については次の通りである．
パラメータ[MATH] [MATH]の初期値は[MATH]とした．
また，[MATH] [MATH]の初期値にはランダムな値を与えた．
[MATH]と[MATH]を更新する際の反復回数と，[MATH]と[MATH]を更新する際の反復回数は，それぞれ[MATH]回とした．
LDAで考慮する語彙数[MATH]は[MATH]とした．
ただし，LDAで考慮する語彙は，初期検索結果に対する重要度を基に選出した．
ここで，初期検索結果[MATH]に対する単語[MATH]の重要度は，[MATH]と定義した．
ただし，[MATH]は[MATH]における[MATH]の文書頻度を表す．
適合性フィードバックの効果は，適合性フィードバック前のランキング（i.e.,初期検索結果のランキング）と，適合性フィードバック後のランキングを比較することで検証できる．
このとき，フィードバックとして使用する文書の扱いに気を付けなければならない[CITE]．
例えば，適合性フィードバック前後のランキングをそのまま比較すると，後者が有利になってしまう．
これは，フィードバックとして与えられた文書（適合であることが分かっている文書）が，適合性フィードバック後のランキングの上位に含まれやすいためである．
そこで，適合性フィードバック前後のランキングを比較する際，フィードバックとして与えられた文書を適合性フィードバック後のランキングから取り除くという方法が考えられる．
しかし，この方法だと，適合性フィードバック前のランキングが有利になってしまう．
これは，適合文書が少ないときに特に問題となる．
以上を踏まえ，実験では，ランキングの精度を測定する際，フィードバックとして使用した文書を各ランキングから取り除いた．
これにより，適合性フィードバック前後のランキングを公平に比較することができる．
ランキング精度の評価尺度には，P@10，Mean Average Precision (MAP)，Normalized Discounted Cumulative Gain at [MATH] (NDCG@10) [CITE]を用いた．
ただし，P@10及びMAPを測定する際は，「高適合」「適合」「部分適合」の文書を正解，「不適合」及び適合度が付与されていない文書を不正解とした．
また，NDCG@10は，「高適合」の文書を[MATH]点，「適合」の文書を[MATH]点，「部分適合」の文書を[MATH]点として算出した．
まず，提案手法が初期検索結果のランキング精度をどの程度改善できるか調査した．
具体的には，初期検索結果のランキング精度と，提案手法によってリランキングを行った後のランキング精度を比較し，提案手法の有効性を検証した．
実験には評価データを使用し，各検索課題の初期検索結果を取得する際は，[REF_ssec:data]節で述べたように，[MATH]TITLE[MATH]タグの最初の[MATH]単語をクエリとして用いた．
また，実験では，[MATH]（式([REF_equ:initial_score])参照）の上位[MATH]件を初期検索結果とした．
提案手法を実行する際は，[MATH]RDOC[MATH]タグの最初の[MATH]文書をフィードバックとして用いた．
なお，これらの文書に含まれる単語数は平均[MATH]語であった．
提案手法に必要な[MATH]つのパラメータ[MATH]，[MATH]，[MATH]の値は，それぞれ[MATH]，[MATH]，[MATH]とした．
これらは，[REF_ssec:experiment0]節で述べる実験の結果を基に決定した．
結果を表[REF_sbtbl:eRF]に示す．
INITは各検索課題に対する初期検索結果のランキング精度の平均値を，OURSは提案手法実行後のランキング精度の平均値を表す．
比較のため，初期検索結果に対してベースラインとなる手法を実行したときの結果も示した．
ZHAIはZhaiらの手法[CITE]を，OURS ([MATH])は提案手法から潜在情報を除いた手法を表す．
ただし，ZHAIとOURS ([MATH])は本質的にはほとんど同じ手法である．
両手法とも，フィードバックの表層の単語分布を文書セット全体の単語分布で補正することでフィードバックモデルを構築し，これを用いてリランキングを行っている．
違うのは単語分布の補正の仕方だけである（前者はEMアルゴリズムを用い，後者はDIRを用いて補正を行っている）．
OURS ([MATH])では，[MATH]とした．
これも，[REF_ssec:experiment0]節で述べる実験の結果を基に決定した．
DICもベースラインとなる手法を表す．
提案手法の核となるアイディアは，テキスト（フィードバック及び検索結果中の各文書）に潜在的に現れうる単語の情報を適合性フィードバックに利用することである．
同義語辞書や関連語辞書などの知識リソースを用いても，同様のアイディアを実現することができる．
DICでは，OURS ([MATH])をベースに，テキスト中の各単語が同義語を持つ場合，その同義語もそのテキストに出現しているとみなした上でリランキングを行った．
ただし，同義知識は，Shibataらの手法[CITE]を用いて例会小学国語辞典[CITE]と岩波国語辞典[CITE]から獲得した．
獲得された同義知識（e.g.,「コンピュータ」＝「電子計算機」，「ポテト」＝「じゃが芋」＝「ばれいしょ」）は[MATH]個であった．
表[REF_sbtbl:eRF]を見ると，すべての尺度において，OURSがINITを大きく上回っている．
例えばP@10は[MATH]改善しており，提案手法が初期検索結果をうまくリランキングできたことが分かる．
また，提案手法は，ZHAIやOURS ([MATH])より高い性能を示した．
ZHAIやOURS ([MATH])は，テキストの表層情報だけを用いて適合性フィードバックを行っている．
一方，提案手法は，テキストの表層情報に加え，テキストの潜在情報も用いて適合性フィードバックを行っている．
提案手法がこれらの手法を上回ったことから，潜在情報が適合性フィードバックに有用であったことが分かる．
さらに，リランキング結果を調査したところ，提案手法が，テキストに表層的には出現しないが潜在的には現れうる単語の情報をうまく利用していることが確認できた．
図[REF_fig:ntcir_subject]の検索課題を例に取ると，「宗教」や「祝日」「聖書」などの単語は，情報要求によく関連するが，フィードバックとして使用した文書には含まれていなかった．
そのため，ZHAIやOURS ([MATH])では，これらの単語の情報を使用することができなかった．
一方，提案手法では，これらの単語がフィードバックにおいてもある程度の確率で現れうると推定できた．
具体的には，「宗教」「祝日」「聖書」は，それぞれ[MATH]，[MATH]，[MATH]の確率で現れうると推定できた．
なお，フィードバックに[MATH]回出現した単語として「クリスマス」や「ＥＡＳＴＥＲ」などがあったが，これらの生起確率の推定値は，それぞれ[MATH]，[MATH]であった．
提案手法では，これらの推定結果を用いることで，これらの単語を含む検索結果中の適合文書を上位にリランキングすることができた．
DICはあまり有効に機能せず，その結果はZHAIやOURS ([MATH])の結果を少し上回る程度であった．
この原因は，我々が構築した同義語辞書のカバレッジにあると思われる．
DICは，よりカバレッジの高い知識リソースが利用できれば（同義語や関連語などの知識をより多く利用できれば），より有効に機能する可能性を持つ．
しかし，そのようなリソースを構築するのは容易ではない．
一方，提案手法でも，単語と単語が関連するという知識を必要とする．
しかし，DICと違って，何のリソースも必要としない．
すなわち，提案手法では，LDAを用いることで，単語と単語が関連するという知識を検索結果から動的に獲得することができる．
[REF_sec:introduction]章の「マック価格」というクエリを例に取ると，このクエリに対する検索結果には「CPU」や「ハードディスク」「ハンバーガー」「ポテト」などの単語が含まれると考えられる．
提案手法では，検索結果に対してLDAを実行することで，「CPU」と「ハードディスク」が関連するという知識や「ハンバーガー」と「ポテト」が関連するという知識を，トピックという形で動的に獲得することができる．
そして，獲得された知識を用いることで，文書に「ハードディスク」という単語が出現していなくても，「CPU」という単語が出現していれば，「ハードディスク」も潜在的にはその文書に現れうると推測できる．
このように，DICと比べると，（カバレッジの高低に関わらず）何のリソースも必要としないという点で，提案手法の方が優れている．
提案手法は擬似適合性フィードバックにも適用可能である．
そこで，これに対するリランキング性能も調査した．
擬似適合性フィードバックでは，初期検索結果の上位[MATH]文書を適合文書とみなし，適合性フィードバックを行う．
実験では，[MATH]として初期検索結果をリランキングし，リランキング前後のランキング精度を比較した．
ただし，擬似適合性フィードバックでは，明示的なフィードバック（適合であることが分かっている文書）は存在しない．
そのため，ランキングの精度を測る際，他の実験のように，[MATH]RDOC[MATH]タグの文書を各ランキングから除くことはしなかった．
結果を表[REF_sbtbl:pRF]に示す．
INITの値が表[REF_sbtbl:eRF]と違うのは，ランキング精度を算出する際，[MATH]RDOC[MATH]タグの文書を除いていないからである．
表[REF_sbtbl:pRF]を見ると，普通の適合性フィードバックに比べると改善の度合いは小さいが，P@10やNDCG@10の値が上昇している．
例えば，P@10では[MATH]の改善が見られる．
このことから，擬似適合性フィードバックにおいても提案手法がある程度機能することが分かる．
現実的には，ユーザが多くのフィードバックを与えてくれるとは考えにくい．
そのため，適合性フィードバックの手法は，フィードバックが少ない状況でも機能するべきである．
この実験では，このような状況をシミュレートし，フィードバックが少なくても提案手法が機能するかを調査した．
具体的には，提案手法に与えるフィードバックを少しずつ減らしていき，リランキング性能がどのように変化するかを調査した．
提案手法に与えるフィードバックの分量[MATH]は，[MATH]とした．
ただし，例えば[MATH]は，フィードバックとして[MATH]文書を用いることを意味している．
また，例えば[MATH]は，フィードバックとして[MATH]適合文書の半分だけを用いることを意味している．
この場合，適合文書中の単語をランダムに半分抽出し，それらを用いて適合性フィードバックを行った．
[MATH]の場合も調査したのは，フィードバックとして文書より小さい単位（e.g.,文書のタイトル，スニペット）が与えられた場合を想定し，このような場合にも提案手法が機能するかを調べたかったからである．
結果を図[REF_fig:F]に示す．
比較のため，提案手法から潜在情報を除いたとき(i.e., OURS ([MATH]))の性能の変化も示した．
また，INITは初期検索結果のランキング精度を表す．
図から，[MATH]が小さいときでも，提案手法が高い性能を示すことが分かる．
例えば[MATH]のとき，提案手法は初期検索結果を[MATH]改善している．
さらに，[MATH]のときでも，[MATH]の改善が見られた．
なお，[MATH]のとき，フィードバック[MATH]に含まれる単語数は平均[MATH]語であった．
一方，OURS ([MATH])を見ると，[MATH]が小さくなるにつれ，ほとんど改善が見られなくなった．
OURS ([MATH])ではテキストの表層情報しか利用していない．
そのため，[MATH]が小さくなるにつれて利用できる情報が少なくなり，初期検索結果を改善できなくなったと考えられる．
一方，提案手法では，表層情報だけでなく潜在情報も利用している．
利用できる情報が多い分，[MATH]が小さいときでも，初期検索結果のランキングを改善することができたと考えられる．
提案手法には[MATH]つのパラメータ[MATH]，[MATH]，[MATH]がある．
[MATH]は[MATH]と[MATH]の混合比を調整するパラメータ（式([REF_equ:hdm])及び式([REF_equ:hfm])参照），[MATH]は[MATH]と[MATH]の混合比を調整するパラメータ（式([REF_equ:nqm])参照），[MATH]はLDAのトピック数である．
[REF_ssec:experiment1]節及び[REF_ssec:experiment2]節で述べた実験では，OURSのパラメータを[MATH]，[MATH]，[MATH]とした．
また，OURS ([MATH])のパラメータを[MATH]とした．
これらの値は予備実験の結果を基に決定した．
提案手法の性能を最大限に発揮するためには，パラメータとリランキング性能の関係について知る必要がある．
予備実験では，この関係を知るため，様々な[MATH]の組み合わせについて提案手法のリランキング性能を調査し，その結果を比較した．
ただし，[MATH]，[MATH]，[MATH]とし，全[MATH]通りの組み合わせについて，調査を行った．
開発データを用いて調査した．
ある[MATH]の組み合わせに対するリランキング性能は，他の実験と同じようにして，これを測定した．
すなわち，開発データ中の各検索課題について初期検索結果を取得し，提案手法を用いてこれらをリランキングした後，全課題におけるP@10の平均値を算出した．
他の実験と同様，クエリには[MATH]TITLE[MATH]タグの最初の[MATH]単語を，フィードバックには[MATH]RDOC[MATH]タグの最初の[MATH]文書を用いた．
結果を表[REF_tbl:experiment0]及び図[REF_fig:K]に示す．
表[REF_tbl:experiment0]は，実験結果を[MATH]についてまとめたものである．
表中の各セルの値は，各[MATH]の組み合わせについて，各[MATH]のP@10を平均したものである．
例えば，[MATH]のセルは，[MATH] [MATH]のP@10の平均値が[MATH]であったことを示している．
各列においてもっともP@10が高いセルは，その値を太字で装飾した．
また，各行においてもっともP@10が高いセルは，その値に下線を引いた．
表から，[MATH] or [MATH]のとき，リランキング性能がもっとも良いことが分かる．
また，[MATH]のとき（潜在情報を考慮しないとき）は，[MATH]が大体[MATH]〜[MATH]のとき，リランキング性能が良い．
一方，[MATH]のとき（潜在情報を考慮したとき）は，[MATH]が大体[MATH]〜[MATH]のとき，リランキング性能が良い．
[MATH]のときより，性能が良くなる[MATH]の値（及びそのときのランキング精度）が大きくなっている．
これは，潜在情報を考慮することで，フィードバックモデルの信頼度が増すことを示唆している．
図[REF_fig:K]は，[MATH]によるリランキング性能の変化を示している．
図では，表[REF_tbl:experiment0]においてリランキング性能が良かった[MATH]つの[MATH]の組み合わせ[MATH]について，[MATH]による性能の変化を示した．
図から，[MATH]が大体[MATH]〜[MATH]のとき，リランキング性能が良いことが分かる．
以上の結果をまとめると，提案手法がその性能を発揮するパラメータは，[MATH] or [MATH]，[MATH]は大体[MATH]〜[MATH]となる．
提案手法では，検索結果中の各文書に対する[MATH]を構築するため，検索結果に対してLDAを実行する．
また，フィードバックに対する[MATH]を構築する際は，フィードバックに対してLDAを実行する．
本節では，これらの処理に要する時間について考察する．
実験では，各検索課題の検索結果（[MATH]文書）に対してLDA（PerlとCを組み合わせて実装）を実行するのに，[MATH]〜[MATH]秒を要した．
この程度の時間であれば，提案手法を実行する上で，問題にはならない．
適合性フィードバックは，(1)システムによる検索結果の提示，(2)ユーザによる検索結果の閲覧，適合文書の選択，(3)適合文書を用いた検索結果のリランキングという三つのステップから成る．
ここで，一般的に考えて，(2)には[MATH]分以上はかかると思われる．
従って，まずユーザに検索結果を提示し，ユーザが検索結果を閲覧している裏でLDAを実行するようなシステムの構成を採れば，(3)に移る前にLDAの実行を終えることができる．
このように，検索結果が[MATH]文書程度であれば，LDAの実行時間は問題にならない．
一方，検索結果は，より大きくなり得る．
検索結果が大きくなると，LDAの実行時間も大きくなってしまう．
これを解決する一つの方法は，ランキングの上位だけを検索結果とすることである．
例えば，多くの文書が検索されても，上位[MATH]文書だけを検索結果とすれば，上述の通り，LDAの実行時間は問題にならない．
別の方法として，変分パラメータの推定を並列化することも考えられる．
LDAの実行時間は，変分パラメータの推定に要する時間が多くを占める．
ここで，各文書に対する変分パラメータは，他の文書に対する変分パラメータと独立である．
従って，各文書に対する変分パラメータの推定を並列化し，LDAの実行時間を削減することができる．
例えば，Nallapatiらは，[MATH]ノードのクラスタを用いることでLDAの実行時間を[MATH]倍高速化できたと報告している[CITE]．
提案手法でも並列化を取り入れることで，LDAの実行時間を削減することができると思われる．
最後に，フィードバックに対してLDAを実行するのに要した時間を報告する．
これは[MATH]秒にも満たないものであった．
例えば，フィードバックが[MATH]文書の場合，実行に要した時間は，わずか[MATH]〜[MATH]秒であった．
従って，フィードバックに対するLDAの実行時間も問題にはならない．
本章では，提案手法の有効性を調査するために行った実験と，その結果について報告する．
実験は，第[MATH]回NTCIRワークショップで構築されたウェブ検索評価用テストセット[CITE]を用いて，これを行った．
テストセットは，[MATH]ページの日本語ウェブ文書と，[MATH]個の検索課題から成る．
検索課題ごとに，約[MATH]文書に，その課題に対する適合度が付与されている．
ただし，適合度は「高適合」「適合」「部分適合」「不適合」のいずれかである．
これらの適合度が付与された文書を用いて，検索結果のランキング精度を測ることができる．
図[REF_fig:ntcir_subject]に検索課題の一例を示す．
各タグが表す意味内容は次の通りである．
\itemsep=-0.1zw
課題番号．
検索システムに入力するであろう単語．
課題作成者によって[MATH]〜[MATH]語がリストアップされている．
左から順に重要．
課題作成者の情報要求を一文で表したもの．
情報要求に適合する代表的な文書のID．
課題作成者によって[MATH]〜[MATH]個がリストアップされている．
実験では，[MATH]TITLE[MATH]タグの単語をクエリとして使用した．
ただし，提案手法では，検索の質を高めるため，クエリを含む文書（クエリを構成する各タームが最低でも[MATH]回以上出現する文書）のみをスコア付けの対象として収集する（[REF_ssec:initial_resuls_acquisition]節参照）．
そのため，[MATH]TITLE[MATH]タグの全ての単語を用いると，多くの検索課題において，検索される文書数が極端に少なくなってしまった．
例えば，課題番号0027，0047，0058などは，それぞれ[MATH]文書，[MATH]文書，[MATH]文書しか検索できなかった．
課題番号0061に至っては[MATH]文書も検索できなかった．
このように検索される文書が少ないと，適合性フィードバックの有効性が検証しにくい．
すなわち，実際に適合性フィードバックによって初期検索結果のランキングが改善されても，その結果がP@10などの評価尺度の値に反映されにくく，適合性フィードバックが有効に働いたかどうかが判断しづらい．
そこで，実験では，この問題を避けるため，十分な検索結果が得られるように，クエリとして使用する単語を[MATH]TITLE[MATH]タグの最初の[MATH]語のみとした．
ただし，「十分」の定義は「[MATH]文書以上」とした．
また，[MATH]RDOC[MATH]タグのIDが付与された文書を，ユーザのフィードバックとして使用した．
上で述べた通り，これらは課題作成者本人によって選択された代表的な適合文書であり，フィードバックとして使用するのに最適と考えられる．
これらの文書は，提案手法の初期検索結果に含まれるとは限らない．
初期検索結果に含まれない場合，これらをユーザのフィードバックとして使用するのは奇異に感じられるかもしれない．
しかし，これらの文書は，仮に初期検索結果に含まれていた場合も，リランキング前後のランキング精度を測定・比較する際，結局ランキングから取り除かれる（[REF_ssec:evaluation_method]節で後述）．
言い換えれば，これらは，初期検索結果に含まれていた場合も，初期検索結果に含まれない場合のように，検索結果中に存在していないものとして扱われる．
このように，どちらの場合でも存在していないものとして扱われることを考えると，これらの文書が初期検索結果に含まれているか含まれていないかは重要ではない．
以上を踏まえ，実験では，これらが初期検索結果に含まれているか含まれていないかは問題にしなかった．
[MATH]個の検索課題のうち，[MATH]個の検索課題（課題番号: 0011, 0018, 0032, 0040, 0044, 0047, 0061）については，実験で使用しなかった．
これは，上で述べたようにクエリとして使用する単語を[MATH]語にしても，十分な文書（i.e., [MATH]文書）が検索できなかったためである．
さらに，残った[MATH]課題を，開発データと評価データに分けて使用した．
開発データは，提案手法のパラメータを最適化するために使用した．
評価データは，提案手法のランキング精度を測定するために使用した．
開発データには[MATH]課題（課題番号：0008〜0017）を，評価データには[MATH]課題（課題番号：0018〜0063）を使用した．
実験を行うため，提案手法に従って適合性フィードバックを行う検索システムを作成した．
実装の詳細は以下の通りである．
検索対象とする文書セット(i.e., [MATH])には，テストセットの[MATH]文書を使用した．
また，文書セット中の各文書について，次の手順に従って文書モデルを構築した．
Shinzatoらの手法[CITE]を用いて本文を抽出し，JUMAN [CITE]を用いて各文を解析する．
解析結果及び式([REF_equ:dir])を用いて，DIRに基づく文書モデルを構築する．
ただし，先行研究[CITE]に倣って，[MATH]とした．
クエリが与えられたら，次の手順に従ってクエリモデルを構築した．
JUMANを用いてクエリを解析する．
解析結果及び式([REF_equ:mle])を用いて，MLEに基づくクエリモデルを構築する．
LDAの実装については次の通りである．
パラメータ[MATH] [MATH]の初期値は[MATH]とした．
また，[MATH] [MATH]の初期値にはランダムな値を与えた．
[MATH]と[MATH]を更新する際の反復回数と，[MATH]と[MATH]を更新する際の反復回数は，それぞれ[MATH]回とした．
LDAで考慮する語彙数[MATH]は[MATH]とした．
ただし，LDAで考慮する語彙は，初期検索結果に対する重要度を基に選出した．
ここで，初期検索結果[MATH]に対する単語[MATH]の重要度は，[MATH]と定義した．
ただし，[MATH]は[MATH]における[MATH]の文書頻度を表す．
適合性フィードバックの効果は，適合性フィードバック前のランキング（i.e.,初期検索結果のランキング）と，適合性フィードバック後のランキングを比較することで検証できる．
このとき，フィードバックとして使用する文書の扱いに気を付けなければならない[CITE]．
例えば，適合性フィードバック前後のランキングをそのまま比較すると，後者が有利になってしまう．
これは，フィードバックとして与えられた文書（適合であることが分かっている文書）が，適合性フィードバック後のランキングの上位に含まれやすいためである．
そこで，適合性フィードバック前後のランキングを比較する際，フィードバックとして与えられた文書を適合性フィードバック後のランキングから取り除くという方法が考えられる．
しかし，この方法だと，適合性フィードバック前のランキングが有利になってしまう．
これは，適合文書が少ないときに特に問題となる．
以上を踏まえ，実験では，ランキングの精度を測定する際，フィードバックとして使用した文書を各ランキングから取り除いた．
これにより，適合性フィードバック前後のランキングを公平に比較することができる．
ランキング精度の評価尺度には，P@10，Mean Average Precision (MAP)，Normalized Discounted Cumulative Gain at [MATH] (NDCG@10) [CITE]を用いた．
ただし，P@10及びMAPを測定する際は，「高適合」「適合」「部分適合」の文書を正解，「不適合」及び適合度が付与されていない文書を不正解とした．
また，NDCG@10は，「高適合」の文書を[MATH]点，「適合」の文書を[MATH]点，「部分適合」の文書を[MATH]点として算出した．
まず，提案手法が初期検索結果のランキング精度をどの程度改善できるか調査した．
具体的には，初期検索結果のランキング精度と，提案手法によってリランキングを行った後のランキング精度を比較し，提案手法の有効性を検証した．
実験には評価データを使用し，各検索課題の初期検索結果を取得する際は，[REF_ssec:data]節で述べたように，[MATH]TITLE[MATH]タグの最初の[MATH]単語をクエリとして用いた．
また，実験では，[MATH]（式([REF_equ:initial_score])参照）の上位[MATH]件を初期検索結果とした．
提案手法を実行する際は，[MATH]RDOC[MATH]タグの最初の[MATH]文書をフィードバックとして用いた．
なお，これらの文書に含まれる単語数は平均[MATH]語であった．
提案手法に必要な[MATH]つのパラメータ[MATH]，[MATH]，[MATH]の値は，それぞれ[MATH]，[MATH]，[MATH]とした．
これらは，[REF_ssec:experiment0]節で述べる実験の結果を基に決定した．
結果を表[REF_sbtbl:eRF]に示す．
INITは各検索課題に対する初期検索結果のランキング精度の平均値を，OURSは提案手法実行後のランキング精度の平均値を表す．
比較のため，初期検索結果に対してベースラインとなる手法を実行したときの結果も示した．
ZHAIはZhaiらの手法[CITE]を，OURS ([MATH])は提案手法から潜在情報を除いた手法を表す．
ただし，ZHAIとOURS ([MATH])は本質的にはほとんど同じ手法である．
両手法とも，フィードバックの表層の単語分布を文書セット全体の単語分布で補正することでフィードバックモデルを構築し，これを用いてリランキングを行っている．
違うのは単語分布の補正の仕方だけである（前者はEMアルゴリズムを用い，後者はDIRを用いて補正を行っている）．
OURS ([MATH])では，[MATH]とした．
これも，[REF_ssec:experiment0]節で述べる実験の結果を基に決定した．
DICもベースラインとなる手法を表す．
提案手法の核となるアイディアは，テキスト（フィードバック及び検索結果中の各文書）に潜在的に現れうる単語の情報を適合性フィードバックに利用することである．
同義語辞書や関連語辞書などの知識リソースを用いても，同様のアイディアを実現することができる．
DICでは，OURS ([MATH])をベースに，テキスト中の各単語が同義語を持つ場合，その同義語もそのテキストに出現しているとみなした上でリランキングを行った．
ただし，同義知識は，Shibataらの手法[CITE]を用いて例会小学国語辞典[CITE]と岩波国語辞典[CITE]から獲得した．
獲得された同義知識（e.g.,「コンピュータ」＝「電子計算機」，「ポテト」＝「じゃが芋」＝「ばれいしょ」）は[MATH]個であった．
表[REF_sbtbl:eRF]を見ると，すべての尺度において，OURSがINITを大きく上回っている．
例えばP@10は[MATH]改善しており，提案手法が初期検索結果をうまくリランキングできたことが分かる．
また，提案手法は，ZHAIやOURS ([MATH])より高い性能を示した．
ZHAIやOURS ([MATH])は，テキストの表層情報だけを用いて適合性フィードバックを行っている．
一方，提案手法は，テキストの表層情報に加え，テキストの潜在情報も用いて適合性フィードバックを行っている．
提案手法がこれらの手法を上回ったことから，潜在情報が適合性フィードバックに有用であったことが分かる．
さらに，リランキング結果を調査したところ，提案手法が，テキストに表層的には出現しないが潜在的には現れうる単語の情報をうまく利用していることが確認できた．
図[REF_fig:ntcir_subject]の検索課題を例に取ると，「宗教」や「祝日」「聖書」などの単語は，情報要求によく関連するが，フィードバックとして使用した文書には含まれていなかった．
そのため，ZHAIやOURS ([MATH])では，これらの単語の情報を使用することができなかった．
一方，提案手法では，これらの単語がフィードバックにおいてもある程度の確率で現れうると推定できた．
具体的には，「宗教」「祝日」「聖書」は，それぞれ[MATH]，[MATH]，[MATH]の確率で現れうると推定できた．
なお，フィードバックに[MATH]回出現した単語として「クリスマス」や「ＥＡＳＴＥＲ」などがあったが，これらの生起確率の推定値は，それぞれ[MATH]，[MATH]であった．
提案手法では，これらの推定結果を用いることで，これらの単語を含む検索結果中の適合文書を上位にリランキングすることができた．
DICはあまり有効に機能せず，その結果はZHAIやOURS ([MATH])の結果を少し上回る程度であった．
この原因は，我々が構築した同義語辞書のカバレッジにあると思われる．
DICは，よりカバレッジの高い知識リソースが利用できれば（同義語や関連語などの知識をより多く利用できれば），より有効に機能する可能性を持つ．
しかし，そのようなリソースを構築するのは容易ではない．
一方，提案手法でも，単語と単語が関連するという知識を必要とする．
しかし，DICと違って，何のリソースも必要としない．
すなわち，提案手法では，LDAを用いることで，単語と単語が関連するという知識を検索結果から動的に獲得することができる．
[REF_sec:introduction]章の「マック価格」というクエリを例に取ると，このクエリに対する検索結果には「CPU」や「ハードディスク」「ハンバーガー」「ポテト」などの単語が含まれると考えられる．
提案手法では，検索結果に対してLDAを実行することで，「CPU」と「ハードディスク」が関連するという知識や「ハンバーガー」と「ポテト」が関連するという知識を，トピックという形で動的に獲得することができる．
そして，獲得された知識を用いることで，文書に「ハードディスク」という単語が出現していなくても，「CPU」という単語が出現していれば，「ハードディスク」も潜在的にはその文書に現れうると推測できる．
このように，DICと比べると，（カバレッジの高低に関わらず）何のリソースも必要としないという点で，提案手法の方が優れている．
提案手法は擬似適合性フィードバックにも適用可能である．
そこで，これに対するリランキング性能も調査した．
擬似適合性フィードバックでは，初期検索結果の上位[MATH]文書を適合文書とみなし，適合性フィードバックを行う．
実験では，[MATH]として初期検索結果をリランキングし，リランキング前後のランキング精度を比較した．
ただし，擬似適合性フィードバックでは，明示的なフィードバック（適合であることが分かっている文書）は存在しない．
そのため，ランキングの精度を測る際，他の実験のように，[MATH]RDOC[MATH]タグの文書を各ランキングから除くことはしなかった．
結果を表[REF_sbtbl:pRF]に示す．
INITの値が表[REF_sbtbl:eRF]と違うのは，ランキング精度を算出する際，[MATH]RDOC[MATH]タグの文書を除いていないからである．
表[REF_sbtbl:pRF]を見ると，普通の適合性フィードバックに比べると改善の度合いは小さいが，P@10やNDCG@10の値が上昇している．
例えば，P@10では[MATH]の改善が見られる．
このことから，擬似適合性フィードバックにおいても提案手法がある程度機能することが分かる．
現実的には，ユーザが多くのフィードバックを与えてくれるとは考えにくい．
そのため，適合性フィードバックの手法は，フィードバックが少ない状況でも機能するべきである．
この実験では，このような状況をシミュレートし，フィードバックが少なくても提案手法が機能するかを調査した．
具体的には，提案手法に与えるフィードバックを少しずつ減らしていき，リランキング性能がどのように変化するかを調査した．
提案手法に与えるフィードバックの分量[MATH]は，[MATH]とした．
ただし，例えば[MATH]は，フィードバックとして[MATH]文書を用いることを意味している．
また，例えば[MATH]は，フィードバックとして[MATH]適合文書の半分だけを用いることを意味している．
この場合，適合文書中の単語をランダムに半分抽出し，それらを用いて適合性フィードバックを行った．
[MATH]の場合も調査したのは，フィードバックとして文書より小さい単位（e.g.,文書のタイトル，スニペット）が与えられた場合を想定し，このような場合にも提案手法が機能するかを調べたかったからである．
結果を図[REF_fig:F]に示す．
比較のため，提案手法から潜在情報を除いたとき(i.e., OURS ([MATH]))の性能の変化も示した．
また，INITは初期検索結果のランキング精度を表す．
図から，[MATH]が小さいときでも，提案手法が高い性能を示すことが分かる．
例えば[MATH]のとき，提案手法は初期検索結果を[MATH]改善している．
さらに，[MATH]のときでも，[MATH]の改善が見られた．
なお，[MATH]のとき，フィードバック[MATH]に含まれる単語数は平均[MATH]語であった．
一方，OURS ([MATH])を見ると，[MATH]が小さくなるにつれ，ほとんど改善が見られなくなった．
OURS ([MATH])ではテキストの表層情報しか利用していない．
そのため，[MATH]が小さくなるにつれて利用できる情報が少なくなり，初期検索結果を改善できなくなったと考えられる．
一方，提案手法では，表層情報だけでなく潜在情報も利用している．
利用できる情報が多い分，[MATH]が小さいときでも，初期検索結果のランキングを改善することができたと考えられる．
提案手法には[MATH]つのパラメータ[MATH]，[MATH]，[MATH]がある．
[MATH]は[MATH]と[MATH]の混合比を調整するパラメータ（式([REF_equ:hdm])及び式([REF_equ:hfm])参照），[MATH]は[MATH]と[MATH]の混合比を調整するパラメータ（式([REF_equ:nqm])参照），[MATH]はLDAのトピック数である．
[REF_ssec:experiment1]節及び[REF_ssec:experiment2]節で述べた実験では，OURSのパラメータを[MATH]，[MATH]，[MATH]とした．
また，OURS ([MATH])のパラメータを[MATH]とした．
これらの値は予備実験の結果を基に決定した．
提案手法の性能を最大限に発揮するためには，パラメータとリランキング性能の関係について知る必要がある．
予備実験では，この関係を知るため，様々な[MATH]の組み合わせについて提案手法のリランキング性能を調査し，その結果を比較した．
ただし，[MATH]，[MATH]，[MATH]とし，全[MATH]通りの組み合わせについて，調査を行った．
開発データを用いて調査した．
ある[MATH]の組み合わせに対するリランキング性能は，他の実験と同じようにして，これを測定した．
すなわち，開発データ中の各検索課題について初期検索結果を取得し，提案手法を用いてこれらをリランキングした後，全課題におけるP@10の平均値を算出した．
他の実験と同様，クエリには[MATH]TITLE[MATH]タグの最初の[MATH]単語を，フィードバックには[MATH]RDOC[MATH]タグの最初の[MATH]文書を用いた．
結果を表[REF_tbl:experiment0]及び図[REF_fig:K]に示す．
表[REF_tbl:experiment0]は，実験結果を[MATH]についてまとめたものである．
表中の各セルの値は，各[MATH]の組み合わせについて，各[MATH]のP@10を平均したものである．
例えば，[MATH]のセルは，[MATH] [MATH]のP@10の平均値が[MATH]であったことを示している．
各列においてもっともP@10が高いセルは，その値を太字で装飾した．
また，各行においてもっともP@10が高いセルは，その値に下線を引いた．
表から，[MATH] or [MATH]のとき，リランキング性能がもっとも良いことが分かる．
また，[MATH]のとき（潜在情報を考慮しないとき）は，[MATH]が大体[MATH]〜[MATH]のとき，リランキング性能が良い．
一方，[MATH]のとき（潜在情報を考慮したとき）は，[MATH]が大体[MATH]〜[MATH]のとき，リランキング性能が良い．
[MATH]のときより，性能が良くなる[MATH]の値（及びそのときのランキング精度）が大きくなっている．
これは，潜在情報を考慮することで，フィードバックモデルの信頼度が増すことを示唆している．
図[REF_fig:K]は，[MATH]によるリランキング性能の変化を示している．
図では，表[REF_tbl:experiment0]においてリランキング性能が良かった[MATH]つの[MATH]の組み合わせ[MATH]について，[MATH]による性能の変化を示した．
図から，[MATH]が大体[MATH]〜[MATH]のとき，リランキング性能が良いことが分かる．
以上の結果をまとめると，提案手法がその性能を発揮するパラメータは，[MATH] or [MATH]，[MATH]は大体[MATH]〜[MATH]となる．
提案手法では，検索結果中の各文書に対する[MATH]を構築するため，検索結果に対してLDAを実行する．
また，フィードバックに対する[MATH]を構築する際は，フィードバックに対してLDAを実行する．
本節では，これらの処理に要する時間について考察する．
実験では，各検索課題の検索結果（[MATH]文書）に対してLDA（PerlとCを組み合わせて実装）を実行するのに，[MATH]〜[MATH]秒を要した．
この程度の時間であれば，提案手法を実行する上で，問題にはならない．
適合性フィードバックは，(1)システムによる検索結果の提示，(2)ユーザによる検索結果の閲覧，適合文書の選択，(3)適合文書を用いた検索結果のリランキングという三つのステップから成る．
ここで，一般的に考えて，(2)には[MATH]分以上はかかると思われる．
従って，まずユーザに検索結果を提示し，ユーザが検索結果を閲覧している裏でLDAを実行するようなシステムの構成を採れば，(3)に移る前にLDAの実行を終えることができる．
このように，検索結果が[MATH]文書程度であれば，LDAの実行時間は問題にならない．
一方，検索結果は，より大きくなり得る．
検索結果が大きくなると，LDAの実行時間も大きくなってしまう．
これを解決する一つの方法は，ランキングの上位だけを検索結果とすることである．
例えば，多くの文書が検索されても，上位[MATH]文書だけを検索結果とすれば，上述の通り，LDAの実行時間は問題にならない．
別の方法として，変分パラメータの推定を並列化することも考えられる．
LDAの実行時間は，変分パラメータの推定に要する時間が多くを占める．
ここで，各文書に対する変分パラメータは，他の文書に対する変分パラメータと独立である．
従って，各文書に対する変分パラメータの推定を並列化し，LDAの実行時間を削減することができる．
例えば，Nallapatiらは，[MATH]ノードのクラスタを用いることでLDAの実行時間を[MATH]倍高速化できたと報告している[CITE]．
提案手法でも並列化を取り入れることで，LDAの実行時間を削減することができると思われる．
最後に，フィードバックに対してLDAを実行するのに要した時間を報告する．
これは[MATH]秒にも満たないものであった．
例えば，フィードバックが[MATH]文書の場合，実行に要した時間は，わずか[MATH]〜[MATH]秒であった．
従って，フィードバックに対するLDAの実行時間も問題にはならない．
