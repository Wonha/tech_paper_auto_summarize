\section{実験と結果}  
係り受け距離の頻度情報に基づいて, いくつかのぺナルティ関数を定義し, 総ペナルティ最小化法に
よる係り受け解析を行った．結果を正解検出率, 一意正解率, 曖昧度減少率お
よび平均候補数によって評価し, ぺナルティ関数に
よる結果の違いを比較検討した．また, 決定論的解析法による解析も行い, 総ペナルティ最小化法の
結果と比較した．

\begin{center}\begin{minipage}{120mm}
\vspace*{3mm}
\noindent N\ :\ 文の文節数\ ; \\
kakari\_n\ :\ 係り文節番号 ( 1≦ kakari\_n ≦  N−1)\ ;\\
uke\_n\ :\ 受け文節番号 ( 2≦ uke\_n ≦  N)\ ;\\
uke\_number\ [\ i\ ]\ :\ 文節番号iの文節を受ける文節の番号\ ;\\
kakari\_uke\_if\ (\ i\ ,\ j\ )\ :\ 文節番号iの文節が文節番号j\ の文節に係り得るか否かをチェックする関数\ .
\bigskip

\noindent program  \ \ \ deterministic\_analysis\ ;    \\
\hspace*{7mm}    var\ \ \ \   kakari\_n\ ,  uke\_n\ :\ integer\ ;\\
\hspace*{16mm}               uke\_number\ [\ i\ ]\ :\ array\ [\ 1\ .\ .\ N\ ] \ \ of\ \  integer\ ;\\
                  begin    \\
\hspace*{10mm}         uke\_number\ [\ N\ ]\  :＝ N\ ＋\  1\ ; \\
\hspace*{10mm}         uke\_number\ [\ N−1\ ]\  :＝ N\ ; \\
\hspace*{10mm}         for \ \ \ kakari\_n :＝ N−2 \ \ \   downto \ \ \ 1 \ \ \ do\\
\hspace*{15mm}         begin \\
\hspace*{20mm}               uke\_n\ :＝ kakari\_n\ ＋\  1\ ;\\
\hspace*{20mm}               while \ \ (uke\_n\ $<=$\ N)\ and \\ 
\hspace*{35mm}(kakari\_uke\_if\ (\ kakari\_n\ ,\ uke\_n)＝ false)\ \ \\
\hspace*{20mm}          do \ \ \ uke\_n :＝ uke\_number\ [\ uke\_n\ ]\ ; \\
\hspace*{20mm}               if   \hspace*{1mm} \ uke\_n ＝ N\ ＋\  1\ \hspace*{3mm}\\
\hspace*{20mm}                    then\ \  \ begin \\ 
\hspace*{37mm}                                 write\ (\ '\ failed\ '\ )\ ;\\
\hspace*{37mm}                                 uke\_number\ [\ kakari\_n\ ]\  :＝ kakari\_n\ ＋\ 1\ \\
\hspace*{33mm}                               end\\
\hspace*{20mm}                    else\ \ \ \ uke\_number\ [\ kakari\_n\ ]\  :＝ uke\_n\\      
\hspace*{15mm}         end\\
                 end\ .
\bigskip

\begin{center}
  図1\ \ \  本研究で用いた決定論的解析アルゴリズム
\end{center}
\end{minipage}\end{center}
\subsection{係り受け規則}
文献\cite{kurohashi}を参考にして, 2文節間の形態素による整合条件を表6に示すように定めた．
\begin{center}
{\hspace*{15mm}表6 \ \ \ 係り受け規則}
\vspace*{2mm}
\\
\footnotesize
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
{} &\multicolumn{7}{c|}{受け文節の頭部の形態素}\\ 
\cline{2-8}
    &    &     &     &{\raisebox{-1.0ex}{述語の}}      &{\raisebox{-1.0ex}{連体形の}} &{\raisebox{-1.0ex}{連体形の}} &{\raisebox{-1.0ex}{連用形の}}    \\
{\raisebox{0.5ex}{係り文節の後部の形態素}}&名詞&{\raisebox{1.5ex}{名詞＋}}&動詞&イ・ナ&イ&ナ&イ・ナ\\
  &                        &{\raisebox{1.5ex}{判定詞}}&                            &{\raisebox{0.8ex}{形容詞}}&{\raisebox{0.8ex}{形容詞}}&{\raisebox{0.8ex}{形容詞}}&{\raisebox{0.8ex}{形容詞}}\\
\hline 
連体詞                  &＋  &            &    &        &        &        &        \\ \hline 
活用語の連用形          &    &  ＋        &  ＋&    ＋  &        &        &        \\ \hline 
活用語の基本形・タ形    &  ＋&            &    &        &        &        &        \\ \hline 
  副詞                  &    &          ＋&  ＋&      ＋&      ＋&      ＋&      ＋\\ \hline 
  助詞                  &    &            &    &        &        &        &        \\ \hline 
\hspace*{10mm}      の                &＋  &  ＋        & ＋ &  ＋    &   ＋   &        &        \\ \hline  
\hspace*{10mm} が、に、より           &    &  ＋        & ＋ &  ＋    &   ＋   &   ＋   &        \\ \hline 
\hspace*{10mm}    へ                  &    &  ＋        & ＋ &        &        &        &        \\ \hline 
\hspace*{10mm}     を                 &    &            & ＋ &        &        &        &        \\ \hline 
\hspace*{10mm}    他の助詞              &    &  ＋        & ＋ &  ＋    &        &        &        \\ \hline 
\end{tabular}
\end{center}
\vspace*{3mm}

実際には, この条件を, 
「受け文節の頭部の形態素」としての「名詞」,「名詞＋判定詞」,「動詞」の
さまざまな変形に対処できるように補強して用いる．
例えば, 「二時半ごろだった」(「数詞＋接尾語＋名詞＋名詞＋助動詞＋助動詞」)は
「名詞＋判定詞」として認識されるようにしている．
ここではこの条件を係り受け規則と呼ぶ．この規則は総ペナルティ最小化法と
決定論的解析法の両方において共通に使用した．
\subsection{ぺナルティ関数の定義}      
係り文節$x$, 受け文節$y$\ に対してぺナルティ関数 $F\ (x ,y)$を次のように定義する．

\[\hspace*{5mm}  F\ (x ,y)＝\left\{
\begin{array}{lll}
\raisebox{2.5ex}{$- \log P\ (x ,y)\ ,\ $}&\mbox{\raisebox{2.5ex}{$P\ (x,y)>0$\  のとき\ ;}}& \\
\mbox{\raisebox{-1.0ex}{c\ ,\ }}&\mbox{\raisebox{-1.0ex}{$P\ (x,y)=0$\  のとき\ }}& \hspace*{15mm}{\raisebox{1.7ex}{(2)}}
\end{array}
\right.
\]

ここで,\ $P\ (x ,y)$\ は\ $x$\ が\ $y$\ に係る頻度から定まる値である．
その具体的な定め方は後で述べる．文節$x,\ y$\ が係り受け規則を      
満たさない場合には $P\ (x ,y)=0$\ と定義する．
このときのぺナルティ値\ c\ は非零の$P\ (x ,y)$で生成される
最大ぺナルティ値より十分大きな値に設定する．

これにより, 文節\ $x,\ y$\ が係り受け規則を満たさない場合には
大きなぺナルティが課せられる．また, 係り受け規則を満たす場合には,\ 
$P\ (x ,y)$が大きい程, ぺナルティは小さくなる．
$P\ (x ,y)$として次の4種類の関数を考え, それぞれに対して実験を行った．
\begin{description}
\item[1] 距離情報なし
    \[P_1\ (x ,y)＝α\] 
$α$は正の定数である．すなわち,\ $(x ,y)$\ が一様分布することを仮定する．
\item[2] 距離の頻度近似式
\[P_2\ (x ,y)＝P\ (d\ (x,y)\ )\] 
ここで,\ $d\ (x,y)$は$x$と$y$\ の係り受け距離であり,\ $P\ (\ ・\ )$は
式(1)である．

\item[3] 係り文節の種類別に求めた頻度分布 

2.3で述べたように,\ 係り受け距離の頻度分布は係り文節の種類に依存する．
また, 係り受け距離が同じでも受け文節が文末か非文末かによって係り受
け頻度は大きく異る\cite{maruyama}．そこで係り文節の種類別に,
 また受け文節が文末か非文末かを区別して, 以下のように
係り受け距離の頻度分布を求めた．

係り文節の種類(表4)を番号\ $m$\ で表す．そして,\ 
\ $T\ (m)$\ を係り文節が第\ $m$\ 種の文節であるような 
係り受け文節対の全体とする:
\vspace*{2mm}\\
\hspace*{12mm}$T(m)\ =\ \{\ (u,v)\ |\ 文節\ u\ は文節\ v\ に係る,\ u\ は第\ $m$\ 種の係り文節 \}$
\vspace*{2mm}\\
\ $T\ (m)$\ の中で,\ 係り文節と受け文節の距離が\ $k$\ であり,\
受け文節が文末であるようなものの全体を$S_l^{m}\ (k)$,\ 同じく
受け文節が非文末であるようなものの全体を$S_n^{m}\ (k)$
とする:
\vspace*{2mm}\\
\hspace*{10mm}$S_l^{m}\ (k)=\ \{\ (u,v)\ |\ (u,v) \in T\ (m),\ d\
(u,v)=k\ ,\ v\ は文末文節\}$\ \ ,
\vspace*{2mm}\\
\hspace*{10mm}$S_n^{m}\ (k)=\ \{\ (u,v)\ |\ (u,v) \in T\ (m),\ d\ (u,v)=k\ ,\ v\ は非文末文節\}$ 
\vspace*{2mm}\\
当然,\ 次の関係がある:
\[\ T\ (m)＝\  \cup_{k}\ (S_l^{m}\ (k)\ \cup\ S_n^{m}\ (k)\ )\]
  そして,\ 以下の式により文節$x$が文節$y$に
係る相対頻度の推定値$P_3\ (x ,y)$を計算する:
\vspace*{1mm}
\[P_3\ (x ,y)＝\left\{
\begin{array}{l}
\mbox{\raisebox{3.0ex}{$|\ S_l^{m(x)}\ (d\ (x,y))｜\ \ /\ ｜T\ (m(x))｜$\ ,\ yが文末文節のとき\ ;}}\\
\mbox{\raisebox{-0.5ex}{$|\ S_n^{m(x)}\ (d\ (x,y))｜\ \ /\ ｜T\ (m(x))｜$\ ,\ yが非文末文節のとき\ }}\\
\end{array}
\right.
\]
\vspace*{1mm}\\
ここで\ $m(x)$\ は係り文節\ $x$\ の種類を表し,\ $|・|$は集合の要素数を表す．
すなわち,\ $P_3\ (x ,y)$は,\ \ $d\ (x,y)=\ k$とするとき,\ $x$\ と同じ種
類の文節を係り文節とする係り受け文節対の中で, 距離が\ $k$\ であるよう
なものが, どれだけの割合で存在するかを示す量であり,\ $y$ が文末か非文
末かに分けて計算される．ここでは, 係り受け文節対の出現頻度が,
 係り文節の種類, 係り受け距離, および受け文節が文末か非文末かの三つの
変数に依存して定まる分布モデルが仮定されていることになる．
\item[4] 補間した頻度分布\\
\hspace*{4mm}
係り受け規則がコーパスを完全にカバーしていないため, ある文節を受ける文節が文末までに存在しない
ことがある．また係り受け規則で許されても, コーパスの希薄性によって,
 係り受け頻度が0となる場合がある．そこで,\ $P_3$\ に次のような一種の補間を施し,その結果を\ $P_4\ (x,y)$とする．
\begin{description}
\item[(1)]\ 上のような問題がない場合\ :
     \[P_4\ (x,y)＝P_3\ (x,y)\]
\item[(2)]\ 上のような問題がある場合\ :
\begin{description}
\item[(a)]  文節 $x$を受ける文節が文末までに存在しない場合:\\
   文節$x$がどの文節に係るかを係り受け距離の頻度
分布に基づくヒューリスティクスで定める．\\
すなわち, 文節$x$に対して, 上の\ {\bf 3}\ で推定された係り受け頻度が
最大となる後続文節を求め,\ $x$がその文節に係ることを許す．また, 他
の文節に係ることは許さない．これは, 文節\ $x$が後続文節\ $y$\ に係る
ことが係り受け規則の上では許されなくても, もし文節間距離が\ $d\ (x,y)$に
等しい係り受け文節対の出現頻度が大きければ, それを許そうという考え方であ
る．その際, 出現頻度が最も大きい後続文節だけに対して文節\ $x$が
係ることを許し, 他の文節に対しては許さないことにする．

実際の計算は次のように行なう．

文節\ $x$に対して, 最大係り受け頻度を与える後続文節を\ $z$ とする:
 \[z=\arg\max\{\ P_3\ (x,y)\ |\ y\ は\ xの後続文節\}\]
このような\ $z$\ が複数個あるときは,\ それらをすべて求める．\ 
そして,\ 文節$x$が後続文節\ $y$\ に係る頻度$P_4\ (x,y)$を
次のように設定する:
\[P_4\ (x,y)=\left\{
\begin{array}{ll}
\raisebox{2.0ex}{$\ P_3\ (x,y)$\ ,}&\mbox{\raisebox{2.0ex}{$y=z$\ のとき;}}\\      
\raisebox{-1.0ex}{\ 0\ ,  }&\mbox{\raisebox{-1.0ex}{$y≠z$\ のとき}}\\                   
\end{array}
\right.
\]

\item[(b)]  $xがy$\ に係ることが,\ 係り受け規則で許されるにもかかわらず,\ $P_{3}\ (x,y)$が0になる場合:
\[P_4\ (x,y)=β\] 
ただし,$β$は
\[\max_{u,\ v}\{- \log P_3\ (u,v)\ \}<- \log β<<\mbox{\ c}\hspace*{20mm}(3)\]  
を満たすような値に設定する．ここで,\ 最左辺の最大値は文節$\ u\ が文節\ v\ $に係ることが係り受け規則で許されるような$u\ ,\ v$\ のすべての組についてとる．
これはいわゆる底上げ(flooring)の技法である．このとき,\ $x$\ が\
$y$\ に係るペナルティは\ $- \log β$\ となるが, 式(3)は, このペ
ナルティが係り受け規則により係り受けが許されない文節対のペナルティより
ははるかに小さく, また, 係り受け頻度から定まる最大ペナルティよりは大
きくなるように\ $β$\ の値を設定することを意味する． 
\end{description}
\end{description}
\end{description}
\subsection{解析実験}
解析実験は学習データから係り受け距離の頻度情報を抽出する学習ステップと
, その頻度情報に基づいて設定したペナルティ関数を用いてテストデータを
係り受け解析し, 結果を評価する解析ステップから成る．\\
まず学習法と各種のパラメータ設定法について説明する．

$P_1$を用いる場合には, 頻度情報は全く使用しない, したがって学習は不
要である．αはどのような値に設定しても解析結果は同じになる．

$P_2$\ はパラメトリックな分布モデルである．学習ステップにおいては,
 学習データを用いてパラメータ\ $a$,\ $b$\ を推定する．

$P_3$\ はノンパラメトリックな分布モデルである．学習ステップにおいて
は, 学習データを用いて, 係り文節の種類\ $m$\ と\ 係り受け距離\ $k$\ に対して$S_l^{m}\ (k)$と
$S_n^{m}\ (k)$を求め,\ 記憶する．
解析ステップにおいては, 各文節\ $x$,\ $y$\ に対し,\ $S_l^{m(x)}\ (k)$と
$S_n^{m(x)}\ (k)$から定義式に基づいて\ $P_3\ (x,y)$\ の値を
計算し,ペナルティ関数の値を設定する．

$P_4$\ は基本的には$P_3$\ から定まるので学習する必要はない．
βの値は式(3)を満たす限りどのような値に設定しても結果に
大きな差はないと考えられるので, 式(3)を満たす値を任意
に選んで使用した．

ペナルティ関数を式(2)によって定義するとき, 定数\ $c$\ を定める
必要がある．これは文節\ $x$\ が文節\ $y$\ に係ることが係り受け
規則により許されないとき, あるいは文節\  $x$\ が文節\ $y$\ に係る
頻度が\ 0\ になるとき\ (\ $P_4$\ によって補間されない限り)文節
\ $x$\ が文節\ $y$\ に係ることを禁止するような大きなペナルティ
を与えるためのものである．したがって, これは  $\infty$ 
とも言うべきものであり, 十分大きな値に設定すれば, どの
ような値に設定しても解析結果に差はない．

このような学習法およびパラメータ設定法を用いて
次のような3種類の実験を行った．

\begin{description}
\item[実験1] $P_1$〜$P_4$を用いてぺナルティ関数を定義し,
総ペナルティ最小化法による解析実験を行った．
この実験では係り受け距離の頻度情報を抽出するための
学習データとテストデータを分離せず,どちらも503文すべてを用いた．
比較のため同じデータを用いた決定論的解析法による解析実験
も併せて行った．

\item[実験2] 実験1で総合的に最も良い結果が得られたのは$P_4$\hspace{-0.25mm}で
定めたぺナルティ関数である．これを用いて,学習データとテストデータを分離した解析実験を行った．
10グループ$A〜J$\ の一つをテストデータとし,残りを学習データとした．
テストデータを$AからJ$\ まで変えて10回の実験を行った．

\item[実験3] 学習データの量と解析結果との関係を調べるため,
テストデータをグループ$J$\ に固定し,学習データを$A,A \cup B,A \cup  B \cup C, \cdots  $
と漸次増加させる実験を行った．純粋に学習データの量が解析結果に与える効果を見い出すため,補間していない$P_3$を用いてぺナルティ関数を定めた．
\end{description}
\subsection{解析結果}
解析結果について述べるため,まず記法と用語を定義する．

\begin{itemize}
\item $M$\ :\ 評価に用いるテスト文の総数\ ;
\item $S_i$\ :\ 番号$i$のテスト文\ ;
\item $L_i$\ :\ 文$S_i$ の文節数\ ;\\
\hspace*{7mm}文$S_i$\ に対する係り受け構造の中の係り受けの総数は\ $L_i-1$\ に等しい．
\item $D_i$\ :\ 長さ$L_i$\ の文節列上に存在し得る係り受け
構造の総数;\\
\hspace*{7mm} $D_i$\ はカタラン数と呼ばれる数列になり,\ 次の式によって計算することができる:
\[D_i＝\frac{1}{L_i}\ _{2(L_i-1)}C_{(L_i-1)}\] 
\hspace*{8mm}ここで\ $_{2(L_i-1)}C_{(L_i-1)}$\ は\ $2(L_i-1)$
個のものから\ $(L_i-1)$\ 個のものをとる組合せの数を表す．\ この式は再帰式
\[D_i\ =\left\{
\begin{array}{ll}
\ {\raisebox{2.0ex}{1,}}&\mbox{\raisebox{2.0ex}{$i=1$\ のとき;}}\\      
\ \sum_{1≦j≦i-1}{D_{i-j}D_j}\ ,  &\mbox{$i≧2$\ のとき}\\                   
\end{array}
\right.
\]
\newline
が成り立つ\cite{ozeki}ことと\ ,母関数の手法\cite{JCC}を用いることにより示すことができる．
\item $R_i$\ :\ 文$S_i$の解析結果\ ,\ すなわち係り受け構造候補の集合\ ;
\item $K_{ij}$\ :\ 文$S_i$に対する$j\ (\ 1≦ j ≦ |R_i|\ )$番目の解析候補の中でコーパスのラベルに示されるものと一致する係り受けの数\ ．
\bigskip
\end{itemize}
{\bf 評価方法}
\vspace*{1mm}

\noindent
結果の評価は,\ \ (1)\ \ 2文節間の係り受けがどの程度正しく検出されたか,\ \ (2)\ \ 係り受け構造が
どの程度正しく検出されたか, という二つの観点から行った．また, 文の検出率が高くても一つの文に
対する解析結果の候補数が多ければ良い解析法とは言えない．このため, 候補数がどれだけ絞られるかを
評価することとした．さらに, 解析を行うことにより, 情報理論的な曖昧さがどれだけ減少するかを
調べ, 全体的な解析効率を評価した．これらの評価を行うため, 以下のようないくつかの評価尺度を定義した． 

\noindent
(1) 係り受けの正しさに着目した評価尺度\bigskip
     \[\ \  文S_i\ の係り受け検出率\ ＝\ \frac{\sum_{j=1}^{|R_i|}K_{ij}}{(L_i-1)×|R_i|}\]
\bigskip
  \[係り受け検出率\ ＝\ \frac{\sum_{i=1}^{M}\sum_{j=1}^{|R_i|}K_{ij}}{\sum_{i=1}^{M}(L_i-1)×|R_i|}\]
\newline
係り受け検出率は,解析結果がラベルと部分的に一致する度合を示す数字である．このような部分的一致も
,結果を意味理解に利用する場合などには有用と思われる．

        
決定論的解析法を用いた解析実験においては,解析の途中で,ある文節を受ける文節が存在しない時には,
直後の文節を受け文節として解析を続けた．その結果をもとにして係り受け検出率を計算した．
\vspace*{3mm}
\\
(2) 係り受け構造の正しさに着目した評価尺度

\noindent(a)文検出数と文検出率
\[  文検出数_k\ ＝\sum_{i\ :\ |R_i|≦k}I_i \]
ここで
\[I_i\ ＝\left\{
\begin{array}{ll}
{\raisebox{2.0ex}{1\ ,}}&\mbox{\raisebox{2.0ex}{$R_i$の中にラベルで指定される係り受け構造と一致する候補が存在する場合;}}\\      
{\raisebox{-0.5ex}{0\ ,}}&  \mbox{\raisebox{-0.5ex}{ 他の場合 }}
\end{array}
\right.
\]
\vspace*{2mm}

$文検出数_k$\ は, 出力された解析結果の候補数が\ $k$\ 以下であり,\ かつ,\ その中に
コーパス中のラベルで指定される係り受け構造と一致するものが含まれるような
テスト文の数である．また, これをテスト文の総数に対する比で表わしたものを,
 $文検出率_k$\ と呼ぶ\ : \bigskip
\[文検出率_k\  ＝\ \frac{文検出数_k}{M}\]
\newline
$文検出数_\infty$ ,\ $文検出率_\infty$\ をそれぞれ単に文検出数,\ 文検出率という．また,\ 
$文検出数_1$ ,\ $文検出率_1$\ をそれぞれ一意正解数,\ 一意正解率と呼ぶことにする．
\ 一意正解数は解析結果が一意的に決定し,\ それがコーパスのラベルと一致したテスト文の数である．

\noindent (b)平均候補数
\[平均候補数\ ＝\ \frac{ \sum_{i=1}^{M}|R_i|×I_i}{\sum_{i=1}^{M}I_i}\]
\newline
\noindent(c)曖昧度減少率\\
      文$S_i$ の係り受け構造の候補数の対数をその文の曖昧度と定義すると,
\begin{table}[b]
\begin{center}
表7 {\ \ \ \ 実験1の結果}
\vspace*{2mm}
\\
\begin{tabular}{ |c|c|c|c|c|c|} 
\hline
\makebox[30mm]{}&\makebox[26mm]{\raisebox{-0.5ex}{文検出数}} &\makebox[14mm]{\raisebox{-0.5ex}{平均}}&\makebox[14mm]{\raisebox{-0.5ex}{曖昧度}}&\makebox[24mm]{\raisebox{-0.5ex}{係り受け}} \\[-2mm]
\makebox[30mm]{\raisebox{1.5ex}{ペナルティ関数}}&\makebox[26mm]{ 文検出率(\%)} &\makebox[14mm]{候補数}&\makebox[14mm]{減少率}&\makebox[24mm]{検出率(\%)} \\\hline
\makebox[30mm]{$P_1(距離情報なし)$}&\makebox[26mm]{432(85.9) } &\makebox[14mm]{41.3}&\makebox[14mm]{0.40}&\makebox[24mm]{61.5} \\
\hline
\makebox[30mm]{$P_2(近似式)$}&\makebox[26mm]{247 (49.1)} &\makebox[14mm]{2.5}&\makebox[14mm]{0.36}&\makebox[24mm]{75.2} \\\hline
\makebox[30mm]{$P_3(種類別)$}&\makebox[26mm]{321(63.8)} &\makebox[14mm]{2.9}&\makebox[14mm]{0.52}&\makebox[24mm]{74.4} \\\hline
\makebox[30mm]{$P_4(補間)$}&\makebox[26mm]{287(57.1)} &\makebox[14mm]{1.1}&\makebox[14mm]{0.47}&\makebox[24mm]{87.1} \\\hline
\hline
\makebox[30mm]{決定論的解析法}&\makebox[26mm]{193(38.4)} &\makebox[14mm]{1.0}&\makebox[14mm]{0.28}&\makebox[24mm]{81.9} \\\hline
\end{tabular}
\end{center}
\end{table}
\vspace*{2mm}

\[解析前の曖昧度＝\log D_i\]

\[解析後の曖昧度＝\left\{
\begin{array}{ll}
\mbox{\raisebox{1.0ex}{$ \log |R_i|\ ,$}}&\mbox{\raisebox{1.0ex}{$R_i$の中にラベルで指定される係り受け構造と}}\\      
             &\mbox{\raisebox{1.0ex}{一致する候補が存在する場合\ ; }}\\      
\raisebox{-0.5ex}{$\log D_i\ ,$  }&\mbox{\raisebox{-0.5ex} {他の場合 }}
\end{array}
\right.
\]
\vspace*{1mm}\\
\hspace*{36mm}$   
＝\log\ (D_i+(\ |R_i|-D_i\ )\ I_i) $ 
\vspace*{2mm}\\
となる．これを用いて,\ 次の量を定義する．\bigskip
\[曖昧度減少率＝\frac{ \sum_{i=1}^{M}\log D_i - \sum_{i=1}^{M}\log\ (D_i+(\ |R_i|-D_i)\ I_i)}{\sum_{i=1}^{M}\log D_i}\]
\newline
曖昧度減少率は文の統語的な曖昧さが解析により減少する度合を表す．
\vspace*{1mm}\\
{\bf 実験結果と分析}
\vspace*{1mm}\\
(1)\ 実験1の結果を表7に示す．また,\ 同実験において候補数を制
限したときの文
検出数(率)を表8に示す．
\begin{table}[t]
\begin{center}
表8 \ \ \ \ {候補数を制限したときの文検出数(率)}
\vspace*{2mm}
\\
\begin{tabular}{ |c|c|c|c|} 
\hline
\makebox[30mm]{}&\multicolumn{3}{c|}{$文検出数_k$\ (率\%)} \\
\cline{2-4}
\makebox[30mm]{ペナルティ関数}&\makebox[20mm]{}&\makebox[20mm]{} &\makebox[28mm]{\raisebox{-1.0ex}{一意正解数(率) }}\\ 
\makebox[30mm]{}&\makebox[20mm]{\raisebox{1.5ex}{$k＝50 $}} &\makebox[20mm]{\raisebox{1.5ex}{$k＝10$}} &\makebox[28mm]{\raisebox{0.5ex}{ ($k＝1$)}}\\\hline
\makebox[30mm]{$P_1(距離情報なし)$}&\makebox[20mm]{364(72.4) }&\makebox[20mm]{235(46.7) }&\makebox[28mm]{31(6.2) }\\
\hline
\makebox[30mm]{$P_2(近似式)$}&\makebox[20mm]{246(48.9)}&\makebox[20mm]{244(48.5) }&\makebox[28mm]{195(38.8)}\\\hline
\makebox[30mm]{$P_3(種類別)$}&\makebox[20mm]{319(63.4)}&\makebox[20mm]{315(62.6) }&\makebox[28mm]{237(47.1)}\\\hline
\makebox[30mm]{$P_4(補間)$}&\makebox[20mm]{287(57.1)}&\makebox[20mm]{287(57.1) }&\makebox[28mm]{264(52.5)}\\\hline
\hline
\makebox[30mm]{決定論的解析法}&\makebox[20mm]{193(38.4)} &\makebox[20mm]{193(38.4)}&\makebox[28mm]{193(38.4)}\\ \hline
\end{tabular}
\end{center}
\end{table}

$P_1$を用いた場合は,\ 他の場合と比べて,\ 文検出率が高い反面,\ 平均候
補数が非常に大きい．また,\ 表8から$文検出率_k$\ (\ $k=10,\ 1$)は$P_2$〜$P_4$を用いた場合の方が,\ $P_1$を用いた場合より高い．したがって,\ 係り受け距離の情報は候補数を絞るのに有効であることがわかった．

$P_3$を用いた場合は,\ $P_2$を用いた場合と比較して,
\ 表7における文検出率,\ および表8における$文検出率_k$\ (\ $k=50,\ 10,\ 1$)が高い．
したがって,\ 係り文節の種類別に求めた係り受け距離の頻度情報は,\ 係り文節を分類せず全体で求めた情報よりも,\ 
各文検出率$_k$を高めるのに有効である．また,\ その結果,\ 曖昧度が減少し
ている．

$P_4$を用いた場合は,\ $P_3$を用いた場合と比べて,\ 表7における係り受け検出率と表8における一意正解率が高くなっている．
したがって,\ 補間は係り受け検出率と一意正解率を上げる効果があることが検証された．
文検出率は低下したが,\ $P_2$を用いた場合よりは高く,
\ 平均候補数は$P_3$を用いた場合の半分以下になっている．したがって,\ 
距離の頻度分布を補間することにより,\ ある程度係り受け規則の不完全さとコーパスの
希薄性の問題を軽減できることがわかった．また,\ 平均候補数は1.1まで絞られており,\ 87.1\%\ というかなり高い
係り受け検出率が得られることから,
\ この解析法を意味理解のための部分解析法として使用できる可能性がある．

\hspace{0.25mm}決定論的解析法を用いた解析実験と比較すると,\ $P_1〜P_4$を用いた場合は表7\hspace{0.1mm}の平均候補数が大きくなったかわりに文検出率がかなり向上している．さらに,\ 表8において,
\ $P_2$〜$P_4$を用いた場合は決定論的解析法を用いた場合の一意正解率を越えるともに
各$文検出率_k$\ が高くなっている．とくに$P_4$を用いた場合は
決定論的解析法を用いた場合と平均候補数がほとんど等しく,
\ 各$文検出率_k$\ ,\ 曖昧度減少率,\ および係り受け検出
率が高くなっている．したがって,\ 係り受け距離の統計的知識を利用した
総ペナルティ最小化法により,\ 決定論的解析法を用いた場合に比べて
解析性能を向上させることができる．
\\
(2)\ 実験2の結果を表9に示す．
\begin{table}[b]
\hspace*{50mm}表9  \ \ \ \ {実験2の結果}
\vspace*{2mm}
{\small
\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
    \begin{tabular}[c]{@{}c@{}} テスト\\[-1.1mm]グループ\\\end{tabular}&
      \makebox[6mm]{A} & \makebox[6mm]{B} &\makebox[6mm]{C} &
      \makebox[6mm]{D} & \makebox[6mm]{E} &\makebox[6mm]{F} &
      \makebox[6mm]{G} & \makebox[6mm]{H} &\makebox[6mm]{I} &
      \makebox[6mm]{J} & 合計 \\ \hline
    文数 &
      50 & 50 & 50 & 50 & 50 & 50 & 50 & 50 & 50 & 53 & 503 \\\hline\hline
    \begin{tabular}[c]{@{}c@{}} 文検出数\\[-1.1mm](率)\\\end{tabular}&
      25 & 27 & 20 & 26 & 25 & 16 & 26 & 26 & 29 & 41 & 261(52\%) \\\hline
    \begin{tabular}[c]{@{}c@{}} 一意正解数\\[-1.1mm](率)\\\end{tabular}&
      21 & 22 & 19 & 19 & 22 & 13 & 18 & 24 & 23 & 37 & 218(43\%) \\\hline
    \begin{tabular}[c]{@{}c@{}} 平均\\[-1.1mm]候補数\\\end{tabular}&
      1.3 & 1.3 & 1.1 & 1.5 & 1.1 & 1.2 & 1.4 & 1.1 & 1.2 & 1.1 &
      1.2 \\\hline
    \begin{tabular}[c]{@{}c@{}} 曖昧度\\[-1.1mm]減少率\\\end{tabular}&
      0.39 & 0.41 & 0.30 & 0.49 & 0.33 & 0.28 & 0.41 & 0.42 & 0.44 & 0.68 &
      0.42 \\\hline
  \end{tabular}
\end{center}
}
\end{table}
この実験は学習データとテストデータを分離した, いわゆるオープン実験である．
文検出率と一意正解率はクローズ実験である実験1の結果に比べると若干低下し,
平均候補数もやや増加している．しかし,\ 1.2という平均候補数は, この解
析結果に対してさらに何らかの後続処理を行う場合, 処理量の上で問題となる
数ではなく,\ 表7に示される決定論的解析法の結果と比べると,文検出数,一意正解数,
曖昧度減少率など全てが高い．したがって,\ 未知文の係り受け解析に対しても
(1)と同様の結論が導かれる．
\\
(3)\ 実験3の結果を表10に示す．これもオープン実験である．学習データの量
が増加するに従って, 一意正解数, 曖昧度減少率が向上するともに, 平均候補数は減少する．
文検出数はほぼ一定に保たれる．
したがって, 学習データ量を増加させることはこのような解析法の性能向上に有効である．
より大きなコーパスを使用することによって, 更に解析性能が向上すること
が期待される．
\begin{center}
表10 \ \ \ \ {実験3の結果}
\vspace*{2mm}
\\
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline 
学習データの量&{\raisebox{-1.0ex}{50}} &{\raisebox{-1.0ex}{100}} &{\raisebox{-1.0ex}{150}}  &{\raisebox{-1.0ex}{200}}&{\raisebox{-1.0ex}{250}}&{\raisebox{-1.0ex}{300}} &{\raisebox{-1.0ex}{350}} &{\raisebox{-1.0ex}{400}}&{\raisebox{-1.0ex}{450}} \\[-2mm]
（文数）      &     &     &      &     &      &      &     &     &  \\  \hline
\hline  
53文の文検出数&  43 & 43  &  42  &  41 &   42 &   41 &  40 &  43 &   43 \\  \hline 
一意正解数    &  16 & 24  &   25 &   25&   28 &   29 &  29 &  29 &   31 \\  \hline 
平均候補数    & 3.8 &2.2  &  2.0 & 1.9 & 1.7  & 1.5  & 1.5 & 1.5 & 1.5  \\  \hline
曖昧度減少率& 0.53&0.61 & 0.62 &0.60 & 0.63 & 0.63 &0.60 &0.67 &0.68\\  \hline
\end{tabular}
\\
\end{center}
