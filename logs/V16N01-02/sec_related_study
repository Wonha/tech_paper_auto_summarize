文書の母語話者性の判別と関連の深い研究分野として，文書の記述言語を推定する言語識別がある．
Cavnarらは，出現頻度上位の文字列とその順位を言語および文書の特徴と考える言語識別を行っている[CITE]．
各言語[MATH]の学習データ文書中での1〜5の長さの文字列のうち出現頻度上位300個の文字列とその順位を求めて，言語[MATH]における順位表を作成しておく．
同様に，識別対象文書[MATH]に対しても順位表を作成する．
[MATH]の順位表中の各文字列の順位と[MATH]の順位表での順位との差の絶対値の和を[MATH]と[MATH]の非類似度[MATH]と考え，[MATH]が最小の言語[MATH]を[MATH]の記述言語として識別する．
これは，前節で述べた枠組みに対して，[MATH]の逆数を尤度[MATH]と考えたことに相当する．
また，前田らは，長さ2の文字列の出現頻度分布をユークリッド空間上のベクトル（頻度ベクトル）であると考え，識別対象文書[MATH]の頻度ベクトルと各言語[MATH]の学習データ文書の頻度ベクトルとの余弦を，[MATH]と言語[MATH]の類似度[MATH]とする言語識別を行っている[CITE]．
これは，前節で述べた枠組みに対して，[MATH]を尤度[MATH]と考えたことに相当する．
行野らは，長い文字列の頻度は統計的な揺らぎが大きいものの，言語を特定する能力が高いと考え，1〜7の長さの文字列を言語および文書の特徴と考える言語識別の手法を提案している[CITE]．
彼らの手法は，識別対象文書[MATH]に出現する1〜7の長さの文字列集合と言語[MATH]の学習データに出現する1〜7の長さの文字列集合の積集合の大きさを[MATH]とする手法である．
長い文字列を特徴として使用した結果，類似言語間の識別や識別対象文書が極めて短い場合の識別でも，Cavanrらの手法，前田らの手法に比べ高い識別精度を実現したと報告している．
Dunningは文字[MATH]-gramモデルにより[MATH]を求め，言語の事前確率を等確率([MATH])と仮定して，ベイズ識別により，[MATH]の属す言語の識別を行っている[CITE]．
ただし，ゼロ頻度問題に対処するため，加算スムージングによる[MATH]-gram確率のスムージングを行っている．
[MATH]を試した結果，[MATH]の場合，つまり，bi-gramモデルの場合が最も識別精度が高かったと報告している．
Sibunらは，長さ[MATH]の文字列の確率分布を言語および文書の特徴と考え（実際には[MATH]または[MATH]を採用している），確率分布間の相違尺度であるKL-Divergenceに基づいた言語識別手法を提案している[CITE]．
確率分布[MATH]と[MATH]のKL-Divergence（Kullback-Leibler距離）[MATH]は
で定義される[CITE]．
Sibunらの手法は，[MATH]が最小の[MATH]を[MATH]の記述言語として識別する手法である．
ただし，[MATH]は文書[MATH]における文字列[MATH]の生起確率，[MATH]は言語クラス[MATH]における文字列[MATH]の生起確率である．
[MATH]は
と推定されるので（[MATH]は[MATH]での文字列[MATH]の出現頻度，[MATH]は記号の全体集合，[MATH]は可能な[MATH]長さの文字列の全体の集合），言語[MATH]における文書[MATH]の生起確率[MATH]を
と大胆に近似するならば，
となる．
つまり，[MATH]が最小の[MATH]を[MATH]の記述言語として識別するSibunらの手法は，式([REF_式：文書の生起確率の大胆な近似])の近似を行った上で，言語の事前確率を等確率と仮定して，ベイズ識別により，[MATH]の属す言語の識別を行うことと等価である．
なお，Sibunらもゼロ頻度問題に対処するために，[MATH]は
のように，加算スムージングによりスムージングしている（[MATH]は非負の定数）．
次に，本論文で扱う文書の母語話者性判別に関する従来研究について述べる．
Tomokiyoらは，長さ[MATH] ([MATH])の記号列（記号としては，単語，品詞，および単語品詞混合の3種を試している）を言語および文書の特徴と考え，文書（あるいは，文書を構成する単語をその品詞に置き換えたもの，文書を構成する一部の単語を品詞に置き換えたもの）の生起確率を式([REF_式：文書の生起確率の大胆な近似])で近似し，ベイズ識別に基づく母語話者／非母語話者クラスの判別を行っている[CITE]．
しかし，彼らは，子供用ニュース記事の音読による発話や観光などに関する自発的発話を，音声認識器によりテキストにした文書，および，人手で書き起こした文書を対象としている．
音読では読み間違いが非母語話者の大きな特徴であり，自発的発話では，使用語彙が母語話者／非母語話者の間の大きな違いである．
一方，我々は，論文などのように十分推敲して作成されているフォーマルな文書を対象としており，母語話者／非母語話者判別に有効な特徴量も異なってくるため，彼らの判別実験結果と直接比較することはできない．
藤井らは，品詞tri-gramモデルを言語モデルとし，ゼロ頻度問題に対処できるSkew Divergenceを用いて英文書の母語話者性の判別を行っている[CITE]．
以下で定義される判別対象文書[MATH]と言語クラス[MATH]（[MATH], [MATH]:母語話者言語クラス，[MATH]:非母語話者言語クラス）との相違度[MATH]
を求め，[MATH]を最小にする[MATH]を[MATH]が属すクラスとして推定する．
ただし，[MATH]は品詞の全体集合，[MATH]は文書[MATH]における条件を[MATH]とする品詞tri-gram分布，[MATH]はクラス[MATH]における条件を[MATH]とする品詞tri-gram分布，[MATH]は確率分布間の相違度である．
確率分布間の相違度[MATH]としてKL Divergenceを用いた場合，[MATH]を最小にする[MATH]は，文書[MATH]の各単語をその品詞で置き換えた品詞列の生起確率を最大にする言語クラスである．
したがって藤井らの手法は，品詞tri-gramモデルを言語モデルとし，言語の事前確率を等確率と仮定して，Bayes識別に基づいて[MATH]が属すクラスを判定する方法と本質的には同じである．
藤井らの手法の特徴は，分布間の相違度[MATH]としてKL Divergenceを用いるのではなく，以下で定義されるSkew Divergence [CITE]を用いている点にある．
Skew Divergenceはゼロ頻度問題に弱いKL Divergenceを改良したものである．
藤井らは，[MATH]を，分布[MATH]（つまり，[MATH]，[MATH]）の推定に用いた学習データのサイズに応じて，[MATH]毎に
と設定している．
彼らは，Skew Divergenceを用いることで，線形補間を施した品詞tri-gram分布によるKL Divergenceを用いた手法および多くの識別問題で高い精度を実現しているSupport Vector Machineを用いた手法よりも有意に高い判別精度を実現できたと報告している．
英文書の母語話者性判別は，母語話者英語，非母語話者英語という類似した言語の識別問題と捉えることもできる．
青木らは，文書を，それを構成する単語を品詞で置き換えた品詞列と見なし，基本的にはKL-Divergenceを用いたSibunらの言語識別手法に基づいて，文書の母語話者性の判別を行っている[CITE]．
「長い文字列も言語特徴とすることで類似言語の識別精度が向上する」という行野らの知見からの予想通り，長い品詞列の頻度情報を利用した場合の判別精度が高く（[MATH]のときが最も精度が高い），藤井らの手法より高精度で母語話者性を判別できたと報告している．
著者らは，青木らの主張と同じく，長い品詞列の頻度情報も利用することが文書の母語話者性判別に有効であると考えている．
藤井らの手法は言語モデルを[MATH]の品詞[MATH]-gramモデルとしてもそのまま適用できる．
しかし，藤井らは，式([REF_Skew:α])を
[MATH]は[MATH]の単調増加関数，
[MATH]，[MATH]
を満たす[MATH]の設定法の一例として用いたに過ぎず，[MATH]を大きくした場合に，式([REF_Skew:α])で良いのかどうかは疑問である．
さらに，[MATH]を大きくしたとき，式([REF_Skew:α])では高い精度が得られない場合に，式([REF_Skew:α])に代えて，上記の性質を満たす[MATH]のどのような関数を[MATH]の設定に用いればよいのかも明らかではない．
一方，青木らの手法は，統計的パターン認識の立場で見るならば，近似式([REF_式：文書の生起確率の大胆な近似])を仮定したベイズ識別による判別法である．
しかし，品詞[MATH]-gramモデルに比べ，式([REF_式：文書の生起確率の大胆な近似])は，近似としては非常に粗い．
[MATH]が大きな品詞[MATH]-gramモデルを言語モデルとして使用し，かつ，ゼロ頻度問題およびスパースネスの問題を克服する新たな母語話者性判別手法を次節で述べる．
