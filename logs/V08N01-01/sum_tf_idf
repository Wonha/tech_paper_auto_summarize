================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:1, score:0.43383] ベクトル空間モデルでは文書を索引語の重みベクトルで表現するが，文書ベクトルは一般に要素数が非常に多く，スパースなベクトルになるため，検索時間の長さや必要なメモリの量が大きな問題となる．
[i:2, score:0.46095] 本論文では，この問題を解決するため，ベクトル空間モデルにおけるベクトルの次元圧縮を行う手法としてランダム・プロジェクションを用いた検索モデルを提案する．
[i:5, score:0.50443] また，ランダム・プロジェクションで次元圧縮に必要な行列を得るために，球面[MATH]平均アルゴリズムで得られる概念ベクトルの利用を提案する．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[i:17, score:0.47017] ベクトル空間モデルを用いた検索システムを新聞記事などの大量の文書データに対して適用した場合，文書データ全体に存在するタームの数が非常に多くなるため，文書ベクトルは高い次元を持つようになる．
[i:33, score:0.52283] 概念ベクトルは文書の内容が似ているベクトル集合の重心で，この概念ベクトルを得る際，高次元でスパースな文書データ集合を高速にクラスタリングすることができる球面[MATH]平均アルゴリズム[CITE]を用いる．
[i:35, score:0.46324] この概念ベクトルをランダム・プロジェクションで用いることにより，任意のベクトルを用いた検索性能と比較して，検索性能が改善されていることを示し，概念ベクトルを利用した次元圧縮の有効性を示す．

================================================================
[section type  : proposed_method]
[section title : ランダム・プロジェクションによるベクトルの次元圧縮]
================================================================
[i:37, score:0.39542] ランダム・プロジェクションは，ひとつの文書データを[MATH]次元空間上のベクトル[MATH]として表現するとき，このベクトルを[MATH]次元空間に射影する手法である．
[i:41, score:0.36992] その結果，[MATH]次元に圧縮した[MATH]を要素とするベクトルが得られる．
[i:42, score:0.40696] 次元圧縮に必要なベクトル[MATH]を列ベクトルとする[MATH]の行列[MATH]を用いると，求める[MATH]次元ベクトルは

================================================================
[section type  : proposed_method]
[section title : 概念ベクトルを用いたランダム・プロジェクション]
================================================================
[i:47, score:0.60238] しかし，任意の正規直交行列を用いる場合，次元圧縮を行う前後のベクトル間距離を保存する効果は得られたとしても，LSIのように，ベクトルの要素が抽象的な意味を持つ索引語の生成や内容的に関連のある文書をまとめる効果があるとは考えられない．
[i:56, score:0.51193] この概念ベクトルによる次元圧縮は，単にベクトルを近似するだけではなく，クラスタに属するベクトル集合の重心を求めることにより，ターム間で特徴づけられる隠れた関連性やタームの同義性と多義性を捉えることができる．
[i:58, score:0.51970] これにより，次元圧縮された行列は文書と概念ベクトルの類似度を表し，元の空間において内容の近い文書は，圧縮した空間においても近くなる可能性がある．
-----------------------------------------------------
  [subsection title : 概念ベクトル]
-----------------------------------------------------
  [i:lead, score:0.25759] ベクトルの集合をベクトル空間にプロットしたとき，同質のベクトルが多く存在する場合を除いて，いくつかのグループに分かれる．
.....
  [i:74, score:0.31873] 概念ベクトルはクラスタに属するベクトルの重心を求めることにより得られ，そのクラスタの内容を表す代表ベクトルである．
  [i:75, score:0.31906] 概念ベクトルを求める例として，正規化された[MATH]個のベクトル[MATH]を，異なる[MATH]個のクラスタ[MATH]にクラスタリングすることを考える．
  [i:76, score:0.29436] このとき，ひとつのクラスタ[MATH]に含まれるベクトル[MATH]の平均である重心[MATH]は以下のように表される．
-----------------------------------------------------
  [subsection title : 目的関数]
-----------------------------------------------------
  [i:lead, score:0.32381] [MATH]平均アルゴリズムでは，目的関数は一般的に概念ベクトルとクラスタに属するベクトルとの距離の和
.....
  [i:79, score:0.32381] [MATH]平均アルゴリズムでは，目的関数は一般的に概念ベクトルとクラスタに属するベクトルとの距離の和
  [i:84, score:0.32084] クラスタの密度は，以下のコーシー・シュワルツの不等式より，任意の単位ベクトル[MATH]に対して，クラスタ[MATH]に含まれるベクトル[MATH]と概念ベクトルとの内積の総和が最大となる．
  [i:85, score:0.26831] また，クラスタの密度は，それに属するベクトル和の距離に等しくなるという特徴を持っている．
-----------------------------------------------------
  [subsection title : 球面 $k$ 平均アルゴリズム]
-----------------------------------------------------
  [i:lead, score:0.26319] [REF_moku]節で示した目的関数[MATH]を最大にするように，ベクトルの集合を反復法によりクラスタリングする．
.....
  [i:92, score:0.33715] このとき，すべての概念ベクトルは正規化されているので，余弦は文書ベクトル[MATH]と概念ベクトル[MATH]の内積を求めることと同値である．
  [i:93, score:0.31941] これにより，前回の繰り返しで求めた概念ベクトル[MATH]から，文書ベクトルが新たな部分集合[MATH]に分割される．
  [i:96, score:0.32464] ここで，[MATH]はクラスタ[MATH]の文書ベクトルの重心を表す．

================================================================
[section type  : experiment_result]
[section title : 実験]
================================================================
[i:104, score:0.14535] 本節では，ランダム・プロジェクションを用いた検索モデルを構築し，その評価として，MEDLINEを用いた検索実験について述べる．
-----------------------------------------------------
  [subsection title : データ]
-----------------------------------------------------
  [i:lead, score:0.14202] 実験で用いたデータは，情報検索システムの評価用テストコレクションであるMEDLINEを利用した．
.....
  [i:106, score:0.21565] MEDLINEは医学・生物学分野における英文の文献情報データベースで，検索の対象となる文書の件数は1033件で，約1Mbyteの容量を持つテキストデータである．
  [i:107, score:0.19201] また，MEDLINEには30個の評価用検索要求文と各要求文に対する正解文書が用意されている．
  [i:108, score:0.17649] MEDLINEに含まれている1033件の文書全体から，前処理として，``a''や``about''などの一般的な439個の英単語を不要語リストに指定して，文書の内容と関係のほとんどない単語は削除した．
-----------------------------------------------------
  [subsection title : 検索実験方法]
-----------------------------------------------------
  [i:lead, score:0.41179] 実験では，MEDLINEから前処理により得られた索引語を要素とする文書ベクトルと検索要求ベクトルを作成し，比較することで検索スコアを計算する．
.....
  [i:111, score:0.41179] 実験では，MEDLINEから前処理により得られた索引語を要素とする文書ベクトルと検索要求ベクトルを作成し，比較することで検索スコアを計算する．
  [i:119, score:0.45912] 作成した概念ベクトルを結合した行列に対し，ランダム・プロジェクションを行い，文書ベクトル，検索要求ベクトルの次元を削減する．
  [i:120, score:0.40594] 次元の削減されたベクトルを用いて，内積の計算を行い，その値を各文書に対する検索スコアとする．

================================================================
[section type  : experiment_result]
[section title : 実験結果および考察]
================================================================
[i:125, score:0.00000] 
-----------------------------------------------------
  [subsection title : 次元数による比較]
-----------------------------------------------------
  [i:lead, score:0.43638] 本実験では，ランダム・プロジェクションにより，ベクトルの次元を100から900まで圧縮した検索モデルについて，検索実験を行った．
.....
  [i:126, score:0.43638] 本実験では，ランダム・プロジェクションにより，ベクトルの次元を100から900まで圧縮した検索モデルについて，検索実験を行った．
  [i:128, score:0.43926] 平均正解率は，ベクトルの次元が大きくなるにつれて増加し，次元数300において，次元圧縮を行わないベクトル空間モデルよりも良い結果となった．
  [i:131, score:0.29945] このことから，効果的な検索を行うためには，全文書数の約半分に次元圧縮を行う必要があることが分かった．
-----------------------------------------------------
  [subsection title : 検索モデル作成時間]
-----------------------------------------------------
  [i:lead, score:0.10122] 検索モデルを作成する時間，および，一つの検索要求に対し，検索を行うために必要な時間を測定した結果を述べる．
.....
  [i:133, score:0.39647] 検索実験には，Ultra Sparc(330MHz)のマシンを使用し，ベクトルの次元を500とした結果，表[REF_time]に示すように，ランダム・プロジェクションを用いた場合，モデルを作成する時間は約11分必要であった．
  [i:134, score:0.42411] LSIの場合，SVDの計算についてはSVDPACKの中で最も高速なLanczos法を利用し，同様にベクトルの次元を500とした結果，モデルを作成する時間は約24分で必要であった．
  [i:143, score:0.40660] しかし，非常に大規模な文書数に対しては，より1回の反復による計算量が増加するため，反復計算を必要とせずに，球面[MATH]平均アルゴリズム並の概念ベクトルを得ることが課題となった．
-----------------------------------------------------
  [subsection title : 他の検索モデルとの比較]
-----------------------------------------------------
  [i:lead, score:0.07957] ランダム・プロジェクションを用いた検索モデルに対して，モデルとしての有効性について評価をする．
.....
  [i:145, score:0.56501] この評価をするために，次元圧縮をしていない元のベクトル空間モデルと特異値分解を用いたLSIによる検索モデルについての検索実験も同時に行い，性能を比較した．
  [i:146, score:0.32538] このとき，比較として用いたLSIは，次元数100として次元圧縮した検索モデルを用いている．
  [i:149, score:0.50947] またグラフの`LSI100'は次元数100のLSI，`VSM'は次元圧縮なしのベクトル空間モデル，`RP500'，`RP700'，`RP900'はランダム・プロジェクションによるそれぞれに示された次元数に圧縮したモデルの実験結果である．
-----------------------------------------------------
  [subsection title : 概念ベクトルの有効性]
-----------------------------------------------------
  [i:lead, score:0.44419] ランダム・プロジェクションで次元圧縮に用いられる概念ベクトルが有効であるかを評価するために，他のベクトルを用いて次元圧縮が行われた場合との検索結果の比較を行った．
.....
  [i:159, score:0.49429] このグラフと平均値から，正規分布の性質を持つ任意のベクトルや文書ベクトルの部分集合を用いて次元圧縮を行った結果とそれぞれ比較すると，概念ベクトルを用いて次元圧縮を行った結果が，明らかに優れていることが分かる．
  [i:162, score:0.49381] 文書ベクトルの部分集合を用いた場合は，次元圧縮後，ベクトル中のいくつかの要素が似通った意味を持っているために，検索性能が下がったと考えられる．
  [i:167, score:0.54204] これらのことにより，情報検索に対してランダム・プロジェクションを用いて次元圧縮を行う場合，内容の近い文書や同義語などのような索引語の特徴を表した概念ベクトルを用いることにより，優れた検索性能が得られることが示された．

================================================================
[section type  : conclusion]
[section title : おわりに]
================================================================
[i:175, score:0.54930] また，ランダム・プロジェクションで次元圧縮に必要な行列を得るために，球面[MATH]平均アルゴリズムで得られる概念ベクトルの利用を提案し，その有効性を検索実験にて評価した．
[i:179, score:0.48890] 今後の研究課題としては，まず，球面[MATH]平均アルゴリズムは初期段階での分割に非常に大きな影響を及ぼす可能性があるため，初期分割に依存しない有効な概念ベクトルの生成方法を考慮し，より有効な次元圧縮を実現が可能であると考えられる．
[i:180, score:0.48101] さらに，より有効な次元圧縮を行うために，評価用データの解答やユーザの評価をフィードバック情報として，概念ベクトルの調節を行った検索モデル[CITE][CITE]を構築することが挙げられる．

