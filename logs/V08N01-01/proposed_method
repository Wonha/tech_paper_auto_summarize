ランダム・プロジェクションによるベクトルの次元圧縮
本節では，ランダム・プロジェクションを用いた
ベクトル空間モデル\cite{Papadimitriou}\cite{Arriaga}についての概観を述べる．
ランダム・プロジェクションは，ひとつの文書データを 
$n$ 次元空間上のベクトル ${\bf u}$ として表現するとき，
このベクトルを $k\ (k < n)$ 次元空間に射影する手法である．
その際， $k$ 個の任意の $n$ 次元ベクトル ${\bf r}_1, \cdots , {\bf r}_k$ 
を用意する．
用意したこれらのベクトルと $n$ 次元ベクトル ${\bf u}$ の内積，
\begin{equation}
{\bf u}'_1 = {\bf r}_1 \cdot {\bf u}, \cdots , {\bf u}'_k ={\bf r}_k \cdot {\bf u}
\end{equation}
をそれぞれ計算する．
その結果，$k$ 次元に圧縮した ${\bf u}'_1, \cdots , {\bf u}'_k$ を
要素とするベクトルが得られる．

次元圧縮に必要なベクトル ${\bf r}_1, \cdots , {\bf r}_k$ を
列ベクトルとする $n \times k$ の行列 ${\bf R}$ を用いると，
求める $k$ 次元ベクトルは
\begin{equation}
{\bf u}' = {\bf R}^T {\bf u}
\end{equation}
となり，
ランダム・プロジェクションは行列計算のみの簡単な形で表現することができる．
この行列 ${\bf R}$ が任意の正規直交行列のとき，すなわち，行列 ${\bf R}$ の
列ベクトルがすべて単位ベクトルで，かつ，相異なる列ベクトルが互いに
直交していれば，ランダム・プロジェクションは射影前後におけるベクトル間距離を
近似的に保存する特性を持っている．

概念ベクトルを用いたランダム・プロジェクション
ランダム・プロジェクションに必要な行列 ${\bf R}$ は，これまでの研究では
正規分布などの確率分布をなす任意の行列が
用いられている\cite{Papadimitriou}\cite{Vempala}\cite{Arriaga}
\cite{Kleinberg}\cite{Blum}\cite{Feige}．
このような行列を用いて任意の部分空間に射影する場合，
次元圧縮を行う前後の任意のベクトル間距離は近似的に保存されることが
示されている\cite{Frankl}\cite{Johnson}．
しかし，任意の正規直交行列を用いる場合，次元圧縮を行う前後のベクトル間距離を
保存する効果は得られたとしても，LSIのように，ベクトルの要素が
抽象的な意味を持つ索引語の生成や
内容的に関連のある文書をまとめる効果があるとは考えられない．
このことから，LSIのような，情報検索に有効な索引語を生成するために，
ランダム・プロジェクションの改良が課題となる．

このような課題を解決するものとして，ランダム・プロジェクションで
ベクトルを次元圧縮をした後，さらに特異値分解を行うことにより，
LSIの効果を得る手法が提案されている\cite{Papadimitriou}．
この手法は，関連文書をまとめる効果を得ると同時に，
特異値分解のみを用いた場合に比べ，モデル作成に必要な時間を
短縮したものである．
しかし，ランダム・プロジェクションと特異値分解は，
共にベクトル間距離を保存する効果を持つ手法であるため，
特異値分解が内容的に関連のある文書，あるいはタームをまとめるために
適用されているとしても，これらの手法を同時に利用することは，
検索モデルを構築する時間に関して，効率の良い手法であるとはいえない．
さらに，非常に大きい次元数をもつ行列について考えた場合，
特異値分解に多くの計算量が必要であることも問題となる．
したがって，特異値分解により誤差を最小とする近似行列を得る代わりに，
誤差は最小ではないものの，ランダム・プロジェクションのみを用いてLSIの
効果を得ることで，より高速に検索モデルが構築できるのではないかと考えられる．

これを実現するために，我々は，ランダム・プロジェクションにおける
行列 ${\bf R}$ に，文書の内容を表現した概念ベクトルを利用することを提案する．
概念ベクトルは，文書ベクトル集合をクラスタリングしてできたクラスタの，
各クラスタに属する文書ベクトルの重心を正規化したベクトルとして表される．
この概念ベクトルによる次元圧縮は，単にベクトルを近似するだけではなく，
クラスタに属するベクトル集合の重心を求めることにより，ターム間で
特徴づけられる隠れた関連性やタームの同義性と多義性を捉えることができる．
クラスタリングにより得られた各クラスタは互いに異なる概念を持ち，
これより得られる概念ベクトルが圧縮した空間の軸となるように用いられる．
これにより，次元圧縮された行列は文書と概念ベクトルの類似度を表し，
元の空間において内容の近い文書は，圧縮した空間においても
近くなる可能性がある．
また，類似しているが，異なるタームを使った文書の場合，
元の空間では近くないが，圧縮した空間では近くなる可能性があり，
検索性能が改善されると考えられる．
さらに，多義語により元の空間において近いとされる文書どうしが
圧縮した空間では遠くに離れ，誤った検索が取り除かれる可能性も期待できる．
このように，これまで単語などが要素であったベクトルが，
文書の内容を要素とするようなベクトルに変換され，
文書を低い次元で，より検索性能が向上するベクトル表現ができると考えられる．

概念ベクトルからなる行列 ${\bf R}$ を求めるために，
球面 $k$ 平均アルゴリズム\cite{Dhillon}と呼ばれるクラスタリング手法を用いる．
球面 $k$ 平均アルゴリズムは，目的関数が局所的に最大となるまで，
高い次元でスパースな文書データ集合をクラスタリングする手法である．
球面 $k$ 平均アルゴリズムでは，ユークリッド空間内でベクトル間のなす角の余弦を
類似度とし，多次元空間の単位円を分割することによりクラスタリングを行う．
これにより，文書ベクトルの集合は指定した数の部分集合に分割され，
各クラスタの中心を計算することで，容易に概念ベクトルを作ることができる．
さらに，このアルゴリズムは文書ベクトルのスパースさを逆に利用して高速に
収束する利点を持ち，得られる概念ベクトルは特異値分解を
用いたものに非常に近いことが示されている\cite{Dhillon}．

しかし，球面 $k$ 平均アルゴリズムにより得られる概念ベクトルは
一般的に直交性を満たしているとは限らないため，概念ベクトルを
ランダム・プロジェクションに適用するには疑問が生じる．
先に述べたように，距離を保存するには正規直交性を満たすベクトルを
利用する必要があるが，この概念ベクトルをランダム・プロジェクションに
適用する場合，直交性を満たしていないとしても独立であれば，
任意の行列においても十分に距離を保存する可能性のあることが示されている
\cite{Arriaga}．
球面 $k$ 平均アルゴリズムでは，内容的に似通ったベクトルをクラスタとして
まとめるため，原理的には独立した概念ベクトルを生成すると考えられる．
このため，直交性に関して，概念ベクトルをランダム・プロジェクションに
適用するのは問題ないと考えられる．

本節では，まず，球面 $k$ 平均アルゴリズムの概要を述べる前に，
クラスタリングにより得られる概念ベクトルについて述べる．

\subsection{概念ベクトル}
ベクトルの集合をベクトル空間にプロットしたとき，
同質のベクトルが多く存在する場合を除いて，いくつかのグループに分かれる．
このようなグループはクラスタと呼ばれ，類似した内容をもつ
ベクトルの集合が形成される．
概念ベクトルはクラスタに属するベクトルの重心を求めることにより得られ，
そのクラスタの内容を表す代表ベクトルである．

概念ベクトルを求める例として，正規化された $N$ 個のベクトル 
${\bf x}_1, {\bf x}_2, \cdots , {\bf x}_N$ を，
異なる $s\ (s < N)$ 個のクラスタ $\pi_1, \pi_2, \cdots , \pi_s$ 
にクラスタリングすることを考える．
このとき，ひとつのクラスタ $\pi_j$ に含まれるベクトル $x_i$ の平均である
重心 ${\bf m}_j$ は以下のように表される．
\begin{equation}
{\bf m}_j = \frac{1}{n_j} \sum_{{\bf x}_i \in \pi_j} {\bf x}_i
\end{equation}
ここで $n_j$ はクラスタ $\pi_j$ に含まれるベクトルの数を表す．
ベクトルの重心は単位長にはなっていないので，そのベクトルの長さで
割ることにより概念ベクトル ${\bf c}_j$ を得る．
\begin{equation}
{\bf c}_j = \frac{{\bf m}_j}{\| {\bf m}_j \|}
\end{equation}

\subsection{目的関数} \label{moku}
$k$ 平均アルゴリズムでは，目的関数は一般的に概念ベクトルと
クラスタに属するベクトルとの距離の和
\begin{equation}
\sum_{{\bf x}_i \in \pi_j} \| {\bf m}_j - {\bf x}_i \|
\end{equation}
を最小にするような概念ベクトルを求める，最小二乗法が用いられる．
球面 $k$ 平均アルゴリズムでは，このような最小化問題ではなく，
ミクロ経済学の分野における，生産計画の最適化問題で扱われている目的関数を
用いている\cite{Kleinberg}．
これは，各クラスタ $\pi_{j} (1 \leq j \leq s)$ の密度を
\begin{equation}
\sum_{{\bf x}_i \in \pi_j} {\bf x}_i^T {\bf c}_j
\end{equation}
とし，クラスタの結合密度の和を目的関数としている．
\begin{equation}
D = \sum_{j =1}^{s} \sum_{{\bf x}_i \in \pi_j} {\bf x}_i^T {\bf c}_j
\end{equation}

クラスタの密度は，以下のコーシー・シュワルツの不等式より，
任意の単位ベクトル ${\bf z}$ に対して，
クラスタ $\pi_j$ に含まれるベクトル $x_i$ と概念ベクトルとの内積の
総和が最大となる．
\begin{equation}
\sum_{{\bf x}_i \in \pi_j} {\bf x}_i^T {\bf z} \leq 
\sum_{{\bf x}_i \in \pi_j} {\bf x}_i^T {\bf c}_j
\end{equation}
また，クラスタの密度は，それに属するベクトル和の距離に
等しくなるという特徴を持っている．
\begin{equation}
\sum_{{\bf x}_i \in \pi_j} {\bf x}_i^T {\bf c}_j = 
\| \sum_{{\bf x}_i \in \pi_j} {\bf x}_i \|
\end{equation}

\subsection{球面 $k$ 平均アルゴリズム}
\ref{moku}節で示した目的関数 $D$ を最大にするように，
ベクトルの集合を反復法によりクラスタリングする．
文書ベクトル ${\bf x}_1, {\bf x}_2, \cdots , {\bf x}_N$ を 
$s$ 個のクラスタ $\pi_1^{\star}, \pi_2^{\star}, \cdots , \pi_s^{\star}$ に
分割するためのアルゴリズムを以下に示す．
\begin{enumerate}
\item すべての文書ベクトルを $s$ 個のクラスタに任意に分割する．
これらの部分集合を $\{ \pi_j^{(0)} \}_{j = 1}^{s}$ とし，
これより求められた概念ベクトルの初期集合を 
$\{ {\bf c}_j^{(0)} \}_{j = 1}^{s}$ とする．
また，$t$ を繰り返しの回数とし，初期値は $t = 0$ である．

\item 各文書ベクトル ${\bf x}_i (1 \leq i \leq N$) に対し，
余弦が最も大きい，最も文書ベクトルに近い概念ベクトルを見つける．
このとき，すべての概念ベクトルは正規化されているので，余弦は
文書ベクトル ${\bf x}_i$ と 概念ベクトル ${\bf c}_j^{(t)}$ の内積を
求めることと同値である．
これにより，前回の繰り返しで求めた概念ベクトル 
$\{ {\bf c}_j^{(t)} \}_{j = 1}^{s}$ から，
文書ベクトルが新たな部分集合 $\{ \pi_j^{(t+1)} \}_{j = 1}^{s}$ に分割される．
\begin{equation}
\pi_j^{(t+1)} = \{ {\bf x}_i: 
{\bf x}_i^T {\bf c}_j^{(t)} \geq {\bf x}_i^T {\bf c}_l^{(t)}\} \ 
(1 \leq l \leq N,\  1 \leq j \leq s)
\end{equation}
ここで，$\pi_j^{(t+1)}$ は概念ベクトル ${\bf c}_j^{(t)}$ に近いすべての
文書ベクトルの集合とする．

\item 新たに導かれた概念ベクトルの長さを正規化する．
\begin{equation}
{\bf c}_j^{(t+1)} = \frac{{\bf m}_j^{(t+1)}}{|| {\bf m}_j^{(t+1)} ||}, 
\ \ \ (1 \leq j \leq s)
\end{equation}
ここで，${\bf m}_j^{(t+1)}$ はクラスタ $\pi_j^{(t+1)}$ の文書ベクトルの
重心を表す．

\item 目的関数 $D^{(t+1)}$ の値を求め．前回の繰り返しにおける目的関数の
値 $D^{(t)}$ との差を計算する．
このとき，
\begin{equation}
\| D^{(t)} - D^{(t+1)} \| \leq 1
\end{equation}
を満たす場合，$\pi_j^{\star} = \pi_j^{(t+1)}$，
${\bf c}_j^{\star} = {\bf c}_j^{(t+1)}$ ($1 \leq j \leq s$) とし，
アルゴリズムを終了する．
停止基準を超えていない場合は，$t$ に1を加え，ステップ2に戻る．
ここで，停止基準における目的関数の差は，文書数が約4000で，
クラスタの数が8よりも大きい場合，収束した時の目的関数は1000を超えることが
これまでの研究で報告されている\cite{Dhillon}．
このため，繰り返しでの1以下の差は無視できるとし，便宜的に1という値を設定した．
\end{enumerate}


