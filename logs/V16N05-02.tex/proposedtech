    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.2}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline






\Volume{16}
\Number{5}
\Month{October}
\Year{2009}

\received{2009}{2}{6}
\revised{2009}{7}{16}
\accepted{2009}{8}{19}

\setcounter{page}{23}


\jtitle{半教師有りクラスタリングを用いたWeb検索結果における\\
	人名の曖昧性解消}
\jauthor{杉山　一成\affiref{Author_1} \and 奥村　　学\affiref{Author_2}}
\jabstract{
人名は検索語として，しばしば検索エンジンに入力される．しかし，
この入力された人名に対して，検索エンジンは，いくつかの同姓同名人物に
ついての Web ページを含む長い検索結果のリストを返すだけである．
この問題を解決するために，Web検索結果における人名の曖昧性解消を
目的とした従来研究の多くは，凝集型クラスタリングを適用している．
一方，本研究では，ある種文書に
類似した文書をマージする半教師有りクラスタリングを用いる．我々の
提案する半教師有りクラスタリングは，種文書を含むクラスタの重心の変動を
抑えるという点において，新規性がある．
}
\jkeywords{Web情報検索，半教師有りクラスタリング，人名の曖昧性解消}

\etitle{Personal Name Disambiguation in Web Search Results Using a Semi-Supervised Clustering Approach}
\eauthor{Kazunari Sugiyama\affiref{Author_1} \and Manabu Okumura\affiref{Author_2}} 
\eabstract{
 Personal names are often submitted to search engines as query keywords.
 However, in response to a personal name query, search engines return
 a long list of search results that contains Web pages about several
 namesakes. In order to address this problem,
 most of the previous works that disambiguate personal names
 in Web search results often employ agglomerative clustering approaches.
 In contrast, we have adopted a semi-supervised clustering approach
 to integrate similar documents into a seed document.
 Our proposed semi-supervised clustering approach is
 novel in that it controls the fluctuation of the centroid of a cluster.
}
\ekeywords{Web information retrieval, Semi-supervised clustering, Personal name disambiguation}

\headauthor{杉山，奥村}
\headtitle{半教師有りクラスタリングを用いたWeb検索結果における人名の曖昧性解消}

\affilabel{Author_1}{シンガポール国立大学計算機科学科}{Department of Computer Science, National University of Singapore}
\affilabel{Author_2}{東京工業大学精密工学研究所}{Precision and Intelligence Laboratory, Tokyo Institute of Technology}



\begin{document}
\maketitle


\section{提案手法}
\label{sec:ProposedMethod}

\ref{sec:Intro}章で述べた凝集型クラスタリングに
基づいた人名の曖昧性解消は，クラスタリングを適切に導いていく基準が
ないため，正確なクラスタリングを行うことは難しい．一方，
これまでに提案されている半教師有りクラスタリングは，
クラスタ数$K$をあらかじめ設定する必要がある
$K$-means アルゴリズム \cite{MacQueen67} を改良することを目的としている．
しかし，本研究においては，Web検索結果における同姓同名人物の
数は，事前にわかっているわけではない．したがって，我々の手法においては，
事前にクラスタ数を設定するのではなく，
新たに生成されたクラスタと，すでに生成されているクラスタ間の
類似度を計算し，これらの値がすべて，あらかじめ設定した閾値よりも小さくなった場合に，
クラスタリングの処理を終え，その時点で生成されているクラスタ数を最終的な
同姓同名人物の数とする．

また，従来の半教師有りクラスタリングアルゴリズムは，
制約を導入したり \cite{Wagstaff00}，\cite{Wagstaff01}，\cite{Basu02}，
距離を学習したり \cite{Klein02}，\cite{Xing03}，\cite{Bar-Hillel03} 
することにのみ着目していた．
しかし，
半教師有りクラスタリングにおいて，より正確なクラスタリング結果を
得るためには，seedページ間への制約の導入とともに，seedページを
含むクラスタの重心の変動の抑制も重要である．
これは，
(1) seedページを導入して半教師有りクラスタリングを行なう場合，
通常の重心の計算法では重心の変動が大きくなる傾向にあり，
クラスタリングの基準となるseedページを導入する効果が得られない，
(2) 重心を完全に固定して半教師有りクラスタリングを行なう場合，
その重心と類似度が高いWebページしかマージされなくなり，
多数の独立したクラスタが生成されやすくなる，
という二つの考えに基づく．
したがって，seedページを含むクラスタの重心の変動を抑えることが
できれば，より適切なクラスタリングが実現できると期待される．

本章では，我々の提案する半教師有りクラスタリングの手法について説明する．

以下，検索結果集合$W_{p}$中のWebページ$p_{i}$の
特徴ベクトル$\boldsymbol{w}^{p_{i}}$ $(i=1,\cdots ,n)$を
式(\ref{Eq:feature vector_org})のように表す．
\begin{equation}
 \boldsymbol{w}^{p_{i}}=(w_{t_{1}}^{p_{i}},w_{t_{2}}^{p_{i}},\cdots, w_{t_{m}}^{p_{i}})
\label{Eq:feature vector_org}
\end{equation}
ここで，$m$は検索結果集合$W_{p}$における単語の
異なり数であり，$t_{k}$ $(k=1,2,\cdots, m)$は，各単語を表す．
予備実験として，(a) Term Frequency (TF)，(b) Inverse Document
Frequency (IDF)，(c) residual IDF (RIDF)，(d) TF-IDF，
(e) $x^{I}$-measure，(f) gain の6つの単語重み付け法を
比較した．これらの単語重み付け法は，それぞれ，次のように定義される．

\clearpage

\noindent
\textbf{(a) Term Frequency (TF)} 

TFは，与えられた文書において，ある単語がどれだけ顕著に
出現するかを示し，この値が大きければ大きいほど，その単語が文書の内容を
よく表現していることを示す．
$tf(t_{k}, p_{i})$をWebページ$p_{i}$における単語$t_{k}$の頻度とする．このとき，
$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，式(\ref{eq: tf})に
よって定義される．
\begin{eqnarray}
 w_{t_{k}}^{p_{i}}=\frac{tf(t_{k}, p_{i})}{\sum_{s=1}^{m}tf(t_{s}, p_{i})} \label{eq: tf}
\end{eqnarray} 

\noindent
\textbf{(b) Inverse Document Frequency (IDF)} 

\cite{Jones73} によって導入されたIDFは，その単語が出現する
文書数が少なければ少ないほど，その単語が出現する文書にとっては，
有用であることを示すスコアである．
このとき，$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，式(\ref{eq: idf})によって定義される．
\begin{eqnarray}
 w_{t_{k}}^{p_{i}}=\log\frac{N}{df(t_{k})} \label{eq: idf}
\end{eqnarray}
ここで，$N$はWebページの総数，$df(t_{k})$は単語$t_{k}$が現れる
Webページ数である．

\noindent
\textbf{(c) Residual Inverse Document Frequency (RIDF)}

Church and Gale \cite{Church95VLC,Church95JNLE} は，
ほとんどすべての単語は，ポアッソンモデルのような独立性に基づいた
モデルに応じて，非常に大きなIDFスコアを持つことを示した．また，
単語の有用性は，推定されるスコアからは
大きな偏差を持つ傾向があるという考えに基づいて導入したスコアがresidual IDFである．
このスコアは，実際のIDFとポアッソン分布によって推定されるIDFとの差として定義される．
$cf_{k}$を文書集合中における単語$t_{k}$の総出現数，$N$をWebページの総数としたとき，
1つのWebページあたりの単語$t_{k}$の平均出現数は，
$\lambda_{k}=\frac{cf_{k}}{N}$と
表される．このとき，
$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，
式(\ref{eq: ridf})によって定義される．
\begin{align}
 w_{t_{k}}^{p_{i}} &= IDF - \log\frac{1}{1-p(0;\lambda_{i})} \nonumber \\
               &= \log\frac{N}{df(t_{k})}+\log(1-p(0;\lambda_{k})) \label{eq: ridf}
\end{align}
ここで，$p$は，パラメータ$\lambda_{k}$を伴うポアッソン分布である．
この手法は，少数の文書のみに出現する単語は，より大きなRIDFスコアを持つ傾向がある．

\noindent
\textbf{(d) TF-IDF}

TF-IDF 法 \cite{Salton83} は，文書中の単語を重み付けするために，情報
検索の研究において広く使われている．TF-IDFは，上述した
(a) TF と (b) IDF に基づいて，式(\ref{eq: tfidf})のように定義される．
\begin{eqnarray}
 w_{t_{k}}^{p_{i}} = \frac{tf(t_{k}, p_{i})}{\sum_{s=1}^{m}tf(t_{s}, p_{i})}\cdot \log\frac{N}{df(t_{k})} \label{eq: tfidf}
\end{eqnarray}
ここで， $tf(t_{k}, p_{i})$と$df(t_{k})$は，それぞれ，
Webページ$p_{i}$における単語$t_{k}$の頻度と，単語$t_{k}$が
出現するWebページ数を表す．また，$N$はWebページの総数である．

\noindent
\textbf{(e) $x^{I}$-measure}

Bookstein and Swanson \cite{Bookstein74} は，単語$t_{k}$に対する
 $x^{I}$-measure というスコアを導入した．
$tf(t_{k}, p_{i})$をWebページ$p_{i}$における単語$t_{k}$の頻度，
$df(t_{k})$を単語$t_{k}$が現れるWebページ数とすると，
$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，
式(\ref{eq: xI})によって定義される．
\begin{eqnarray}
 w_{t_{k}}^{p_{i}}=tf(t_{k}, p_{i})-df(t_{k}) \label{eq: xI}
\end{eqnarray}
この手法は，同程度の出現頻度である2つの単語のうち，
少数の文書に集中して出現する単語ほど，高いスコアを示す．

\noindent
\textbf{(f) gain}

一般に，IDFは単語の重要性を表すと考えられているが，
Papineni \cite{Papineni01} は，IDFは単語の特徴を表す最適な重みに過ぎず，
単語の重要性とは異なるものであるため，
利得を単語の重要性と考え，gainを提案した．本手法では，
$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，
式(\ref{eq: gain})によって定義される．
\vspace{-0.5\baselineskip}
\begin{eqnarray}
 w_{t_{k}}^{p_{i}}=\frac{df(t_{k})}{N}\left(\frac{df(t_{k})}{N}-1-\log\frac{df(t_{k})}{N}\right) \label{eq: gain}
\end{eqnarray}
ここで，$df(t_{k})$は，単語$t_{k}$が現れるWebページ数を，
$N$はWebページの総数を示す．本手法では，ほとんど出現しない単語と，
非常に頻出する単語は，両方とも低いスコアとなり，中頻度の単語は
高いスコアとなる．

上述した(a)〜(f)の単語重み付け手法の中で，本研究においては，``(f) gain''が
最も効果的な単語の重み付け法であることがわかったため，これを本研究に
おける単語の重み付け法として用いる．
さらに，クラスタ$C$の重心ベクトル
$\boldsymbol{G}^{C}$を式(\ref{Eq:centroid vector})のように定義する．
\begin{eqnarray}
 \boldsymbol{G}^{C}=(g^{C}_{t_{1}},g^{C}_{t_{2}},\cdots, g^{C}_{t_{m}})
\label{Eq:centroid vector}
\end{eqnarray}
ここで，$g^{C}_{t_{k}}$は$\boldsymbol{G}^{C}$に
おける各単語の重みであり，$t_{k}$ $(k=1,2,\cdots, m)$は各単語を表す．
なお，以下で述べるクラスタリング手法では，2つのクラスタ$C_{i}$，
$C_{j}$間の類似度$sim(C_{i},C_{j})$を，式(\ref{eq:sim})によって
計算する．
\begin{eqnarray}
 sim(C_{i},C_{j})=
\frac{\boldsymbol{G}^{C_{i}}\cdot\boldsymbol{G}^{C_{j}}}{|\boldsymbol{G}^{C_{i}}|\cdot |\boldsymbol{G}^{C_{j}}|}
\label{eq:sim}
\end{eqnarray}
ただし，$\boldsymbol{G}^{C_{i}}$，$\boldsymbol{G}^{C_{j}}$は，それぞれ，
クラスタ$C_{i}$，$C_{j}$の
重心ベクトルを表す．


\subsection{凝集型クラスタリング}
\label{subsec:AggCls}

凝集型クラスタリングにおいては，はじめに各Webページを，
\pagebreak
個々のクラスタとして設定する．次に，二つのクラスタ間の類似度が，
あらかじめ設定された閾値より小さくなるまで，
類似度が最大となる二つのクラスタをマージして
新たなクラスタを生成する．図\ref{Fig:AggClsAlgorithm}に凝集型
クラスタリングのアルゴリズムを示す．

\begin{figure}[b]
\begin{center}
\includegraphics{16-4ia3f1.eps}
\end{center}
\caption{凝集型クラスタリングアルゴリズム}\label{Fig:AggClsAlgorithm}
\end{figure}

このアルゴリズムでは，あるクラスタ$C_{i}$ (要素数$n_{i}$)
を最も類似したクラスタ$C_{j}$ (要素数$n_{j}$) にマージした後の，
新たなクラスタ$C^{new}$の重心ベクトル
$\boldsymbol{G}^{new}$は，
式(\ref{eq:new agg-centroid})のように定義される．
\begin{eqnarray}
 \boldsymbol{G}^{new}=\frac{\sum_{\boldsymbol{w}^{p}\in C_{i}}\boldsymbol{w}^{p}+\sum_{\boldsymbol{w}^{p}\in C_{j}}\boldsymbol{w}^{p}}{n_{i}+n_{j}} \label{eq:new agg-centroid}
\end{eqnarray}


\subsection{提案する半教師有りクラスタリング} \label{subsec:SSCls}

一般に，seedページを含むクラスタ$C_{s_{j}}$
と，seedページを含まないクラスタ$C_{i}$の類似度が
大きい場合には，両者を新たなクラスタとしてマージすべきであるが，
両者の距離が大きい場合には，通常の重心の計算法では，
重心の変動が大きくなる傾向にある．そこで，
はじめに，あるクラスタ$C_{i}$(重心ベクトル$\boldsymbol{G}^{C_{i}}$)を，
seedページを含むクラスタ$C_{s_{j}}$(重心ベクトル$\boldsymbol{G}^{C_{s_{j}}}$)に
マージする際，
これらのクラスタの重心間の距離
$D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}})$に
基づいて，Webページ$p$の特徴ベクトル$\boldsymbol{w}^{p}\in C_{i}$を
重み付けする．次に，この重み付けした特徴ベクトルを用いて重心の計算を
行なうことで上述した傾向を防ぎ，重心の変動を抑える．

まず，これまでに$k_{j}$個のクラスタがマージされたseedページを含むクラスタ
$C_{s_{j}}^{(k_{j})}$ (要素数$n_{s_{j}}$) に対して，
クラスタ$C_{i}$ (要素数$n_{i}$) が$k_{j}+1$回目に
マージされるクラスタであるとする．
なお，クラスタ$C_{s_{j}}^{(0)}$の要素は，初期のseedページとなる．

\noindent
\textbf{(1)} この$C_{s_{j}}^{(k_{j})}$にマージされるクラスタ$C_{i}$に
含まれる各要素について，
$C_{s_{j}}^{(k_{j})}$の重心$\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}}$と，
クラスタ$C_{i}$の重心$\boldsymbol{G}^{C_{i}}$間の距離尺度
$D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})$を用いて，
クラスタ$C_{i}$に含まれるWebページの特徴ベクトル
$\boldsymbol{w}^{p_{l}}_{C_{i}}$ $(l=1,\cdots ,n_{i})$を
重み付けし，その後に生成されるクラスタを$C_{i'}$ (要素数$n_{i'}$) 
とする．このとき，$C_{i'}$の要素となる重み付けした後のWebページの
特徴ベクトル$\boldsymbol{w}^{p_{l}}_{C_{i'}}$は，式(\ref{eq:TransferedCor})で表される．
\begin{eqnarray}
\boldsymbol{w}^{p_{l}}_{C_{i'}}
=\frac{\boldsymbol{w}^{p_{l}}_{C_{i}}}{D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})+c} \label{eq:TransferedCor}
\end{eqnarray}
本研究では，$D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})$
として，(i)ユークリッド距離，(ii)マハラノビス距離，
(iii)適応的マハラノビス距離，の三つの距離尺度を比較する．
また，$c$は$D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})$が
0に非常に近い値となったとき，
$\boldsymbol{w}^{p}$の各要素が極端に大きな値となることを防ぐ
ために導入した定数である．
この$c$の値の影響については，3.3.1節で
述べる．

\noindent
\textbf{(2)} 次に，seedページを含むクラスタ$C_{s_{j}}^{(k_{j})}$
(要素数$n_{s_{j}}$)に$C_{i'}$(要素数$n_{i'}$)の
要素を追加し，クラスタ$C_{s_{j}}^{(k_{j}+1)}$
(要素数$n_{s_{j}}+n_{i'}$)を作成する．
\[
C_{s_{j}}^{(k_{j}+1)}=
\{
\boldsymbol{w}^{p_{1}}_{C_{s_{j}}^{(k_{j})}},\cdots
,\boldsymbol{w}^{p_{{n}_{s_{j}}}}_{C_{s_{j}}^{(k_{j})}},
\boldsymbol{w}^{p_{1}}_{C_{i'}},\cdots ,\boldsymbol{w}^{p_{n_{{i'}}}}_{C_{i'}} \}
\]


\noindent
\textbf{(3)} このとき，$k_{j}+1$回目のクラスタをマージしたクラスタ$C_{s_{j}}^{(k_{j}+1)}$の
重心$\boldsymbol{G}^{C_{s_{j}}^{(k_{j}+1)}}$は，式(\ref{eq:NewG})のように
計算される．ここで，式(\ref{eq:TransferedCor})において，マージされる
クラスタの特徴ベクトル$\boldsymbol{w}^{p_{l}}_{C_{i}}$に重み付けを
しているため，重み付き平均の計算と
なるように，$n_{i'}$にも同様の重みを乗じている．
\begin{eqnarray}
\boldsymbol{G}^{C_{s_{j}}^{(k_{j}+1)}}=\frac{\sum_{\boldsymbol{w}^{p}\in C_{s_{j}}^{(k_{j}+1)}}\boldsymbol{w}^{p}}{n_{{s_{j}}}+n_{i'}\times\frac{1}{D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})+c}}
\label{eq:NewG}
\end{eqnarray}

このように本研究では，seedページを含むクラスタを重視してクラスタリングの
基準を明確にし，正確なクラスタリングを行うことを目的とする．
もし，2つのクラスタが種用例を含まないのであれば，
新たなクラスタの重心ベクトル$\boldsymbol{G}^{new}$は，
式(\ref{eq:new centroid(agg)})のように計算される．
\begin{eqnarray}
 \boldsymbol{G}^{new}=\frac{\sum_{\boldsymbol{w}^{p}\in C_{i}}\boldsymbol{w}^{p}+\sum_{\boldsymbol{w}^{p}\in C_{j}}\boldsymbol{w}^{p}}{n_{i}+n_{j}}
 \label{eq:new centroid(agg)}
\end{eqnarray}

本研究では，seedページを含むクラスタに，それと最も類似したクラスタを
マージする際，seedページを含むクラスタの
重心の変動を抑える半教師有りクラスタリングを適用して，
Web検索結果における人名の曖昧性を解消する．
従来の半教師有りクラスタリングの手法のうち，
制約を導入する手法では，クラスタの基準となる重心についての
検討は見逃されており，また，距離を学習する手法では，
特徴空間が大域的に変換される．
一方，我々の手法は，seedページを含むクラスタの
重心の変動を抑え，その重心を局所的に調整できる効果が期待される．
なお，seedページを導入することで，検索結果を改善することは，
適合性フィードバック\cite{Rocchio71}に類似した手法であると考えられる．
しかし，適合性フィードバックでは，検索結果中の文書に
対して，ユーザが判断した適合文書・非適合文書に
基づいた検索語の修正を目的としているのに対し，本手法は，
あらかじめ設定したseedページに基づいて，検索結果の改善，特に本研究に
おいては，検索結果のクラスタリング精度の改善を目的としている点が異なる．

また，検索結果をクラスタリングする
検索エンジンとして，``Clusty''\footnote{http://clusty.com}
が挙げられる．しかし，そのクラスタリングされた検索結果には，
適合しないWebページが含まれることも
多く，クラスタリングを行う上で，何らかの基準が必要である．
すなわち，本研究のように，
seedページをクラスタリングの基準として導入し，かつ，そのseedページを
含むクラスタの重心を抑えることで，その基準を保つような手法が必要であると
考えられる．

図\ref{Fig:SSClsAlgorithm}に，我々の提案する半教師有りクラスタリング
アルゴリズムの詳細を示す．なお，提案する半教師有りクラスタリングでは，
対象とするすべてのWebページが，いずれかのseedページを含むクラスタに
マージされるのではなく，seedページを含まないクラスタにもマージされる
ことに，注意されたい（図\ref{Fig:SSClsAlgorithm}下から7行目，
``else if''以降参照）．

\begin{figure}[p]
\begin{center}
\includegraphics{16-4ia3f2.eps}
\end{center}
\caption{提案する半教師有りクラスタリングアルゴリズム} \label{Fig:SSClsAlgorithm}
\end{figure}

ここで，本研究において比較する式(\ref{eq:TransferedCor})直後に
述べた(i)，(ii)，(iii)の3つの距離尺度は，それぞれ，以下のように
定義される．


\noindent
\textbf{(i) ユークリッド距離}

式(\ref{eq:TransferedCor})において，ユークリッド距離を導入した場合，
seedページを含むクラスタの重心ベクトル$\boldsymbol{G}^{C_{s}}$と，
あるクラスタ$C$の重心ベクトル$\boldsymbol{G}^{C}$間の距離
$D(\boldsymbol{G}^{C_{s}},\boldsymbol{G}^{C})$は，
式(\ref{Eq:centroid vector})に基づいて，
式(\ref{Eq:Euclidean disrance})のように定義される．
\begin{eqnarray}
 D(\boldsymbol{G}^{C_{s}},\boldsymbol{G}^{C})=\sqrt{\sum_{k=1}^{m}(g^{C_{s}}_{t_{k}}-g^{C}_{t_{k}})^{2}}
\label{Eq:Euclidean disrance}
\end{eqnarray}

\noindent
\textbf{(ii) マハラノビス距離}

マハラノビス距離は，データ集合の相関を考慮した尺度である
という点において，ユークリッド距離とは異なる．
したがって，ユークリッド距離を用いるよりも
マハラノビス距離を用いた方が，クラスタの重心の変動を，
より効果的に抑えられることが期待される．

式(\ref{eq:TransferedCor})において，マハラノビス距離を導入した場合，
seedページを含むクラスタ$C_{s}$の重心ベクトル$\boldsymbol{G}^{C_{s}}$と，
あるクラスタ$C$の重心ベクトル$\boldsymbol{G}^{C}$間の距離
$D(\boldsymbol{G}^{C_{s}},\boldsymbol{G}^{C})$は，
式(\ref{Eq:Mahalanobis distance})のように定義される．
\begin{eqnarray}
D(\boldsymbol{G_{C_{(s)}}},\boldsymbol{G_{C}})
=\sqrt{(\boldsymbol{G}^{C_{s}}-\boldsymbol{G}^{C})^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{G}^{C_{s}}-\boldsymbol{G}^{C})} \label{Eq:Mahalanobis distance}
\end{eqnarray}
ここで，$\boldsymbol{\Sigma}$は，seedページを含むクラスタ$C_{s}$
の要素によって定義される共分散行列である．

すなわち，クラスタ$C_{s}$内の要素を，
\[
 C_{s}=\{\boldsymbol{w}^{p_{1}}_{C_{s}},\boldsymbol{w}^{p_{2}}_{C_{s}},\cdots ,\boldsymbol{w}^{p_{m}}_{C_{s}}\}
\]
と表せば，重心ベクトル$\boldsymbol{G}^{C_{s}}$，
\[
 \boldsymbol{G}^{C_{s}}=\frac{1}{m}\sum_{i=1}^{m}\boldsymbol{w}^{p_{i}}_{C_{s}}
\]
を用いて，共分散$\Sigma_{ij}$を式(\ref{eq:CovMHD})のように
定義することができる．
\begin{eqnarray}
 \Sigma_{ij}=\frac{1}{m}\sum_{i=1}^{m}(\boldsymbol{w}^{p_{i}}_{C_{s}}-\boldsymbol{G}^{C_{s}})
(\boldsymbol{w}^{p_{j}}_{C_{s}}-\boldsymbol{G}^{C_{s}})^{T} \label{eq:CovMHD}
\end{eqnarray}
以上から，共分散行列$\boldsymbol{\Sigma}$は，
\[
 \boldsymbol{\Sigma}=\left[
 \begin{array}{@{\,}cccc@{\,}}
  \Sigma_{11} & \Sigma_{12} & \cdots & \Sigma_{1m} \\
  \Sigma_{21} & \Sigma_{22} & \cdots & \Sigma_{2m} \\
  \vdots      & \vdots      & \ddots & \vdots \\
  \Sigma_{m1} & \Sigma_{m2} & \cdots & \Sigma_{mm}
 \end{array}
\right]
\]
と表すことができる．

\noindent
\textbf{(iii)適応的マハラノビス距離}

(ii)のマハラノビス距離は，クラスタ内の要素数が
少ないときに，共分散が大きくなる傾向がある．
そこで，seedページを含むあるクラスタ$C_{s_{j}}$について，
このクラスタに含まれるWebページの特徴ベクトル間の
非類似度を局所最小化することを考える．
この局所最小化で得られる分散共分散行列を
用いて計算した
$C_{s_{j}}$の重心ベクトル$\boldsymbol{G}^{C_{s_{j}}}$と，
このクラスタにマージされるクラスタ$C_{l}$の
重心ベクトル$\boldsymbol{G}^{C_{l}}$間の距離が，
適応的マハラノビス距離 \cite{Diday77} である．
この分散共分散行列は，次のように導出される．

\noindent
\textbf{(1)}まず，クラスタ$C_{s_{j}}$において，このクラスタ
に含まれるWebページの特徴ベクトル
$\boldsymbol{w}^{p_{i}}$と，それ以外の
特徴ベクトル$\boldsymbol{v}$ $(\boldsymbol{w}^{p_{i}}\neq \boldsymbol{v})$との
非類似度$d_{s_{j}}(\boldsymbol{w}^{p_{i}},\boldsymbol{v})$を，
式(\ref{eq:intra-cls})により定義する．
\begin{eqnarray}
d_{s_{j}}(\boldsymbol{w}^{p_{i}},\boldsymbol{v})=
(\boldsymbol{w}^{p_{i}}-\boldsymbol{v})^{T}\boldsymbol{M}_{s_{j}}^{-1}(\boldsymbol{w}^{p_{i}}-\boldsymbol{v}) \label{eq:intra-cls}
\end{eqnarray}
ただし，$\boldsymbol{M}_{s_{j}}$は$C_{s_{j}}$の分散共分散行列を表す．
すなわち，クラスタ$C_{s_{j}}$内の要素を，
\[
 C_{s_{j}}=\{\boldsymbol{w}^{p_{1}}_{C_{s_{j}}},\boldsymbol{w}^{p_{2}}_{C_{s_{j}}},\cdots ,\boldsymbol{w}^{p_{m}}_{C_{s_{j}}}\}
\]
\pagebreak
と表せば，重心ベクトル$\boldsymbol{G}^{C_{s_{j}}}$，
\[
 \boldsymbol{G}^{C_{s_{j}}}=\frac{1}{m}\sum_{i=1}^{m}\boldsymbol{w}^{p_{i}}_{C_{s_{j}}}
\]
を用いて，共分散$M_{ij}$を式(\ref{eq:CovAMHD})のように定義することができる．
\begin{eqnarray}
 M_{ij}=\frac{1}{m}\sum_{i=1}^{m}(\boldsymbol{w}^{p_{i}}_{C_{s_{j}}}-\boldsymbol{G}^{C_{s_{j}}})
(\boldsymbol{w}^{p_{j}}_{C_{s_{j}}}-\boldsymbol{G}^{C_{s_{j}}})^{T} \label{eq:CovAMHD}
\end{eqnarray}
以上から，共分散行列$\boldsymbol{M_{s_{j}}}$は，
\[
 \boldsymbol{M_{s_{j}}}=\left[
 \begin{array}{@{\,}cccc@{\,}}
  M_{11} & M_{12} & \cdots & M_{1m} \\
  M_{21} & M_{22} & \cdots & M_{2m} \\
  \vdots      & \vdots      & \ddots & \vdots \\
  M_{m1} & M_{m2} & \cdots & M_{mm}
 \end{array}
\right]
\]
と表すことができる．

\noindent
\textbf{(2)} 次に，目的関数
\begin{align*}
 \Delta_{s_{j}}(\boldsymbol{v},\boldsymbol{M}_{s_{j}})
	&= \sum_{\boldsymbol{w}^{p_{i}}\in C_{s_{j}}}d_{s_{j}}
	(\boldsymbol{w}^{p_{i}},\boldsymbol{v})\\
  & = \sum_{\boldsymbol{w}^{p_{i}}\in C_{s_{j}}}
	(\boldsymbol{w}^{p_{i}}-\boldsymbol{v})^{T}\boldsymbol{M}_{s_{j}}^{-1}
	(\boldsymbol{w}^{p_{i}}-\boldsymbol{v})
\end{align*}
を定義し，これを局所最小化するような$C_{{s}_{j}}$の代表点の特徴ベクトル
$\boldsymbol{L}_{s_{j}}$と分散共分散行列$\boldsymbol{S}_{{s}_{j}}$を求める．

\noindent
(i) まず，クラスタ$C_{s_{j}}$の要素により定義される共分散
行列$\boldsymbol{M}_{s_{j}}$を固定し，
$\Delta_{s_{j}}$を最小化する$\boldsymbol{L}_{s_{j}}$を求める．
\begin{eqnarray}
\boldsymbol{L}_{s_{j}}=
\arg\min_{\boldsymbol{v}}\sum_{\boldsymbol{w}^{p_{i}}\in C_{s_{j}}}
(\boldsymbol{w}^{p_{i}}-\boldsymbol{v})^{T}\boldsymbol{M}_{s_{j}}^{-1}(\boldsymbol{w}^{p_{i}}-\boldsymbol{v}) \label{eq:Lj}
\end{eqnarray}
式(\ref{eq:Lj})において，クラスタ$C_{s_{j}}$の重心$G$に最も近い点
$G'$の特徴ベクトルを$\boldsymbol{v}_{G'}$と表せば，
$\boldsymbol{L}_{s_{j}}=\boldsymbol{v}_{G'}$と求めることができる．

\noindent
(ii) 次に，
(i)で求めた代表点の特徴ベクトル
$\boldsymbol{L}_{s_{j}}=\boldsymbol{v}_{G'}$
を固定する．ここで，
$det(\boldsymbol{M}_{s_{j}})=1$のもとで，
$\Delta_{s_{j}}$を局所最小化する$\boldsymbol{S}_{s_{j}}$を求める．
\begin{eqnarray}
\boldsymbol{S}_{s_{j}}=\arg\min_{\boldsymbol{M}_{s_{j}}}\sum_{\boldsymbol{w}^{p_{i}}\in C_{s_{j}}}
(\boldsymbol{w}^{p_{i}}-\boldsymbol{v}_{G'})^{T}
\boldsymbol{M}_{s_{j}}^{-1}(\boldsymbol{w}^{p_{i}}-\boldsymbol{v}_{G'}) \label{eq:dj}
\end{eqnarray}
この$\boldsymbol{S}_{s_{j}}$は，クラスタ$C_{s_{j}}$の
共分散行列$\boldsymbol{M}_{s_{j}}$を用いて，
式(\ref{eq:AdpCov})によって与えられることが，文献 \cite{Diday77} に
より示されている．
\pagebreak
\begin{eqnarray}
\boldsymbol{S}_{s_{j}}=(det(\boldsymbol{M}_{s_{j}}))^{1/m}\boldsymbol{M}_{s_{j}}^{-1}
\label{eq:AdpCov}
\end{eqnarray}
ただし，$det(\boldsymbol{M}_{s_{j}}) \neq 0$であり，$m$は検索結果
集合における単語の異なり数を表す．

以上から，seedページを含むあるクラスタ$C_{s_{j}}$において，
Webページ間の非類似度を局所最小化することを考慮した
分散共分散行列$\boldsymbol{S}_{s_{j}}$を求めることができる．
この$\boldsymbol{S}_{s_{j}}$を用いて，$C_{s_{j}}$の
重心ベクトル$\boldsymbol{G}^{C_{s_{j}}}$と，このクラスタに
マージされるべきクラスタ$C_{l}$の重心ベクトル$\boldsymbol{G}^{C_{l}}$
間の適応的マハラノビス距離は，
式(\ref{Eq:Adapt. Mahalanobis distance})のように定義される．
\begin{eqnarray}
D(\boldsymbol{G}^{C_{s_{j}}},\boldsymbol{G}^{C_{l}})=
\sqrt{(\boldsymbol{G}^{C_{s_{j}}}-\boldsymbol{G}^{C_{l}})^{T}\boldsymbol{S}_{s_{j}}^{-1}(\boldsymbol{G}^{C_{s_{j}}}-\boldsymbol{G}^{C_{l}})} \label{Eq:Adapt. Mahalanobis distance}
\end{eqnarray}
なお，式(\ref{Eq:Adapt. Mahalanobis distance})は，
上述した\textbf{(1)}〜\textbf{(2)}による
クラスタ$C_{s_{j}}$におけるWebページ間の非類似度を
考慮して得られた式(\ref{eq:AdpCov})の
分散共分散行列$\boldsymbol{S}_{s_{j}}$を
適用している点で，式(\ref{Eq:Mahalanobis distance})とは異なる．


\section{むすび}
\label{sec:Conclusion}

本論文では，Web検索結果における人名の曖昧性を解消するため，
seedページを含むクラスタの重心の変動を抑える
半教師有りクラスタリングの手法を提案した．
実験の結果，最良な場合において，
[purity:0.80，inverse purity:0.83，$F_{0.5}$:0.81，
$F_{0.2}$:0.82]の評価値が得られた．
今回は，上位に順位付けされる検索エンジンの出力結果が
異なることを想定して実験を行った．すなわち，同一人物の
seedページ間にも``cannot-link''の制約が導入されている
可能性がある．しかし，クラスタが生成される過程で，seedページ以外の人物の
ページがクラスタ内の要素として支配的になり，最終的には比較的正確なクラスタが
生成されることが観察された．同一人物のseedページ間でも，その人物を正確に
表現しているページ，そうでないページがあることによるためであると
考えられる．したがって，その人物についてより正確に
記述されたWebページをseedページとして選択することが，今後の課題の
一つとして挙げられる．
また，Web検索結果における人名の曖昧性解消の精度を高めるには，
その人物を特徴付ける単語の重みが大きくなるように，
Webページの特徴ベクトルを作成して，クラスタリングを行なう
ことが重要である．
そのために，特に，seedページの内容に適合する人物のページが集まるように，
より的確なseedページの特徴ベクトルを作成するための
手法を開発してクラスタリングを行なうことも，
今後の課題として挙げられる．






\bibliographystyle{jnlpbbl_1.4}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{Artiles, Gonzalo, \BBA\ Sekine}{Artiles
  et~al.}{2007}]{Artiles07}
Artiles, J., Gonzalo, J., \BBA\ Sekine, S. \BBOP 2007\BBCP.
\newblock \BBOQ The SemEval-2007 WePS Evaluation: Establishing a Benchmark for
  the Web People Search Task.\BBCQ\
\newblock In {\Bem Proc. of the Semeval 2007, Association for Computational
  Linguistics (ACL)}, \mbox{\BPGS\ 64--69}.

\bibitem[\protect\BCAY{Bar-Hillel, Hertz, \BBA\ Shental}{Bar-Hillel
  et~al.}{2003}]{Bar-Hillel03}
Bar-Hillel, A., Hertz, T., \BBA\ Shental, N. \BBOP 2003\BBCP.
\newblock \BBOQ Learning Distance Functions Using Equivalence Relations.\BBCQ\
\newblock In {\Bem Proc. of the 20th International Conference on Machine
  Learning (ICML 2003)}, \mbox{\BPGS\ 577--584}.

\bibitem[\protect\BCAY{Basu, Banerjee, \BBA\ Mooney}{Basu
  et~al.}{2002}]{Basu02}
Basu, S., Banerjee, A., \BBA\ Mooney, R. \BBOP 2002\BBCP.
\newblock \BBOQ Semi-supervised Clustering by Seeding.\BBCQ\
\newblock In {\Bem Proc. of the 19th International Conference on Machine
  Learning (ICML 2002)}, \mbox{\BPGS\ 27--34}.

\bibitem[\protect\BCAY{Bekkerman, El-Yaniv, \BBA\ McCallum}{Bekkerman
  et~al.}{2005}]{Bekkerman-ICML05}
Bekkerman, R., El-Yaniv, R., \BBA\ McCallum, A. \BBOP 2005\BBCP.
\newblock \BBOQ Multi-way Distributional Clustering via Pairwise
  Interactions.\BBCQ\
\newblock In {\Bem Proc. of the 22nd International Conference on Machine
  Learning (ICML2005)}, \mbox{\BPGS\ 41--48}.

\bibitem[\protect\BCAY{Bollegala, Matsuo, \BBA\ Ishizuka}{Bollegala
  et~al.}{2006}]{Bollegala06}
Bollegala, D., Matsuo, Y., \BBA\ Ishizuka, M. \BBOP 2006\BBCP.
\newblock \BBOQ Extracting Key Phrases to Disambiguate Personal Names on the
  Web.\BBCQ\
\newblock In {\Bem Proc. of the 7th International Conference on Computational
  Linguistics and Intelligent Text Processing (CICLing 2006)}, \mbox{\BPGS\
  223--234}.

\bibitem[\protect\BCAY{Bookstein \BBA\ Swanson}{Bookstein \BBA\
  Swanson}{1974}]{Bookstein74}
Bookstein, A.\BBACOMMA\ \BBA\ Swanson, D.~R. \BBOP 1974\BBCP.
\newblock \BBOQ Probabilistic Models for Automatic Indexing.\BBCQ\
\newblock {\Bem Journal of the American Society for Information Science}, {\Bbf
  25}  (5), \mbox{\BPGS\ 312--318}.

\bibitem[\protect\BCAY{Bunescu \BBA\ Pasca}{Bunescu \BBA\
  Pasca}{2006}]{Bunescu06}
Bunescu, R.\BBACOMMA\ \BBA\ Pasca, M. \BBOP 2006\BBCP.
\newblock \BBOQ Using Encyclopedic Knowledge for Named Entity
  Disambiguation.\BBCQ\
\newblock In {\Bem Proc. of the 11th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2006)}, \mbox{\BPGS\ 9--16}.

\bibitem[\protect\BCAY{Chen \BBA\ Martin}{Chen \BBA\ Martin}{2007}]{Chen07}
Chen, Y.\BBACOMMA\ \BBA\ Martin, J. \BBOP 2007\BBCP.
\newblock \BBOQ CU-COMSEM: Exploring Rich Features for Unsupervised Web
  Personal Name Disambiguation.\BBCQ\
\newblock In {\Bem Proc. of the Semeval 2007, Association for Computational
  Linguistics (ACL)}, \mbox{\BPGS\ 125--128}.

\bibitem[\protect\BCAY{Church \BBA\ Gale}{Church \BBA\
  Gale}{1995a}]{Church95VLC}
Church, K.~W.\BBACOMMA\ \BBA\ Gale, W.~A. \BBOP 1995a\BBCP.
\newblock \BBOQ Inverse Document Frequency (IDF): A Measure of Deviation from
  Poisson.\BBCQ\
\newblock In {\Bem Proc. of the 3rd Workshop on Very Large Corpora},
  \mbox{\BPGS\ 121--130}.

\bibitem[\protect\BCAY{Church \BBA\ Gale}{Church \BBA\
  Gale}{1995b}]{Church95JNLE}
Church, K.~W.\BBACOMMA\ \BBA\ Gale, W.~A. \BBOP 1995b\BBCP.
\newblock \BBOQ Poisson Mixtures.\BBCQ\
\newblock {\Bem Journal of Natural Language Engineering}, {\Bbf 1}  (2),
  \mbox{\BPGS\ 163--190}.

\bibitem[\protect\BCAY{Diday \BBA\ Govaert}{Diday \BBA\
  Govaert}{1977}]{Diday77}
Diday, E.\BBACOMMA\ \BBA\ Govaert, G. \BBOP 1977\BBCP.
\newblock \BBOQ Classification Automatique Avec Distances Adaptatives.\BBCQ\
\newblock {\Bem R.A.I.R.O. Informatique Computer Science}, {\Bbf 11}  (4),
  \mbox{\BPGS\ 329--349}.

\bibitem[\protect\BCAY{Elmacioglu, Tan, Yan, Kan, \BBA\ Lee}{Elmacioglu
  et~al.}{2007}]{Elmacioglu07}
Elmacioglu, E., Tan, Y.~F., Yan, S., Kan, M.-Y., \BBA\ Lee, D. \BBOP 2007\BBCP.
\newblock \BBOQ PSNUS: Web People Name Disambiguation by Simple Clustering with
  Rich Features.\BBCQ\
\newblock In {\Bem Proc. of the Semeval 2007, Association for Computational
  Linguistics (ACL)}, \mbox{\BPGS\ 268--271}.

\bibitem[\protect\BCAY{Hotho, N{\"{u}}rnberger, \BBA\ Paa\ss}{Hotho
  et~al.}{2005}]{Hotho05}
Hotho, A., N{\"{u}}rnberger, A., \BBA\ Paa\ss, G. \BBOP 2005\BBCP.
\newblock \BBOQ A Brief Survey of Text Mining.\BBCQ\
\newblock {\Bem GLDV-Journal for Computational Linguistics and Language
  Technology}, {\Bbf 20}  (1), \mbox{\BPGS\ 19--62}.

\bibitem[\protect\BCAY{Jones}{Jones}{1973}]{Jones73}
Jones, K.~S. \BBOP 1973\BBCP.
\newblock \BBOQ Index Term Weighting.\BBCQ\
\newblock {\Bem Information Strage and Retrieval}, {\Bbf 9}  (11), \mbox{\BPGS\
  619--633}.

\bibitem[\protect\BCAY{Klein, Kamvar, \BBA\ Manning}{Klein
  et~al.}{2002}]{Klein02}
Klein, D., Kamvar, S.~D., \BBA\ Manning, C.~D. \BBOP 2002\BBCP.
\newblock \BBOQ From Instance-level Constraints to Space-level Constraints:
  Making the Most of Prior Knowledge in Data Clustering.\BBCQ\
\newblock In {\Bem Proc. of the 19th International Conference on Machine
  Learning (ICML 2002)}, \mbox{\BPGS\ 307--314}.

\bibitem[\protect\BCAY{MacQueen}{MacQueen}{1967}]{MacQueen67}
MacQueen, J. \BBOP 1967\BBCP.
\newblock \BBOQ Some Methods for Classification and Analysis of Multivariate
  Observations.\BBCQ\
\newblock In {\Bem Proc. of the 5th Berkeley Symposium on Mathmatical
  Statistics and Probability}, \mbox{\BPGS\ 281--297}.

\bibitem[\protect\BCAY{Mann \BBA\ Yarowsky}{Mann \BBA\ Yarowsky}{2003}]{Mann03}
Mann, G.~S.\BBACOMMA\ \BBA\ Yarowsky, D. \BBOP 2003\BBCP.
\newblock \BBOQ Unsupervised Personal Name Disambiguation.\BBCQ\
\newblock In {\Bem Proc. of the 7th Conference on Natural Language Learning
  (CoNLL-2003)}, \mbox{\BPGS\ 33--40}.

\bibitem[\protect\BCAY{Papineni}{Papineni}{2001}]{Papineni01}
Papineni, K. \BBOP 2001\BBCP.
\newblock \BBOQ Why Inverse Document Frequency?\BBCQ\
\newblock In {\Bem Proc. of the 2nd Meeting of the North American Chapter of
  the Association for Computational Linguistics (NAACL 2001)}, \mbox{\BPGS\
  25--32}.

\bibitem[\protect\BCAY{Pedersen, Purandare, \BBA\ Kulkarni}{Pedersen
  et~al.}{2005}]{Pedersen05}
Pedersen, T., Purandare, A., \BBA\ Kulkarni, A. \BBOP 2005\BBCP.
\newblock \BBOQ Name Discrimination by Clustering Similar Contexts.\BBCQ\
\newblock In {\Bem Proc. of the 6th International Conference on Computational
  Linguistics and Intelligent Text Processing (CICLing~2005)}, \mbox{\BPGS\
  226--237}.

\bibitem[\protect\BCAY{Popescu \BBA\ Magnini}{Popescu \BBA\
  Magnini}{2007}]{Popescu07}
Popescu, O.\BBACOMMA\ \BBA\ Magnini, B. \BBOP 2007\BBCP.
\newblock \BBOQ IRST-BP: Web People Search Using Name Entities.\BBCQ\
\newblock In {\Bem Proc. of the Semeval 2007, Association for Computational
  Linguistics (ACL)}, \mbox{\BPGS\ 195--198}.

\bibitem[\protect\BCAY{Porter}{Porter}{1980}]{Porter80}
Porter, M.~F. \BBOP 1980\BBCP.
\newblock \BBOQ An Algorithm for Suffix Stripping.\BBCQ\
\newblock {\Bem Program}, {\Bbf 14}  (3), \mbox{\BPGS\ 130--137}.

\bibitem[\protect\BCAY{Remy}{Remy}{2002}]{Remy02}
Remy, M. \BBOP 2002\BBCP.
\newblock \BBOQ Wikipedia: The Free Encyclopedia.\BBCQ\
\newblock {\Bem Online Information Review}, {\Bbf 26}  (6), \mbox{\BPG\ 434}.

\bibitem[\protect\BCAY{Rocchio}{Rocchio}{1971}]{Rocchio71}
Rocchio, J. \BBOP 1971\BBCP.
\newblock \BBOQ Relevance Feedback in Information Retrieval.\BBCQ\
\newblock In Salton, G.\BED, {\Bem The Smart Retrieval System: Experiments in
  Automatic Document Processing}, \mbox{\BPGS\ 313--323}. Prentice-Hall,
  Englewood Cliffs, NJ.

\bibitem[\protect\BCAY{Salton \BBA\ McGill}{Salton \BBA\
  McGill}{1983}]{Salton83}
Salton, G.\BBACOMMA\ \BBA\ McGill, M.~J. \BBOP 1983\BBCP.
\newblock {\Bem Introduction to Modern Information Retrieval}.
\newblock McGraw-Hill.

\bibitem[\protect\BCAY{Shental, Hertz, Weinshall, \BBA\ Pavel}{Shental
  et~al.}{2002}]{Shental02}
Shental, N., Hertz, T., Weinshall, D., \BBA\ Pavel, M. \BBOP 2002\BBCP.
\newblock \BBOQ Adjustment Learning and Relevant Component Analysis.\BBCQ\
\newblock In {\Bem Proc. of the 7th European Conference on Computer Vision
  (ECCV 2002)}, \mbox{\BPGS\ 776--792}.

\bibitem[\protect\BCAY{Wagstaff \BBA\ Cardie}{Wagstaff \BBA\
  Cardie}{2000}]{Wagstaff00}
Wagstaff, K.\BBACOMMA\ \BBA\ Cardie, C. \BBOP 2000\BBCP.
\newblock \BBOQ Clustering with Instance-level Constraints.\BBCQ\
\newblock In {\Bem Proc. of the 17th International Conference on Machine
  Learning (ICML 2000)}, \mbox{\BPGS\ 1103--1110}.

\bibitem[\protect\BCAY{Wagstaff, Rogers, \BBA\ Schroedl}{Wagstaff
  et~al.}{2001}]{Wagstaff01}
Wagstaff, K., Rogers, S., \BBA\ Schroedl, S. \BBOP 2001\BBCP.
\newblock \BBOQ Constrained K-means Clustering with Background Knowledge.\BBCQ\
\newblock In {\Bem Proc. of the 18th International Conference on Machine
  Learning (ICML 2001)}, \mbox{\BPGS\ 577--584}.

\bibitem[\protect\BCAY{Xing, Ng, Jordan, \BBA\ Russell}{Xing
  et~al.}{2003}]{Xing03}
Xing, E.~P., Ng, A.~Y., Jordan, M.~I., \BBA\ Russell, S.~J. \BBOP 2003\BBCP.
\newblock \BBOQ Distance Metric Learning with Application to Clustering with
  Side-Information.\BBCQ\
\newblock {\Bem Advances in Neural Information Processing Systems}, {\Bbf 15},
  \mbox{\BPGS\ 521--528}.

\end{thebibliography}

\begin{biography}
\bioauthor{杉山　一成}{
1998年横浜国立大学工学部電子情報工学科卒業．
2000年同大学院工学研究科電子情報工学
専攻博士前期課程修了．KDDI（株）勤務を経て，
2004年奈良先端科学技術大学院大学情報科学研究科博士
後期課程修了，博士（工学）．（株）日立製作所勤務を経て，
2006年東京工業大学精密工学研究所研究員，
2009年シンガポール国立大学計算機科学科研究員，
現在に至る．情報検索，自然言語処理に関する研究に従事．
電子情報通信学会，情報処理学会，人工知能学会，IEEE，ACM，AAAI 各会員．
}
\bioauthor{奥村　　学}{
1984年東京工業大学工学部情報工学科卒業．1989年同大学院博士
課程修了．工学博士．同年，東京工業大学工学部情報工学科助手．
1992年北陸先端科学技術大学院大学情報科学研究科助教授，
2000年東京工業大学精密工学研究所助教授，2009年同教授，
現在に至る．自然言語処理，知的情報提示技術，語学学習支援，テキスト評価分析，
テキストマイニングに関する研究に従事．
電子情報通信学会，情報処理学会，人工知能学会，言語処理学会，認知科学会，計量国語学会，
 AAAI，ACL 各会員．
}

\end{biography}


\biodate


\end{document}
