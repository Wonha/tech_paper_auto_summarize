提案手法
\label{sec:ProposedMethod}

\ref{sec:Intro}章で述べた凝集型クラスタリングに
基づいた人名の曖昧性解消は，クラスタリングを適切に導いていく基準が
ないため，正確なクラスタリングを行うことは難しい．一方，
これまでに提案されている半教師有りクラスタリングは，
クラスタ数$K$をあらかじめ設定する必要がある
$K$-means アルゴリズム \cite{MacQueen67} を改良することを目的としている．
しかし，本研究においては，Web検索結果における同姓同名人物の
数は，事前にわかっているわけではない．したがって，我々の手法においては，
事前にクラスタ数を設定するのではなく，
新たに生成されたクラスタと，すでに生成されているクラスタ間の
類似度を計算し，これらの値がすべて，あらかじめ設定した閾値よりも小さくなった場合に，
クラスタリングの処理を終え，その時点で生成されているクラスタ数を最終的な
同姓同名人物の数とする．

また，従来の半教師有りクラスタリングアルゴリズムは，
制約を導入したり \cite{Wagstaff00}，\cite{Wagstaff01}，\cite{Basu02}，
距離を学習したり \cite{Klein02}，\cite{Xing03}，\cite{Bar-Hillel03} 
することにのみ着目していた．
しかし，
半教師有りクラスタリングにおいて，より正確なクラスタリング結果を
得るためには，seedページ間への制約の導入とともに，seedページを
含むクラスタの重心の変動の抑制も重要である．
これは，
(1) seedページを導入して半教師有りクラスタリングを行なう場合，
通常の重心の計算法では重心の変動が大きくなる傾向にあり，
クラスタリングの基準となるseedページを導入する効果が得られない，
(2) 重心を完全に固定して半教師有りクラスタリングを行なう場合，
その重心と類似度が高いWebページしかマージされなくなり，
多数の独立したクラスタが生成されやすくなる，
という二つの考えに基づく．
したがって，seedページを含むクラスタの重心の変動を抑えることが
できれば，より適切なクラスタリングが実現できると期待される．

本章では，我々の提案する半教師有りクラスタリングの手法について説明する．

以下，検索結果集合$W_{p}$中のWebページ$p_{i}$の
特徴ベクトル$\boldsymbol{w}^{p_{i}}$ $(i=1,\cdots ,n)$を
式(\ref{Eq:feature vector_org})のように表す．
\begin{equation}
 \boldsymbol{w}^{p_{i}}=(w_{t_{1}}^{p_{i}},w_{t_{2}}^{p_{i}},\cdots, w_{t_{m}}^{p_{i}})
\label{Eq:feature vector_org}
\end{equation}
ここで，$m$は検索結果集合$W_{p}$における単語の
異なり数であり，$t_{k}$ $(k=1,2,\cdots, m)$は，各単語を表す．
予備実験として，(a) Term Frequency (TF)，(b) Inverse Document
Frequency (IDF)，(c) residual IDF (RIDF)，(d) TF-IDF，
(e) $x^{I}$-measure，(f) gain の6つの単語重み付け法を
比較した．これらの単語重み付け法は，それぞれ，次のように定義される．

\clearpage

\noindent
\textbf{(a) Term Frequency (TF)} 

TFは，与えられた文書において，ある単語がどれだけ顕著に
出現するかを示し，この値が大きければ大きいほど，その単語が文書の内容を
よく表現していることを示す．
$tf(t_{k}, p_{i})$をWebページ$p_{i}$における単語$t_{k}$の頻度とする．このとき，
$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，式(\ref{eq: tf})に
よって定義される．
\begin{eqnarray}
 w_{t_{k}}^{p_{i}}=\frac{tf(t_{k}, p_{i})}{\sum_{s=1}^{m}tf(t_{s}, p_{i})} \label{eq: tf}
\end{eqnarray} 

\noindent
\textbf{(b) Inverse Document Frequency (IDF)} 

\cite{Jones73} によって導入されたIDFは，その単語が出現する
文書数が少なければ少ないほど，その単語が出現する文書にとっては，
有用であることを示すスコアである．
このとき，$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，式(\ref{eq: idf})によって定義される．
\begin{eqnarray}
 w_{t_{k}}^{p_{i}}=\log\frac{N}{df(t_{k})} \label{eq: idf}
\end{eqnarray}
ここで，$N$はWebページの総数，$df(t_{k})$は単語$t_{k}$が現れる
Webページ数である．

\noindent
\textbf{(c) Residual Inverse Document Frequency (RIDF)}

Church and Gale \cite{Church95VLC,Church95JNLE} は，
ほとんどすべての単語は，ポアッソンモデルのような独立性に基づいた
モデルに応じて，非常に大きなIDFスコアを持つことを示した．また，
単語の有用性は，推定されるスコアからは
大きな偏差を持つ傾向があるという考えに基づいて導入したスコアがresidual IDFである．
このスコアは，実際のIDFとポアッソン分布によって推定されるIDFとの差として定義される．
$cf_{k}$を文書集合中における単語$t_{k}$の総出現数，$N$をWebページの総数としたとき，
1つのWebページあたりの単語$t_{k}$の平均出現数は，
$\lambda_{k}=\frac{cf_{k}}{N}$と
表される．このとき，
$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，
式(\ref{eq: ridf})によって定義される．
\begin{align}
 w_{t_{k}}^{p_{i}} &= IDF - \log\frac{1}{1-p(0;\lambda_{i})} \nonumber \\
               &= \log\frac{N}{df(t_{k})}+\log(1-p(0;\lambda_{k})) \label{eq: ridf}
\end{align}
ここで，$p$は，パラメータ$\lambda_{k}$を伴うポアッソン分布である．
この手法は，少数の文書のみに出現する単語は，より大きなRIDFスコアを持つ傾向がある．

\noindent
\textbf{(d) TF-IDF}

TF-IDF 法 \cite{Salton83} は，文書中の単語を重み付けするために，情報
検索の研究において広く使われている．TF-IDFは，上述した
(a) TF と (b) IDF に基づいて，式(\ref{eq: tfidf})のように定義される．
\begin{eqnarray}
 w_{t_{k}}^{p_{i}} = \frac{tf(t_{k}, p_{i})}{\sum_{s=1}^{m}tf(t_{s}, p_{i})}\cdot \log\frac{N}{df(t_{k})} \label{eq: tfidf}
\end{eqnarray}
ここで， $tf(t_{k}, p_{i})$と$df(t_{k})$は，それぞれ，
Webページ$p_{i}$における単語$t_{k}$の頻度と，単語$t_{k}$が
出現するWebページ数を表す．また，$N$はWebページの総数である．

\noindent
\textbf{(e) $x^{I}$-measure}

Bookstein and Swanson \cite{Bookstein74} は，単語$t_{k}$に対する
 $x^{I}$-measure というスコアを導入した．
$tf(t_{k}, p_{i})$をWebページ$p_{i}$における単語$t_{k}$の頻度，
$df(t_{k})$を単語$t_{k}$が現れるWebページ数とすると，
$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，
式(\ref{eq: xI})によって定義される．
\begin{eqnarray}
 w_{t_{k}}^{p_{i}}=tf(t_{k}, p_{i})-df(t_{k}) \label{eq: xI}
\end{eqnarray}
この手法は，同程度の出現頻度である2つの単語のうち，
少数の文書に集中して出現する単語ほど，高いスコアを示す．

\noindent
\textbf{(f) gain}

一般に，IDFは単語の重要性を表すと考えられているが，
Papineni \cite{Papineni01} は，IDFは単語の特徴を表す最適な重みに過ぎず，
単語の重要性とは異なるものであるため，
利得を単語の重要性と考え，gainを提案した．本手法では，
$\boldsymbol{w}^{p_{i}}$の各要素$w_{t_{k}}^{p_{i}}$は，
式(\ref{eq: gain})によって定義される．
\vspace{-0.5\baselineskip}
\begin{eqnarray}
 w_{t_{k}}^{p_{i}}=\frac{df(t_{k})}{N}\left(\frac{df(t_{k})}{N}-1-\log\frac{df(t_{k})}{N}\right) \label{eq: gain}
\end{eqnarray}
ここで，$df(t_{k})$は，単語$t_{k}$が現れるWebページ数を，
$N$はWebページの総数を示す．本手法では，ほとんど出現しない単語と，
非常に頻出する単語は，両方とも低いスコアとなり，中頻度の単語は
高いスコアとなる．

上述した(a)〜(f)の単語重み付け手法の中で，本研究においては，``(f) gain''が
最も効果的な単語の重み付け法であることがわかったため，これを本研究に
おける単語の重み付け法として用いる．
さらに，クラスタ$C$の重心ベクトル
$\boldsymbol{G}^{C}$を式(\ref{Eq:centroid vector})のように定義する．
\begin{eqnarray}
 \boldsymbol{G}^{C}=(g^{C}_{t_{1}},g^{C}_{t_{2}},\cdots, g^{C}_{t_{m}})
\label{Eq:centroid vector}
\end{eqnarray}
ここで，$g^{C}_{t_{k}}$は$\boldsymbol{G}^{C}$に
おける各単語の重みであり，$t_{k}$ $(k=1,2,\cdots, m)$は各単語を表す．
なお，以下で述べるクラスタリング手法では，2つのクラスタ$C_{i}$，
$C_{j}$間の類似度$sim(C_{i},C_{j})$を，式(\ref{eq:sim})によって
計算する．
\begin{eqnarray}
 sim(C_{i},C_{j})=
\frac{\boldsymbol{G}^{C_{i}}\cdot\boldsymbol{G}^{C_{j}}}{|\boldsymbol{G}^{C_{i}}|\cdot |\boldsymbol{G}^{C_{j}}|}
\label{eq:sim}
\end{eqnarray}
ただし，$\boldsymbol{G}^{C_{i}}$，$\boldsymbol{G}^{C_{j}}$は，それぞれ，
クラスタ$C_{i}$，$C_{j}$の
重心ベクトルを表す．


\subsection{凝集型クラスタリング}
\label{subsec:AggCls}

凝集型クラスタリングにおいては，はじめに各Webページを，
\pagebreak
個々のクラスタとして設定する．次に，二つのクラスタ間の類似度が，
あらかじめ設定された閾値より小さくなるまで，
類似度が最大となる二つのクラスタをマージして
新たなクラスタを生成する．図\ref{Fig:AggClsAlgorithm}に凝集型
クラスタリングのアルゴリズムを示す．

\begin{figure}[b]
\begin{center}
\includegraphics{16-4ia3f1.eps}
\end{center}
\caption{凝集型クラスタリングアルゴリズム}\label{Fig:AggClsAlgorithm}
\end{figure}

このアルゴリズムでは，あるクラスタ$C_{i}$ (要素数$n_{i}$)
を最も類似したクラスタ$C_{j}$ (要素数$n_{j}$) にマージした後の，
新たなクラスタ$C^{new}$の重心ベクトル
$\boldsymbol{G}^{new}$は，
式(\ref{eq:new agg-centroid})のように定義される．
\begin{eqnarray}
 \boldsymbol{G}^{new}=\frac{\sum_{\boldsymbol{w}^{p}\in C_{i}}\boldsymbol{w}^{p}+\sum_{\boldsymbol{w}^{p}\in C_{j}}\boldsymbol{w}^{p}}{n_{i}+n_{j}} \label{eq:new agg-centroid}
\end{eqnarray}


\subsection{提案する半教師有りクラスタリング} \label{subsec:SSCls}

一般に，seedページを含むクラスタ$C_{s_{j}}$
と，seedページを含まないクラスタ$C_{i}$の類似度が
大きい場合には，両者を新たなクラスタとしてマージすべきであるが，
両者の距離が大きい場合には，通常の重心の計算法では，
重心の変動が大きくなる傾向にある．そこで，
はじめに，あるクラスタ$C_{i}$(重心ベクトル$\boldsymbol{G}^{C_{i}}$)を，
seedページを含むクラスタ$C_{s_{j}}$(重心ベクトル$\boldsymbol{G}^{C_{s_{j}}}$)に
マージする際，
これらのクラスタの重心間の距離
$D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}})$に
基づいて，Webページ$p$の特徴ベクトル$\boldsymbol{w}^{p}\in C_{i}$を
重み付けする．次に，この重み付けした特徴ベクトルを用いて重心の計算を
行なうことで上述した傾向を防ぎ，重心の変動を抑える．

まず，これまでに$k_{j}$個のクラスタがマージされたseedページを含むクラスタ
$C_{s_{j}}^{(k_{j})}$ (要素数$n_{s_{j}}$) に対して，
クラスタ$C_{i}$ (要素数$n_{i}$) が$k_{j}+1$回目に
マージされるクラスタであるとする．
なお，クラスタ$C_{s_{j}}^{(0)}$の要素は，初期のseedページとなる．

\noindent
\textbf{(1)} この$C_{s_{j}}^{(k_{j})}$にマージされるクラスタ$C_{i}$に
含まれる各要素について，
$C_{s_{j}}^{(k_{j})}$の重心$\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}}$と，
クラスタ$C_{i}$の重心$\boldsymbol{G}^{C_{i}}$間の距離尺度
$D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})$を用いて，
クラスタ$C_{i}$に含まれるWebページの特徴ベクトル
$\boldsymbol{w}^{p_{l}}_{C_{i}}$ $(l=1,\cdots ,n_{i})$を
重み付けし，その後に生成されるクラスタを$C_{i'}$ (要素数$n_{i'}$) 
とする．このとき，$C_{i'}$の要素となる重み付けした後のWebページの
特徴ベクトル$\boldsymbol{w}^{p_{l}}_{C_{i'}}$は，式(\ref{eq:TransferedCor})で表される．
\begin{eqnarray}
\boldsymbol{w}^{p_{l}}_{C_{i'}}
=\frac{\boldsymbol{w}^{p_{l}}_{C_{i}}}{D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})+c} \label{eq:TransferedCor}
\end{eqnarray}
本研究では，$D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})$
として，(i)ユークリッド距離，(ii)マハラノビス距離，
(iii)適応的マハラノビス距離，の三つの距離尺度を比較する．
また，$c$は$D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})$が
0に非常に近い値となったとき，
$\boldsymbol{w}^{p}$の各要素が極端に大きな値となることを防ぐ
ために導入した定数である．
この$c$の値の影響については，3.3.1節で
述べる．

\noindent
\textbf{(2)} 次に，seedページを含むクラスタ$C_{s_{j}}^{(k_{j})}$
(要素数$n_{s_{j}}$)に$C_{i'}$(要素数$n_{i'}$)の
要素を追加し，クラスタ$C_{s_{j}}^{(k_{j}+1)}$
(要素数$n_{s_{j}}+n_{i'}$)を作成する．
\[
C_{s_{j}}^{(k_{j}+1)}=
\{
\boldsymbol{w}^{p_{1}}_{C_{s_{j}}^{(k_{j})}},\cdots
,\boldsymbol{w}^{p_{{n}_{s_{j}}}}_{C_{s_{j}}^{(k_{j})}},
\boldsymbol{w}^{p_{1}}_{C_{i'}},\cdots ,\boldsymbol{w}^{p_{n_{{i'}}}}_{C_{i'}} \}
\]


\noindent
\textbf{(3)} このとき，$k_{j}+1$回目のクラスタをマージしたクラスタ$C_{s_{j}}^{(k_{j}+1)}$の
重心$\boldsymbol{G}^{C_{s_{j}}^{(k_{j}+1)}}$は，式(\ref{eq:NewG})のように
計算される．ここで，式(\ref{eq:TransferedCor})において，マージされる
クラスタの特徴ベクトル$\boldsymbol{w}^{p_{l}}_{C_{i}}$に重み付けを
しているため，重み付き平均の計算と
なるように，$n_{i'}$にも同様の重みを乗じている．
\begin{eqnarray}
\boldsymbol{G}^{C_{s_{j}}^{(k_{j}+1)}}=\frac{\sum_{\boldsymbol{w}^{p}\in C_{s_{j}}^{(k_{j}+1)}}\boldsymbol{w}^{p}}{n_{{s_{j}}}+n_{i'}\times\frac{1}{D(\boldsymbol{G}^{C_{i}},\boldsymbol{G}^{C_{s_{j}}^{(k_{j})}})+c}}
\label{eq:NewG}
\end{eqnarray}

このように本研究では，seedページを含むクラスタを重視してクラスタリングの
基準を明確にし，正確なクラスタリングを行うことを目的とする．
もし，2つのクラスタが種用例を含まないのであれば，
新たなクラスタの重心ベクトル$\boldsymbol{G}^{new}$は，
式(\ref{eq:new centroid(agg)})のように計算される．
\begin{eqnarray}
 \boldsymbol{G}^{new}=\frac{\sum_{\boldsymbol{w}^{p}\in C_{i}}\boldsymbol{w}^{p}+\sum_{\boldsymbol{w}^{p}\in C_{j}}\boldsymbol{w}^{p}}{n_{i}+n_{j}}
 \label{eq:new centroid(agg)}
\end{eqnarray}

本研究では，seedページを含むクラスタに，それと最も類似したクラスタを
マージする際，seedページを含むクラスタの
重心の変動を抑える半教師有りクラスタリングを適用して，
Web検索結果における人名の曖昧性を解消する．
従来の半教師有りクラスタリングの手法のうち，
制約を導入する手法では，クラスタの基準となる重心についての
検討は見逃されており，また，距離を学習する手法では，
特徴空間が大域的に変換される．
一方，我々の手法は，seedページを含むクラスタの
重心の変動を抑え，その重心を局所的に調整できる効果が期待される．
なお，seedページを導入することで，検索結果を改善することは，
適合性フィードバック\cite{Rocchio71}に類似した手法であると考えられる．
しかし，適合性フィードバックでは，検索結果中の文書に
対して，ユーザが判断した適合文書・非適合文書に
基づいた検索語の修正を目的としているのに対し，本手法は，
あらかじめ設定したseedページに基づいて，検索結果の改善，特に本研究に
おいては，検索結果のクラスタリング精度の改善を目的としている点が異なる．

また，検索結果をクラスタリングする
検索エンジンとして，``Clusty''\footnote{http://clusty.com}
が挙げられる．しかし，そのクラスタリングされた検索結果には，
適合しないWebページが含まれることも
多く，クラスタリングを行う上で，何らかの基準が必要である．
すなわち，本研究のように，
seedページをクラスタリングの基準として導入し，かつ，そのseedページを
含むクラスタの重心を抑えることで，その基準を保つような手法が必要であると
考えられる．

図\ref{Fig:SSClsAlgorithm}に，我々の提案する半教師有りクラスタリング
アルゴリズムの詳細を示す．なお，提案する半教師有りクラスタリングでは，
対象とするすべてのWebページが，いずれかのseedページを含むクラスタに
マージされるのではなく，seedページを含まないクラスタにもマージされる
ことに，注意されたい（図\ref{Fig:SSClsAlgorithm}下から7行目，
``else if''以降参照）．

\begin{figure}[p]
\begin{center}
\includegraphics{16-4ia3f2.eps}
\end{center}
\caption{提案する半教師有りクラスタリングアルゴリズム} \label{Fig:SSClsAlgorithm}
\end{figure}

ここで，本研究において比較する式(\ref{eq:TransferedCor})直後に
述べた(i)，(ii)，(iii)の3つの距離尺度は，それぞれ，以下のように
定義される．


\noindent
\textbf{(i) ユークリッド距離}

式(\ref{eq:TransferedCor})において，ユークリッド距離を導入した場合，
seedページを含むクラスタの重心ベクトル$\boldsymbol{G}^{C_{s}}$と，
あるクラスタ$C$の重心ベクトル$\boldsymbol{G}^{C}$間の距離
$D(\boldsymbol{G}^{C_{s}},\boldsymbol{G}^{C})$は，
式(\ref{Eq:centroid vector})に基づいて，
式(\ref{Eq:Euclidean disrance})のように定義される．
\begin{eqnarray}
 D(\boldsymbol{G}^{C_{s}},\boldsymbol{G}^{C})=\sqrt{\sum_{k=1}^{m}(g^{C_{s}}_{t_{k}}-g^{C}_{t_{k}})^{2}}
\label{Eq:Euclidean disrance}
\end{eqnarray}

\noindent
\textbf{(ii) マハラノビス距離}

マハラノビス距離は，データ集合の相関を考慮した尺度である
という点において，ユークリッド距離とは異なる．
したがって，ユークリッド距離を用いるよりも
マハラノビス距離を用いた方が，クラスタの重心の変動を，
より効果的に抑えられることが期待される．

式(\ref{eq:TransferedCor})において，マハラノビス距離を導入した場合，
seedページを含むクラスタ$C_{s}$の重心ベクトル$\boldsymbol{G}^{C_{s}}$と，
あるクラスタ$C$の重心ベクトル$\boldsymbol{G}^{C}$間の距離
$D(\boldsymbol{G}^{C_{s}},\boldsymbol{G}^{C})$は，
式(\ref{Eq:Mahalanobis distance})のように定義される．
\begin{eqnarray}
D(\boldsymbol{G_{C_{(s)}}},\boldsymbol{G_{C}})
=\sqrt{(\boldsymbol{G}^{C_{s}}-\boldsymbol{G}^{C})^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{G}^{C_{s}}-\boldsymbol{G}^{C})} \label{Eq:Mahalanobis distance}
\end{eqnarray}
ここで，$\boldsymbol{\Sigma}$は，seedページを含むクラスタ$C_{s}$
の要素によって定義される共分散行列である．

すなわち，クラスタ$C_{s}$内の要素を，
\[
 C_{s}=\{\boldsymbol{w}^{p_{1}}_{C_{s}},\boldsymbol{w}^{p_{2}}_{C_{s}},\cdots ,\boldsymbol{w}^{p_{m}}_{C_{s}}\}
\]
と表せば，重心ベクトル$\boldsymbol{G}^{C_{s}}$，
\[
 \boldsymbol{G}^{C_{s}}=\frac{1}{m}\sum_{i=1}^{m}\boldsymbol{w}^{p_{i}}_{C_{s}}
\]
を用いて，共分散$\Sigma_{ij}$を式(\ref{eq:CovMHD})のように
定義することができる．
\begin{eqnarray}
 \Sigma_{ij}=\frac{1}{m}\sum_{i=1}^{m}(\boldsymbol{w}^{p_{i}}_{C_{s}}-\boldsymbol{G}^{C_{s}})
(\boldsymbol{w}^{p_{j}}_{C_{s}}-\boldsymbol{G}^{C_{s}})^{T} \label{eq:CovMHD}
\end{eqnarray}
以上から，共分散行列$\boldsymbol{\Sigma}$は，
\[
 \boldsymbol{\Sigma}=\left[
 \begin{array}{@{\,}cccc@{\,}}
  \Sigma_{11} & \Sigma_{12} & \cdots & \Sigma_{1m} \\
  \Sigma_{21} & \Sigma_{22} & \cdots & \Sigma_{2m} \\
  \vdots      & \vdots      & \ddots & \vdots \\
  \Sigma_{m1} & \Sigma_{m2} & \cdots & \Sigma_{mm}
 \end{array}
\right]
\]
と表すことができる．

\noindent
\textbf{(iii)適応的マハラノビス距離}

(ii)のマハラノビス距離は，クラスタ内の要素数が
少ないときに，共分散が大きくなる傾向がある．
そこで，seedページを含むあるクラスタ$C_{s_{j}}$について，
このクラスタに含まれるWebページの特徴ベクトル間の
非類似度を局所最小化することを考える．
この局所最小化で得られる分散共分散行列を
用いて計算した
$C_{s_{j}}$の重心ベクトル$\boldsymbol{G}^{C_{s_{j}}}$と，
このクラスタにマージされるクラスタ$C_{l}$の
重心ベクトル$\boldsymbol{G}^{C_{l}}$間の距離が，
適応的マハラノビス距離 \cite{Diday77} である．
この分散共分散行列は，次のように導出される．

\noindent
\textbf{(1)}まず，クラスタ$C_{s_{j}}$において，このクラスタ
に含まれるWebページの特徴ベクトル
$\boldsymbol{w}^{p_{i}}$と，それ以外の
特徴ベクトル$\boldsymbol{v}$ $(\boldsymbol{w}^{p_{i}}\neq \boldsymbol{v})$との
非類似度$d_{s_{j}}(\boldsymbol{w}^{p_{i}},\boldsymbol{v})$を，
式(\ref{eq:intra-cls})により定義する．
\begin{eqnarray}
d_{s_{j}}(\boldsymbol{w}^{p_{i}},\boldsymbol{v})=
(\boldsymbol{w}^{p_{i}}-\boldsymbol{v})^{T}\boldsymbol{M}_{s_{j}}^{-1}(\boldsymbol{w}^{p_{i}}-\boldsymbol{v}) \label{eq:intra-cls}
\end{eqnarray}
ただし，$\boldsymbol{M}_{s_{j}}$は$C_{s_{j}}$の分散共分散行列を表す．
すなわち，クラスタ$C_{s_{j}}$内の要素を，
\[
 C_{s_{j}}=\{\boldsymbol{w}^{p_{1}}_{C_{s_{j}}},\boldsymbol{w}^{p_{2}}_{C_{s_{j}}},\cdots ,\boldsymbol{w}^{p_{m}}_{C_{s_{j}}}\}
\]
\pagebreak
と表せば，重心ベクトル$\boldsymbol{G}^{C_{s_{j}}}$，
\[
 \boldsymbol{G}^{C_{s_{j}}}=\frac{1}{m}\sum_{i=1}^{m}\boldsymbol{w}^{p_{i}}_{C_{s_{j}}}
\]
を用いて，共分散$M_{ij}$を式(\ref{eq:CovAMHD})のように定義することができる．
\begin{eqnarray}
 M_{ij}=\frac{1}{m}\sum_{i=1}^{m}(\boldsymbol{w}^{p_{i}}_{C_{s_{j}}}-\boldsymbol{G}^{C_{s_{j}}})
(\boldsymbol{w}^{p_{j}}_{C_{s_{j}}}-\boldsymbol{G}^{C_{s_{j}}})^{T} \label{eq:CovAMHD}
\end{eqnarray}
以上から，共分散行列$\boldsymbol{M_{s_{j}}}$は，
\[
 \boldsymbol{M_{s_{j}}}=\left[
 \begin{array}{@{\,}cccc@{\,}}
  M_{11} & M_{12} & \cdots & M_{1m} \\
  M_{21} & M_{22} & \cdots & M_{2m} \\
  \vdots      & \vdots      & \ddots & \vdots \\
  M_{m1} & M_{m2} & \cdots & M_{mm}
 \end{array}
\right]
\]
と表すことができる．

\noindent
\textbf{(2)} 次に，目的関数
\begin{align*}
 \Delta_{s_{j}}(\boldsymbol{v},\boldsymbol{M}_{s_{j}})
	&= \sum_{\boldsymbol{w}^{p_{i}}\in C_{s_{j}}}d_{s_{j}}
	(\boldsymbol{w}^{p_{i}},\boldsymbol{v})\\
  & = \sum_{\boldsymbol{w}^{p_{i}}\in C_{s_{j}}}
	(\boldsymbol{w}^{p_{i}}-\boldsymbol{v})^{T}\boldsymbol{M}_{s_{j}}^{-1}
	(\boldsymbol{w}^{p_{i}}-\boldsymbol{v})
\end{align*}
を定義し，これを局所最小化するような$C_{{s}_{j}}$の代表点の特徴ベクトル
$\boldsymbol{L}_{s_{j}}$と分散共分散行列$\boldsymbol{S}_{{s}_{j}}$を求める．

\noindent
(i) まず，クラスタ$C_{s_{j}}$の要素により定義される共分散
行列$\boldsymbol{M}_{s_{j}}$を固定し，
$\Delta_{s_{j}}$を最小化する$\boldsymbol{L}_{s_{j}}$を求める．
\begin{eqnarray}
\boldsymbol{L}_{s_{j}}=
\arg\min_{\boldsymbol{v}}\sum_{\boldsymbol{w}^{p_{i}}\in C_{s_{j}}}
(\boldsymbol{w}^{p_{i}}-\boldsymbol{v})^{T}\boldsymbol{M}_{s_{j}}^{-1}(\boldsymbol{w}^{p_{i}}-\boldsymbol{v}) \label{eq:Lj}
\end{eqnarray}
式(\ref{eq:Lj})において，クラスタ$C_{s_{j}}$の重心$G$に最も近い点
$G'$の特徴ベクトルを$\boldsymbol{v}_{G'}$と表せば，
$\boldsymbol{L}_{s_{j}}=\boldsymbol{v}_{G'}$と求めることができる．

\noindent
(ii) 次に，
(i)で求めた代表点の特徴ベクトル
$\boldsymbol{L}_{s_{j}}=\boldsymbol{v}_{G'}$
を固定する．ここで，
$det(\boldsymbol{M}_{s_{j}})=1$のもとで，
$\Delta_{s_{j}}$を局所最小化する$\boldsymbol{S}_{s_{j}}$を求める．
\begin{eqnarray}
\boldsymbol{S}_{s_{j}}=\arg\min_{\boldsymbol{M}_{s_{j}}}\sum_{\boldsymbol{w}^{p_{i}}\in C_{s_{j}}}
(\boldsymbol{w}^{p_{i}}-\boldsymbol{v}_{G'})^{T}
\boldsymbol{M}_{s_{j}}^{-1}(\boldsymbol{w}^{p_{i}}-\boldsymbol{v}_{G'}) \label{eq:dj}
\end{eqnarray}
この$\boldsymbol{S}_{s_{j}}$は，クラスタ$C_{s_{j}}$の
共分散行列$\boldsymbol{M}_{s_{j}}$を用いて，
式(\ref{eq:AdpCov})によって与えられることが，文献 \cite{Diday77} に
より示されている．
\pagebreak
\begin{eqnarray}
\boldsymbol{S}_{s_{j}}=(det(\boldsymbol{M}_{s_{j}}))^{1/m}\boldsymbol{M}_{s_{j}}^{-1}
\label{eq:AdpCov}
\end{eqnarray}
ただし，$det(\boldsymbol{M}_{s_{j}}) \neq 0$であり，$m$は検索結果
集合における単語の異なり数を表す．

以上から，seedページを含むあるクラスタ$C_{s_{j}}$において，
Webページ間の非類似度を局所最小化することを考慮した
分散共分散行列$\boldsymbol{S}_{s_{j}}$を求めることができる．
この$\boldsymbol{S}_{s_{j}}$を用いて，$C_{s_{j}}$の
重心ベクトル$\boldsymbol{G}^{C_{s_{j}}}$と，このクラスタに
マージされるべきクラスタ$C_{l}$の重心ベクトル$\boldsymbol{G}^{C_{l}}$
間の適応的マハラノビス距離は，
式(\ref{Eq:Adapt. Mahalanobis distance})のように定義される．
\begin{eqnarray}
D(\boldsymbol{G}^{C_{s_{j}}},\boldsymbol{G}^{C_{l}})=
\sqrt{(\boldsymbol{G}^{C_{s_{j}}}-\boldsymbol{G}^{C_{l}})^{T}\boldsymbol{S}_{s_{j}}^{-1}(\boldsymbol{G}^{C_{s_{j}}}-\boldsymbol{G}^{C_{l}})} \label{Eq:Adapt. Mahalanobis distance}
\end{eqnarray}
なお，式(\ref{Eq:Adapt. Mahalanobis distance})は，
上述した\textbf{(1)}〜\textbf{(2)}による
クラスタ$C_{s_{j}}$におけるWebページ間の非類似度を
考慮して得られた式(\ref{eq:AdpCov})の
分散共分散行列$\boldsymbol{S}_{s_{j}}$を
適用している点で，式(\ref{Eq:Mahalanobis distance})とは異なる．


