未知語分類システムの構成を図[REF_fig:system]に示す．
未知語分類システムは，単語を意味的に分類した分類体系の1つであるNTTシソーラス[CITE]と，未知語をNTTシソーラスのノードに分類するための未知語分類処理により構成されている．
また，未知語分類処理においては，複数の国語辞書や新聞などから機械的に構築した大規模な知識ベースである概念ベース[CITE]と，概念と概念の関連の強さを定量的に評価する関連度計算[CITE]（以下，これらを合わせて連想メカニズムと呼ぶ）を用いることにより，未知語とNTTシソーラスのノードとの関連付けを行っている．
なお本論文では，未知語とNTTシソーラスのノードに対して関連度計算を行うために，概念化という処理を行っている．
概念化とは，ある単語に属性と重みの集合を与えることである．
本論文では，常識的な会話処理において用いられる一般名詞については，ノードとリーフをあわせて13万語の以上の単語が収録されているシソーラスと，約9万語の概念を収録する概念ベースを用いることで対応することができると考え，未知語に関する表現として，固有名詞を扱う．
さらに，固有名詞の中でも，1つの単語のみから人間がその単語の意味を判断できる固有名詞を扱う．
例えば，「Gショック」は「時計」，「クイニーアマン」は「パン」であると判断できる．
逆に，「イオン」であれば，「企業」と判断する人間だけでなく，「電離現象」と判断する人間もいると考えられる．
このように，人間が一意に判断できないことは，判断する手法が存在しないと考え，多義的な要素を持つ固有名詞については扱わないものとしている．
また，[REF_acquiring_attribute_of_unknown_word]節で述べる未知語の概念化では，未知語の属性とその重みの獲得をWebから行う．
そのため検索にヒットしない，つまり，Webに存在しない未知語は扱わないものとしている．
本章では，本研究を構成する技術であるシソーラス，連想メカニズム，および，属性の重み付け手法について述べる．
シソーラスとは，単語を意味的に分類した分類体系である．
シソーラスの多くは木構造を持ち，名詞の集合を分類した名詞シソーラスや用言の集合を分類した用言シソーラスなどがある．
また，木構造の葉（以下，リーフと呼ぶ）のみに単語が所属する分類シソーラスと根及び中間ノードにも単語が所属する上位下位シソーラスがある．
本論文では，木構造を持つ名詞シソーラスであり，上位下位シソーラスの1つであるNTTシソーラス[CITE]を用いる．
NTTシソーラスは一般名詞の意味的用法を表す2710個のノードの上位—下位関係，全体—部分関係が木構造で示されたものである．
ノードに所属する名詞として約13万語のリーフが分類されている．
図[REF_fig:thesaurus]にNTTシソーラスの木構造の一部を示す．
本論文では，未知語を最も詳しく説明するノードに分類するという考えから，未知語を分類するノードを最下位ノード（1926個）に限定している．
さらにその中で，固有名詞である未知語が分類されることはないと判断できるノードを人手で削除している．
なお判断基準としては，3名の被験者に各最下位ノードに未知語が分類されるノードか否かを判断してもらい，そのうち3名全員が未知語は分類されないと判断したノードを削除している．
結果，使用するノード数は385個となっている．
表[REF_table:filtering_node]に選別したノードの一例を示す．
連想メカニズムは概念ベースと関連度計算により構成されており，概念ベース[CITE]は，ある単語から語意の展開を行い，関連度計算[CITE]は，語意の展開結果を利用し，単語の間にある関連性の強さを数値として表す手法である．
概念ベースとは複数の国語辞書や新聞などから機械的に構築した単語（概念）とその意味特徴を表す単語（属性）の集合からなる知識ベースである．
概念には属性とその重要性を表す重みが付与されている．
概念ベースには約9万語の概念が収録されており，1つの概念に平均約30個の属性が存在する．
ある概念[MATH]は属性[MATH]とその重み[MATH]の対の集合として，式[REF_eq:concept_base]で表される．
任意の一次属性[MATH]は，その概念ベース中の概念表記の集合に含まれている単語で構成されている．
したがって，一次属性は必ずある概念表記に一致するため，さらにその一次属性を抽出することができる．
これを二次属性と呼ぶ．
概念ベースにおいて，「概念」は[MATH]次までの属性の連鎖集合により定義されている（図[REF_fig:concept_base]）．
本論文では，[REF_acquiring_attribute_of_unknown_word]節で述べる未知語の概念化，および，[REF_acquiring_attribute_of_node]節で述べるシソーラスのノードの概念化に概念ベースを用いている．
関連度とは，概念と概念の関連の強さを定量的に評価するものである．
概念と概念の間にある関連性を定量的に評価する手法として，ベクトル空間モデルが広く用いられている．
しかし，本論文では，概念を定義する属性集合とその重みを含めた一致度に基づいた関連度計算方式を利用している．
これは，関連度計算方式が有限ベクトル空間によるベクトル空間モデルよりも良好な結果が得られるという報告がなされているためである[CITE]．
本論文では重み比率付き関連度計算方式を使用し，実験を行う[CITE]．
任意の概念[MATH]，[MATH]について，それぞれ一次属性を[MATH]，[MATH]とし，対応する重みを[MATH]，[MATH]とする．
また，概念[MATH]，[MATH]の属性数を[MATH]個，[MATH]個[MATH]とする．
A={(a_i,u_i) \mid i=1〜L}
B={(b_j,v_j) \mid j=1〜M}
このとき，概念[MATH]，[MATH]の重み比率付き一致度[MATH]を以下の式[REF_eq:MatchWR1]，[REF_eq:MatchWR2]で定義する．
MatchWR(A,B)=\sum_{a_i=b_j}\min(u_i,v_j)
\min(\alpha,\beta)=
\alpha & (\beta > \alpha)
\beta & (\alpha\geq\beta)
概念[MATH]，[MATH]の属性[MATH]，[MATH]に対し，[MATH]となる属性（概念[MATH]，[MATH]に共通する属性）があった場合，共通する属性の重みの共通部分，つまり，小さい重み分のみ一致するとの考えに基づいている．
定義から明らかなように，両概念の属性と重みが完全に一致する場合に，一致度は1.0となる．
次に，属性の少ない方の概念を[MATH]とし[MATH]，概念[MATH]の属性を基準とする．
そして概念[MATH]の属性を，概念[MATH]の各属性との重み比率付一致度[MATH]の和が最大になるように並び替える．
これによって，概念[MATH]の一次属性と概念[MATH]の一次属性の対応する組を決める．
対応にあふれた概念[MATH]の属性は無視する（この時点では組み合わせは[MATH]個）．
ただし，一次属性同士が一致する（概念表記が同じ）ものがある場合([MATH])は，別扱いにする．
これは概念ベースには約9万の概念が存在し，属性が一致することは稀であるという考えに基づく．
従って，属性の一致の扱いを別にすることにより，属性が一致した場合を大きく評価する．
具体的には，対応する属性の重み[MATH]，[MATH]の大きさを重みの小さい方にそろえる．
このとき，重みの大きい方はその値から小さい方の重みを引き，もう一度，他の属性と対応をとることにする．
例えば，[MATH]で[MATH]とすれば，対応が決定するのは[MATH]と[MATH]であり，[MATH]はもう一度他の属性と対応させる．
このように対応を決めて，対応の取れた属性の組み合わせ数を[MATH]個とする．
重み比率付き関連度とは，重み比率付き一致度を比較する概念の各属性間で算出し，その和の最大値を求めることで計算する．
これを以下の式[REF_eq:DoA]により定義する．
以下，重み比率付き関連度を関連度と略し，この関連度[CITE]を用いる．
関連度の値は概念間の関連の強さを0〜1の間の連続値で表す．
1に近づくほど関連が強い．
概念[MATH]と[MATH]に対して関連度計算を行った例を表[REF_table:degree_of_association]に挙げる．
最後に，概念「机」と「椅子」を例に用いて，関連度の計算例を説明する．
概念「机」と「椅子」の一次属性および二次属性を表[REF_table:example_primary_attribute]，表[REF_table:example_secondary_attribute]に示す．
まず，概念「机」と「椅子」の一致度の計算を行う．
例えば，概念「机」の一次属性「学校」と概念「椅子」の一次属性「木」は，「木造」という共通する属性を持っているため，一致度は以下のように計算される．
同様に全ての一次属性の組み合わせについて一致度を計算した結果を表[REF_table:example_dom_matrix]に示す．
次に，関連度の計算を行う．
関連度の計算は，まず属性が完全に一致している部分から行われる．
続いて，一致度の大きい部分から順に対応を決める．
この場合，表[REF_table:example_dom_matrix]から一次属性「勉強」と「勉強」，「学校」と「教室」，「本棚」と「勉強」の順に対応が決まることになる．
結果，関連度は次式のように計算される．
DoA(机,椅子) & = 1.0\times(0.3+0.3)\times(0.3/0.3)/0.2+0.4\times(0.6+0.3)\times(0.3/0.6)/2
&　{} +0.1\times(0.1+0.2)\times(0.1/0.2)/2
& =0.3975
本論文では，[REF_narrowing_node]節で述べる未知語とシソーラスのノードとの関連の強さの判断に関連度計算を用いる．
本節では，本論文が提案する手法で用いる，対象としている文書に出現する単語の重み付け手法であるTF・IDF[CITE]とSWeb-idf[CITE]について述べる．
TF・IDFによる重み付けとは，対象としている単語の頻度と網羅性に基づいた重み付け手法である．
文書[MATH]における索引語[MATH]の重み[MATH]は以下の式[REF_eq:tf]によって得られる．
[MATH]は文書[MATH]における索引語[MATH]の出現頻度である．
また，[MATH]は検索対象文書数[MATH]と索引語[MATH]が出現する文書の数[MATH]によって決まり，式[REF_eq:idf]によって定義される．
本論文では，[REF_acquiring_attribute_of_node]節で述べるシソーラスのノード属性の概念化，および，[REF_determining_node]節で述べるノード動詞の構築にTF・IDFを用いている．
SWeb-idf (Statics Web-Inverse Document Frequency)とは，Web上の単語のIDFを統計的に調べたIDF値である．
まず，無作為に選んだ固有名詞1000語を作成する．
表[REF_table:proper_noun]に無作為に選択した固有名詞の一部を示す．
この作成した1000語に対して個々に検索エンジンで検索を行い，1語につき検索上位10件の検索結果ページの内容を取得する．
よって，得られた検索結果ページ数は10000ページとなる．
この10000ページから，複数の国語辞書や新聞などから概念（単語）を抽出した知識ベースである概念ベースの収録語数である約9万語とほぼ同等の単語数が得られたことから，獲得した10000ページをWebの全情報情報空間とみなしている．
そして，その中での単語のIDF値を表すSWeb-idfは，式[REF_eq:SWeb-idf]で求められる．
これらにより得られた単語とそのIDF値をデータベースに登録した．
なお[MATH]項は，全文書空間（10000ページ）に出現する概念[MATH]の頻度である．
獲得したSWeb-idfの値の例を表[REF_table:SWeb-idf]に挙げる．
なお，固有名詞の選び方を変えてもSWeb-idfの値に大きな変化は見られないという報告がなされている[CITE]．
本論文では，[REF_acquiring_attribute_of_unknown_word]節で述べる未知語の概念化にSWeb-idfを用いている．
本論文が提案する手法では，未知語を入力した後に，未知語とシソーラスのノードに対して関連度計算を行うために，未知語の概念化およびシソーラスのノードの概念化を行う．
そして，概念化された未知語およびノードを用いて未知語が所属するシソーラスのノード決定を行う．
処理の流れとしては，まず，未知語を入力した後に，未知語とノードに対して関連度計算を行うために，未知語とノードの概念化を行う．
次に，概念化された未知語およびノードに対して関連度計算を行い，所属候補ノードを絞り込む．
さらに，ノード動詞および共起ヒットを用いて未知語が所属するべきノードを決定する．
図[REF_fig:flow_node_mapping]に未知語をシソーラスのノードへ分類する流れを示す．
以下の手順により，未知語の概念化を行うために，未知語の属性とその重みをWebから獲得する．
入力された未知語をキーワードとして検索エンジンを用いて検索を行い，検索上位100件の検索結果ページの内容を取得する．
HTMLタグなど不要な情報を取り除いた文書群に対して，形態素解析ソフト「茶筌」を用いて形態素解析を行い，自立語を抽出する．
得られた自立語の中から概念ベースに存在する単語のみを未知語の属性として抜き出す．
得られた属性の頻度にSWeb-idf（[REF_SWeb-idf]節参照）の値を掛け合わせたものを属性の重みとし，得られた重み順に並び替える．
なお，SWeb-idfのDBに存在しない属性については，Web上にあまり存在しない単語と考え，SWeb-idf値の最大値を掛け合わせている．
表[REF_table:attribute_unknown_word]に未知語を概念化した例を示す．
入力された未知語の属性とその重みはWebの検索を用いて獲得したが（[REF_acquiring_attribute_of_unknown_word]節参照），比較対象であるシソーラスのノードは概念ではないため，関連度計算による比較を行うことができない．
そのため，シソーラスのノードの概念化を以下の手順で行う[CITE]．
ノードに所属する全てのリーフに対して概念ベースを参照し，リーフを概念とみなすことでその一次属性を取得する（図[REF_fig:acquiring_attribute_of_node]）．
(1)の作業を全てのノードに対して行う．
リーフを概念とみなすことで取得した一次属性に対して，TF・IDFを利用（[REF_tf_idf]節参照）して各属性の重みを求める．
具体的には，取得した一次属性の重みをTFとみなし，また，全てのノードの数を式[REF_eq:idf]の[MATH]，取得した一次属性が出現するノードの数を式[REF_eq:idf]の[MATH]とみなしてIDFを求める．
そして，得られた重み順に属性を並び替える．
表[REF_table:attribute_node]にシソーラスのノードの1つである「時計」を概念化した例を示す．
以下の手順により，処理回数を少なくするためにシソーラスのノードの絞込みを行う．
[REF_acquiring_attribute_of_unknown_word]節で説明した手法を用いて，概念化を行った未知語[MATH]を式[REF_eq:query]で定義する．
なお，[MATH]が属性，[MATH]がその重みである．
シソーラスのノード集合[MATH]を式[REF_eq:node]で定義する．
また，[REF_acquiring_attribute_of_node]節で説明した手法を用いて，概念化を行ったシソーラスのノード[MATH]を式[REF_eq:node_attribute]で定義する．
なお，[MATH]が属性，[MATH]がその重みである．
概念化を行った未知語[MATH]とシソーラスの各ノード[MATH]に対して関連度計算を行い，関連度[MATH]を求める．
そして，0.02以上の関連度を持つノードを所属候補ノードとする．
よって，所属候補ノード集合[MATH]は以下の式[REF_eq:candidate_node]で定義される．
なお，関連度の閾値0.02は，0.0から0.05まで0.001毎に変化させて実験を行った結果，最も高い精度を得られた値を閾値として採用したものである．
この実験については，[REF_threshold_evaluation]節で述べる．
また，閾値によりノード数を385個から10個程度に絞り込むことができ，[REF_determining_node]節で述べるノード動詞や共起ヒットを用いる処理において，処理回数を20分に1以下にすることに成功している．
[REF_narrowing_node]節の処理により求めた所属候補ノード集合[MATH]に対してノード動詞や共起ヒットを用いたノード決定を行う．
NTTシソーラスは作成者がある分類基準に従って単語を体系的に分類したものである．
そのため，NTTシソーラスには「あるノードに所属するリーフは，そのリーフの直後に現れる助詞を伴う動詞が同じである」という関係が存在する．
例えば，ノード「茶」に属するリーフ「番茶」や「麦茶」などには，「番茶を飲む」や「麦茶を飲む」など直後に現れる助詞を伴う動詞が共に「を飲む」であることが分かる．
ノード動詞とはこの関係を利用して，ノードに設定したキーワードのことであり，ノード決定に利用する．
具体的には，入力された未知語にノードごとに対応する助詞を伴う動詞（ノード動詞）を連結したキーワードを検索エンジンに入力し，HIT数を獲得する．
そして，獲得したHIT数を[REF_determining_node_method]節で述べるノード得点の算出に利用する．
例えば，未知語が「マイルドセブン」，所属候補ノードが「たばこ」である場合，ノード「たばこ」のノード動詞である「を吸う」を連結した「マイルドセブンを吸う」というキーワードの検索を検索エンジンで行ったときのHIT数を求める．
以下にノード動詞の構築方法を示す．
ノードに属しているリーフをすべて抜き出す．
それぞれのリーフをキーワードとして検索エンジンで検索し，各リーフについて検索上位1000件の検索結果ページを取得する．
そして，その文書内でキーワードの直後に出現する「格助詞+動詞（サ変名詞を含む）」部分を全て抜き出す．
(2)の操作を全てのノードに対して行う．
(3)で得られた「格助詞＋動詞（サ変名詞を含む）」に対して，TF・IDF（[REF_tf_idf]節参照）を利用して，重みを求める．
具体的には，取得した「格助詞+動詞（サ変名詞を含む）」の数をTFとみなし，また，全てのノードの数を式[REF_eq:idf]の[MATH]，「格助詞+動詞（サ変名詞を含む）」が出現するノードの数を式[REF_eq:idf]の[MATH]とみなしてIDFを求める．
そして，最も大きな重みを持つ「格助詞＋動詞（サ変名詞を含む）」をノード動詞に決定する．
表[REF_table:node_verb]に構築したノード動詞の例を示す．
「単語の意味は，どのような単語と共起するかという観点から特徴付けられる」というHarrisの分布仮説から[CITE]，関係のある2語は，ある文書に共に出現すると考えられる．
そこで，未知語とノード名のAnd検索を検索エンジンを行い，HIT数を獲得する．
そして，獲得したHIT数を[REF_determining_node_method]節で述べるノード得点の算出に利用する．
例えば，未知語が「マイルドセブン」，所属候補ノードが「たばこ」である場合，「マイルドセブン」と「たばこ」のAnd検索を検索エンジンで行ったときのHIT数を求める．
未知語の所属ノードを決定する計算式を式[REF_eq:determining_node_method]に示す．
所属候補ノード[MATH]の中でノード得点[MATH]が最も高いノードを所属ノードとする．
[MATH]は未知語[MATH]と[MATH]の関連度，[MATH]は未知語にノード動詞を連結したキーワードの検索を検索エンジンで行ったときのHIT数，[MATH]は未知語とノード名のAnd検索を検索エンジンで行ったときのHIT数を表す．
以下に未知語「Gショック」および「クイニーアマン」を例に，所属ノード決定手法における式[REF_eq:determining_node_method]の結果をノード得点上位5個まで例示したものを表[REF_table:calculation_example_doa]，[REF_table:calculation_example_node_verb]，[REF_table:calculation_example_coincidence_hit]，[REF_table:calculation_example_node_value]に示す．
表[REF_table:calculation_example_doa]が未知語とノード得点上位5個のノードとの関連度，表[REF_table:calculation_example_node_verb]が未知語のノード得点上位5個のノードが持つノード動詞とノード動詞を用いたときのHIT数，表[REF_table:calculation_example_coincidence_hit]が共起ヒットを用いたときのHIT数，表[REF_table:calculation_example_node_value]が未知語のノード得点上位5個のノードに与えられたノード得点を表している．
ここでは既存手法として，ベクトル空間法に基づく手法について説明する．
この手法では，シソーラスにはNTTシソーラス[CITE]，学習データ及び未知語データにはEDRコーパス[CITE]の共起辞書を用いている．
EDRコーパスは22万文からなる文章のデータベースであり，係り受け関係にある単語対を抽出した共起辞書を用いている．
ベクトル空間法に基づく手法は，シソーラスの各ノードの特徴ベクトルと未知語の特徴ベクトルの類似度をベクトル間の余弦を用いて算出し，類似度の高いノードに未知語を分類する．
最も単純なベクトル空間法では，特徴ベクトルは名詞と動詞の共起頻度によるベクトルである．
ノードの特徴ベクトルの各要素は，そのノードに属する名詞と動詞との共起頻度を足し合わせたものであり，未知語の特徴ベクトルの各要素は，未知語と動詞の共起頻度そのものとなっている．
以下に，ベクトル空間法を詳しく説明する．
ベクトル空間法では，式[REF_eq:vector_space_method1]，[REF_eq:vector_space_method2]，[REF_eq:vector_space_method3]によって未知語[MATH]を分類するノードが決定される．
式[REF_eq:vector_space_method1]よりベクトル空間法では，未知語の特徴ベクトル[MATH]と余弦の値が最高になる特徴ベクトル[MATH]に対応するノード[MATH]に未知語[MATH]を分類する．
式[REF_eq:vector_space_method1]，[REF_eq:vector_space_method2]，[REF_eq:vector_space_method3]についての説明を行う．
まず，シソーラスに既に分類されている名詞（リーフ）[MATH]の集合[MATH]，シソーラスのノード[MATH]の集合[MATH]，共起を考慮する動詞[MATH]の集合[MATH]を以下に定義する．
また，[MATH]は未知語を表している．
\mathit{NOUN} & ={noun}_1, \mathit{noun}_2, \cdots, \mathit{noun}_i, \cdots, \mathit{noun}_{noun}_\mathit{num}
\mathit{NODE} & ={node}_1, \mathit{node}_2, \cdots, \mathit{node}_i, \cdots, \mathit{node}_{node}_\mathit{num}
\mathit{VERB} & ={verb}_1, \mathit{verb}_2, \cdots, \mathit{verb}_i, \cdots, \mathit{node}_{verb}_\mathit{num}
次に，ノード[MATH]と動詞[MATH]が共起したことを表す1つの学習データを以下に定義する．
[MATH]は[MATH]個の学習データからなる系列である．
学習データを生成するために用いる元々の文章の中では，名詞[MATH]と動詞[MATH]が共起しているが，学習データを生成する時点で名詞と動詞の二項組[MATH]をノードと動詞の二項組[MATH]に変換する．
なお，ノード[MATH]は名詞[MATH]が属するノードであり，複数のノードに属する場合は複数の二項組に変換する．
したがって，未知語[MATH]が属するノード[MATH]と未知語[MATH]と共起した動詞[MATH]の系列[MATH]は，以下のように表すことができる．
しかし，[MATH]は未知であり，実際に観測される未知語データは未知語[MATH]と共起した動詞[MATH]の系列[MATH]の二項組[MATH]である．
よって，未知語分類問題は学習データ[MATH]と未知語データ[MATH]を観測したもとで未知語[MATH]が属するノード[MATH]を推定する問題となる．
[MATH]は，学習データ[MATH]と未知語データ[MATH]を引数に取り，未知語[MATH]を分類するべきノードを決定する関数を表す．
[MATH]はノード[MATH]の特徴ベクトル，[MATH]は未知語[MATH]の特徴ベクトルである．
また，[MATH]は学習データ[MATH]中の[MATH]の数でノード[MATH]と動詞[MATH]が共起した回数，[MATH]は未知語データ[MATH]の[MATH]中の[MATH]の数で未知語[MATH]と動詞[MATH]が共起した回数を表す．
[MATH]はベクトル間の余弦の値を求める関数，[MATH]はベクトル[MATH]間の内積，[MATH]はベクトル[MATH]のノルムである．
[b] d_ {(w,z)^N , (\mathit{unknown}, y^M) } & = \arg\max_{node}_i (\mathit{vec}(\mathit{node}_i), \mathit{vec}(\mathit{unknown}))
& = \arg\max_{node}_i \left{vec}(\mathit{node}_i) \cdot\mathit{vec}(\mathit{unknown}) \mathit{vec}(\mathit{node}_i) \parallel\parallel\mathit{vec}(\mathit{unknown}) \parallel \right
[b] \mathit{vec}(\mathit{node}_i) = & \bigl{co}\bigl( (\mathit{node}_i,\mathit{verb}_1) \mid(w,z)^Z \bigr), \mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_2) \mid(w,z)^Z \bigr), \cdots,
& \mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_i) \mid(w,z)^Z \bigr), \cdots, \mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_{verb}_\mathit{num}) \mid(w,z)^Z \bigr)\bigr
[b] \mathit{vec}(\mathit{node}_i) = & \bigl{co}(\mathit{verb}_1 \mid y^M), \mathit{co}(\mathit{verb}_2 \mid y^M)), \cdots,
& \mathit{co}(\mathit{verb}_i \mid y^M) \cdots, \mathit{co} (\mathit{verb}_{verb}_\mathit{num} \mid y^M) \bigr
なお，上記のような単純に共起頻度を用いるベクトル空間法以外に，各共起頻度に重み付けを行うTF・IDF法を導入したベクトル空間法も提案されており，情報検索などの分野において実用化されている手法は，TF・IDF法を導入したベクトル空間法である[CITE]．
TF・IDF法を導入したベクトル空間法では，式[REF_eq:vector_space_method2]および式[REF_eq:vector_space_method3]において，特徴ベクトルの第[MATH]要素に[MATH]を掛け合わせたものを特徴ベクトルとして採用し，その上で式[REF_eq:vector_space_method1]を用いて未知語の分類を行う．
ただし，[MATH]は動詞[MATH]との共起頻度が1以上のノードの数である．
比較実験の方法を以下に示す[CITE]．
NTTシソーラスに既に分類されている名詞（リーフ）の中で概念ベース（[REF_concept_base]節参照）に存在する単語から1000語を未知語と仮定して抽出する．
NTTシソーラスに属している残りのリーフとEDRコーパス頻出動詞上位500語との共起回数を算出し，学習データを作成する．
さらに，NTTシソーラスから取り出しておいた1000語の未知語について，学習データと同様にEDRコーパス頻出動詞上位500語との共起回数を共起辞書から算出し，1000個の未知語データを作成する．
学習データと未知語データをもとにベクトル空間法（式[REF_eq:vector_space_method1]）を用いて，各未知語に対する所属ノードを出力する．
また，本論文で提案する手法（式[REF_eq:determining_node_method]）を用いて，各未知語に対する所属ノードを出力する．
抽出された未知語とその未知語が所属するノードの例を表[REF_table:testset_leaf]に示す．
図[REF_fig:comparison_result]に実験結果を示す．
図[REF_fig:comparison_result]のCosは共起頻度のみによるベクトル空間法，TF・IDFはTF・IDF法を導入したベクトル空間法，提案手法が本論文で提案している手法に対応する．
本実験において，未知語が元のNTTシソーラスにおいて分類されていたノードに分類できた場合を正解とする．
また，未知語が複数のノードに所属していた場合には，出力したノードがその中のどれか1つと一致すれば，正解とみなしている．
なお，図[REF_fig:plural_precision]と同様に，横軸は考慮した累積のノードの数を表している．
また，縦軸は考慮しているノードの中に1つでも正解ノードを得た未知語を正解，1つも正解ノードを得られなかった未知語を不正解として算出した精度を表している．
図[REF_fig:comparison_result]より，提案手法の精度は共起頻度によるベクトル空間法(Cos)より13〜30%高く，TF・IDF法を導入したベクトル空間法(TF・IDF)に対しても10〜20%高くなっており，提案手法がベクトル空間法に基づく手法よりも優れた結果を示している．
本来，本論文で提案している手法は，固有名詞を中心とする既存のシソーラスに分類されていない未知語に対して有効な手法である．
その一方で，本実験で用いた既存のシソーラス（NTTシソーラス）から抽出した仮想的な未知語の実体は一般的な単語である．
一般的な単語は多くの文書で使用されるため，[REF_acquiring_attribute_of_unknown_word]節で説明した手法を用いると，広範囲にわたるページから属性を獲得することになる．
その結果，獲得できる属性にばらつきが生じ，適切な属性を獲得することが困難である．
そのため，本論文で提案している手法は，本実験に対しては不利な部分があるといえる．
この点を踏まえると，本実験において不利な部分を持っているにも関わらず，提案手法は良好な結果が得られたといえる．
したがって，本論文で提案する手法が未知語に限らず，一般的な単語に対しても柔軟に機能することを示しているといえる．
未知語分類システムの構成を図[REF_fig:system]に示す．
未知語分類システムは，単語を意味的に分類した分類体系の1つであるNTTシソーラス[CITE]と，未知語をNTTシソーラスのノードに分類するための未知語分類処理により構成されている．
また，未知語分類処理においては，複数の国語辞書や新聞などから機械的に構築した大規模な知識ベースである概念ベース[CITE]と，概念と概念の関連の強さを定量的に評価する関連度計算[CITE]（以下，これらを合わせて連想メカニズムと呼ぶ）を用いることにより，未知語とNTTシソーラスのノードとの関連付けを行っている．
なお本論文では，未知語とNTTシソーラスのノードに対して関連度計算を行うために，概念化という処理を行っている．
概念化とは，ある単語に属性と重みの集合を与えることである．
本論文では，常識的な会話処理において用いられる一般名詞については，ノードとリーフをあわせて13万語の以上の単語が収録されているシソーラスと，約9万語の概念を収録する概念ベースを用いることで対応することができると考え，未知語に関する表現として，固有名詞を扱う．
さらに，固有名詞の中でも，1つの単語のみから人間がその単語の意味を判断できる固有名詞を扱う．
例えば，「Gショック」は「時計」，「クイニーアマン」は「パン」であると判断できる．
逆に，「イオン」であれば，「企業」と判断する人間だけでなく，「電離現象」と判断する人間もいると考えられる．
このように，人間が一意に判断できないことは，判断する手法が存在しないと考え，多義的な要素を持つ固有名詞については扱わないものとしている．
また，[REF_acquiring_attribute_of_unknown_word]節で述べる未知語の概念化では，未知語の属性とその重みの獲得をWebから行う．
そのため検索にヒットしない，つまり，Webに存在しない未知語は扱わないものとしている．
本章では，本研究を構成する技術であるシソーラス，連想メカニズム，および，属性の重み付け手法について述べる．
シソーラスとは，単語を意味的に分類した分類体系である．
シソーラスの多くは木構造を持ち，名詞の集合を分類した名詞シソーラスや用言の集合を分類した用言シソーラスなどがある．
また，木構造の葉（以下，リーフと呼ぶ）のみに単語が所属する分類シソーラスと根及び中間ノードにも単語が所属する上位下位シソーラスがある．
本論文では，木構造を持つ名詞シソーラスであり，上位下位シソーラスの1つであるNTTシソーラス[CITE]を用いる．
NTTシソーラスは一般名詞の意味的用法を表す2710個のノードの上位—下位関係，全体—部分関係が木構造で示されたものである．
ノードに所属する名詞として約13万語のリーフが分類されている．
図[REF_fig:thesaurus]にNTTシソーラスの木構造の一部を示す．
本論文では，未知語を最も詳しく説明するノードに分類するという考えから，未知語を分類するノードを最下位ノード（1926個）に限定している．
さらにその中で，固有名詞である未知語が分類されることはないと判断できるノードを人手で削除している．
なお判断基準としては，3名の被験者に各最下位ノードに未知語が分類されるノードか否かを判断してもらい，そのうち3名全員が未知語は分類されないと判断したノードを削除している．
結果，使用するノード数は385個となっている．
表[REF_table:filtering_node]に選別したノードの一例を示す．
連想メカニズムは概念ベースと関連度計算により構成されており，概念ベース[CITE]は，ある単語から語意の展開を行い，関連度計算[CITE]は，語意の展開結果を利用し，単語の間にある関連性の強さを数値として表す手法である．
概念ベースとは複数の国語辞書や新聞などから機械的に構築した単語（概念）とその意味特徴を表す単語（属性）の集合からなる知識ベースである．
概念には属性とその重要性を表す重みが付与されている．
概念ベースには約9万語の概念が収録されており，1つの概念に平均約30個の属性が存在する．
ある概念[MATH]は属性[MATH]とその重み[MATH]の対の集合として，式[REF_eq:concept_base]で表される．
任意の一次属性[MATH]は，その概念ベース中の概念表記の集合に含まれている単語で構成されている．
したがって，一次属性は必ずある概念表記に一致するため，さらにその一次属性を抽出することができる．
これを二次属性と呼ぶ．
概念ベースにおいて，「概念」は[MATH]次までの属性の連鎖集合により定義されている（図[REF_fig:concept_base]）．
本論文では，[REF_acquiring_attribute_of_unknown_word]節で述べる未知語の概念化，および，[REF_acquiring_attribute_of_node]節で述べるシソーラスのノードの概念化に概念ベースを用いている．
関連度とは，概念と概念の関連の強さを定量的に評価するものである．
概念と概念の間にある関連性を定量的に評価する手法として，ベクトル空間モデルが広く用いられている．
しかし，本論文では，概念を定義する属性集合とその重みを含めた一致度に基づいた関連度計算方式を利用している．
これは，関連度計算方式が有限ベクトル空間によるベクトル空間モデルよりも良好な結果が得られるという報告がなされているためである[CITE]．
本論文では重み比率付き関連度計算方式を使用し，実験を行う[CITE]．
任意の概念[MATH]，[MATH]について，それぞれ一次属性を[MATH]，[MATH]とし，対応する重みを[MATH]，[MATH]とする．
また，概念[MATH]，[MATH]の属性数を[MATH]個，[MATH]個[MATH]とする．
A={(a_i,u_i) \mid i=1〜L}
B={(b_j,v_j) \mid j=1〜M}
このとき，概念[MATH]，[MATH]の重み比率付き一致度[MATH]を以下の式[REF_eq:MatchWR1]，[REF_eq:MatchWR2]で定義する．
MatchWR(A,B)=\sum_{a_i=b_j}\min(u_i,v_j)
\min(\alpha,\beta)=
\alpha & (\beta > \alpha)
\beta & (\alpha\geq\beta)
概念[MATH]，[MATH]の属性[MATH]，[MATH]に対し，[MATH]となる属性（概念[MATH]，[MATH]に共通する属性）があった場合，共通する属性の重みの共通部分，つまり，小さい重み分のみ一致するとの考えに基づいている．
定義から明らかなように，両概念の属性と重みが完全に一致する場合に，一致度は1.0となる．
次に，属性の少ない方の概念を[MATH]とし[MATH]，概念[MATH]の属性を基準とする．
そして概念[MATH]の属性を，概念[MATH]の各属性との重み比率付一致度[MATH]の和が最大になるように並び替える．
これによって，概念[MATH]の一次属性と概念[MATH]の一次属性の対応する組を決める．
対応にあふれた概念[MATH]の属性は無視する（この時点では組み合わせは[MATH]個）．
ただし，一次属性同士が一致する（概念表記が同じ）ものがある場合([MATH])は，別扱いにする．
これは概念ベースには約9万の概念が存在し，属性が一致することは稀であるという考えに基づく．
従って，属性の一致の扱いを別にすることにより，属性が一致した場合を大きく評価する．
具体的には，対応する属性の重み[MATH]，[MATH]の大きさを重みの小さい方にそろえる．
このとき，重みの大きい方はその値から小さい方の重みを引き，もう一度，他の属性と対応をとることにする．
例えば，[MATH]で[MATH]とすれば，対応が決定するのは[MATH]と[MATH]であり，[MATH]はもう一度他の属性と対応させる．
このように対応を決めて，対応の取れた属性の組み合わせ数を[MATH]個とする．
重み比率付き関連度とは，重み比率付き一致度を比較する概念の各属性間で算出し，その和の最大値を求めることで計算する．
これを以下の式[REF_eq:DoA]により定義する．
以下，重み比率付き関連度を関連度と略し，この関連度[CITE]を用いる．
関連度の値は概念間の関連の強さを0〜1の間の連続値で表す．
1に近づくほど関連が強い．
概念[MATH]と[MATH]に対して関連度計算を行った例を表[REF_table:degree_of_association]に挙げる．
最後に，概念「机」と「椅子」を例に用いて，関連度の計算例を説明する．
概念「机」と「椅子」の一次属性および二次属性を表[REF_table:example_primary_attribute]，表[REF_table:example_secondary_attribute]に示す．
まず，概念「机」と「椅子」の一致度の計算を行う．
例えば，概念「机」の一次属性「学校」と概念「椅子」の一次属性「木」は，「木造」という共通する属性を持っているため，一致度は以下のように計算される．
同様に全ての一次属性の組み合わせについて一致度を計算した結果を表[REF_table:example_dom_matrix]に示す．
次に，関連度の計算を行う．
関連度の計算は，まず属性が完全に一致している部分から行われる．
続いて，一致度の大きい部分から順に対応を決める．
この場合，表[REF_table:example_dom_matrix]から一次属性「勉強」と「勉強」，「学校」と「教室」，「本棚」と「勉強」の順に対応が決まることになる．
結果，関連度は次式のように計算される．
DoA(机,椅子) & = 1.0\times(0.3+0.3)\times(0.3/0.3)/0.2+0.4\times(0.6+0.3)\times(0.3/0.6)/2
&　{} +0.1\times(0.1+0.2)\times(0.1/0.2)/2
& =0.3975
本論文では，[REF_narrowing_node]節で述べる未知語とシソーラスのノードとの関連の強さの判断に関連度計算を用いる．
本節では，本論文が提案する手法で用いる，対象としている文書に出現する単語の重み付け手法であるTF・IDF[CITE]とSWeb-idf[CITE]について述べる．
TF・IDFによる重み付けとは，対象としている単語の頻度と網羅性に基づいた重み付け手法である．
文書[MATH]における索引語[MATH]の重み[MATH]は以下の式[REF_eq:tf]によって得られる．
[MATH]は文書[MATH]における索引語[MATH]の出現頻度である．
また，[MATH]は検索対象文書数[MATH]と索引語[MATH]が出現する文書の数[MATH]によって決まり，式[REF_eq:idf]によって定義される．
本論文では，[REF_acquiring_attribute_of_node]節で述べるシソーラスのノード属性の概念化，および，[REF_determining_node]節で述べるノード動詞の構築にTF・IDFを用いている．
SWeb-idf (Statics Web-Inverse Document Frequency)とは，Web上の単語のIDFを統計的に調べたIDF値である．
まず，無作為に選んだ固有名詞1000語を作成する．
表[REF_table:proper_noun]に無作為に選択した固有名詞の一部を示す．
この作成した1000語に対して個々に検索エンジンで検索を行い，1語につき検索上位10件の検索結果ページの内容を取得する．
よって，得られた検索結果ページ数は10000ページとなる．
この10000ページから，複数の国語辞書や新聞などから概念（単語）を抽出した知識ベースである概念ベースの収録語数である約9万語とほぼ同等の単語数が得られたことから，獲得した10000ページをWebの全情報情報空間とみなしている．
そして，その中での単語のIDF値を表すSWeb-idfは，式[REF_eq:SWeb-idf]で求められる．
これらにより得られた単語とそのIDF値をデータベースに登録した．
なお[MATH]項は，全文書空間（10000ページ）に出現する概念[MATH]の頻度である．
獲得したSWeb-idfの値の例を表[REF_table:SWeb-idf]に挙げる．
なお，固有名詞の選び方を変えてもSWeb-idfの値に大きな変化は見られないという報告がなされている[CITE]．
本論文では，[REF_acquiring_attribute_of_unknown_word]節で述べる未知語の概念化にSWeb-idfを用いている．
本論文が提案する手法では，未知語を入力した後に，未知語とシソーラスのノードに対して関連度計算を行うために，未知語の概念化およびシソーラスのノードの概念化を行う．
そして，概念化された未知語およびノードを用いて未知語が所属するシソーラスのノード決定を行う．
処理の流れとしては，まず，未知語を入力した後に，未知語とノードに対して関連度計算を行うために，未知語とノードの概念化を行う．
次に，概念化された未知語およびノードに対して関連度計算を行い，所属候補ノードを絞り込む．
さらに，ノード動詞および共起ヒットを用いて未知語が所属するべきノードを決定する．
図[REF_fig:flow_node_mapping]に未知語をシソーラスのノードへ分類する流れを示す．
以下の手順により，未知語の概念化を行うために，未知語の属性とその重みをWebから獲得する．
入力された未知語をキーワードとして検索エンジンを用いて検索を行い，検索上位100件の検索結果ページの内容を取得する．
HTMLタグなど不要な情報を取り除いた文書群に対して，形態素解析ソフト「茶筌」を用いて形態素解析を行い，自立語を抽出する．
得られた自立語の中から概念ベースに存在する単語のみを未知語の属性として抜き出す．
得られた属性の頻度にSWeb-idf（[REF_SWeb-idf]節参照）の値を掛け合わせたものを属性の重みとし，得られた重み順に並び替える．
なお，SWeb-idfのDBに存在しない属性については，Web上にあまり存在しない単語と考え，SWeb-idf値の最大値を掛け合わせている．
表[REF_table:attribute_unknown_word]に未知語を概念化した例を示す．
入力された未知語の属性とその重みはWebの検索を用いて獲得したが（[REF_acquiring_attribute_of_unknown_word]節参照），比較対象であるシソーラスのノードは概念ではないため，関連度計算による比較を行うことができない．
そのため，シソーラスのノードの概念化を以下の手順で行う[CITE]．
ノードに所属する全てのリーフに対して概念ベースを参照し，リーフを概念とみなすことでその一次属性を取得する（図[REF_fig:acquiring_attribute_of_node]）．
(1)の作業を全てのノードに対して行う．
リーフを概念とみなすことで取得した一次属性に対して，TF・IDFを利用（[REF_tf_idf]節参照）して各属性の重みを求める．
具体的には，取得した一次属性の重みをTFとみなし，また，全てのノードの数を式[REF_eq:idf]の[MATH]，取得した一次属性が出現するノードの数を式[REF_eq:idf]の[MATH]とみなしてIDFを求める．
そして，得られた重み順に属性を並び替える．
表[REF_table:attribute_node]にシソーラスのノードの1つである「時計」を概念化した例を示す．
以下の手順により，処理回数を少なくするためにシソーラスのノードの絞込みを行う．
[REF_acquiring_attribute_of_unknown_word]節で説明した手法を用いて，概念化を行った未知語[MATH]を式[REF_eq:query]で定義する．
なお，[MATH]が属性，[MATH]がその重みである．
シソーラスのノード集合[MATH]を式[REF_eq:node]で定義する．
また，[REF_acquiring_attribute_of_node]節で説明した手法を用いて，概念化を行ったシソーラスのノード[MATH]を式[REF_eq:node_attribute]で定義する．
なお，[MATH]が属性，[MATH]がその重みである．
概念化を行った未知語[MATH]とシソーラスの各ノード[MATH]に対して関連度計算を行い，関連度[MATH]を求める．
そして，0.02以上の関連度を持つノードを所属候補ノードとする．
よって，所属候補ノード集合[MATH]は以下の式[REF_eq:candidate_node]で定義される．
なお，関連度の閾値0.02は，0.0から0.05まで0.001毎に変化させて実験を行った結果，最も高い精度を得られた値を閾値として採用したものである．
この実験については，[REF_threshold_evaluation]節で述べる．
また，閾値によりノード数を385個から10個程度に絞り込むことができ，[REF_determining_node]節で述べるノード動詞や共起ヒットを用いる処理において，処理回数を20分に1以下にすることに成功している．
[REF_narrowing_node]節の処理により求めた所属候補ノード集合[MATH]に対してノード動詞や共起ヒットを用いたノード決定を行う．
NTTシソーラスは作成者がある分類基準に従って単語を体系的に分類したものである．
そのため，NTTシソーラスには「あるノードに所属するリーフは，そのリーフの直後に現れる助詞を伴う動詞が同じである」という関係が存在する．
例えば，ノード「茶」に属するリーフ「番茶」や「麦茶」などには，「番茶を飲む」や「麦茶を飲む」など直後に現れる助詞を伴う動詞が共に「を飲む」であることが分かる．
ノード動詞とはこの関係を利用して，ノードに設定したキーワードのことであり，ノード決定に利用する．
具体的には，入力された未知語にノードごとに対応する助詞を伴う動詞（ノード動詞）を連結したキーワードを検索エンジンに入力し，HIT数を獲得する．
そして，獲得したHIT数を[REF_determining_node_method]節で述べるノード得点の算出に利用する．
例えば，未知語が「マイルドセブン」，所属候補ノードが「たばこ」である場合，ノード「たばこ」のノード動詞である「を吸う」を連結した「マイルドセブンを吸う」というキーワードの検索を検索エンジンで行ったときのHIT数を求める．
以下にノード動詞の構築方法を示す．
ノードに属しているリーフをすべて抜き出す．
それぞれのリーフをキーワードとして検索エンジンで検索し，各リーフについて検索上位1000件の検索結果ページを取得する．
そして，その文書内でキーワードの直後に出現する「格助詞+動詞（サ変名詞を含む）」部分を全て抜き出す．
(2)の操作を全てのノードに対して行う．
(3)で得られた「格助詞＋動詞（サ変名詞を含む）」に対して，TF・IDF（[REF_tf_idf]節参照）を利用して，重みを求める．
具体的には，取得した「格助詞+動詞（サ変名詞を含む）」の数をTFとみなし，また，全てのノードの数を式[REF_eq:idf]の[MATH]，「格助詞+動詞（サ変名詞を含む）」が出現するノードの数を式[REF_eq:idf]の[MATH]とみなしてIDFを求める．
そして，最も大きな重みを持つ「格助詞＋動詞（サ変名詞を含む）」をノード動詞に決定する．
表[REF_table:node_verb]に構築したノード動詞の例を示す．
「単語の意味は，どのような単語と共起するかという観点から特徴付けられる」というHarrisの分布仮説から[CITE]，関係のある2語は，ある文書に共に出現すると考えられる．
そこで，未知語とノード名のAnd検索を検索エンジンを行い，HIT数を獲得する．
そして，獲得したHIT数を[REF_determining_node_method]節で述べるノード得点の算出に利用する．
例えば，未知語が「マイルドセブン」，所属候補ノードが「たばこ」である場合，「マイルドセブン」と「たばこ」のAnd検索を検索エンジンで行ったときのHIT数を求める．
未知語の所属ノードを決定する計算式を式[REF_eq:determining_node_method]に示す．
所属候補ノード[MATH]の中でノード得点[MATH]が最も高いノードを所属ノードとする．
[MATH]は未知語[MATH]と[MATH]の関連度，[MATH]は未知語にノード動詞を連結したキーワードの検索を検索エンジンで行ったときのHIT数，[MATH]は未知語とノード名のAnd検索を検索エンジンで行ったときのHIT数を表す．
以下に未知語「Gショック」および「クイニーアマン」を例に，所属ノード決定手法における式[REF_eq:determining_node_method]の結果をノード得点上位5個まで例示したものを表[REF_table:calculation_example_doa]，[REF_table:calculation_example_node_verb]，[REF_table:calculation_example_coincidence_hit]，[REF_table:calculation_example_node_value]に示す．
表[REF_table:calculation_example_doa]が未知語とノード得点上位5個のノードとの関連度，表[REF_table:calculation_example_node_verb]が未知語のノード得点上位5個のノードが持つノード動詞とノード動詞を用いたときのHIT数，表[REF_table:calculation_example_coincidence_hit]が共起ヒットを用いたときのHIT数，表[REF_table:calculation_example_node_value]が未知語のノード得点上位5個のノードに与えられたノード得点を表している．
ここでは既存手法として，ベクトル空間法に基づく手法について説明する．
この手法では，シソーラスにはNTTシソーラス[CITE]，学習データ及び未知語データにはEDRコーパス[CITE]の共起辞書を用いている．
EDRコーパスは22万文からなる文章のデータベースであり，係り受け関係にある単語対を抽出した共起辞書を用いている．
ベクトル空間法に基づく手法は，シソーラスの各ノードの特徴ベクトルと未知語の特徴ベクトルの類似度をベクトル間の余弦を用いて算出し，類似度の高いノードに未知語を分類する．
最も単純なベクトル空間法では，特徴ベクトルは名詞と動詞の共起頻度によるベクトルである．
ノードの特徴ベクトルの各要素は，そのノードに属する名詞と動詞との共起頻度を足し合わせたものであり，未知語の特徴ベクトルの各要素は，未知語と動詞の共起頻度そのものとなっている．
以下に，ベクトル空間法を詳しく説明する．
ベクトル空間法では，式[REF_eq:vector_space_method1]，[REF_eq:vector_space_method2]，[REF_eq:vector_space_method3]によって未知語[MATH]を分類するノードが決定される．
式[REF_eq:vector_space_method1]よりベクトル空間法では，未知語の特徴ベクトル[MATH]と余弦の値が最高になる特徴ベクトル[MATH]に対応するノード[MATH]に未知語[MATH]を分類する．
式[REF_eq:vector_space_method1]，[REF_eq:vector_space_method2]，[REF_eq:vector_space_method3]についての説明を行う．
まず，シソーラスに既に分類されている名詞（リーフ）[MATH]の集合[MATH]，シソーラスのノード[MATH]の集合[MATH]，共起を考慮する動詞[MATH]の集合[MATH]を以下に定義する．
また，[MATH]は未知語を表している．
\mathit{NOUN} & ={noun}_1, \mathit{noun}_2, \cdots, \mathit{noun}_i, \cdots, \mathit{noun}_{noun}_\mathit{num}
\mathit{NODE} & ={node}_1, \mathit{node}_2, \cdots, \mathit{node}_i, \cdots, \mathit{node}_{node}_\mathit{num}
\mathit{VERB} & ={verb}_1, \mathit{verb}_2, \cdots, \mathit{verb}_i, \cdots, \mathit{node}_{verb}_\mathit{num}
次に，ノード[MATH]と動詞[MATH]が共起したことを表す1つの学習データを以下に定義する．
[MATH]は[MATH]個の学習データからなる系列である．
学習データを生成するために用いる元々の文章の中では，名詞[MATH]と動詞[MATH]が共起しているが，学習データを生成する時点で名詞と動詞の二項組[MATH]をノードと動詞の二項組[MATH]に変換する．
なお，ノード[MATH]は名詞[MATH]が属するノードであり，複数のノードに属する場合は複数の二項組に変換する．
したがって，未知語[MATH]が属するノード[MATH]と未知語[MATH]と共起した動詞[MATH]の系列[MATH]は，以下のように表すことができる．
しかし，[MATH]は未知であり，実際に観測される未知語データは未知語[MATH]と共起した動詞[MATH]の系列[MATH]の二項組[MATH]である．
よって，未知語分類問題は学習データ[MATH]と未知語データ[MATH]を観測したもとで未知語[MATH]が属するノード[MATH]を推定する問題となる．
[MATH]は，学習データ[MATH]と未知語データ[MATH]を引数に取り，未知語[MATH]を分類するべきノードを決定する関数を表す．
[MATH]はノード[MATH]の特徴ベクトル，[MATH]は未知語[MATH]の特徴ベクトルである．
また，[MATH]は学習データ[MATH]中の[MATH]の数でノード[MATH]と動詞[MATH]が共起した回数，[MATH]は未知語データ[MATH]の[MATH]中の[MATH]の数で未知語[MATH]と動詞[MATH]が共起した回数を表す．
[MATH]はベクトル間の余弦の値を求める関数，[MATH]はベクトル[MATH]間の内積，[MATH]はベクトル[MATH]のノルムである．
[b] d_ {(w,z)^N , (\mathit{unknown}, y^M) } & = \arg\max_{node}_i (\mathit{vec}(\mathit{node}_i), \mathit{vec}(\mathit{unknown}))
& = \arg\max_{node}_i \left{vec}(\mathit{node}_i) \cdot\mathit{vec}(\mathit{unknown}) \mathit{vec}(\mathit{node}_i) \parallel\parallel\mathit{vec}(\mathit{unknown}) \parallel \right
[b] \mathit{vec}(\mathit{node}_i) = & \bigl{co}\bigl( (\mathit{node}_i,\mathit{verb}_1) \mid(w,z)^Z \bigr), \mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_2) \mid(w,z)^Z \bigr), \cdots,
& \mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_i) \mid(w,z)^Z \bigr), \cdots, \mathit{co}\bigl( (\mathit{node}_i,\mathit{verb}_{verb}_\mathit{num}) \mid(w,z)^Z \bigr)\bigr
[b] \mathit{vec}(\mathit{node}_i) = & \bigl{co}(\mathit{verb}_1 \mid y^M), \mathit{co}(\mathit{verb}_2 \mid y^M)), \cdots,
& \mathit{co}(\mathit{verb}_i \mid y^M) \cdots, \mathit{co} (\mathit{verb}_{verb}_\mathit{num} \mid y^M) \bigr
なお，上記のような単純に共起頻度を用いるベクトル空間法以外に，各共起頻度に重み付けを行うTF・IDF法を導入したベクトル空間法も提案されており，情報検索などの分野において実用化されている手法は，TF・IDF法を導入したベクトル空間法である[CITE]．
TF・IDF法を導入したベクトル空間法では，式[REF_eq:vector_space_method2]および式[REF_eq:vector_space_method3]において，特徴ベクトルの第[MATH]要素に[MATH]を掛け合わせたものを特徴ベクトルとして採用し，その上で式[REF_eq:vector_space_method1]を用いて未知語の分類を行う．
ただし，[MATH]は動詞[MATH]との共起頻度が1以上のノードの数である．
比較実験の方法を以下に示す[CITE]．
NTTシソーラスに既に分類されている名詞（リーフ）の中で概念ベース（[REF_concept_base]節参照）に存在する単語から1000語を未知語と仮定して抽出する．
NTTシソーラスに属している残りのリーフとEDRコーパス頻出動詞上位500語との共起回数を算出し，学習データを作成する．
さらに，NTTシソーラスから取り出しておいた1000語の未知語について，学習データと同様にEDRコーパス頻出動詞上位500語との共起回数を共起辞書から算出し，1000個の未知語データを作成する．
学習データと未知語データをもとにベクトル空間法（式[REF_eq:vector_space_method1]）を用いて，各未知語に対する所属ノードを出力する．
また，本論文で提案する手法（式[REF_eq:determining_node_method]）を用いて，各未知語に対する所属ノードを出力する．
抽出された未知語とその未知語が所属するノードの例を表[REF_table:testset_leaf]に示す．
図[REF_fig:comparison_result]に実験結果を示す．
図[REF_fig:comparison_result]のCosは共起頻度のみによるベクトル空間法，TF・IDFはTF・IDF法を導入したベクトル空間法，提案手法が本論文で提案している手法に対応する．
本実験において，未知語が元のNTTシソーラスにおいて分類されていたノードに分類できた場合を正解とする．
また，未知語が複数のノードに所属していた場合には，出力したノードがその中のどれか1つと一致すれば，正解とみなしている．
なお，図[REF_fig:plural_precision]と同様に，横軸は考慮した累積のノードの数を表している．
また，縦軸は考慮しているノードの中に1つでも正解ノードを得た未知語を正解，1つも正解ノードを得られなかった未知語を不正解として算出した精度を表している．
図[REF_fig:comparison_result]より，提案手法の精度は共起頻度によるベクトル空間法(Cos)より13〜30%高く，TF・IDF法を導入したベクトル空間法(TF・IDF)に対しても10〜20%高くなっており，提案手法がベクトル空間法に基づく手法よりも優れた結果を示している．
本来，本論文で提案している手法は，固有名詞を中心とする既存のシソーラスに分類されていない未知語に対して有効な手法である．
その一方で，本実験で用いた既存のシソーラス（NTTシソーラス）から抽出した仮想的な未知語の実体は一般的な単語である．
一般的な単語は多くの文書で使用されるため，[REF_acquiring_attribute_of_unknown_word]節で説明した手法を用いると，広範囲にわたるページから属性を獲得することになる．
その結果，獲得できる属性にばらつきが生じ，適切な属性を獲得することが困難である．
そのため，本論文で提案している手法は，本実験に対しては不利な部分があるといえる．
この点を踏まえると，本実験において不利な部分を持っているにも関わらず，提案手法は良好な結果が得られたといえる．
したがって，本論文で提案する手法が未知語に限らず，一般的な単語に対しても柔軟に機能することを示しているといえる．
