英語等の言語では，単語を認識単位とした連続音声認識システムが構成されている．
しかし，日本語の文は連続した文字列から構成されているため，単語の定義が明らかでなく，適切な認識単位が定かではない．
連続音声認識の適切な単位として，以下の条件を満たすが重要であると考えられる．
認識単位から読みの対応が明確であること
日本語の場合，漢字の読みや，助詞の「は」「へ」など，その文字だけでは読みが決定できない場合が多い．
できる限りヴァリエーションの少ない読みが決定できる認識単位を選択することが，連続音声認識の探索空間の削減のために有効である．
ポーズ（無音区間）の挿入位置が限定できること
連続音声といっても，発声文にはポーズが挿入される．
連続音声認識では，認識単位間にはポーズの挿入が可能であるとして認識を行うため，連続音声認識の探索空間の削減および認識率の向上のためには，認識単位をできるだけ長くしてポーズが挿入される個所を少なくし，かつ認識単位中にはポーズは挿入されないように決定するのが好ましい．
言語理解システムとの整合性が良いこと
音声認識のアプリケーションとして，機械翻訳等の言語理解と結合した音声理解システムが考えられる．
言語理解には形態素解析・構文解析等の処理が必要である．
このため，認識誤りにより認識結果の解析が不能に陥ることが少ない単位が望まれる．
以上の条件をよく満たす単位として，形態素が挙げられる．
しかし，形態素を単位としてN-gramを構築する場合，テキストデータから形態素を切り出す必要があり，この作業をすべて手作業で行うにはかなりのコストが必要となり，N-gram構築上の大きな問題となる．
このため，形態素切り出し作業を省くことを目的として，文字あるいは文字列を単位とした手法が提案されているが[CITE][CITE][CITE]，上記３条件には適合せず，連続音声認識にとって好ましい単位とは言えない．
従って，テキストデータから大量の形態素データを得るためには，自動で形態素解析を行う必要があると考えられる．
また，音声認識の対象となる文は基本的に話し言葉であり，文章の読み上げなどの場合を除いては，通常の発話において書き言葉を喋ることはまずありえない．
このため，連続音声認識用の言語モデル構築のためには，話し言葉のテキストデータに対して形態素解析が可能でなければならない．
現在形態素解析の手法としては，形態素間の接続ルールや重み付けを人手で作成するルールベースの手法が広く用いられているが，接続ルールは通常書き言葉を対象に作成されており，音声認識で必要な話し言葉に対して，十分な性能が得られない可能性がある．
このため，本論文では，統計的な言語モデルを用いた形態素解析の手法を採用した．
統計的モデルは，データから自動的に構築できるため，接続ルールや重み付けを手作業で行うことと比較して，話し言葉に対しても適用が容易であると考えられる．
また，統計的言語モデルは，連続音声認識の言語モデルとしても盛んに使用されているため，形態素解析にも統計的モデルを用いることにより，認識用言語モデルに適した形態素解析結果が得られると考えられる．
日本語の形態素解析は，文の文字列[MATH]から，それに対応する形態素列[MATH]を獲得することである．
統計的手法では，[MATH]に対して最も高い確率を与える形態素列[MATH]を探索することにより形態素解析を実現する．
これは，以下の式で与えられる．
ベイズ則により本式は下式のように変形される．
本式において，[MATH]は右式の最大値を与えるためには無関係な量である．
従って，式[REF_eqn:Bayes1]は下式と等価となる．
右辺の確率[MATH]は形態素から文字列を与える確率であるが，これは，形態素の表記と文字列が一致する場合は必ず[MATH]であり，一致しない場合は[MATH]である．
また，確率[MATH]は，形態素列[MATH]の生成確率である．
従って，統計的手法による形態素解析は，与えられた文字列と一致する全ての形態素列の中から，生成確率が最も高くなる形態素列を探索することによって実現できる．
形態素列[MATH]を[MATH]とすると，その生成確率[MATH]は次のように表される．
本式において，[MATH]は[MATH]番目[MATH]から番目までの形態素列を表す．
右辺の確率を直接求めるのは困難であるから，各形態素は，直前のN-1形態素から確率的に予測できると近似する．
これが，形態素N-gramである．
N-gramを用いると，上式の形態素列[MATH]の遷移確率は次のように近似される．
N-gramは，Nが大きくなるほど，パラメータ数が飛躍的に増大するため，通常は直前の形態素から次の形態素を予測するBigram(2-gram)，および直前の2形態素から次の形態素を予測するTrigram(3-gram)程度がよく使用される．

形態素N-gramのパラメータ数，すなわち形態素遷移の組合せは[MATH]（[MATH]は語い）であり，Nを大きくするとパラメータ数が格段に多くなるため，それぞれの値の推定が困難になる．
例えば，語いが10,000語の時，Trigramのパラメータ数は[MATH]となり，それぞれのパラメータを推定するためには，数兆語からなるテキストデータが必要となるが，これほどの大規模のデータを収集することは事実上不可能に近い．
実際には，平滑化[CITE][CITE]の手法を用いてデータ上に出現しない形態素遷移に対しても，確率を与えることができるが，その結果，実際には有り得ない形態素の遷移に対しても比較的高い確率を与える可能性がある．
この問題を解決するため，品詞を単位としたN-gramが使用される場合がある[CITE]．
これは，形態素の代りに品詞間の遷移を考えることによりパラメータ数を削減し，推定量の信頼性を高めるものである．
[MATH]形態素からなる文の生成確率は一般に下式で表される．
（[MATH]は[MATH]の属する品詞を，[MATH]は[MATH]番目から[MATH]番目の形態素列に対応する品詞列を表す）上式で，[MATH]は直前のN-1形態素列に対応する品詞列から次の形態素の属する品詞への遷移確率を表し，[MATH]は，次品詞から次形態素が出現する確率を表す．
品詞数を100とした時，Trigramの全ての品詞間の遷移の組は[MATH]であるから，形態素N-gramに比べてパラメータ数は極めて少なく，比較的信頼できる遷移確率が求めることができる．
しかし，品詞単位でのモデルでは，それぞれの形態素特有の接続関係を表現することができないため，言語モデルとしての性能は劣ると考えられる．
また，N-gramの単位を結合させ部分的に長くする手法も提案されている[CITE]．
日本語の場合は特定の形態素列を結合させてN-gramの単位として扱い，固定長のN-gramと比較して，局所的にNを大きくさせる効果があり，パラメータ数の増大を抑えながら，より長い範囲の形態素間の関係を表現するものである．
[MATH]形態素からなる文の生成確率は一般に下式で表される．
但し，[MATH]は文章の[MATH]番目の形態素列（単独の形態素も含める）を意味する．
また，[MATH]は形態素列に分割した際の形態素列の個数を表し，[MATH]である．
可変長形態素列N-gramは，長い範囲の形態素間の連接関係を表現するのには有効であるが，パラメータ数が同じNの形態素N-gramより多くなり，少量の学習データから，信頼性の高いパラメータ推定を行うのは困難である．
本論文では，形態素解析のための言語モデルとして，品詞と可変長形態素列の複合N-gramを使用することを提案する．
品詞と可変長形態素列の複合N-gramは，品詞N-gramを基本としたN-gramであるが，品詞全体の性質とは異なった性質を呈する特殊な形態素はその品詞から分離させ独立して扱う．
さらに，結合させることにより，言語モデルの精度を向上させる特定の形態素列は結合させた単位として扱う．
従って，品詞と可変長形態素列の複合N-gramは，前節で示した品詞N-gramと可変長形態素列N-gramとのそれぞれの長所を生かしながら，それぞれの短所を補い合うことにより，少ないパラメータで高い予測精度が得ることを可能とした言語モデルである．
N=2の場合，すなわちBigramを例にして，形態素N-gram，品詞N-gram，可変長形態素列N-gram，複合N-gramとの比較を図[REF_fig:probability comparison]に示す．
複合N-gramは，品詞クラス，形態素，形態素列を同時に扱うため複雑なモデルとなるが，本論文では，表現を簡単にするため，複合N-gramを次の３種類のクラス間のN-gramとして表現する．
品詞クラス
独立した１形態素のみで構成されるクラス
１形態素列で構成されるクラス
\def\probability_FIG {} \probability_FIGこのクラス分類を用いると，複合N-gramによる文の生成確率は，下式のクラスN-gramの形で与えることができる．
但し，[MATH]は文章を上記のクラス分類を用いた場合の，t番目の形態素列（単独の形態素も含める）を意味する．
また，[MATH]は文章の形態素列の個数を表し，[MATH]である．
例として，次の文章（５形態素）を考える．
「わたくし-橋本-と-言い-ます」
「橋本」は出現頻度が高くないため．
固有名詞クラスとして扱う方が適切であると考えられる．
「わたくし」および「と」は日本語の文章で頻繁に出現する形態素であるため．
品詞クラスより分離して単独で扱う．
また，「言い-ます」は日本語で頻繁に用いられるフレーズであるため．
結合させて一単位として扱う方が効果的であると考えられる．
従って，この文章の生成確率は，次の式で与えられる．
但し，[MATH]はそれぞれ，クラスA) B) C)に属していることを表す．
B)およびC)のクラスは，形態素（列）とクラスの出現頻度は等しいため([MATH])，上式は次のように変形することができ，複合N-gramと等価であることがわかる．
より少ないパラメータで次形態素予測精度の高い効率的な複合N-gramを得るためには，初期クラスから独立させる形態素，および結合させる形態素列を適切に選択する必要がある．
本論文では，品詞クラスを初期クラスとし，初期クラスからの形態素独立によるクラス分離，および形態素列結合によるクラス分離の２種類のクラス分離を逐次的に行うことによって，複合N-gramのためのクラス分類を決定する方法を提案する．
形態素独立，および形態素列結合候補の決定は，式[REF_eqn:entropy]により求められるエントロピーを最小にさせる候補を１つのみ選択する．
エントロピーはあいまいさを表す尺度であり，また，エントロピーを[MATH]としたときパープレキシティは[MATH]で与えられる．
すなわち，エントロピーが小さいことはあいまいさが小さく，また，次形態素予測の分岐も少なく，言語モデルの精度が高いことを意味する．
従って，クラス分離を行う際に，常にエントロピーを最小にする候補を選択する本手法は，より少ないパラメータで精度の高い複合N-gramを生成するために適した手法であると考えられる．
なお，本手法において，エントロピーの減少は常に正になることが保証されており，クラス分離によって，学習データに関してエントロピーは単調に減少する．
本論文では，未知語の形態素解析を行うために，品詞クラス[MATH]に対して，同一品詞の未知語のためのクラス[MATH]を導入する．
クラス[MATH]は，任意の文字を１次を出力するクラスであり，同一未知語クラス[MATH]が連続した場合は，それらをまとめて一つの未知語とみなす．
図[REF_fig:UnknownWord]に，「政瀧」という未知語を含んだ文の品詞Bigramを使用した形態素解析の処理例を示す．
以下に，[MATH]に関する確率の導出を行う．
\def\unknown_FIG {} \unknown_FIG Turing推定によると，データ上に[MATH]回出現する形態素は，次式の[MATH]回と推定される．
ただし，[MATH]はデータ上に[MATH]回出現した形態素の種類数を表す．
従って，[MATH]回出現する形態素[MATH]の品詞からの出現確率[MATH]は，
となる．
これを，クラス[MATH]に属する全ての形態素について計算し，[MATH]から引いた残りが品詞[MATH]から未知語出現する確率[MATH]である．
品詞[MATH]の未知語の文字[MATH]の出現する確率[MATH]は，本来文字毎に与えられるべきであるが，固有名詞等では，学習データにも出現しない文字が出現する可能性もあり文字毎に正確な確率を与えるのは困難であるため，全ての文字が等しい確率で出現するとし，未知語出現確率[MATH]から均等に割り当てる．
ただし，[MATH]は文字の種類数とする．
また，[MATH]は，未知語が連続する確率である．
本モデルにより生成される未知語の長さは二項分布に従うため，実際の未知語の長さも二項分布に従うという仮定を設けると，その品詞に属する語[MATH]の文字列[MATH]より[MATH]は下式により求められる．
