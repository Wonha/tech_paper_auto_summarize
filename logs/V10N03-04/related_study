自然言語処理では個々の問題を分類問題として定式化し，帰納学習の手法を利用して，
その問題を解決するというアプローチが大きな成功をおさめている．
しかしこのアプローチには帰納学習で必要とされる訓練データを用意しなければ
ならないという大きな問題がある．
この問題に対して，近年，少量のラベル付き訓練データから得られる分類器の精度を，
大量のラベルなし訓練データによって高めてゆく教師なし学習が散見される．
代表的な手法として，Co-training\cite{blum98} と，EM アルゴリズムを利用した手法\cite{nigam00}がある．
Co-training は2つの独立した属性 A と B を設定し，一方の属性 A から構築される
分類器を利用して，ラベルなしデータにラベル（クラス）を付与する．その中から信頼性のある
ラベルが付与されたデータをラベル付き訓練データに加える．このようにして追加されたラベル付き訓練データは，
もう一方の属性 B から見るとランダムなサンプルにラベル付けされたデータとして振る舞うので，
属性 B から構築される分類器の精度が高まる．これをお互いに作用し合うことで，
分類器の精度が高められる．一方，
EM アルゴリズムは，部分的に欠損値のある不完全な観測データ\( x_1, x_2, \cdots, x_N \)から，
そのデータを発生する確率モデル\( P_{\theta}(x) \)を推定する手法である．
\( P_{\theta}(x) \)は未知パラメータ\( \theta \)を含み，
\( P_{\theta}(x) \)の推定は，\( \theta \)の推定に帰着される．
分類問題の教師なし学習では，ラベル付き訓練データが完全な観測データ，
ラベルなし訓練データがラベルを欠損値とした不完全な観測データとなる．
EM アルゴリズムは，現時点での\( \theta \)を使って，
モデル\( P_{\theta}(c|x_i) \) のもとでの\( \log P_{\hat{\theta}}(x_i,c) \)の
期待値を取る（E-step）．次に，この期待値を最大にするような\( \hat{\theta} \)を求める（M-Step）．
\( \hat{\theta} \)を新たな\( \theta\)として先の E-step と M-step を繰り返す．
ここで\( c \)は欠損値となるラベルである．
EM アルゴリズムはパラメータ\( \theta \)とモデル\( P_\theta (x) \)を
適切に設定することで，隠れマルコフモデルや文脈自由文法のパラメータ推定，
あるいは名詞と動詞間の関係クラスの教師なし学習\cite{rooth}\cite{torisawa}などに利用できる．
そして，Nigam らは文書分類を題材にモデル\( P_\theta (x) \)を Naive Bayes のモデル，
\( \theta \)をラベル\( c \)のもとで素性\( f \)が起る条件付き確率\( p(f | c) \)に設定
することで，教師なし学習を試みている\cite{nigam00}．
score of this paragraph is 5
