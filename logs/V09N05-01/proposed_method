
分類問題において，正例，負例の2つのクラスに属す学習データのベクトル集合を，
とする．
ここで[MATH]はデータ[MATH]の特徴ベクトルで，一般的に[MATH]次元の素性ベクトル[MATH]で表現される．
[MATH]はデータ[MATH]が正例([MATH])あるいは負例([MATH])のいずれかを表わす値である．
パターン認識とは，この学習データ[MATH]から，クラスラベル出力[MATH]への識別関数[MATH]を導出することにある．
SVMでは，以下のような[MATH]次元Euclid空間上の平面で正例，負例を分離することを考える．
この時，近接する正例と負例の間の間隔(マージン)ができるだけ大きいほうが，汎化能力が高く，精度よく評価データを分類できる．
図[REF_fig:hyperplane]に，2次元空間上の正例(白丸)，負例(黒丸)を分離する問題を例にこのマージン最大化の概略を表す．
図[REF_fig:hyperplane]中の実線は式([REF_eq:hyperplane])の分離平面を示す．
一般にこのような分離平面は無数に存在し，図[REF_fig:hyperplane]に示す2つの分離平面はどちらも学習データを誤りなく分離している．
分離平面に平行する2つの破線は分離平面が傾き[MATH]を変化させないまま平行移動したときに，分類誤りなく移動できる境界を示す．
この2つの破線間の距離をマージンと呼び，SVMはマージンが最大となる分離平面を求める戦略を採用している．
図[REF_fig:hyperplane]の例では，右の分離平面が左の分離平面にくらべて大きなマージンを持っており，精度よくテスト事例を分離できることを意味している．
実際に2つの破線を求めてみる．
破線は，正例([MATH])もしくは負例([MATH])のラベルを出力する境界面になるように正規化を行えば，
で与えられる．
さらにマージン[MATH]は，分離平面上の任意の点[MATH]から各破線までの距離の和であり，[MATH]は，[MATH]を満たすため，
となる．
このマージンを最大化するためには，[MATH]を最小化すればよい．
つまり，この問題は以下の制約付き最適化問題を解くことと等価となる．
ここで，2つの破線上の分類を決定づける事例をサポートベクターと呼び，サポートベクター以外の事例は実際の学習結果に影響を及ぼさない．
さらに，一般的な分類問題においては，学習データを線形分離することが困難な場合ある．
このような場合，各素性の組み合わせを考慮し，より高次元な空間に学習データを写像すれば線形分離が容易になる．
実際の証明は省略するがSVMの学習，分類アルゴリズムは事例間の内積しか使用しない．
この点を生かし，各事例間の内積を任意のKernel関数におきかえることで，SVMは低次元中の非線形分類問題を高次元中の線形分離問題としてみなし分類を行うことが可能となっている．
多くのKernel関数が提案されているが，我々は以下の式で与えられる[MATH]次の多項式Kernel関数を用いた．
[MATH]次の多項式関数は[MATH]個までの素性の組み合わせ(共起)を考慮した学習モデルと見なすことができる．
ここで，汎化能力に関する一般的な理論について考察する．
学習データおよびテストデータがすべて独立かつ同じ分布[MATH]から生成されたと仮定すると，識別関数[MATH]のテストデータに対する汎化誤差[MATH]，学習データに対する誤差[MATH]は以下のように与えられる．
さらに，[MATH]には以下のような関係が成立することが知られている[CITE]．
[Vapnik]学習データの事例数を[MATH]，モデルのVC次元を[MATH]とする時，汎化誤差[MATH]は，[MATH]の確率で以下の上限値を持つ．
ここでVC次元[MATH]とは，モデルの記述能力，複雑さを表すパラメータである．
式([REF_eq:svm_gen])の右辺をVC boundと呼び，汎化誤差を小さくするには，VC boundをできるだけ小さくすればよい．
従来からある多くの学習アルゴリズムは，モデルの複雑さであるVC次元[MATH]を固定し，学習データに対するエラー率を最小にするような戦略をとる．
そのため，適切に[MATH]を選ばないとテストデータを精度良く分類できない．
また適切な[MATH]の選択は一般的に困難である．
一方SVMは，学習データに対するエラー率をSoft MarginやKernel関数を使って固定し，そのうえで右辺の第二項を最小化する戦略をとる．
実際に式([REF_eq:svm_gen])の右辺第二項に注目すると，[MATH]に対して増加関数となっている．
つまり，汎化誤差[MATH]を小さくするには，[MATH]をできるだけ小さくすればよい．
SVMではVC次元[MATH]とマージン[MATH]には以下の関係が成立することが知られている[CITE]．
[Vapnik]事例の次元数を[MATH]，マージンを[MATH]，全事例を囲む球面の最小直径を[MATH]とすると，SVMのVC次元[MATH]は，以下の上限値を持つ．
式([REF_eq:teiri])から，[MATH]を最小にするためには，マージンを最大にすればよく，これはSVMがとる戦略そのものであることが分かる．
また，学習データの次元数が十分大きければ，VC次元[MATH]は，学習データの次元数に依存しない．
さらに，[MATH]は，使用するKernel関数によって決まるため，式([REF_eq:teiri])はKernel関数の選択の指針を与える能力も持ちあわせていることが知られている[CITE]．
また，Vapnikは式([REF_eq:svm_gen])とは別に，SVMに固有のエラー率の上限を与えている．
[Vapnik] [MATH]をLeave-One-Outによって評価されるエラー率とする場合
となる．
Leave-One-Outとは，[MATH]個の学習データのうち1個をとりのぞいてテストデータとし，残り[MATH]を使って学習することをすべてのデータについて[MATH]回繰り返すことで，未知データに対するエラー率を予測する手法である．
式([REF_eq:loo])は容易に証明可能である．
つまり，SVMの特徴としてsupport vector以外の事例は最終の識別関数には一切影響を及ぼさない．
そのため個々のsupport vectorすべてが誤ったときが最悪のケースとなり，式([REF_eq:loo])が導かれる．
このboundは，単純明解で汎化誤差のおおまかな値を予測することを可能にする．
しかし，support vectorの数が増えても汎化能力が向上する事例もあり，式([REF_eq:loo])の汎化誤差の予測能力は式([REF_eq:svm_gen])には劣ることが知られている．

Chunk同定の際，各chunkの状態をどう表現するかが問題となる．
一つの手法として，各chunk同定を分割問題とみなし，各単語の間(ギャップ)にタグを付与する手法が考えられる．
しかし，この手法は単語とは別の位置にタグを付与する必要があり，従来からある形態素解析などのタグ付けタスクとは異なる枠組が必要となる．
その一方で，各単語にchunkの状態を示すタグを付与する手法がある．
この手法は，従来からあるタグ付け問題と同じ枠組でモデル化ができる利点がある．
後者の単語にタグを付与する表現法として，以下2種類の手法が提案されている．
Inside/Outside
この手法は英語のbase NP同定でよく用いられる手法の一つである[CITE]．
この手法では，chunkの状態として以下の3種類を設定する．
さらにTjong Kim Sangらは，上記のモデルをIOB1と呼び，このモデルを基にIOB2/IOE1/IOE2の3種類の表現方法を提案している[CITE]．
Start/End
この手法は日本語固有名詞抽出において用いられた手法[CITE]で，各単語に付与するタグとして以下の5種類を設定する．
これら5種類のタグ付け手法を英語の単名詞句抽出(base NP chunking)を例に以下に示す．
各chunkに対し，そのchunkの役割を示すタグを付与する場合は，B/E/I/O/Sといったchunkの状態を示すタグと，役割を示すタグを'-'で連結し新たなタグを導入することによって表現する．
例えば，IOB2モデルにおいて，動詞句(VP)の先頭の単語はB-VPというタグを付与すればよい．
基本的にSVMは2値分類器である．
そのため，chunkのタグ表現のように多値の分類問題を扱うためにはSVMに対し何らかの拡張を行う必要がある．
一般に，2値分類器を多値分類器に拡張する手法として，以下に述べる2種類の手法がある．
一つは，one class vs. all othersと呼ばれる手法で，[MATH]クラスの分類問題に対し，あるクラスかそれ以外かを分類する計[MATH]種類の分類器を作成する手法である．
もう一つは，pairwise法であり，各クラス2つの組み合わせを分類する[MATH]種類の分類器を作成し，最終的にそれらの多数決でクラスを決定する手法である．
また，DietterichやAllweinらは，上記の二つを含む形で，二値分類を多値分類器に拡張するための統一的な手法を提案している[CITE]．
本稿では，多値分類器への拡張手法としてpairwise法を採用した．
採用の理由として以下が挙げられる．
一般に，SVMは[MATH] ([MATH]は学習データのサイズ)の学習コストを要求する．
そのために，個々の二値分類器に用いられる学習データのサイズが小さければ，学習コストを大幅に削減することができる．
pairwise法は，one class vs. othersに比べ多くの二値各分類器を作成するが，各二値分類器に用いられる学習データは少量であり，全体的に学習のコストを小さくすることができる．
pairwise法が実験的に良い結果が得られたという報告[CITE]がある．
chunkタグの学習に用いる素性としては，現在の単語およびその周辺の単語や品詞といった文脈を用いる．
具体的には，位置[MATH]のchunkタグ[MATH]の推定を行う素性として[MATH]自身の単語と品詞，および右2つ，左2つの単語と品詞を用いた．
また，左2つのchunkタグも素性として使用した．
さらに，解析方向を逆(右向きから左向き)にし，右2つのchunkを素性として使用することも考えられる．
本稿では，これら2つの解析手法を前向き解析/後ろ向き解析と呼び区別する．
一般に，左2つ(後ろ向きの場合は右2つ)のchunkタグは学習データに対しては付与されているが，テストデータに対しては付与されていない．
そこで実際の解析時には，これらの素性は左から右向きに(後向きの場合は右から左に)解析しながら動的に追加していくこととした．
このような処理は，一種の動的計画法(DP)と考えることができる．
すなわち，全体として最尤なchunkタグ列は，各chunkタグに付与されるある種のスコアの和が最大になるようなタグ列を選択することにより決定される．
さらに，動的計画法を行う際に，解析のビーム幅を指定することで曖昧性の候補の爆発を抑えることができる．
CoNLL2000のshared taskにおいて，我々はスコアとしてpairwise時の投票数，また，ビーム幅を5として解析を行っている[CITE]．
本稿では，このような曖昧性を考慮したビーム幅付きの解析は行わず，ビーム幅1の決定的な解析を行った．
その理由としては以下が挙げられる．
我々の詳細な調査の結果，ビーム幅を大きく設定しても，顕著な精度向上に繋がらず，決定的な解析でも十分な解析精度が得られることが分かった．
本稿の目的は，後述する重み付き多数決の手法を比較することであり，単純な設定にすることで，個々の重み付け手法の相違点を明確にすることができる．
重み付き多数決とは，1つの学習器で出力を得るのではなく，学習データ，学習データの表現方法，素性の選択手法，学習アルゴリズム，あるいは学習アルゴリズムのパラメータ等の異なる複数の学習器を線形結合して出力を得るアルゴリズムのことを指す．
このような重み付き多数決の手法は，潜在的にマージン最大化の効果があり，汎化能力の高い強学習アルゴリズムを作成できることが理論的にも実験的にも明らかになっている．
ここで，多数決がなぜ精度向上に繋がるのか，その簡単な証明を行う．
重み付き多数決に用いる学習器の1つを[MATH]，さらに学習すべき対象(正解)を[MATH]とする．
また，[MATH]の[MATH]個を均一な重み[MATH]で線形結合した学習器を[MATH]とする．
この時，[MATH]と[MATH]，および[MATH]と[MATH]の二乗誤差の期待値には以下のような関係が成立する．
ただし[MATH]は，[MATH]の期待値を表現する．
式[REF_eq:w]より，多数決を行った学習器の二乗誤差の期待値のが，単独に学習した学習器の期待値より小さくなることが分かる．
ここでは，証明を簡単にするために均一な重みとしたが，不均一な重みの場合に対する一般化も可能である．
詳細については文献[CITE]を参照されたい．
この重み付き多数決の概念の一つとしてBoosting [CITE]があり，自然言語処理の多くのタスクに応用され高い精度を示している．
Chunk同定問題においても，重み付き多数決の手法が適用されている．
例えば，Tjong Kim Sangらは，base NP同定の問題に対し，弱学習アルゴリズムにMBL，ME，IGTree等の7種類のアルゴリズム，さらにIOB1/IOB2/IOE1/IOE2の4種類の表現を用いて独立に学習した複数のモデルの重み付き多数決を行うことで，個々のモデルのどれよりも高精度の結果が得られたと報告している[CITE]．
本稿では，弱学習アルゴリズムにSVMを用い，IOB1/IOB2/IOE1/IOE2の4種類の表現，さらに解析方向(前向き/後ろ向き)の合計[MATH]種類の重み付け多数決を行うことで精度向上を試みる．
IOB1/IOB2/IOE1/IOE2には，それぞれ次のような特徴がある．
IOB1/IOE2は，chunkが連続したときのみ他とは異なるタグ(B/E)が付与される．
つまり，chunkが連続するような事例に特化した学習が行われる．
また，IOB2/IOE2は，chunkの開始/終了位置に他とは違ったタグ(B/E)が付与される．
これらは，chunkの開始/終了位置に特化した学習が行われるさらに，chunk中の主辞(Head)となる単語は，chunkの成立に必要不可欠であるために，他の単語に比べ頻出し，同定が容易である．
主辞がchunkの先頭にある場合は，前向きに解析を行うことで，主辞が最初に決定され，その結果が後続するタグの素性に影響を及ぼすため，全体として高い精度が期待できる．
逆に，主辞がchunkの末尾にある場合は，後ろ向きに解析を行ったほうが高い精度が得られる．
前向き/後ろ向きとは，すべてのchunkの主辞が先頭/末尾にあると仮定し，それぞれの仮定に特化した学習手法である．
このように，chunkの表現方法，及び解析方向の異なる複数の学習器を作成することで，それぞれ視点の異なる複数の学習器が作成される．
一般に，複数の学習器の性質が異なれば異なるほど，多数決の結果の精度が高くなるために，単独のタグ表現方法，及び解析方向の手法より高い精度が期待できる．
重み付き多数決を行う場合，各モデルの重みをどう決定するかが問題となる．
真のテストデータに対する精度を用いることで良い結果を得ることができるが，一般に真のテストデータを評価することは不可能である．
Boostingでは学習データの頻度分布を変更しながら，各ラウンドにおける学習データに対する精度を重みとしている．
しかしながら，SVMは，Soft Marginパラメータ，Kernel関数の選択次第で，学習データを完全に分離することができ，単純に学習データに対する精度を重みにすることは困難である．
本稿では，重み付き多数決の重みとして以下の4種類の手法を提案し，それぞれの手法の精度や計算量などを考察する．
均一重み
これは，すべてのモデルに対し均一の重みを付与する手法である．
最も単純な手法であり，他の手法に対するベースラインとなる．
交差検定
学習データを[MATH]等分し，[MATH]を学習データ，残りの[MATH]をテストとして評価する．
この処理を[MATH]回行い，それぞれの精度の平均を各モデルの重みとして利用する．
VC bound
式([REF_eq:svm_gen])，式([REF_eq:teiri])を用いてVC boundを計算し，その値から正解率の下限を推定し，重みとする手法である．
ただし，式([REF_eq:teiri])における全事例を囲む最小直径[MATH]は各学習データから原点までのノルム最大値を用いて近似を行った．
Leave-One-Out(L-O-O) bound
式([REF_eq:loo])のLeave-One-Out boundを求め，正解率の下限を推定し，重みとする手法である．
実際の解析は以下のように行った．
学習データをIOB1/IOB2/IOE1/IOE2の各表現に変換する．
4つの表現に対し，前向き解析，後ろ向き解析の計[MATH]種類のモデルを作成し，SVMで独立に学習する．
8種類のモデルに対し，VC bound，Leave-One-Out boundを計算し重みを求める．
交差検定に関しては，(1)，(2)の処理を各分割したデータに対して行い，各ラウンドのタグ付け精度の平均を重みとする．
実際の実験では，交差検定における分割数[MATH]は，5とした．
合計8種類のモデルを用いて学習データとは別のテストデータを解析する．
個々の8種類のモデルが出力するchunkの表現は，それぞれ異なるため，そのままでは多数決を行うことができない．
多数決を行うためには，個々の結果を1つの統一表現に変換する必要がある．
この目的のために，解析後のデータをIOB1/IOB2/IOE1/IOE2の各表現に再び変換する．
IOB1/IOB2/IOE1/IOE2の個々に変換された結果に対し，タグレベルで合計8種類の重みつき多数決を行う．
つまり各重み付けの手法に対し，IOB1/IOB2/IOE1/IOE2の4種類の表現方法で評価した結果を得ることとなる．
最終的に，[MATH] (統一表現のタイプ) [MATH] [MATH] (重み付けの方法) [MATH]種類の結果を得ることとなる．
重み付き多数決の候補として，IOBES前向き解析とIOBES後向き解析の各モデルを参加させることは可能であるが，我々はそのような実験を行わなかった．
その理由として，推定すべきクラスの数がIOB1，IOB2，IOE1，IOE2モデルは3に対し，IOBESモデルは5と異なり，VC boundや，Leave-One-Out boundを同じ条件で比較することが困難なことが挙げられる．
IOBES前向き解析とIOBES後向き解析の各モデルの実験は，IOB1，IOB2，IOE1，IOE2の各モデルとの精度を比較するために行った．
