
評価実験の際に用いるデータや設定するパラメータについて述べる．
また提案法と同じく複数文を要約するシステムである従来研究を比較対象として説明する．
この節では実験で使用する用例データベース，パラメータの調整，テストに用いたデータについて述べる．
\ul{用例データベース}用例データベース内の用例には日経ニュースメールNikkei-goo\toolref{言語資源:nikkei-goo}から配信されているニュースの要約文を用いた．
このニュース要約文は人手で作成されているものであり，1999年12月から2007年12月までに収集した27036件を用いた．
1文あたりの平均形態素数は23.1形態素，平均文節数は6.6文節である．
\ul{パラメータ調整用のデータ}従来手法，提案法のシステムにおけるパラメータを調整するためのデータとして，日本経済新聞1999年のデータ121件を用いた．
またこの新聞データの日付やタイトル情報を利用し，用例データベースと同じ形式である日経ニュースメールNikkei-goo\toolref{言語資源:nikkei-goo}の中から記事タイトルと日付情報が一致したものを正解データとして利用した．
このパラメータ調整用のデータは入力ニュース記事1件に対して，正解要約文1件が1対1で存在する．
1記事当たりの平均文数は10.6文であり，1文当たりの平均形態素数は30.6形態素，平均文節数は9.6文節である．
\ul{テストデータ}日本経済新聞1998年のデータ200件を用いた．
このデータはパラメータ調整用のデータとは異なり，オープンテストである．
この200件のうち100件は1記事あたりの文数が3文以下で比較的短いものである．
また他の100件は1記事あたりの文数が4文以上10文以下の長めの記事である．
テストデータの詳細は表[REF_表:テストデータ詳細]に示す．
なおテストデータ200件には3人が独立で作成した正解データ（人手で作成した要約文）が存在する．
つまりこのデータは入力ニュース記事1件に対して，正解要約文3件が1対3で存在する．
要約文の自動評価ではこれら複数の正解データを使用して[REF_節:自動評価]節に示すBLEUとROUGEにて評価を行う．
提案法で使用するパラメータの調整方法について述べる．
調整するパラメータは文節の組合せ時のノードとエッジのバランスを取るためのものであり，式([REF_nodescore])と式([REF_edge])の[MATH]である．
このパラメータの調整には[REF_節:使用するデータ]節のパラメータ調整用のデータを用いた．
ここで対応文節の組合せを行うには，まず類似用例文を獲得し，文節の対応付けをしなくてはならない．
本論文では類似用例文の検索精度によらずパラメータを調整するため，正解データを類似用例文として使用した．
そして提案法により文節の対応付けを行い，パラメータを適宜変動させることで組合せを行った．
パラメータは式([REF_nodescore])と式([REF_edge])の[MATH]をそれぞれ0.1刻みで変動を行った．
調整用のデータ121件で出力された要約文と正解データでのBLEU値の合計が最も高かったパラメータをテストで用いる．
このBLEU値は要約や翻訳で用いられている自動評価手法であり，[REF_節:自動評価]節で詳細を述べる．
調整したパラメータを表[REF_表:提案法パラメータ]に示す．
提案法と同じく複数文の語を使用して1文に要約するHori [CITE]の手法を従来手法として挙げる．
この手法は複数文要約を複数の入力文中に含まれる単語列から部分単語列を抽出する問題として定式化することによって1文の要約文を作成している．
またこの要約手法は要約率を自由に設定することができる．
そこで本論文では従来手法が出力する要約文の形態素数と提案法が出力する要約文と形態素数を同じに設定して比較実験を行う．
本論文ではシステムが出力した要約文を評価するために2つの評価方法を用いた．
まず自動要約の分野でよく用いられている自動評価方法，もう1つは出力した要約文を人手で評価する方法である．
本論文ではBLEUスコア[CITE]とROUGEスコア[CITE]を用いて自動評価を行う．
これらは入力に対してあらかじめ作成した正解要約文とシステムが出力した要約文を比較し，正解にどれだけ近い文が得られたか評価することによってシステムの優劣を測るものである．
\ul{BLEUスコア} BLEUスコアは1 gramから4 gramまでの適合率の重み付き和で以下の式で定義され，複数の正解文にも対応した評価尺度である．
BLEU(sys,ref) & = \mathit{BP}\cdotexp\left(\sum_{n=1}^{4}\frac{1}{n}\log p_n\right)
p_n & = \frac^{M_n}_{j=1}\min(s^n_j,\max_{k=1,\dots,L}r_{jk}^n)^{M_n}_{j=1}s^n_j
式([REF_bleu2])の[MATH]はシステムが出力した要約文に含まれる[MATH]番目の[MATH]gramの出現数を表す([MATH])．
また[MATH]はその[MATH]番目の[MATH]gramが[MATH]番目の正解要約文に出現する数である([MATH])．
このとき[MATH]は正解要約文の数である．
よって[REF_節:パラメータの調整]節のパラメータの調整時は正解要約文が1つであるため[MATH]，テスト時には正解要約文が3つ存在するため[MATH]となる．
またこの評価式は適合率であるため，システムが出力した文が正解文に対してあまりにも短いと評価を不当に上げてしまう恐れがある．
そのためペナルティ([MATH])が導入されている．
しかし，要約文の評価では短い出力文である方が高圧縮であり一般的に良いとされ，この[MATH]は課さない場合が多い．
そのため本論文でも同様に[MATH]として評価を行う．
\ul{ROUGEスコア} ROUGEスコアは正解要約文とシステムが出力した要約文を比較して[MATH]gram再現率を算出することにより正解文にどの程度近いかを評価することができる．
以下にROUGEスコアの式を示す．
式([REF_rouge])の分母は正解要約文に含まれる[MATH]gram総数であり，分子はシステムと正解要約文で一致した[MATH]gramの総数である．
Lin [CITE]によると1 gram再現率または2 gram再現率を測ったときに人手の評価と相関の高い結果が得られたとしている．
つまりROUGE-1またはROUGE-2の場合である．
そのため本論文でもこのROUGEスコア(ROUGE-1及びROUGE-2)を用いて自動評価を行う．
なお[REF_節:使用するデータ]節で示したようにテストデータには1件の入力ニュース記事に対して3つの正解要約文が存在する．
そのためROUGE尺度による評価実験では入力ニュース記事1件に対して，システムが出力した要約文と正解要約文それぞれとでROUGE-1及びROUGE-2を算出し，最も高い値が得られたものを評価値として採用する．
人手による評価では，評価者3人が提案法と従来手法それぞれが出力した要約文を可読性と内容の適切性の2点について評価を行った．
\ul{可読性の評価}可読性の評価では評価者3人が独立にシステムが出力した要約文のみを読み，表[REF_表:可読性の評価]の指標に基づいて4段階評価を行った．
この評価ではシステムが作成した要約文が日本語として読み易いかを評価するものであり，値が小さいほど可読性は良い．
また評価者に与えた教示は以下の通りである．
システムが出力した要約文を読み，表[REF_表:可読性の評価]に基づいて，評価値を付与しなさい．
また，以下に示す例のように述語（「逮捕」及び「述べた」）はどちらも「ガ，デ，ヲ格」を取るが，例2の文は明らかに日本語として不適切な表現（述べた）があるため，4文節中1文節を変更しなくてはならない．
そのため例2の場合は評価2となる．
例1）新潟県警が/○○疑惑で/××容疑者を/\ul{逮捕。
}/例2）新潟県警が/○○疑惑で/××容疑者を/\ul{述べた。
}/
\ul{内容の適切性評価}内容の適切性評価では可読性の評価を行った同じ評価者3人が入力のニュース記事とシステムが出力した要約文を読んで，表[REF_表:内容の適切性評価]の指標に基づいて内容の適切性評価を行った．
この評価値は小さい方が内容の適切性は良い．
また評価者に与えた教示は以下の通りである．
システムが出力した要約文を読む前に，システムに入力した記事のみを読んで，自分が要約文に必要だと考える内容を考えなさい．
その後，自分が考えた内容とシステムが出力した要約文を比較して，表[REF_表:内容の適切性評価]に基づいて，評価値を付与しなさい．
