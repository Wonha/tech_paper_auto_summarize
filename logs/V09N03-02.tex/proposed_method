語句抽出
現在研究されている語句抽出システムは，ほとんどが対象を名詞に準じた単語列に限定したものである．
これは，抽出の対象となる語句は未知語や専門用語が主であり，どちらも名詞がその大半を占めるためである．
未知語，専門用語，固有名詞などはドメイン固有の語句と言ってよいが，ドメイン固有の語句となりうるのは新語や複合語がほとんどで，例えば助詞のように新語の出現しないものや活用語のようにドメインによってはほとんど新語がないものなどは抽出対象となりにくく，名詞に準じる語句を抽出対象とすることでかなりの未知語，専門用語などを取得することが可能である．
また，対象とする品詞を限定することで抽出処理に必要なルールが削減され，ノイズの軽減に繋がるという利点がある．
\subsection{名詞句抽出}
抽出対象は主に，名詞の推定と名詞句の推定とに二分されており，特に名詞の推定では専門用語など複合語の推定を行なうものと固有名詞などの未知語を認識するものに分けられる．

名詞句の推定を目的とした研究としては， Argamon らの提案したサブパターン概念を利用する手法\cite{argamon98}などが挙げられる．

専門用語などドメイン固有の語句の抽出では， tf・idf モデルなど語句の出現頻度を利用する手法と $n$-gram など文字の共起頻度を利用する手法がある．
湯本らは unigram および隣接 bigram の出現頻度を利用して複合名詞の認識を行なう手法を提案した\cite{yumoto01ipsjnl}．
湯本らの抽出対象は専門用語であったが，彼らは専門用語として複合名詞のみを考え，名詞のみに着目した隣接 bigram を利用して複合名詞の推定を行なった．
専門用語となる語句の大部分を複合名詞が占めること，頻出複合名詞の構成要素である名詞は共起頻度が高く，また特定の単語同士の並びが多いことを考えると，この手法は効率的だと言える．
Frantzi らは英語を対象として C-value を利用して複合語を認識する手法を提案し，さらにこれにコーパスサイズや重みなどを利用してランク付けを行なった\cite{frantzi97}．

Cha らは未登録語発見のための形態素パターン辞書を利用して未登録語の認識およびタグ付けを行なう手法を提案した\cite{cha98}．
固有名詞の認識では，最大エントロピー法を利用して固有名詞の切り出しを行なう Borthwick の手法\cite{borthwick99}や，固有名詞の前後に出現しやすい語をトリガーワードとして固有名詞の認識を行なう手法\cite{kitani94,hisamitsu97}，トリガーワードと構文解析情報を利用する福本らの手法\cite{fukumoto98}などがある．
また Cucerzan らは文字の並びの情報を利用して少ない訓練データからの固有名詞の推定を可能にする手法を提案し，ルーマニア語や英語など複数の言語に対して有効であることを示した\cite{cucerzan99}．
\subsection{文字単位の統計情報を利用した辞書未登録語抽出}
文字の共起情報を利用する手法としては， $n$-gram の画期的な抽出法を提案しこれを利用して文中の文字列の塊を認識する長尾らの手法\cite{nagao94} などが挙げられる．
中渡瀬らは $n$-gram 統計を利用して辞書未登録語を自動獲得する手法を提案した\cite{nakawatase98}．
中渡瀬らは任意の文字列の頻度を正規化する手法を提案し，これを用いて語の境界を決定することで，辞書未登録語を獲得している．
中渡瀬らはこの手法で漢字未登録語の 43\% の取得に成功したと報告している．
中渡瀬らの手法では評価対象を漢字未登録語に限定しているが，これは漢字とその他の字種の出現頻度分布が異なるためで，正規化を行なう中渡瀬らの手法では漢字での精度が高いためである．

延澤らは品詞タグや文法などに頼らず機械的に取得可能な文字間の統計情報のみを利用して文の切り分けを行なう手法を提案している\cite{nobesawa96coling}．
延澤らはこの手法を利用してドメイン固有の文字列の自動抽出を試みており，口語文章のような非文を多く含むコーパスに対しても有効であることを示した\cite{nobesawa00coling,nobesawa01anlp}他，固有名詞抽出など抽出対象を絞った場合などについても有効であるとしている\cite{nobesawa99}．
文字列の抽出はその後の利用を見込んだものであるが，延澤らの手法では文字単位での処理を行なっているため抽出される文字列は単語，複合語，言い回しなどサイズがさまざまである．
自然言語処理においては一般に単語または形態素が処理単位とされている．
単語は多義性を持つものも多く，単語が最適な処理単位と言えるかは疑問が残る\cite{fung98}．
その意味で，特定の処理単位を設定することは処理の精度に悪影響を与えている可能性もあるが，さまざまな処理単位を同時に扱う手法は確立されておらず，処理単位の特定が必要であるのが現状である．
このため，さまざまな処理単位の文字列が同時に出力とされる延澤らの手法を用いて出力された文字列は，そのままでは他のツールでの利用が困難である．
そこで本稿では，延澤らの手法の問題点を克服し，この手法を利用して辞書未登録語を抽出することで辞書ベースのツールの精度の向上を図る．
システム概要
本稿で提案するシステムは，対象ドメインのコーパスからシンプルな手法でドメイン固有の語句を抽出する延澤らの手法\cite{nobesawa01anlp}を応用したものであり，辞書ベースの自然言語処理ツールの支援を目的として 2 方向からのアプローチを試みる．
\subsection{システム概要}
本稿では，辞書ベースの形態素解析ツールに対して統計情報を利用することでその精度の向上を図るため，以下の 2 つのアプローチを試みた．
\begin{itemize}
\item システム M: 形態素解析ツールへの組み込みのための統計情報利用システム
\begin{quote}
形態素解析中に統計情報を利用してドメイン固有の語句を認識するシステムを形態素解析ツールに組み込むことで形態素解析時の誤解析を削減．
\end{quote}
\item システム D: 統計情報を利用した辞書の作成システム
\begin{quote}
形態素解析の前処理として対象ドメイン固有の文字列の辞書登録を行なうことで形態素解析時の誤解析を削減．
\end{quote}
\end{itemize}
どちらのアプローチも対象ドメインの訓練コーパスから得た統計情報を利用することで頻出文字列の認識を実現し，これに起因する解析誤りの削減を図るものである．

本稿では形態素解析ツールとして日本語形態素解析ツール茶筌 ver.2.2.3\cite{chasen}を採用した．
また，統計情報としては文字間の共起情報を採用した．
\subsection{共起関係抽出}
文字間の共起情報が頻出文字列認識に有用であるとの延澤らの主張\cite{nobesawa01anlp}に基づき，本稿では対象ドメイン固有の頻出文字列の抽出に利用する統計情報として，文字間の共起情報を採用した．
そこで，前処理として訓練コーパス中の各二文字ペアの共起頻度を数え上げる．

本システムは訓練コーパスに全く制限を設けない．
品詞情報などの付加情報を一切利用しないため，形態素解析や構文解析，タグ付けなども必要としない．

訓練コーパス中の文字共起頻度の数え上げには d-bigram 確率モデル\cite{tsutsumi93}を利用した．
d-bigram とは距離を考慮した bigram モデルであり， {\tt abbc} という文字列の場合，隣接する ({\tt a}, {\tt b}) などだけでなく {\tt a} と {\tt c} のように離れて出現する二文字の共起関係も取得する．
この例では {\tt a} と {\tt c} は距離 3 となり ({\tt a}, {\tt c}; 3) のように表される．
隣接 bigram では視野が非常に狭く文脈情報が利用できないという欠点があり，特に文字レベルでの利用はノイズが大きい．
これに対し， d-bigram モデルは距離の情報を保有することでこの問題に対処しており，例えば 3 単語の並び (trigram) も十分に評価できることが示されている\cite{tsutsumi96}．
さらに，同じ文中であっても離れて出現する文字同士は近接して出現する文字同士に比べて関係が薄いと考えることができる\cite{church89acl}という主張に基づき， d-bigram の取得，利用に際して距離の上限および距離の影響力を設定することが可能である．
システム M: 茶筌への組み込み
本稿で提案するシステムは，日本語を対象とした形態素解析ツール・茶筌に統計情報を利用した文字列抽出モジュール (システム M) を組み込むことで統計情報の活用を図るものである．
これは茶筌に特化した手法ではなく，茶筌本体の構造を改変するものではない．
\subsection{茶筌での統計情報の利用}
茶筌は辞書ベースの形態素解析ツールであり，文単位で処理を行なう．
図 \ref{fig:flo-o} に茶筌による形態素解析の流れを示す．
入力であるテストコーパスは一文ずつ処理され，形態素解析結果が出力される．
形態素解析処理においては，事前に準備された辞書を利用する．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=flo-o.eps,scale=0.21}
\caption{茶筌のみでの形態素解析}
\label{fig:flo-o}
\end{center}
\end{figure}

図 \ref{fig:flo-m} に本稿で提案する統計情報利用システムを茶筌に組み込んだ場合の形態素解析の流れを示す．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=flo-m.eps,scale=0.21}
\caption{システム M を組み込んだ茶筌による形態素解析}
\label{fig:flo-m}
\end{center}
\end{figure}
本稿で提案するシステムではまず茶筌に有繋文字列抽出モジュールを組み込むことにより文字列の認識を行ない (\ref{sec:ukninshiki} 節)，抽出された文字列に専用の品詞名を付けることで辞書の見出し語と同等に扱うことができるようにする (\ref{sec:ukriyou} 節)．

認識する文字列は延澤らの提案した有繋文字列\cite{nobesawa96coling}と呼ばれるもので，文字間の共起情報のみから一塊と推測された文字列である．
本システムを組み込むことで，茶筌の持つ辞書の他に，その文中に含まれる有繋文字列を形態素の候補として利用することが可能となる．
辞書に掲載されている語句が有繋文字列として抽出された場合は，辞書の情報を優先する．
従って，辞書既登録語句は有繋文字列として抽出されることはない．
\subsection{形態素解析時における有繋文字列の認識}
\label{sec:ukninshiki}
文中の $i$ 番目の文字と $i+1$ 番目の文字の間の有繋評価値 $\uk(i)$ の算出式を式 (\ref{exp:uk}) に示す\cite{nobesawa96coling}．
ただし， $w_i$ は文 $w$ の $i$ 番目の文字， $d$ は 2 文字間の距離， $d_{max}$ は $d$ の最大値， $g(d)$ は距離の影響に対する重み付け関数であり，本稿では $d_{max} = 5$， $g(d) = d^{-2}$ とした\cite{sano96}．
\begin{eqnarray}\label{exp:uk}
\uk(i) = \sum_{d=1}^{d_{max}}\sum_{j=i-(d-1)}^{i} \mi_d(w_j,w_{(j+d)};d) \times g(d)
\end{eqnarray}
また， 2 文字間の相互情報量の計算式を d-bigram に対応するよう拡張したものとして式 (\ref{exp:mid}) を利用した\cite{nobesawa96coling}．
ただし， $x$, $y$ は各文字， $d$ は 2 文字間の距離， $P(x)$ は文字 $x$ が出現する確率， $P(x,y;d)$ は d-bigram ($x$, $y$; $d$) が起こる確率とする．
\begin{eqnarray}\label{exp:mid}
\mi_d(x,y;d) = log_2\frac{P(x,y;d)}{P(x)P(y)}
\end{eqnarray}

図 \ref{fig:mountain-valley} に有繋評価値を利用した文字列認識の例を示す\cite{nobesawa96coling}．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=mountain-valley-half.eps,scale=0.21}
\caption{有繋評価値を利用した文字列認識}
\label{fig:mountain-valley}
\end{center}
\end{figure}
図の横軸が入力文，縦軸が有繋評価値を示す．
横軸のアルファベットは入力文中の各文字を示す．
文中の各隣接文字ペア間の有繋評価値は，隣接文字ペアの共起頻度が高いほど高くなる．
従って図の中で評価値を繋いだ線が山状になっている部分は共起する可能性の高い部分であり，一塊の文字列である可能性が高い．
これに着目し山状の部分を抽出することで，文中の文字列の認識を行なう．
\subsection{形態素解析時における有繋文字列の利用}
\label{sec:ukriyou}
形態素解析中 \ref{sec:ukninshiki} 節の手法で認識された有繋文字列は専用の品詞およびコストが設定され既存の辞書の登録語と同等として形態素解析処理に利用される．
有繋文字列は特定の品詞に対応するものではないが，個々の有繋文字列に対してその品詞の推定を行なうことはシステムの実時間性を損ねるため，品詞「有繋文字列」を新設しこれに対して予め品詞情報を設定しておく．
実際に認識される文字列は名詞またはそれに準じるものがほとんどであるため，品詞「有繋文字列」の接続はすべて名詞接続とした．

茶筌では各語句に形態素コストが設定されている．
有繋文字列は文字間の共起情報によって決定するものであり，一塊の文字列であると評価する際の評価値の高さがそれぞれ異なる．
そこで，評価値によって有繋文字列を 5 段階に分類し，段階ごとに形態素コストを設定することで，評価値の高いものを優先的に利用できるように設定する．
\subsection{統計情報を組み込んだ茶筌による形態素解析例}
図 \ref{fig:ex1newspaper} に，本システムを茶筌に組み込んだ場合の実行例を挙げる．
図 \ref{fig:ex1newspaper} の上段が茶筌のみで解析を行なった場合，下段がシステム M を組み込んで解析を行なった場合の切り分け結果である．
下線は辞書未登録語を，太字は有繋文字列として抽出された部分を示す．
\begin{figure}[hbt]
\begin{center}
\begin{tabular}{ll}
茶筌のみ  & 自然 / 言語 / 処理 / の / 分野 / で / は /\\
          & 統計 / 情報 / として / \underline{bigram} / な / ど /\\
          & n / - / \underline{gram} / が / よく / 用い / られる / ．\\
&\\
本システム& {\gt 自然言語} / 処理 / の / 分野 / で / は /\\
          & {\gt 統計情報} / として / {\bf bigram} / など /\\
          & {\bf n-gram} / が / よく / 用い / られる / ．\\
\end{tabular}
\caption{本システムによる切り分けの例}
\label{fig:ex1newspaper}
\end{center}
\end{figure}
図 \ref{fig:ex1newspaper} では辞書未登録語 2 文字列が本システムを利用することで有繋文字列として抽出されている．
「 bigram 」のように辞書未登録語がそのままの形で一語である場合，この部分の切り分け結果は正解と変わらないため他の部分の解析結果への影響がない場合が多いが，図 \ref{fig:ex1newspaper} の例のように他の部分へ影響を与える場合もある．
この例では本システムを利用し「 bigram 」の品詞が「有繋文字列」となったことで「など」が正しく認識されている．
「 n-gram 」は茶筌のみを利用した場合「 n (記号)」「 - (記号)」「 gram (未知語)」に分割された．
複数の字種から成る未知語の場合は字種ごとに区切られる場合がほとんどである．
システム M では字種情報を利用せずすべての字種の文字を同様に扱うため，字種の替わり目で誤分割されず，「 n-gram 」の認識に成功した．

またシステム M を利用することで「自然言語」「統計情報」などの複合語も多く認識された．
「自然言語」は，複合語「自然言語処理」の一部分であるが，「自然言語」自体一塊で複合語を形成し「自然言語処理」の構成要素となると考えられる．
システム D: 辞書への組み込み
訓練コーパスから取得した共起情報をそのまま利用する手法ではノイズの問題が防げない．
この問題を解決するため，本章では共起情報をそのまま利用するのではなく，共起情報を利用して辞書登録候補文字列を抽出しこれを事前に辞書に登録する手法を提案する．

図 \ref{fig:flo-d} に本章で提案する辞書作成システムを利用して事前に作成した有繋文字列辞書を茶筌の辞書に組み込んだ場合による形態素解析の流れを示す．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=flo-d.eps,scale=0.21}
\caption{システム D を利用した茶筌による形態素解析}
\label{fig:flo-d}
\end{center}
\end{figure}
基本的な流れは図 \ref{fig:flo-o} と同じだが，利用する辞書は茶筌の基本辞書に有繋文字列辞書を組み込んだものとなっている．
この有繋文字列辞書は訓練コーパスから作成したものであり，この辞書を組み込むことによってドメイン固有の文字列を形態素解析処理で利用する．

本章で作成する辞書は茶筌の辞書の補完という位置付けであり，辞書既登録語は登録しない．
また，茶筌が元々持つ辞書の改変を行なうこともない．
\subsection{登録文字列の属性設定}
\label{sec:morphcost}
辞書登録文字列の属性は以下のように決定する．
\paragraph{品詞}
登録文字列それぞれに対して適切な品詞を人手で設定することは多大な労力を必要とするだけでなく，その適切さの評価や曖昧性の問題などが存在するため，本稿では登録文字列はすべて同じ品詞とした．
登録文字列に割り振る品詞として「有繋文字列」を新設した．
品詞「有繋文字列」と他の品詞との接続コストの設定は茶筌の既存の品詞「名詞」中の「一般」カテゴリに準拠することとした．
\paragraph{形態素コスト}
個々の登録文字列の形態素コストはその文字列の頻度情報などの情報に基づいて個々に設定することとする．
この関数で利用するパラメータは，本稿で提案する複数の有繋文字列辞書作成手法に依存するものとする．

形態素コスト $c_i$ の算出式を式 (\ref{exp:morphcost}) に示す．
ここで $i$ は文字列， $x_i$ は文字列 $i$ の情報を示す値であり， $x_i$ に適用する値を変化させることで各辞書の特徴を形態素コストに反映させる．
\begin{eqnarray}
\label{exp:morphcost}
c_i = \left[-\frac{c_{max}}{x_{max} - 1} \times x_i + \frac{c_{max} \times x_{max}}{x_{max} - x_{min}}\right]
\end{eqnarray}
形態素コストは「コーパス内に 1 回出現する文字列の形態素コストを 4,000 とする」とする茶筌の定義に基づき，下限 $c_{min}$ を 0，上限 $c_{max}$ を 4,000 または 8,000 とする．
$x_{min}$ および $x_{max}$ は各辞書で利用する $x_i$ によって決まる．
\subsection{登録文字列の選択}
\label{sec:jisho}
本稿では辞書に登録する文字列の選択手法を 4 種類用意し， 4 つの辞書を作成した (表 \ref{tab:jisho})．
\begin{table}[ht]
\begin{small}
\begin{center}
\caption{有繋文字列登録のための辞書一覧}
\label{tab:jisho}
\begin{tabular}{l|lll}
&作成基準            &選別&コスト計算の基準\\
\hline
辞書 \idl&一人の評価による選別&人手&文字列出現頻度\\
辞書 \chk&複数人による評価得点&人手&評価得点\\
辞書 \frq&出現頻度            &自動&文字列出現頻度\\
辞書 \pos&品詞情報            &自動&品詞出現頻度\\
\end{tabular}
\end{center}
\end{small}
\end{table}
本稿では辞書登録の対象を名詞に準じる文字列に絞る．
\subsubsection{辞書 \idl: 一人の評価による登録文字列選択}
\label{sec:whatisidl}
訓練コーパスから抽出された有繋文字列を一人の手によってすべての候補をチェックし，
そのままで辞書登録可能な有繋文字列，過接合有繋文字列から適切な部分を切り出した文字列の 2 種類の文字列を選択した．
過分割有繋文字列については，分割され削除されていた部分が容易に推測できる場合であっても，登録文字列としなかった．
登録文字列の切り出しの対象は，名詞，複合名詞，数式，数値 (単位も含む)，意味のある記号の羅列，英単語の羅列とし，
茶筌既登録語は登録文字列から除外した．

各登録文字列 $i$ の形態素コスト $c_i$ は，式 (\ref{exp:morphcost}) に $x_i$ として $i$ の候補文字列としての出現頻度 $f_i$ を適用して算出した．
他の選択手法と異なり完全に人手で確認しているためノイズの心配がないことから， $c_{max}$ は 4,000 とした．

ユーザ個人が一人で選択する場合，登録語とする基準をユーザ個人で設定できるため，複数人で選別を行なう場合のようなばらつきや基準の統一といった問題がない．
しかし，ドメインが大きくなれば登録候補語も増加するため，一個人がこの選別を行なうことは大きな労力となる．
\subsubsection{辞書 \chk: 複数人による評価を利用した登録文字列登録}
\label{sec:whatischk}
11 名の被験者に登録候補文字列のリストを提示し，登録すべきもの，登録すべきか迷うもの，登録すべきでないもの，判断できないものの  4 段階に分類してもらい，それぞれ 2，1，0 点として集計を行なった．
「判断不能」は評価から外すものとした．
評価得点が 0 となった文字列は，被験者全員が登録すべきでないと判断したものであるため，登録候補としない．

各登録文字列 $i$ の形態素コスト $c_i$ は，式 (\ref{exp:morphcost}) に $x_i$ として $i$ の評価得点 $s_i$， $c_{max} = 8,000$ を適用して算出した．

対象ドメインに詳しい複数の人間が選別を行なうことで，一人一人の労力の軽減が図れるだけでなく，適切な候補語選択がなされると考えられる．
しかし選択を行なう人の専門分野や考え方などの相違から，候補語の絞り込みが難しくなる場合もあり得る．
\subsubsection{辞書 \frq: 出現頻度による登録文字列選択}
\label{sec:whatisfrq}
自動的に選別を行なう場合の最もシンプルな手法は，登録候補になんらかの順位付けを行ないそれに従って登録文字列を決定するものである．
評価値は共起情報を基に算出するため，出現頻度の高い文字列は評価値も高くなる傾向があり，この二点は独立ではない．
従って，本稿では出現頻度のみを基準として辞書登録文字列の選択および形態素コストの設定を行なう．

各登録文字列 $i$ の形態素コスト $c_i$ は，式 (\ref{exp:morphcost}) に $x_i$ として $i$ の候補文字列としての出現頻度 $f_i$， $c_{max} = 8,000$ を適用して算出した．
出現頻度が 1 の文字列はノイズである可能性があるため登録文字列から外し，出現頻度 2 の時形態素コストは最大の 8,000 を採るように設定した．
\subsubsection{辞書 \pos: 品詞情報による登録文字列選択}
\label{sec:whatispos}
登録候補文字列を茶筌に掛けて形態素解析を施し，得られた品詞情報を利用して登録文字列を決定する．
各登録文字列 $i$ の形態素コスト $c_i$ は，式 (\ref{exp:morphcost}) に $x_i$ として $i$ に対応する品詞列の候補文字列としての出現頻度 $t_i$， $c_{max} = 8,000$ を適用して算出した．
システム M+D: 辞書登録と切り分け処理の併用
処理の段階で動的に有繋文字列を認識し利用するシステム M では，ノイズを完全に防ぐことは不可能である．
ノイズを抑えるためには，動的な処理でなく，事前に必要な有繋文字列を辞書登録してしまう方法が有効である．
辞書登録を行なうことでドメイン固有の文字列を辞書に反映させることが可能となるが，完全な辞書の作成は不可能であるという辞書ベースの手法の問題点の完全な解決にはならない．
また本稿で利用する d-bigram 確率モデルは bigram 情報の積み重ねであるため特に複合語やこれに類するものの認識において間に入る語句を柔軟に扱えるという利点があるが，辞書登録では d-bigram の持つ柔軟性が失われる．
これらの問題を解決するために，辞書登録と切り分け処理の併用が考えられる．
事前にドメイン固有文字列の辞書登録を行ない，さらに補助として組み込みの切り分けシステムを利用することで，頻出語句の認識が可能な上，ノイズの減少を図ることが可能となる．
図 \ref{fig:flo-md} に本稿で提案する統計情報利用システムと辞書作成システムを利用した有繋文字列辞書の両方を茶筌に組み込んだ場合による形態素解析の流れを示す．
\begin{figure}[hbt]
\begin{center}
\epsfile{file=flo-md.eps,scale=0.21}
\caption{システム M，D 両方を組み込んだ茶筌による形態素解析}
\label{fig:flo-md}
\end{center}
\end{figure}

表 \ref{tab:threshold} にシステム M を組み込んだ実験での閾値ごとの形態素コストを示す．
\begin{table}[hbt]
\begin{center}
\caption{システム M での閾値および形態素コスト}
\label{tab:threshold}
\begin{small}
\begin{tabular}{r|rr}
閾値&\multicolumn{2}{c}{形態素コスト}\\
    &実験 M & 実験 M+D \\
\hline
\hline
11& 1,000&10,000\\
 9& 3,000&15,000\\
 7& 4,000&20,000\\
 5& 5,000&25,000\\
 3&20,000&30,000\\
\end{tabular}
\end{small}
\end{center}
\end{table}
実験  M+D ではシステム  M をシステム D の補完の立場で利用するため実験 M に比べて形態素コストを大きく設定している．
