\section{実験}
\label{sec:experiments}

\subsection{実験概要}
\label{sec:overview}

\subsubsection*{実験事項}

三つの実験を行った．実験1では，定性的な評価として，種々の形態素解析シ
ステムの解析結果，および，人手修正されたコーパスについて尺度$L$を適用
し，目視により適用結果を評価した．実験2では，訓練コーパスのサイズを変
えたときの，尺度$L$の過分割検出精度を定量的に評価した．実験3では，五つ
の尺度(尺度$L$/相互情報量/尤度比/改良Dice係数\cite{kitamura97}/Yates補
正された$\chi^2$)について過分割の検出精度を定量的に比較した．

\subsubsection*{確率推定の際の設定}

\paragraph{教師なし学習/教師あり学習}
尺度$L$を求めるためには，(\ref{eq:n})式の確率を求める必要があるので，
形態素に分割された訓練コーパスが必要である．そのようなコーパスとしては，
形態素解析システムにより分割されたコーパスをそのまま用いる場合(教師な
し学習)と，形態素解析結果の誤りを人手で修正したコーパスを用いる場合(教
師あり学習)の二通りが考えられる．そのため，実験1,2,3では，この二つの場
合について，尺度$L$の過分割検出精度などを調べた．

\paragraph{パラメータ推定法} (\ref{eq:n})式の確率を求めるには，$n$を設
定し，かつ，確率推定法も適当に決める必要がある．そのために，本稿では，
実験1と実験2においては，n-gram確率推定のためのツールとして広く使われて
いるCMU-Cambridge Toolkit\cite{clarkson97}を用いて，$n=3$の場合につい
て，バックオフスムージングにより推定した．このときのディスカウント法は
Witten-Bell discounting\cite{placeway93}を用い，カットオフは，文字バイ
グラムと文字トライグラムの双方で1とした\footnote{CUM-Cambridge Toolkit
  は，与えられたn-gramである$c_{i-n+1},\ldots,c_{i}$の頻度が0のとき，
  そのn-gram確率$\Pr(c_i | c_{i-n+1},\ldots,c_{i-1})$を(n-1)-gram確率
  $\Pr(c_i | c_{i-n+2},\ldots,c_{i-1})$から推定する．これをバックオフ
  スムージングという\cite{Kita96}．バックオフスムージングには，種々の
  方法があるが，それらは，ディスカウントといって，頻度が0より大きい
  n-gramの頻度から幾らか割引いて，割引いた分を頻度が0のn-gramに分け与
  える方法により特徴付けられる(これにより頻度が0のn-gramの確率が0より
  大きくなる)．そのディスカウントの一手法が Witten-Bell discounting で
  ある．また，カットオフとは，ある値$x$以下の頻度で生起したn-gramの頻
  度を0として確率を計算する場合の$x$のことである．カットオフ以下の頻度
  のn-gramは，頻度が0として扱われるが，バックオフスムージングにより0よ
  り大きい確率が付与される．}．一方，実験3においては，最尤推定により求
めた確率により尺度$L$を計算した．その理由は，尺度$L$以外の尺度において
は，通常，最尤推定を用いて，確率を計算しているので，それに合せるためで
ある．また，比較を簡単にするために，$n=2$の場合について各種の尺度を比
較した．

\subsubsection*{コーパス}

実験1,2,3で共通に用いるコーパスは京都大学テキストコーパス
version2.0\cite{kurohashi98}である．京都大学テキストコーパスは，CD-毎
日新聞95年度版から約2万文を抽出したものであり，形態素・構文解析されて
いる．このコーパスを均等に2分割し，実験に用いた．以下では，その一方を
京大コーパスAと呼び，他方を京大コーパスBと呼ぶ．京大コーパスAは主に確
率推定のための訓練コーパスとして用い，京大コーパスBは主に過分割の検出
精度を評価するためのテストコーパスとして用いた．

\subsection{実験1：目視による尺度$L$の評価}
\label{sec:look}

実験1では，定性的な評価として，種々の形態素解析システムの解析結果，お
よび，人手修正されたコーパスについて尺度$L$を適用し，目視により適用結
果を評価した．

\subsubsection*{実験材料：コーパスと形態素解析システム}

\paragraph{教師なし学習の場合}

教師なし学習では，確率推定用の訓練コーパスと過分割検出用のテストコーパ
スとが同一である．つまり，確率を推定したコーパス中における過分割を検出
する．

このときのコーパスとしては，京大コーパスBとEDR日本語コーパスversion
1.5\cite{edr95}の全文を用いた．なお，EDR日本語コーパスは，新聞・雑誌・
辞典などの流通文書から1文単位でとられた約21万文からなるコーパスであり，
各文は，形態素・構文・意味解析されている．

これらのコーパスにおける生の文を分割する形態素解析システムとしては，公
開されている形態素解析システムのうちから，JUMAN version
3.5\cite{kurohashi97}，茶筅version 1.51\cite{matsumoto97}, すもも
version 1.3\cite{washizaka97}を用いた．これらの形態素解析システムは，
全て，規則に基づいて形態素解析をするものである．なお，これらの形態素解
析システムを用いるときには，ただ一つの(ベストの)解析結果を出力させた．

これらのコーパスと形態素解析システムとの組み合わせは，EDRコーパスに対
しては，三つの形態素解析システム全てを適用したが，京大コーパスBについ
ては，JUMANのみを適用した\footnote{こうした理由は，京大コーパスは主に
  実験2,3における定量的な評価に使用することを意図したものであり，実験1
  では，EDRコーパスを主な対象としたからである．}．また，二つのコーパス
の元々の分割(人手修正済みの分割)についても試した．すなわち，全部で6種
の形態素分割に対して尺度$L$を適用した．

\paragraph{教師あり学習の場合}

教師あり学習では，確率推定用の訓練コーパスと過分割検出用のテストコーパ
スとが異なる．

実験1では，京大コーパスAの元々の分割(JUMANの解析結果を人手修正したもの)
を訓練データとして(\ref{eq:n})式の確率を推定した．そして，その推定値を
利用して，JUMANにより形態素解析された京大コーパスBに対して尺度$L$を適
用した．

\subsubsection*{実験方法}

7種(=教師なし6種+教師あり1種)の形態素分割のそれぞれに対して，その全て
の分割点について，前後の形態素から尺度$L$を計算した．たとえば，「休/憩
室/は/広い/。」のように分割されている文については，四つの分割点におい
て，それぞれ，$L(休,憩室)$，$L(憩室,は)$，$L(は,広い)$，$L(広い,。)$を
計算した．このとき，(\ref{eq:n})式の確率は，\ref{sec:overview}節で述べ
たように，$n=3$としてバックオフスムージングにより計算した．

\subsubsection*{実験結果}

7種の形態素分割のそれぞれに対して，全ての分割点を尺度$L$により降順に
(同一尺度値の場合はランダムにtieを解消して)ソートし，その上位から異な
り150個を選んだ\footnote{たとえば，「休/憩室/は/広い/。」と「休/憩室/
  は/狭い/。」という文があるとき，それぞれの文について，$L(休,憩室)$が
  求まるが，この二つの$L$は同じ文字列を同じように分割しているので，異
  なりとしては一つである．ただし，二つの分割点を比べたとき，分割点の前
  後の形態素の字面は同じであっても，品詞が異なる場合には異なる分割点と
  して扱った．すなわち，たとえば，$L(A,B)$と$L(a,b)$という二つの分割点
  があり，字面上は，$A=a$,$B=b$であったとしても，$A$と$a$の品詞が異な
  るか，$B$と$b$の品詞が異なる場合には，それらの分割点は，異なるものと
  して扱った．そうした理由は，尺度$L$が検出できるのは過分割だけであっ
  ても，我々が実際に興味があるのは品詞付けの誤りを含めた形態素解析結果
  の誤りだからである．}．そして，それぞれの異なりについて，一個の分割
点を無作為に抽出し，それが過分割であるかを判定した\footnote{過分割かど
  うかの判定，すなわち，形態素解析システムによる分割点が実際に切って良
  いかどうかの判定は，複合名詞について困難であるが，もしも，形態素への
  分割の結果として生じる括弧付けが，筆者の内省に基づいた括弧付けと交差
  する(cross bracketing)なら，その分割点による分割は誤り(過分割)とする．
  たとえば，「東南アジアツアー」は，筆者の内省によれば(((東南)アジア)
  ツアー)という構造をしているので，「東南/アジアツアー」という分割は過
  分割とする．なぜなら，この分割では，((東南) (アジアツアー))という構
  造になるので，括弧が交差するからである．一方，「東南アジア/ツアー」
  は((東南アジア)ツアー)という構造なので正解とする．なお，分割の正誤の
  判定が困難なものについては，品詞を参照し，もし品詞が誤っていたら分割
  も誤りとした．ただし，上位異なり150個については，付録の表
  \ref{tab:edr}から表\ref{tab:cv}にある例と同様に，形態素の途中で分割
  されているものがほとんどであるので，分割の正誤の判定に迷うような例は
  少ない．}．なお，判定は筆者による．

\begin{table}[htbp]
  \begin{center}
    \caption{上位異なり150分割点における過分割の数}
    \begin{tabular}{|l|l|l|c|}
      \hline
      学習方法 & コーパス         & 解析システム     & 過分割の数 \\
      \hline
      \hline
      教師なし & EDR              & 人手(元々の分割) & 43 \\
      &                           & JUMAN            & 126\\
      &                           & 茶筌             & 128\\
      &                           & すもも           & 125\\
      \cline{2-4}
      &          京大コーパスB    & 人手(元々の分割) & 49\\
      &                           & JUMAN            & 98\\
      \hline
      教師あり & 京大コーパスB    & JUMAN            & 125\\
      \hline
    \end{tabular}
    \label{tab:errs}
  \end{center}
\end{table}

判定した150個の分割点について，それが実際に過分割であった数を表
\ref{tab:errs}に示す．表から分かるように，これら150個の中に過分割が占
める割合は非常に高い．たとえば，表\ref{tab:errs}では，茶筅には128個の
過分割がある．一方，平均的には，茶筌の分割が過分割であるのは，1.5％以
下であると言ってよい\footnote{\cite{fuchi98}によると，茶筌のEDRコーパ
  スにおける形態素解析結果の適合率(=100$\times$(茶筌の形態素解析結果の
  形態素で正解と一致したものの数/茶筌の形態素解析結果の形態素の総数))
  は，字面が一致していた場合を一致とすると，98.5％である．ここで，形態
  素の適合率は，実験2でも示すように，分割点の適合率よりも低くなる．な
  ぜなら，形態素が一致するためには，その前後の分割点も一致しなくてはな
  らないため，形態素が一致するというのは，分割点の一致よりも厳しい条件
  であるからである．そのため，分割点が過分割であるのは1.5％以下と言っ
  て良い．}．つまり，茶筅の150個の分割点のうちで，過分割は，平均的には，
$150 \times 0.015=2.25$個以下である．よって，茶筅の解析結果から128個の
過分割を検出するためには，平均的には，$(128/2.25) \times 150 \simeq
8533$個以上の分割点を調べなければならないことになる．同様なことが，他
の形態素解析システムによる分割結果，あるいは，人手で修正された分割結果
についても言える．これより，尺度$L$を用いることにより，形態素解析結果
から過分割を効率的に抽出できるといえる．なお，表\ref{tab:errs}において，
JUMANで解析された京大コーパスBからの過分割検出結果について，教師なし学
習の場合と教師あり学習の場合とを比べると，教師あり学習の方が検出個数が
多い．これは，教師あり学習の方が，(\ref{eq:n})式の値を正確に推定できる
からであると解釈できる．

さらに，教師なし学習の場合の6種の形態素分割のそれぞれについて，上位異
なり150個中の過分割から上位12個の過分割を付録の表\ref{tab:edr}と表
\ref{tab:kyoto2.0}に示す．表で「数」とある欄には，そのような過分割を含
む文の数がある．また，「形態素/品詞」とある二つの欄は，尺度$L$を計算し
た分割点の前後の形態素と品詞を示す．なお，品詞は，それぞれの形態素解析
システムの品詞である．また，表の解析結果は，各解析システムが一つだけ解
析結果を出力した場合のものである．もし，複数の解析結果も出力するように
すれば，表中の文について，当該の形態素解析システムが正解を含む解を出す
ことはある．

これらの表に示されている過分割の中には，何らかの規則性があるとすぐに分
るものもある．たとえば，EDRコーパスの元々の分割に含まれる過分割(表
\ref{tab:edr})においては，「引き下げ/よう」が「引き下/げ/よう」と分割
されていたり，「掲げ/、」が「掲/げ/、」のように分割されるなど，動詞の
語幹が分割される例が大半である\footnote{EDRコーパスの元々の分割におい
  ては，「掲げ、」という文字列を含む文が14例あるが，そのうち表
  \ref{tab:edr}の1例のみが，「掲/げ/、」という分割であり，その他の13例
  は，「掲げ/、」という分割である．このことは「掲/げ/、」が過分割であ
  ることを傍証している．これと同様なことが，表\ref{tab:edr}のその他の
  例についても言える．なお，「掲げ/、」という分割を含む例には，「虹/を
  /描/い/た/旗/を/掲げ/、/高らか/に/歌/う/。」や「独特/の/理想/を/掲げ
  /、/実行/し/た/人/だっ/た/。」のような例がある．}．一方，EDRコーパス
に対する茶筌の過分割では，「結果」が「結(普通名詞) /果(普通名詞)」と分
割されていたり，「考えて」が「考(普通名詞)/えて(普通名詞)」と分割され
ているが，このような例に含まれる規則性は，もしあったとしても，容易には
分らない．

いずれにしろ，尺度$L$を使うことにより，ある程度の量の，形態素解析結果
の過分割が，教師なし学習により容易に抽出できることが分かる．このような
例を集めるのは人手では手間が掛る．また，尺度$L$は人手修正後のコーパス
に残る過分割も検出できるため，コーパス作成・整備の際の補助ツールとして
も役立つと考える．

また，付録の表\ref{tab:cv}には，教師あり学習の場合について，尺度$L$の
値が上位12個の過分割を示す．ここで，教師あり学習の結果である表
\ref{tab:cv}におけるJUMANの過分割と，教師なし学習の結果である表
\ref{tab:kyoto2.0}におけるJUMANの過分割とを比べると，表\ref{tab:cv}に
おいては「護/煕」など固有名詞が占める割合が多いが，表
\ref{tab:kyoto2.0}では固有名詞は一つ(「若/乃/花」)しか存在しないことが
わかる．表\ref{tab:kyoto2.0}に固有名詞が少ないのは，固有名詞は未知語で
ある場合が他の品詞と比べて多いため，常に過分割される場合も多くなり，そ
の結果として尺度$L$の値が小さくなる場合が多いためである．このように，
形態素解析システムが常に過分割してしまうような場合を検出するためには，
人手修正済みコーパスが必要であると言える．

\subsection{実験2：過分割検出精度の定量的評価}
\label{sec:size}

教師なし学習により何か統計的に興味のある言語現象を発見するような応用
\cite[など]{shiNnou95,ikehara95,shimohata95,hisamitsu97}においては，新
聞記事などの大規模なコーパスが比較的用意に入手できるので，訓練コーパス
のサイズは深刻な問題ではない．これは本稿における過分割検出の場合でも同
様である．しかし，教師あり学習の場合には，訓練コーパスを構築するのはコ
ストが掛るため，なるべく小さな訓練コーパスであることが望ましい．そこで，
実験2では，主に教師あり学習の場合を対象として，訓練コーパスのサイズと
過分割検出精度との関係を調べた．ただし，教師なし学習の場合についても，
教師あり学習と比較するために，訓練コーパスのサイズと過分割検出精度との
関係を同様に調べた．

\subsubsection*{実験材料：コーパス}

確率推定用の訓練コーパスとしては京大コーパスAを用い，過分割検出の精度
を調べるテストコーパスとしてはJUMANにより分割された京大コーパスBを用い
た．このことは，教師あり学習と教師なし学習とで共通である．ただし，教師
あり学習では京大コーパスAの元々の分割から(\ref{eq:n})式の確率を推定し，
教師なし学習では，京大コーパスAをJUMANにより形態素解析した結果から
(\ref{eq:n})式の確率を推定した\footnote{教師なし学習において，京大コー
  パスAを訓練コーパスにした場合と，京大コーパスBをJUMANにより形態素解
  析した結果を訓練コーパスとした(訓練コーパスとテストコーパスが同一の)
  場合とでは，(後述する(\ref{eq:examination})式で定義する分割点調査率の
  意味における)過分割検出精度は，ほぼ等しい．}．

\paragraph{テストコーパスの各種統計}

テストコーパスである京大コーパスBについて，その元々の分割を正解と看倣
して\footnote{実験1で見付けた過分割についても修正はしていない．}，分割
の正誤を判定したときの統計を表\ref{tab:stat}に示す．

\begin{table}[htbp]
  \begin{center}
    \caption{京大コーパスBにおける分割点についての統計}
    \begin{tabular}{|lr|}
      \hline
      正解における分割点の数 & 232572 \\
      JUMANによる分割点の数 & 233048 \\
      一致した分割点の数 & 231816 \\
      分割点の再現率 & 99.7％ \\
      分割点の適合率 & 99.5％ \\
      \hline
      過分割の数 &  1232 \\
      分割不足の数 & 756 \\
      分割の間違いの数(過分割の数+分割不足の数) & 1988\\
      100$\times$(過分割の数/分割の間違いの数) & 62.0％ \\
      100$\times$(過分割の数/JUMANによる分割点の数) & 0.5％ \\
      \hline
    \end{tabular}
    \label{tab:stat}
  \end{center}
\end{table}

表\ref{tab:stat}より，分割の間違いに占める過分割は62.0％である．加えて，
過分割の周辺には分割不足も起りやすいと言えるので，過分割が検出できれば，
その周囲も調べることにより，分割誤りの多くが検出できると言える．

しかし，分割点の再現率(=100$\times$(一致した分割点の数/正解における分
割点の数))と適合率(=100$\times$(一致した分割点の数/JUMANによる分割点の
数))は，それぞれ，99.7％，99.5％と非常に高い\footnote{参考のため，
  \cite{nagata94}の基準による，形態素の再現率(=100$\times$(一致した形
  態素の数/正解における形態素の数))と適合率(=100$\times$(一致した形態
  素の数/JUMANによる形態素の数))を求めると，それぞれ，99.1％と98.9％に
  なる(ただし，字面が一致していれば形態素が一致したと看倣す)．これらか
  らも分るように，形態素の再現率と適合率とは分割点のものに比べて低い．}．
また，JUMANの分割点全体の中で過分割である分割点は0.5％(=100％$-$適合率)
であるので，過分割を見付けるのは人手では困難であると考える．

\subsubsection*{実験方法}

約1万文からなる訓練コーパスから，約1000，2000, ..., 10000文を選び，そ
れぞれの場合について，$n=3$としてバックオフスムージングにより
(\ref{eq:n})式の確率を推定し，それを利用して約1万文からなるテストコー
パスにおける全分割点の尺度$L$の値を計算した．そして，全ての分割点を尺
度$L$により降順にソートし，上位の分割点から，過分割かどうかを，テスト
コーパスの元々の分割を正解として調べた．

\subsubsection*{実験結果}

まず，全訓練データを使用した場合についての実験結果を述べ，次に訓練デー
タを1000文ずつ増加した場合についての実験結果を述べる．

\paragraph{全訓練データを使用した場合}

図\ref{fig:10000}には，約1万文の訓練データ全てを使って確率推定した場合
について，教師あり学習と教師なし学習のそれぞれについて，過分割検出の再
現率(percent recall)に対する適合率(percent precision)および分割点調査
率(percent examination)を示す．ここで，
\begin{displaymath}
  \label{eq:recall}
  再現率 = 100 \times \frac{検出された過分割の数}{テストコーパスにおける過分割の数},
\end{displaymath}
\begin{displaymath}
  \label{eq:precision}
  適合率 = 100 \times \frac{検出された過分割の数}{尺度Lの上位から順番に調べた分割点の数},
\end{displaymath}
\begin{equation}
  \label{eq:examination}
  分割点調査率 = 100 \times \frac{尺度Lの上位から順番に調べた分割点の数}{テストコーパスにおける全分割点の数}.
\end{equation}

\begin{figure}[htbp]
  \begin{center}
    \epsfile{file=10000.eps}
    \caption{再現率と適合率/分割点調査率}
    \label{fig:10000}
  \end{center}
\end{figure}

図\ref{fig:10000}の 教師あり学習の場合の適合率(supervised-precision)お
よび教師なし学習の場合の適合率(unsupervised-precision)のプロットから分
かるように，上位における過分割検出の適合率は非常に高い．たとえば，再現
率が10.0％のとき，適合率は，教師あり学習の場合に90.5％であり，教師なし
学習の場合に46.8％であるが，これらは，JUMANの分割点全体の中で過分割が
占めるパーセンテージである0.5％の，180倍以上，および，90倍以上である．
この適合率の高さは，実験1での結果を裏付けるものである．

また，図\ref{fig:10000}の教師あり学習の場合の分割点調査率
(supervised-examination)および教師なし学習の場合の分割点調査率
(unsupervised-examination)から分かるように，一部の分割点を調べるだけで
多くの過分割を検出できると言える．たとえば，全体の過分割のなかから再現
率50％で過分割を見付けるためには，教師あり学習の場合には，全分割点の
0.5％を調べればよく，教師なし学習の場合には，全分割点の2.0％を調べれば
よい．さらに，90％の過分割を見付けるためには，教師あり学習の場合には，
全分割点の7.8％を調べればよく，教師なし学習の場合には，全分割点の12.2
％を調べればよい．一方，もし，無作為に分割点を調べるという方法により，
過分割を検出しようとしたならば，50％の過分割を見付けるためには，平均的
には，全分割点の50％を調べる必要があり，90％の過分割を見付けるためには，
90％の分割点を調べる必要がある．

以上より，尺度$L$を使うことにより，過分割の検出が効率良くできると言え
る．

なお，再現率，適合率，分割点調査率の間には
\begin{equation}
  \label{eq:rel}
  適合率  = K \times \frac{再現率}{分割点調査率}.
\end{equation}
という関係が成立する．ただし，$K$はテストコーパスに固有の定数であり，
\begin{displaymath}
K = 100 \times \frac{テストコーパスにおける過分割の数}{テストコーパスにおける全分割点の数}.
\end{displaymath}
(\ref{eq:rel})式から，分割点調査率と再現率が決まれば適合率が決まること
が分かる(e.g.,分割点調査率が小さければ適合率は高い)．そのため，以下で
は，再現率に対する分割点調査率のみに基づいて過分割の検出精度を評価する．
そして，同一の再現率に対して分割点調査率が小さいとき過分割の検出精度が
高いと言い，その逆のときに過分割の検出精度が低いと言うことにする．

\paragraph{1000文ずつ訓練データを増やした場合}

図\ref{fig:incr-exam}には，過分割検出の再現率が25,50,75％の場合
(recall25,recall50,recall75)について，教師あり学習の場合と教師なし学習
の場合における，訓練文数(Num. of training sentences)と分割点調査率の関
係を示す．


\begin{figure}[htbp]
  \begin{center}
\vspace{1mm}
    \epsfile{file=exam25-50-75.eps}
\vspace{2mm}
    \caption{訓練データを増やした場合の再現率と分割点調査率}
    \label{fig:incr-exam}
  \end{center}
\end{figure}

図\ref{fig:incr-exam}から，教師あり学習の場合
(supervised-recall25,50,75)については，訓練文数が増加すると，再現率が
50％と75％においては，分割点調査率が明確に減少していると言える．また，
再現率が25％についても緩やかに分割点調査率は減少している．一方，教師な
し学習の場合(unsupervised-recall25,50,75)については，訓練文数が増えて
いっても，2000文以上については，分割点調査率は(若干の変動はあるが)ほ
ぼ横ばいである．

このことは，教師あり学習については，訓練データが多くなれば多くなるだけ，
(\ref{eq:n})式の確率を精密に推定できるが，教師なし学習については，訓練
データが多くなったとしても，その確率推定に対する効果は，教師あり学習の
場合に比べれば，小さいことを示している．

\vspace{17mm}

\subsection{実験3：各種尺度の比較}
\label{sec:comp}

実験3では，尺度$L$，相互情報量，尤度比，改良Dice係数\cite{kitamura97}，
Yates補正された$\chi^2$，の五つの尺度について，過分割の検出精度を比較
した．ここで，尤度比，改良Dice係数，Yates補正された$\chi^2$は，
\cite{hisamitsu97}において，有用な括弧表現を抽出するために有効であると
された尺度である．また，尤度比は，\cite{kageura97}でも，2文字間の連関
の尺度として，漢字列の分割に有効であることが示されている．

以下では，まず，本実験のテストコーパスとした京大コーパスBについて，そ
こでの分割点の出現頻度の統計について述べる．この出現統計は，あとで，各
尺度間の過分割検出精度の違いを説明するときの資料に用いる．次に，各尺度
を定義し比較する．

\subsubsection*{テストコーパスにおける分割点の出現統計}

形態素$A$の最後の文字を$a$,形態素$B$の最初の文字を$b$とし，$a$と$b$に
挟まれるような分割点を，前後1文字で区別される分割点と呼ぶ．実験3では，
分割点といえば，前後1文字で区別される分割点のこととする．つまり，
「ab/cd」と「xb/cy」のような分割点は，分割点の前後1文字が同じであるの
で，区別しないで同一タイプの分割点として扱う．

表\ref{tab:freq}は，テストコーパスとした京大コーパスBにおける分割点に
ついて，過分割である分割点とそうでない分割点のそれぞれに対して，出現頻
度ごとの，分割点の異なり数などを調べたものである．ここで，分割点の総数
を$F$,頻度$r$における分割点の異なり数を$k_r$とすると，頻度$r$における
延べ数は$f_r = r \times k_r $であり，$F = \sum_r f_r$である．表
\ref{tab:freq}では，頻度$r$における「延べ％」は$100 \times f_r/F$であ
り，「累積％」は$\sum_{s=1}^{r} 100 \times f_s/F$である．

\begin{table}[htbp]
  \begin{center}
    \caption{京大コーパスBにおける分割点の出現統計}
    
    \begin{tabular}{|c||ccc|ccc|}
      \hline
      & \multicolumn{3}{c|}{過分割である分割点}
      & \multicolumn{3}{c|}{過分割でない分割点}\\
      \cline{2-7}
      \raisebox{1.5ex}[0pt]{頻度} & 異なり数 & 延べ％ & 累積％ & 異なり数 & 延べ％ & 累積％ \\
      \hline
      1 & 645 &52.4 &52.4  &24180 &10.4 & 10.4\\
      2 & 104 &16.9 &69.2  &7187  &6.2  & 16.6\\
      3 & 29  &7.1  &76.3  &3579  &4.6  & 21.3\\
      4 & 11  &3.6  &79.9  &2133  &3.7  & 24.9\\
      5 & 9   &3.7  &83.5  &1413  &3.0  & 28.0\\
      6 & 5   &2.4  &86.0  &1062  &2.7  & 30.7\\
      7 & 2   &1.1  &87.1  &764   &2.3  & 33.0\\
      8 & 3   &1.9  &89.0  &644   &2.2  & 35.3\\
      9 & 1   &0.7  &89.8  &496   &1.9  & 37.2\\
      10& 1   &0.8  &90.6  &411   &1.8  & 39.0\\
      \hline
  11以上& 8   &9.4  &100.0 &3586  &61.0 & 100.0 \\
      \hline
    \end{tabular}
    \label{tab:freq}
  \end{center}
\end{table}

表\ref{tab:freq}から，過分割である分割点の出現頻度は，そうでない場合に
比べて，低頻度であると言える．これは，過分割である分割点の数自体が少な
いことが主な原因である．また，過分割である場合とそうでない場合の分布の
様子を比べると，過分割である分割点の場合には，頻度が1か2であるような場
合が全体の50％以上を占めていることから分かるように，低頻度の方に分布が
偏っている．

\subsubsection*{各尺度の定義}

まず，$n=2$として，(\ref{eq:n})式を用いて，(\ref{eq:L})式を変形すると，
\begin{equation}
  \label{eq:Ln2}
  L(A,B) = \log \frac{\Pr(a_k,b_1)}{\Pr(a_k,\E)\Pr(\B,b_1)}
\end{equation}
となる．一方，(\ref{eq:mi})式を同様に変形すると
\begin{displaymath}
  MI(A,B) = \log \frac{\Pr(\E,\B)}{\Pr(\E)\Pr(\B)}
\end{displaymath}
という無意味な値になるので，区切り文字とそれに隣接する文字は特に強く結
合すると仮定し，
\begin{displaymath}
  \Pr(\B, c_1,\ldots, c_k, \E) = \Pr(\B, c_1)\Pr(c_2|\B, c_1)\cdots\Pr(\E, c_k|c_{k-1})
\end{displaymath}
のような変形をすると，
\begin{equation}
  \label{eq:MIn2}
  MI(A,B) = \log \frac{\Pr(a_k,\E,\B,b_1)}{\Pr(a_k,\E)\Pr(\B,b_1)}
\end{equation}
となる．なお，以下では，$a=a_k$,$b=b_1$とする．

(\ref{eq:MIn2})式から，$n=2$においては，相互情報量$MI(A,B)$は，$a \E$
と$\B b$をそれぞれ一つの項と看做せば，この2項に関する通常の相互情報量
の式と一致することがわかる．そこで，尤度比，改良Dice係数，Yates補正さ
れた$\chi^2$についても，これら2項に基づいて，その値を計算する．

以下では，\cite{hisamitsu97}に基づいて，尤度比，改良Dice係数，Yates補
正された$\chi^2$を定義する．また，尺度$L$と相互情報量についても，確率
を最尤推定した形で定義する．

各尺度を定義する準備として，まず，$f_{ij}(i,j=1,2)$は，分割表で示すと
\begin{quote}
  \begin{tabular}{|l|c|c|}
    \hline
    & 後続文字が$\B b$ & 後続文字が$\B b$以外 \\
    \hline
    先行文字が$a \E$     & $f_{11}$           & $f_{12}$ \\
    \hline
    先行文字が$a \E$以外 & $f_{21}$           & $f_{22}$ \\
    \hline
  \end{tabular}
\end{quote}
である．より厳密には，$f(\cdots)$を文字列の頻度とし，$v,w,x,y$を，$\B$
と$\E$を含む任意の文字としたとき，
\begin{displaymath}
  \begin{array}{rcl}
    f_{11}& = &f(a, \E, \B, b)\\
    f_{12}& = &\sum_{xy \ne \B b} f(a,\E,x,y)\\
    f_{21}& = &\sum_{vw \ne a \E} f(v,w,\B,b)\\
    f_{22}& = &\sum_{vwxy} f(v,w,x,y) - f_{11} - f_{12} - f_{21}
  \end{array}
\end{displaymath}
である．また，
\begin{displaymath}
  \begin{array}{rcl}
    f_{i.}& = & f_{i1}+f_{i2}\\
    f_{.j}& = & f_{1j}+f_{2j}\\
    F     & = &\sum_{i,j}f_{ij}
  \end{array}
\end{displaymath}
である．

\paragraph{尤度比}

ここでの「尤度比」は，$a \E$と$\B b$の2項が従属とした場合と独立とした
場合との最尤推定量による尤度比であり，
\begin{equation}
  \label{eq:lambda}
  \lambda = 2 \sum_{i,j} f_{ij}\left \{\log\frac{f_{ij}}{F} - \log\frac{f_{i.}f_{.j}}{F^2} \right \}
\end{equation}
である．なお，上式では，分割点のソートに無関係な項は除いてある．

$\lambda$は，2項が従属して生起する度合が強いとき，正で大きな値をとる．
しかし，これだけでは必ずしも共起強度が強いとは言えない．たとえば，
\begin{tabular}{|l|l|}
  \hline
  10 & 1   \\
  \hline
  1  & 10  \\
  \hline
\end{tabular}
と
\begin{tabular}{|l|l|}
  \hline
  1  & 10   \\
  \hline
  10  & 1  \\
  \hline
\end{tabular}
は同じ$\lambda$となる．これらのうち前者は共起強度が強いが，後者は弱
い(反発している)．このことを考慮して，$\lambda>0$のときには，
\cite{kageura97}と同様に，Yuleの
$Y (=\frac{\sqrt{f_{11}f_{22}}-\sqrt{f_{12}f_{21}}}{\sqrt{f_{11}f_{22}}+\sqrt{f_{12}f_{21}}})$
の符合を付けることにより，分割点をソートした．

\paragraph{Yates補正された$\chi^2$}

$\lambda$と同様に独立性の判定に用いられる尺度である．
\begin{equation}
  \label{eq:chi2}
  \chi^2 = \frac{F(|f_{11}f_{22}-f_{12}f_{21}|-F/2)^2}{f_{1.}f_{2.}f_{.1}f_{.2}}
\end{equation}
なお，$\chi^2$に関しても，$\lambda$と同様な理由から，Yuleの$Y$の符合
を付けて分割点をソートした．

\paragraph{改良Dice係数}

\cite{kitamura97}で，対訳単語間の類似度として，提案されている尺度であ
る．
\begin{equation}
  \label{eq:dice}
  \mbox{改良Dice係数} = (\log f_{11})\frac{2f_{11}}{f_{1.}+f_{.1}}.
\end{equation}

\paragraph{相互情報量}
(\ref{eq:MIn2})式より，分割点のソートに無関係な項は除くと，
\begin{equation}
  \label{eq:mi2}
  MI^\prime = \log \frac{f_{11}}{f_{1.}f_{.1}}.
\end{equation}

\paragraph{尺度$L$}
(\ref{eq:Ln2})式より，分割点のソートに無関係な項は除くと，
\begin{equation}
  \label{eq:L2}
  L^\prime = \log \frac{f(a,b)}{f_{1.}f_{.1}}
\end{equation}

ここで，上記の各尺度について，もし，$f_{ij}=0$，あるいは，$f(a,b)=0$と
なる場合には，それぞれを0.1として計算した．

\subsubsection*{教師なし学習の場合での各尺度の比較}

各尺度について，JUMANにより形態素解析された京大コーパスBを訓練およびテ
ストコーパスとして，過分割の再現率に対する分割点調査率を評価した結果を
図\ref{fig:unsup-cmp}に示す．

図\ref{fig:unsup-cmp}から分るように，改良Dice係数(Dice)，
$\lambda$(lambda)，$\chi^2$(chi＾2)の分割点調査率は，$L^\prime$や
$MI^\prime$と比べて大きい．すなわち，過分割検出精度は低い．この原因は，
これらの尺度が，統計的に有意と言えないような低頻度の共起関係をノイズと
して排除するような尺度であるからである．すなわち，頻度が1とか2とかの共
起関係の尺度値は，これらの尺度では大きくならない\footnote{特に，改良
  Dice係数では，頻度が1の共起関係については，値が0となる}ため，(表
\ref{tab:freq}に示されるように)低頻度である過分割が排除されるためであ
る．

このような性質は，\cite{hisamitsu97}や\cite{kageura97}や
\cite{kitamura97}のような，一般的に共起強度が高い共起関係を必要とする
ような応用に対しては適していたが，低頻度事象である過分割を検出するには
適さない．

一方，$MI^\prime$の過分割検出精度は，再現率50％程度のところまでは，
$L^\prime$とほぼ同じである(実際には若干低い)．これは，$MI^\prime$が低
頻度の共起関係を過大評価する\cite{hisamitsu97}からであろう．つまり，再
現率が低いところでは，低頻度で，かつ，共起強度の強い表現を選択的に拾っ
てくるが，そのようなものは過分割であることが多いため，検出精度が高いと
解釈できる．しかし，再現率が上ってくると，比較的頻度が高い過分割も増え
てくるため，共起強度だけでは，過分割なのか，そうでない分割かが区別でき
なくなり，検出精度が下がると言える．

これらの尺度に対して，$L^\prime$は，分割されるか分割されないかを直接モ
デル化した尺度であるため，再現率が高くなっても検出精度が高いものと考え
る．

\begin{figure}[htbp]
  \begin{center}
\vspace{4mm}
    \epsfile{file=measure-exam.eps}
\vspace{1mm}
    \caption{過分割の再現率と分割点調査率(教師なし学習)}
    \label{fig:unsup-cmp}
  \end{center}
\end{figure}


\begin{figure}[htbp]
  \begin{center}
\vspace{3mm}
    \epsfile{file=sup-measure-exam.eps}
\vspace{1mm}
    \caption{過分割の再現率と分割点調査率(教師あり学習)}
    \label{fig:sup-cmp}
  \end{center}
\end{figure}
\vspace{-3mm}
なお，筆者は，予備実験として，相互情報量とYate補正した$\chi^{2}$を，隣
接する形態素間について，(文字ではなく)形態素を単位とする2項関係に基づ
いて計算してみたが，その性質は尺度$L$とは非常に異なっていた．相互情報
量の性質とYate補正した$\chi^{2}$の性質とは，互いに若干は異なるが，おお
まかには，二つの尺度とも，固有名詞(「福沢/諭吉」など)や四字熟語(「不眠
/不休」など)を取ってくる傾向が強かった．これらの隣接形態素は，それ自体
は有用な表現ではあるが，これらの隣接形態素間の分割が間違っているわけで
はないので，本稿での目的である過分割の検出には適さない．

その他，共起や定型表現を抽出する研究として，特に文字列レベルに関係する
ものでは，\cite[など]{shiNnou95,ikehara95,shimohata95}がある．これらの
研究では，大量の生テキストコーパスから，統計量を用いることにより，「に
関して」や「に対しては」などの定型的な表現を抽出する．これらの表現は有
用な表現ではあるが，「に対して」を「に/対/し/て」と分割しても，過分割
ではないことからも分るように，これらの手法は，本稿での目的である過分割
の検出には適さない．

\subsubsection*{教師あり学習の場合での各尺度の比較}

各尺度に対して，京大コーパスAの元々の分割を訓練コーパス，京大コーパスB
をテストコーパスとして，過分割の再現率に対する分割点率調査を評価した結
果を図\ref{fig:sup-cmp}に示す．

図\ref{fig:sup-cmp}では，図\ref{fig:unsup-cmp}と同様に尺度$L$の分割点
調査率が一番小さい．そして，図\ref{fig:sup-cmp}と図\ref{fig:unsup-cmp}
を比べると，尺度$L$については，図\ref{fig:sup-cmp}の教師あり学習の方が
図\ref{fig:unsup-cmp}の教師なし学習の場合よりも分割点調査率が小さい．

一方，尺度$L$以外の尺度については，教師あり学習の方が分割点調査率は大
きくなっている．これは，教師あり学習の場合の方が，教師なし学習の場合よ
りも，テストコーパスにおいて，過分割の前後の文字の共起強度が小さいこと
を示している．この理由は，教師あり学習においては，訓練コーパスで過分割
であるような分割点が人手により除かれているため，テストコーパスで過分割
であるような分割点は訓練コーパスで出現することが稀となり，その結果，共
起強度が小さくなるからである．このことから，尺度$L$以外の尺度について
は，教師あり学習をしても過分割検出精度が高くならないことが分かる．


