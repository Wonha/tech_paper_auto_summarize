統計的手法による仮名漢字変換[CITE]は，キーボードから直接入力可能な入力記号[MATH]の正閉包[MATH]を入力として，日本語の文字[MATH]の正閉包である変換候補[MATH]を確率値[MATH]の降順に提示する．
文献\Cite{確率的モデルによる仮名漢字変換}では文を単語列[MATH]とみなし，これを単語[MATH]を単位とする言語モデルと仮名漢字モデルに分解して実現する方法を提案している．
本節では，まずこれについて説明し，次に単語と読みを組とする言語モデルによる方法を提案し定式化する．
文献\Cite{確率的モデルによる仮名漢字変換}では，変換候補を[MATH]で順序付けすることを提案しており，これを次の式が示すように，単語を単位とする言語モデルと仮名漢字モデルに分解する．
ここで，後述するパラメータ推定のために，単語と入力記号列との対応関係は各単語において独立であるとの仮定をおく．
さらに，分母[MATH]は出力に依らないので，分子だけを以下のようにモデル化する．
P(\Bdma{y}|\Bdma{w})P(\Bdma{w}) = \prod_{i=1}^{h} P(\Bdma{y}_{i}|w_{i})P(w_{i}|\Bdma{w}_{i-n+1}^{i-1}) \nonumber
P(\Bdma{y}_{i}|w_{i})P(w_{i}|\Bdma{w}_{i-n+1}^{i-1}) =
P(w_{i}|\Bdma{w}_{i-n+1}^{i-1})P(\Bdma{y}_{i}|w_{i}) & \text{if} w_{i} \inW
P(\UW|\Bdma{w}_{i-n+1}^{i-1})M_{y,n}(\Bdma{y}_{i}) & \text{if} w_{i} \not\inW
ここで[MATH]は確率的言語モデルの語彙を表す．
簡単のために，この式の中の[MATH]は，文頭に対応する特別な記号\BTであり，これは文末[MATH]も表す．
この式の[MATH]と[MATH]は，語彙に\BTと未知語記号\UWを加えた[MATH]上の[MATH]-gramモデルである．
パラメータは，単語に分割されたコーパスから以下の式を用いて最尤推定する．
この式中の[MATH]は，事象[MATH]のコーパスにおける頻度を表す．
\figref{figure:LMA}が示すように，この学習コーパスには自動単語分割の結果であることが多いが，自動単語分割器の学習に用いたタグ付きコーパスが利用可能な場合にはこれを加えることもある．
\equref{eqnarray:KKConv1}の[MATH]は，単語単位の仮名漢字モデルであり，パラメータは，単語に分割されかつ各単語に入力記号列が付与されたコーパスから以下の式を用いて最尤推定する．
\equref{eqnarray:KKConv1}から分かるように，単語単位の仮名漢字モデルでは，単語と入力記号列との対応関係が各単語において独立であると仮定している．
この仮定により，比較的少量の入力記号列付与済みコーパスからある程度信頼できるパラメータを推定することができる．
\equref{eqnarray:KKConv1}の[MATH]は，未知語モデルであり，入力記号の集合に単語の両端を表す記号を加えた[MATH]上の[MATH]-gramモデルで実現される．
このパラメータは低頻度の単語に対応する入力記号列から推定する．
本論文では，言語モデルの単位を単語と入力記号列の組[MATH]とすることを提案する．
その上で，以下の式のように[MATH]をモデル化する．
分母[MATH]は出力に依らないので，分子だけを以下のようにモデル化する．
P(\Bdma{u}) = \prod_{i=1}^{h} P(u_{i}|\Bdma{u}_{i-n+1}^{i-1}) \nonumber
P(u_{i}|\Bdma{u}_{i-n+1}^{i-1}) =
P(u_{i}|\Bdma{u}_{i-n+1}^{i-1}) & \text{if}  u_{i} \inU
P(\UU|\Bdma{u}_{i-n+1}^{i-1})M_{u,n}(u_{i}) & \text{if}  u_{i} \not\inU
ここで[MATH]は言語モデルの語彙（単語と入力記号列の組の集合）を表す．
この式の中の[MATH]と[MATH]は，単語を単位とする場合と同様に，文頭と文末に対応する記号\BTである．
また\UUは未知の組を表す記号である．
\equref{eqnarray:KKConv2}の[MATH]は未知語モデルである．
従来手法と同様に，大きな学習コーパスを用いれば実際の使用における未知語率は極めて低く，また未知語に対する正確な仮名漢字変換は困難であると考えて，アルファベット[MATH]上の未知語モデルの代わりにアルファベット[MATH]上の未知語モデル[MATH]を用いることとする．
これは，\equref{eqnarray:KKConv1}と共通である．
以上から，提案手法の仮名漢字変換は，以下の式のようになる．
ここで[MATH]は[MATH]の入力記号列である．
なお，[MATH]の代わりに[MATH]を用いることは以下の式で与えられる近似であり，[MATH]であるので，入力記号列のみからなる文字列を未知語として出力することになる．
この式の[MATH]のパラメータは，学習コーパスにおける語彙[MATH]に含まれない表記と入力記号列の組の入力記号列から推定する．
これは，学習コーパスにおける未知の組の単語を入力記号列に置き換えた結果から[MATH]を推定しているのと同じである．
\equref{eqnarray:KKConv3}の[MATH]と[MATH]は，語彙に\BTと\UUを加えた[MATH]上の[MATH]-gramモデルである．
パラメータは，単語に分割されかつ入力記号列が付与されたコーパスから以下の式を用いて最尤推定する．
\figref{figure:LMB}が示すように，この学習コーパスには自動単語分割・読み付与の結果を用いることができる．
さらに自動単語分割器や読み付与の学習に用いたタグ付きコーパスが利用可能な場合にはこれを加えることもできる（\figref{figure:LMB}の点線）．
単語を単位とする従来手法と同程度の信頼性となるパラメータを推定するために，従来手法においてパラメータ推定に用いられる単語に分割されたコーパスと同程度の量の単語に分割されかつ入力記号列が付与されたコーパスが必要である．
換言すれば，自動単語分割と同程度の精度で入力記号列を推定するシステムが必要である．
これまで，各単語に対する入力記号列（読み）をその文脈に応じて十分な精度で推定する研究やフリーウェアがなかったために，提案手法は現実的ではなかったと思われる．
次節では，この方法を説明し，さらに入力記号列を確率的に付与することで，入力記号列の推定誤りの影響を緩和する方法を提案する．
仮名漢字変換や音声認識のための言語モデルは，単語分割済みコーパスと生コーパスの自動単語分割結果から構築される．
この節では，まずこの過程を概説する．
次に，前節で提案したモデルのパラメータをより正確に推定するために，単語に入力記号列や発音などのタグを確率的に付与することを提案する．
仮名漢字変換や音声認識のための単語を単位とする言語モデル作成においては，これらを適用する分野のコーパスが必須である．
一般に，コーパスには単語境界情報がないので，自動単語分割器[CITE]や形態素解析器[CITE]を用いて文を言語モデルの単位に分割し，その結果に対して単語[MATH]-gram頻度を計数する（\figref{figure:LMA}参照）\inhibitglue．
なお，これら自動単語分割器や形態素解析器などの自然言語処理システムは，単語分割済みあるいは品詞付与済みのコーパスから学習することが多い．
その場合には，これら自然言語処理システムの学習コーパスも言語モデルの学習コーパスに加えることができる（\figref{figure:LMA}の点線）が，実際には，これら自然言語処理システムはツールとして配布され，辞書追加程度の適応しかなされず，自然言語処理システムの学習コーパスが言語モデルの学習に利用されることは少ない．
形態素解析は，日本語の自然言語処理の第一段階として研究され，ルールに基づく方法が一定の成果を上げた[CITE]．
同じ頃，統計的手法[CITE]が提案され，アルゴリズムとデータの分離に成功した．
統計的手法は，ルールに基づく方法と同等かそれ以上の精度を達成しており，現在では主流になっている．
さらに，フリーソフトとして公開され，容易に利用可能となっている．
このような背景から，仮名漢字変換や音声認識のための言語モデル作成のために，形態素解析が用いられている．
結果的に，単語（表記）と品詞の組を言語モデルの単位とすることが多い．
しかしながら，仮名統計的漢字変換や音声認識等の実現には品詞情報は不要であり，形態素解析器の学習コーパス作成のコストを不必要に増大させるのみである．
また，英語等の単語間に空白を置く言語の音声認識においては，言語モデルの単位として当然単語が用いられる．
日本語においても単語を言語モデルの単位とする音声認識の取り組みがあり，十分な認識精度を報告している[CITE]．
以上の考察から，本論文では，単語と品詞の組を言語モデルの単位とする手法は，単語を単位とする手法に含まれるとして，以下の議論を展開する．
言語モデルの構築においては，適応対象の分野の大量のテキストに対する統計をとることが非常に有用である．
このため，形態素解析や自動単語分割等の自動処理が必須であるが，自動処理の結果は一定量の誤りを含む．
この単語分割誤りによる悪影響を緩和するために，確率的に単語に分割することが提案されている[CITE]．
この手法では，自動単語分割器によって各文字の間に単語境界がある確率を付与し，その確率を参照して計算される単語[MATH]-gramの期待頻度を用いて言語モデルが構築される．
実用上は，モンテカルロシミュレーションのように，各文字間に対して都度発生させた乱数と単語境界確率の比較結果から単語境界か否かを決定することで得られる擬似確率的単語分割コーパスから従来法と同様に言語モデルが構築される[CITE]．
前節で，仮名漢字変換のための言語モデルの単位として単語と入力記号列の組を用いることを提案した．
この考え自体は特に新規ではなく，以前から存在している．
実際，音声認識において，数詞のあとの「本」など一部の高頻度語に文脈に応じた発音を付与する後処理が行われている[CITE]．
また，発音レベルでの書き起こしが得られる場合に，単語と発音の対応を推定し，単語と品詞と発音の組を単位をとする言語モデルを構築する研究もある[CITE]．
しかしながら，この考えを一般的な場合において実現するためには，高精度の自動読み推定システムが必要である．
前述の形態素解析の研究とその成果であるフリーソフトにおいては，読みの推定は軽視されており，文脈に応じた読みを高い精度で出力する研究やフリーソフトはなかった．
このため，単語と入力記号列の組や単語と発音の組を単位とする言語モデルは一般的な意味で実現されていなかった．
前節で提案した単語と入力記号列の組を単位とする言語モデルの構築においては，コーパスを単語に分割し，文脈に応じた読みを付与することができるKyTea（京都テキスト解析ツールキット）[CITE]を用いて適応対象の分野のテキストを自動的に単語と入力記号列の組の列に変換する（\figref{figure:LMB}参照）．
その結果から\equref{equation:UM}を用いて単語と入力記号列の組の[MATH]-gram確率を推定する．
KyTeaの詳細は\appref{appe:kytea}に記述した．
自動読み推定の結果は，形態素解析や自動単語分割等の自動処理の場合と同様に，一定量の誤りを含む．
学習コーパスに含まれる読み推定誤りは，言語モデルや仮名漢字モデル，あるいは発音辞書に悪影響を及ぼす．
特に，ある単語に対して至る所で同じ誤った読みを付与する場合には，非常に重大な問題となる．
この問題を回避するために，確率的単語分割と同様に，単語に対する入力記号列付与や発音付与を確率的に行うことを提案する．
すなわち，読み推定においては，ある単語に対する読みを決定的に推定するのではなく，可能な読みとその確率値を返すようにする．
より一般的には，単語に対する読みや品詞などのタグ付与を，ある基準で最適となる唯一のタグを出力する処理ではなく，タグ[MATH]と確率値[MATH]の組の列[MATH]を出力する処理へと一般化する．
この際，タグの確率値は，周辺の他の単語のタグと独立であるとの仮定をおく．
この結果得られるコーパスを確率的タグ付与コーパスと呼ぶ．
確率的タグ付与コーパスの文[MATH]は，以下のように，各単語に可能なタグと確率値の組の列が付与されている．
\langle w_{1},(\pair{t_{1,1}}{p_{1,1}},\pair{t_{1,2}}{p_{1,2}}, \cdots,\pair{t_{1,k_1}}{p_{1,k_1}}) \rangle
\langle w_{2},(\pair{t_{2,1}}{p_{2,1}},\pair{t_{2,2}}{p_{2,2}}, \cdots,\pair{t_{2,k_2}}{p_{2,k_2}}) \rangle
\vdots
\langle w_{h},(\pair{t_{h,1}}{p_{h,1}},\pair{t_{h,2}}{p_{h,2}}, \cdots,\pair{t_{h,k_h}}{p_{h,k_h}}) \rangle
ここで，[MATH]と[MATH]はそれぞれ，[MATH]番目の単語の[MATH]番目のタグとその確率を表す．
このような確率的タグ付与コーパスにおける単語とタグの組の[MATH]-gramの1回の出現あたりの頻度[MATH]は，以下の式で計算される期待頻度として定義される．
この値をコーパスにおけるすべての出現箇所に渡って合計した結果が単語とタグの組の列[MATH]の期待頻度である．
単語とタグの組の[MATH]-gram確率は，この期待頻度の相対値として定義される．
仮名漢字変換のための言語モデル構築では，タグとして単語に対応する入力記号列を用いる．
確率的入力記号列付与のためのモデルは，単語ごとに入力記号列が付与されたコーパスからロジスティック回帰などの点予測器を推定しておくことで実現できる．
確率的単語分割の場合と同様に，確率的タグ付与コーパスに対する単語とタグの組の列の頻度の計算は，決定的タグ付与コーパスに対する頻度計算と比べてはるかに多い計算を要する．
実際，対象となる組の列としての頻度が[MATH]回とすると，\equref{equation:STCFreq}による期待頻度の計算は，各出現箇所における[MATH]回の浮動小数点の積を実行し（[MATH]回の乗算），その結果の総和を[MATH]回の加算により算出することになる．
通常の決定的タグ付与コーパスに対する頻度の計算は，[MATH]回のインクリメントで済むことを考えると，非常に大きな計算コストが必要である．
また，非常に小さい期待頻度の単語とタグの組の列が多数生成され，これによる計算コストの増大も起こる．
このような計算コストの問題は，次に述べる擬似確率的タグ付与コーパスによって近似的に解決される．
擬似確率的タグ付与コーパスは，各単語に対して都度発生させた乱数とタグの確率の比較結果から当該単語のタグを唯一に決定することで得られる単語とタグの組の列である．
この手続きを複数回繰り返して得られるコーパスに対して頻度を計数することで確率的タグ付与コーパスの期待頻度の近似値が得られる．
このときの繰り返し回数を倍率と呼ぶ．
擬似確率的タグ付与コーパスは，確率的単語分割コーパス[CITE]と同様に一種のモンテカルロ法となっており，近似誤差に関しては以下の議論が同様に可能である．
モンテカルロ法による[MATH]次元の単位立方体[MATH]上の定積分[MATH]の数値計算法では，単位立方体[MATH]上の一様乱数[MATH]を発生させて[MATH]とする．
このとき，誤差[MATH]は次元[MATH]によらずに[MATH]に比例する程度の速さで減少することが知られている．
擬似確率的タグ付与コーパスにおける単語とタグの組の[MATH]-gram頻度の計算はこの特殊な場合である．
すなわち，\equref{equation:STCFreq}の値は，[MATH]次元の単位立方体中の矩形の部分領域（[MATH]番目の軸方向の長さが[MATH]）の体積である．
したがって，誤差は[MATH]の値によらずに[MATH]に比例する程度の速さで減少する．
本論文で用いた自動読み推定[CITE]は，コーパスに基づく方法であり，単語に分割された文を入力とし，単語毎に独立に以下の分類に基づいて読み推定が行われる．
学習コーパスに出現しているか
はい
読みが唯一か複数か
複数[MATH]ロジスティック回帰[CITE]を用いて読みを選択
唯一[MATH]その読みを選択
いいえ
辞書に入っているか
はい[MATH]最初の項目の読みを選択
いいえ[MATH]文字と読みの2-gramモデルによる最尤の読みを選択
複数の読みが可能でその確率が必要な場合には，ロジスティック回帰の出力確率や文字と読みの2-gramモデルによる生成確率を正規化した値を利用する．
分類器の学習に用いたコーパスは，現代日本語書き言葉均衡コーパス[CITE]であり，辞書はUniDic [CITE]である．
学習コーパスとして33,147文（899,025単語，1,292,249文字）を用い，テストコーパスとして同一分野の3,681文（98,634単語，141,655文字）を用いた場合の読み推定精度を測定した．
評価基準は，入力記号単位の適合率と再現率である．
その結果，一般的な手法である単語と読みを組とする3-gramモデル\Cite{N-gramモデルを用いた音声合成のための読み及びアクセントの同時推定}の適合率と再現率はそれぞれ99.07%と99.12%であり，本論文で用いた自動読み推定の適合率と再現率はそれぞれ99.19%と99.26%であった．
この結果から，本論文で用いた自動読み手法は，既存手法と同程度の精度となっていることがわかる．
