



\documentstyle[epsf,jnlpbbl,version]{jnlp_j}

\def\atari(#1,#2,#3){}

\setcounter{page}{229}
\setcounter{巻数}{7}
\setcounter{号数}{4}
\setcounter{年}{2000}
\setcounter{月}{10}
\受付{2000}{4}{4}
\再受付{2000}{6}{6}
\採録{2000}{6}{30}

\setcounter{secnumdepth}{2}

\title{複数決定リストの順次適用による文節まとめあげ}
\author{白木 伸征\affiref{chuken} \and 梅村 祥之\affiref{chuken} \and 原田 義久\affiref{chuken}\affiref{nagoya}}

\headauthor{白木, 梅村, 原田}
\headtitle{複数決定リストの順次適用による文節まとめあげ}

\affilabel{chuken}{株式会社 豊田中央研究所}{Toyota Central Research and Development Laboratories Incorporated}
\affilabel{nagoya}{現在，名古屋商科大学，商学部}{Faculty of Commerce and Business Administration, Nagoya University of Commerce and Business Administration}

\jabstract{
  近年の高度情報化の流れにより，自動車にも種々の情報機器が搭載
  されるようになり，その中で音声認識・合成の必要性が高まってい
  る．本研究は音声合成を行うための日本語解析の中で基本となる，
  文節まとめあげに関する研究報告である．
  従来の文節まとめあげは，人手規則による手法と機械学習による手
  法の二つに大きく分けられる．前者は，長年の努力により非常に高
  い精度を得られているが，入力データ形式が固定であるために柔軟
  性に欠け，人手で規則を作成・保守管理するため多大な労力を要し，
  車載情報機器へ実装するには問題が大きい．また後者は，それらの
  問題に柔軟に対処できるが，精度を向上させるためにアルゴリズム
  が複雑化しており，その結果開発期間が延長するなどの問題が生じ，
  車載情報機器には不向きである．
  そこで本研究は，決定リストを用いる手法を発展させ，複数の決定
  リストを順に適用するだけという非常に簡明な文節まとめあげの手
  法を提案する．決定リストの手法は非常に単純であるが，それだけ
  では高い精度が得られない．そこで，決定リストを一つではなく複
  数作成し，それぞれのリストを最適な順序に並べて利用することに
  より精度向上を図った．この結果，京大コーパスの最初の10000文
  を学習コーパス，残りの約10000文をテストコーパスとして実験を
  行ったところ，非常に簡明な手法ながら，99.38\%という高い精度
  を得られた．
}

\jkeywords{文節まとめあげ, 決定リスト}

\etitle{Bunsetsu Identification with \\ Sequential Use of Plural Decision Lists}
\eauthor{Nobuyuki Shiraki\affiref{chuken} \and Yoshiyuki Umemura\affiref{chuken} \and Yoshihisa Harata\affiref{chuken}\affiref{nagoya}} 

\eabstract{
  Recent information-oriented society becomes to need Car-Multi-Media
  systems. In the systems, speech recognition and synthesis systems are
  also necessary. We aimed to improve Bunsetsu Identification which
  is important for them.
  There are two types of traditional Bunsetsu Identification methods:
  one is a method which uses handmade rules and the other is a method 
  which uses machine learning.
  The former has high accuracy rate, but there are some problems
  especially for Car-Multi-Media systems.
  For example, the method is not flexible because it needs fixed inputs,
  and the method needs a lot of efforts to keep identification rules
  because all rules are made by hand.
  The latter is robust for these problems, but the algorithms are
  much more complex to improve accuracy, so there are some problems for
  Car-Multi-Media systems.
  Therefore, we propose a new method that uses plural decision lists
  sequentially. The Decision List method is very simple, but
  it does not have very high accuracy rate. Then, we use not 'one'
  decision list but 'plural' decision lists 'sequentially'.
  We made some experiments using 10,000 sentences as a training corpus,
  and 10,000 sentences as a test corpus in Kyoto-University-Corpus.
  As the result, the accuracy rate was 99.38\%.
}

\ekeywords{bunsetsu identification, decision list}


\def\dfrac#1#2{}
\def\bm#1{}
\def\bold{}


\begin{document}
\thispagestyle{empty}
\maketitle



\section{従来の研究}\label{sec:従来の研究}

文節まとめあげに関する従来の研究は，人手により文節まとめあげの規則を書き下す方法と，大規模コーパスから機械学習により得た統計情報を利用する方法の2種類に大きく分けられる．
これらの手法について以下で説明する．


\subsection{人手規則による文節まとめあげ}

人手により作成した文節まとめあげの規則を利用する最もよく知られているツールに，knpがある．
knpは文節に関する規則を人手で網羅することにより，99\%以上という非常に高精度な文節まとめあげを実現している．
knpの文節まとめあげの規則は906行のファイルに148種類の規則が記述されている
\footnote{
  knpは係り受け解析ツールであるが，係り受け解析の前に文節まとめあげを行っているため，その部分だけの数値を利用した．
  }
．
knpへの入力は形態素解析ツールjuman
\cite{juman3.5}
の出力に限定されており，文節まとめあげの規則もその形式に基づいて作成されている．
そのため，juman以外の形態素解析ツールの出力形式で利用するためには，規則をすべて書き直す必要がある．
人手規則による文節まとめあげは，このように多数の規則を人手で修正・追加を繰り返さなければならず，大きな労力が必要という問題がある．

しかしながら，車載情報機器の形態素解析部の出力形式はそれぞれ機種によって異なり，knpを車載情報機器に実装するためには規則をすべて書き直さなければならず，規則の保守管理も容易ではないため，問題が大きい．


\subsection{機械学習による文節まとめあげ}\label{subsec:機械学習手法}

人手規則による文節まとめあげの持つ問題に対処でき，最近最も盛んに研究されているのが，大規模コーパスから機械学習により得た統計情報を利用して文節まとめあげを行う手法である
\cite{Zhang1998,Asahara1999,Murata2000}
．
機械学習による手法は，大規模コーパスから文節区切りの規則を学習し，それにより文節まとめあげを行う．
そのため人手により規則を保守管理する必要がなく，また形態素解析ツールの出力形式に依存しないという利点がある
．

ただし機械学習の手法でも，学習用のコーパスを準備するという労力は必要である．
しかし，京大コーパス
\cite{KyotoCorpus}
などの大規模コーパスの構文情報を，形態素解析ツールの各出力形式に変換するのは，文節区切りの情報だけに限定するため容易である．
また人手により規則を作成する場合，プログラミングの専門的な知識が必要であるうえ，規則を改良するためには多くの試行錯誤が必要となる．
それに対し，コーパスの作成を行う場合は，コーパスの原文を形態素解析した結果がほぼ100\%に近い精度であり，それを文節に区切るだけでコーパスが得られるので特別に専門的知識は必要ない．
また，単にコーパスの量を増やすだけで精度を向上させることができる．
これらのことから，機械学習の手法は必要な労力が少ないといえる．

機械学習を用いる文節まとめあげには様々な種類があるが，これまでに最も精度の高い結果を得ているのが，村田らによる研究である
\cite{Murata2000}
．
村田らは，決定リストを用いた文節まとめあげの手法に排反な規則を組み合わせた手法を提案している．

決定リストは，規則をある優先順位を決めて1次元に並べたリストのことである．
そしてそのリストを順に探索して一番最初に適用された規則のみを用いて解析を行う手法である．
決定リストの要素としてよく用いられるのは，大規模コーパスから学習した結果であり，それを並べる優先順位としては確率が主に用いられる．

例えば，図\ref{fig:決定リスト}のような決定リストにより，「うまく, 日本語, 文, を, 解析, する, ．」という形態素解析済みの文を処理する方法について考える．
「うまく(形容詞)」と「日本語(名詞)」という情報から，「うまく」＋「日本語」という規則が最初に適用されるため，この部分は「文節に区切る」と決定される．
リストの下位に「形容詞」＋「名詞」は「文節に区切らない」という規則があるが，決定リストはリストの上位の要素から適用するため，この規則は無視される．

\begin{figure}
  \begin{center}
    \begin{tabular}{r@{ }c@{ }l|c|c}
      \multicolumn{3}{c|}{規則} & 確率 & 文節区切り\\
      \hline
      「うまく」 & ＋ & 「日本語」 & 100\% & 区切る\\
      「うまい」 & ＋ & 「日本語」 &  95\% & 区切らない\\
      & $\vdots$ & & $\vdots$ & $\vdots$\\
      「形容詞」 & ＋ & 「名詞」   &  70\% & 区切らない\\
      & $\vdots$ & & $\vdots$ & $\vdots$\\
    \end{tabular}
    \caption{決定リストの例}
    
    
    
    \label{fig:決定リスト}
  \end{center}
\end{figure}

村田らの手法は，文節に区切るあるいは区切らない確率が100\%である規則を排反な規則と呼び，決定リストの手法に排反な規則を組み合わせて文節まとめあげを行う．
確率が100\%でない規則を適用するのは，あらかじめ誤る可能性のあるものを利用するということになるため，高い精度を望むことができない．
そのため排反な規則を重要視しなければならない，と主張している．
図\ref{fig:村田手法}のような前後4つの形態素の4種類の情報を152種類組み合わせて，それにより決定リストを作成する．
決定リストの要素を並べる順序は，まず確率でソートして，同じ確率のものは頻度順にソートする．

\begin{figure}
  \begin{center}
    \begin{tabular}{cccc}
      二つ前 & 一つ前  & 一つ後 & 二つ後\\
      \hline
            & 情報A & 情報A &\\
      情報A & 情報B & 情報B & 情報A\\
      情報B & 情報C & 情報C & 情報B\\
            & 情報D & 情報D &\\
    \end{tabular}

    素性 
    $ \cdots ~~ \left(
      \begin{array}{ll}
        情報A: & 品詞\\
        情報B: & 品詞＋品詞細分類\\
        情報C: & 品詞＋品詞細分類＋意味情報\\
        情報D: & 品詞＋品詞細分類＋意味情報＋単語表記\\
      \end{array}
      \right)
    $
    \caption{村田らの手法で用いる情報}
    
    
    
    \label{fig:村田手法}
  \end{center}
\end{figure}

例えば，ある形態素の隙間の文節区切りを決定する時に図\ref{fig:村田リスト}のような規則のパターンが一致して適用可能である場合，決定リストの手法であれば，最初の規則Aが適用されるため「文節に区切らない」と決定される．
しかし規則B，C，Dを見ると，各規則ごとの頻度は規則Aと比べると小さいが，それぞれの頻度を足しあわせると規則Aの頻度よりも大きい．
そのため，規則B，C，Dに従って「文節に区切る」と決定する方が望ましいと考えられる．
このように，排反な規則，つまり確率が100\%となる規則の頻度を足しあわせ，その頻度により文節区切りの決定を行う．

\begin{figure}
  \begin{center}
    \begin{tabular}{cccrrc}
      \hline
      規則  & パターン  &               & 確率 & 頻度 & 文節区切り\\
      \hline
      A & a & $\Rightarrow$ & 100\%  & 34  & 区切らない\\
      B & b & $\Rightarrow$ & 100\%  & 33  & 区切る\\
      C & c & $\Rightarrow$ & 100\%  & 25  & 区切る\\
      D & d & $\Rightarrow$ & 100\%  & 19  & 区切る\\
      E & e & $\Rightarrow$ & 81.3\% & 123 & 区切る\\
      F & f & $\Rightarrow$ & 76.9\% & 13  & 区切る\\
      G & g & $\Rightarrow$ & 57.4\% & 540 & 区切らない\\
      $\vdots$ & $\vdots$\\
    \end{tabular}
    \caption{村田らの手法の説明}
    
    
    
    \label{fig:村田リスト}
  \end{center}
\end{figure}

この手法による文節まとめあげは，最高で99.17\%という高い精度を得ている．
しかしこの手法は，図\ref{fig:村田手法}のような情報の組み合わせが152種類もある．
京大コーパス中のデータは，1文平均約23の形態素の隙間があるため，1つの形態素の隙間に対して152種類の組み合せを考慮すると，1文あたり$152\times23=約3500回$もの処理をしなければならない．
このようにアルゴリズムが複雑なため，新たに車載情報機器に実装するためには長い開発期間を要し，また規則の学習に長い時間を要するため保守管理にも時間がかかり，さらにデータ量が膨大になるなど様々な問題がある．
そのため，車載情報機器には不向きであるといえる．

\ref{sec:文節まとめあげ}章では，これらの問題を解決するために考案した新しい手法について述べる．



\section{本研究の文節まとめあげの手法}\label{sec:文節まとめあげ}

\subsection{複数決定リストの順次適用による文節まとめあげ}\label{subsec:複数決定リスト}

本研究では，従来手法の問題点を解決するために次の点に着目した．

\begin{itemize}
\item 学習が容易で用いる情報の数が少ないこと
\item 学習結果を利用して文節をまとめる方法が従来手法より簡明であること
\item 精度が従来手法と同程度かそれ以上となること
\end{itemize}
これらを実現するために，{\bold 複数決定リストの順次適用による文節まとめあげ}という新しい手法を考案した．

機械学習を用いる従来手法では，大規模コーパスから得られた様々なn-gram(主に2-gramから4-gram)が利用されている．
本手法では，1つの形態素の隙間に対して6種類のn-gramのそれぞれの決定リストだけを考慮するという非常に簡明な方法を用いる．
具体的には，品詞，単語表記，品詞細分類，単語表記＋品詞の4種類の形態素2-gramと，品詞，単語表記の2種類の形態素3-gramを要素とする決定リストを利用する
\footnote{
  品詞細分類の3-gramと単語表記＋品詞の3-gramは，予備実験を行ったところ結果に変化がなかったため用いなかった．
  }
(図\ref{fig:本手法})．
以下では，これらのn-gramを要素とする決定リストを，n-gramリストと呼ぶ．
文節まとめあげの処理は，村田らと同様に形態素解析済みのテキストに対して行い，形態素の隙間ごとにその前後の形態素の情報からn-gramリストを調べて文節を区切るか区切らないかを決定する処理とした．

\begin{figure}
  \begin{center}
    \begin{tabular}{l|ccccc}
      &
      二つ前   &     & 一つ前         &     & 一つ後\\
      \hline
      単語表記 2-gram &
      ~        &     & 単語表記       & --- & 単語表記\\
      品詞 2-gram &
      ~        &     & 品詞           & --- & 品詞\\
      品詞細分類 2-gram &
      ~        &     & 品詞細分類     & --- & 品詞細分類\\
      単語表記＋品詞 2-gram &
      ~        &     & 単語表記＋品詞 & --- & 単語表記＋品詞\\
      単語表記 3-gram &
      単語表記 & --- & 単語表記       & --- & 単語表記\\
      品詞 3-gram &
      品詞     & --- & 品詞           & --- & 品詞\\
    \end{tabular}
    \caption{本手法で用いる情報}
    
    
    
    \label{fig:本手法}
  \end{center}
\end{figure}

例えば，2-gramの場合には$Y$，$Z$という連続する2つの単語の$Y$と$Z$の間，3-gramの場合には$X$，$Y$，$Z$という連続する3つの単語の$Y$と$Z$の間に注目し，その間の文節区切りを次のように決定する
\footnote{
  3-gramを考慮する場合，$X$，$Y$，$Z$という3つの単語の$X$と$Y$の間も考慮する必要があると思われるが，本手法は処理が簡明であることを最大の目標としたため，ここでは考慮しない．
  }
．

\begin{enumerate}
\item $X$，$Y$，$Z$の形態素を得る．
\item 図\ref{fig:本手法}の6種類のn-gramリストを順番に調べる
  \footnote{
    n-gramリストの適用順は\ref{sec:実験}章の実験で最適化を行う．
    }
  ．
  \begin{enumerate}
  \item n-gramリスト中に規則が見つかり，文節に区切る数が区切らない数よりも多い場合には区切りを入れ，少ない場合には区切らないこととする．
    この段階で$Y$と$Z$の文節区切りを確定し，処理を終了する．
  \item n-gramリスト中に規則が見つからない場合，または文節に区切る数と区切らない数が等しい場合には，次のn-gramリストを調べる．
  \end{enumerate}
\item 6種類すべてのn-gramリストを調べた結果，文節に区切るか区切らないか確定しない場合，デフォルト処理として文節に区切るものとする
\footnote{
  デフォルト処理を文節に区切らないとした場合は，予備実験により精度が下がることがわかったため，文節に区切るものとした．
  }
．
\end{enumerate}

本手法の最大の特徴は，このように{\bold 6種類のn-gramリストを順番に調べるだけで文節まとめあげを行う}，という非常に簡明な点である．
村田らの手法では\ref{subsec:機械学習手法}節で示したように，1つの形態素の隙間に対して約3500回もの処理をしなければならない．
しかし，本手法では最大で$6\times23=138回$の処理でよいため，村田らの手法と比べて約$\dfrac{1}{25}$の処理量で文節まとめあげを行うことができる．


\subsection{n-gramリストの取得方法}\label{subsec:n-gramリスト取得}

各n-gramリストの要素は，大規模コーパスから機械学習によって得る．
本研究では，学習コーパスとして京大コーパス
\cite{KyotoCorpus}
を利用した．
京大コーパスにはあらかじめ詳細な形態素の情報と文節区切りの情報が付与されているので，形態素の隙間ごとに文節に区切る数と区切らない数を数えて，それを確率の高い順に並べて保持する．
ただし，確率には文節に区切る確率か文節に区切らない確率の2種類があるが，高い方の確率を基準としてリストに並べた．
つまり，リストの最下位は確率50\%となる．

以上のようにして得られた品詞2-gramの学習結果の決定リストの例を図\ref{tab:学習結果例}に示す．

\begin{figure}
  \begin{center}
    \begin{tabular}{rr@{ }c@{ }lc}
      確率 & \multicolumn{3}{c}{規則} & 区切り\\
      \hline
      100\% & 「連体詞」 & ＋ & 「連体詞」 & 区切る\\
      100\% & 「連体詞」 & ＋ & 「名詞」   & 区切る\\
      $\vdots$ & & $\vdots$ & & $\vdots$\\
      55\% & 「特殊」 & ＋ & 「特殊」   & 区切る\\
      52\% & 「特殊」 & ＋ & 「接続詞」 & 区切らない\\
    \end{tabular}
    \caption{品詞2-gramの決定リスト}
    
    
    
    \label{tab:学習結果例}
  \end{center}
\end{figure}



\end{document}
