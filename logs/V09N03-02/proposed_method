現在研究されている語句抽出システムは，ほとんどが対象を名詞に準じた単語列に限定したものである．
これは，抽出の対象となる語句は未知語や専門用語が主であり，どちらも名詞がその大半を占めるためである．
未知語，専門用語，固有名詞などはドメイン固有の語句と言ってよいが，ドメイン固有の語句となりうるのは新語や複合語がほとんどで，例えば助詞のように新語の出現しないものや活用語のようにドメインによってはほとんど新語がないものなどは抽出対象となりにくく，名詞に準じる語句を抽出対象とすることでかなりの未知語，専門用語などを取得することが可能である．
また，対象とする品詞を限定することで抽出処理に必要なルールが削減され，ノイズの軽減に繋がるという利点がある．
抽出対象は主に，名詞の推定と名詞句の推定とに二分されており，特に名詞の推定では専門用語など複合語の推定を行なうものと固有名詞などの未知語を認識するものに分けられる．
名詞句の推定を目的とした研究としては，Argamonらの提案したサブパターン概念を利用する手法[CITE]などが挙げられる．
専門用語などドメイン固有の語句の抽出では，tf・idfモデルなど語句の出現頻度を利用する手法と[MATH]-gramなど文字の共起頻度を利用する手法がある．
湯本らはunigramおよび隣接bigramの出現頻度を利用して複合名詞の認識を行なう手法を提案した[CITE]．
湯本らの抽出対象は専門用語であったが，彼らは専門用語として複合名詞のみを考え，名詞のみに着目した隣接bigramを利用して複合名詞の推定を行なった．
専門用語となる語句の大部分を複合名詞が占めること，頻出複合名詞の構成要素である名詞は共起頻度が高く，また特定の単語同士の並びが多いことを考えると，この手法は効率的だと言える．
Frantziらは英語を対象としてC-valueを利用して複合語を認識する手法を提案し，さらにこれにコーパスサイズや重みなどを利用してランク付けを行なった[CITE]．
Chaらは未登録語発見のための形態素パターン辞書を利用して未登録語の認識およびタグ付けを行なう手法を提案した[CITE]．
固有名詞の認識では，最大エントロピー法を利用して固有名詞の切り出しを行なうBorthwickの手法[CITE]や，固有名詞の前後に出現しやすい語をトリガーワードとして固有名詞の認識を行なう手法[CITE]，トリガーワードと構文解析情報を利用する福本らの手法[CITE]などがある．
またCucerzanらは文字の並びの情報を利用して少ない訓練データからの固有名詞の推定を可能にする手法を提案し，ルーマニア語や英語など複数の言語に対して有効であることを示した[CITE]．
文字の共起情報を利用する手法としては，[MATH]-gramの画期的な抽出法を提案しこれを利用して文中の文字列の塊を認識する長尾らの手法[CITE]などが挙げられる．
中渡瀬らは[MATH]-gram統計を利用して辞書未登録語を自動獲得する手法を提案した[CITE]．
中渡瀬らは任意の文字列の頻度を正規化する手法を提案し，これを用いて語の境界を決定することで，辞書未登録語を獲得している．
中渡瀬らはこの手法で漢字未登録語の43%の取得に成功したと報告している．
中渡瀬らの手法では評価対象を漢字未登録語に限定しているが，これは漢字とその他の字種の出現頻度分布が異なるためで，正規化を行なう中渡瀬らの手法では漢字での精度が高いためである．
延澤らは品詞タグや文法などに頼らず機械的に取得可能な文字間の統計情報のみを利用して文の切り分けを行なう手法を提案している[CITE]．
延澤らはこの手法を利用してドメイン固有の文字列の自動抽出を試みており，口語文章のような非文を多く含むコーパスに対しても有効であることを示した[CITE]他，固有名詞抽出など抽出対象を絞った場合などについても有効であるとしている[CITE]．
文字列の抽出はその後の利用を見込んだものであるが，延澤らの手法では文字単位での処理を行なっているため抽出される文字列は単語，複合語，言い回しなどサイズがさまざまである．
自然言語処理においては一般に単語または形態素が処理単位とされている．
単語は多義性を持つものも多く，単語が最適な処理単位と言えるかは疑問が残る[CITE]．
その意味で，特定の処理単位を設定することは処理の精度に悪影響を与えている可能性もあるが，さまざまな処理単位を同時に扱う手法は確立されておらず，処理単位の特定が必要であるのが現状である．
このため，さまざまな処理単位の文字列が同時に出力とされる延澤らの手法を用いて出力された文字列は，そのままでは他のツールでの利用が困難である．
そこで本稿では，延澤らの手法の問題点を克服し，この手法を利用して辞書未登録語を抽出することで辞書ベースのツールの精度の向上を図る．
本稿で提案するシステムは，対象ドメインのコーパスからシンプルな手法でドメイン固有の語句を抽出する延澤らの手法[CITE]を応用したものであり，辞書ベースの自然言語処理ツールの支援を目的として2方向からのアプローチを試みる．
本稿では，辞書ベースの形態素解析ツールに対して統計情報を利用することでその精度の向上を図るため，以下の2つのアプローチを試みた．
システムM:形態素解析ツールへの組み込みのための統計情報利用システム
形態素解析中に統計情報を利用してドメイン固有の語句を認識するシステムを形態素解析ツールに組み込むことで形態素解析時の誤解析を削減．
システムD:統計情報を利用した辞書の作成システム
形態素解析の前処理として対象ドメイン固有の文字列の辞書登録を行なうことで形態素解析時の誤解析を削減．
どちらのアプローチも対象ドメインの訓練コーパスから得た統計情報を利用することで頻出文字列の認識を実現し，これに起因する解析誤りの削減を図るものである．
本稿では形態素解析ツールとして日本語形態素解析ツール茶筌ver.2.2.3[CITE]を採用した．
また，統計情報としては文字間の共起情報を採用した．
文字間の共起情報が頻出文字列認識に有用であるとの延澤らの主張[CITE]に基づき，本稿では対象ドメイン固有の頻出文字列の抽出に利用する統計情報として，文字間の共起情報を採用した．
そこで，前処理として訓練コーパス中の各二文字ペアの共起頻度を数え上げる．
本システムは訓練コーパスに全く制限を設けない．
品詞情報などの付加情報を一切利用しないため，形態素解析や構文解析，タグ付けなども必要としない．
訓練コーパス中の文字共起頻度の数え上げにはd-bigram確率モデル[CITE]を利用した．
d-bigramとは距離を考慮したbigramモデルであり，abbcという文字列の場合，隣接する(a, b)などだけでなくaとcのように離れて出現する二文字の共起関係も取得する．
この例ではaとcは距離3となり(a, c; 3)のように表される．
隣接bigramでは視野が非常に狭く文脈情報が利用できないという欠点があり，特に文字レベルでの利用はノイズが大きい．
これに対し，d-bigramモデルは距離の情報を保有することでこの問題に対処しており，例えば3単語の並び(trigram)も十分に評価できることが示されている[CITE]．
さらに，同じ文中であっても離れて出現する文字同士は近接して出現する文字同士に比べて関係が薄いと考えることができる[CITE]という主張に基づき，d-bigramの取得，利用に際して距離の上限および距離の影響力を設定することが可能である．
本稿で提案するシステムは，日本語を対象とした形態素解析ツール・茶筌に統計情報を利用した文字列抽出モジュール(システムM)を組み込むことで統計情報の活用を図るものである．
これは茶筌に特化した手法ではなく，茶筌本体の構造を改変するものではない．
茶筌は辞書ベースの形態素解析ツールであり，文単位で処理を行なう．
図[REF_fig:flo-o]に茶筌による形態素解析の流れを示す．
入力であるテストコーパスは一文ずつ処理され，形態素解析結果が出力される．
形態素解析処理においては，事前に準備された辞書を利用する．
図[REF_fig:flo-m]に本稿で提案する統計情報利用システムを茶筌に組み込んだ場合の形態素解析の流れを示す．
本稿で提案するシステムではまず茶筌に有繋文字列抽出モジュールを組み込むことにより文字列の認識を行ない([REF_sec:ukninshiki]節)，抽出された文字列に専用の品詞名を付けることで辞書の見出し語と同等に扱うことができるようにする([REF_sec:ukriyou]節)．
認識する文字列は延澤らの提案した有繋文字列[CITE]と呼ばれるもので，文字間の共起情報のみから一塊と推測された文字列である．
本システムを組み込むことで，茶筌の持つ辞書の他に，その文中に含まれる有繋文字列を形態素の候補として利用することが可能となる．
辞書に掲載されている語句が有繋文字列として抽出された場合は，辞書の情報を優先する．
従って，辞書既登録語句は有繋文字列として抽出されることはない．
文中の[MATH]番目の文字と[MATH]番目の文字の間の有繋評価値[MATH]の算出式を式([REF_exp:uk])に示す[CITE]．
ただし，[MATH]は文[MATH]の[MATH]番目の文字，[MATH]は2文字間の距離，[MATH]は[MATH]の最大値，[MATH]は距離の影響に対する重み付け関数であり，本稿では[MATH]，[MATH]とした[CITE]．
また，2文字間の相互情報量の計算式をd-bigramに対応するよう拡張したものとして式([REF_exp:mid])を利用した[CITE]．
ただし，[MATH], [MATH]は各文字，[MATH]は2文字間の距離，[MATH]は文字[MATH]が出現する確率，[MATH]はd-bigram ([MATH], [MATH]; [MATH])が起こる確率とする．
図[REF_fig:mountain-valley]に有繋評価値を利用した文字列認識の例を示す[CITE]．
図の横軸が入力文，縦軸が有繋評価値を示す．
横軸のアルファベットは入力文中の各文字を示す．
文中の各隣接文字ペア間の有繋評価値は，隣接文字ペアの共起頻度が高いほど高くなる．
従って図の中で評価値を繋いだ線が山状になっている部分は共起する可能性の高い部分であり，一塊の文字列である可能性が高い．
これに着目し山状の部分を抽出することで，文中の文字列の認識を行なう．
形態素解析中[REF_sec:ukninshiki]節の手法で認識された有繋文字列は専用の品詞およびコストが設定され既存の辞書の登録語と同等として形態素解析処理に利用される．
有繋文字列は特定の品詞に対応するものではないが，個々の有繋文字列に対してその品詞の推定を行なうことはシステムの実時間性を損ねるため，品詞「有繋文字列」を新設しこれに対して予め品詞情報を設定しておく．
実際に認識される文字列は名詞またはそれに準じるものがほとんどであるため，品詞「有繋文字列」の接続はすべて名詞接続とした．
茶筌では各語句に形態素コストが設定されている．
有繋文字列は文字間の共起情報によって決定するものであり，一塊の文字列であると評価する際の評価値の高さがそれぞれ異なる．
そこで，評価値によって有繋文字列を5段階に分類し，段階ごとに形態素コストを設定することで，評価値の高いものを優先的に利用できるように設定する．
図[REF_fig:ex1newspaper]に，本システムを茶筌に組み込んだ場合の実行例を挙げる．
図[REF_fig:ex1newspaper]の上段が茶筌のみで解析を行なった場合，下段がシステムMを組み込んで解析を行なった場合の切り分け結果である．
下線は辞書未登録語を，太字は有繋文字列として抽出された部分を示す．
図[REF_fig:ex1newspaper]では辞書未登録語2文字列が本システムを利用することで有繋文字列として抽出されている．
「bigram」のように辞書未登録語がそのままの形で一語である場合，この部分の切り分け結果は正解と変わらないため他の部分の解析結果への影響がない場合が多いが，図[REF_fig:ex1newspaper]の例のように他の部分へ影響を与える場合もある．
この例では本システムを利用し「bigram」の品詞が「有繋文字列」となったことで「など」が正しく認識されている．
「n-gram」は茶筌のみを利用した場合「n (記号)」「- (記号)」「gram (未知語)」に分割された．
複数の字種から成る未知語の場合は字種ごとに区切られる場合がほとんどである．
システムMでは字種情報を利用せずすべての字種の文字を同様に扱うため，字種の替わり目で誤分割されず，「n-gram」の認識に成功した．
またシステムMを利用することで「自然言語」「統計情報」などの複合語も多く認識された．
「自然言語」は，複合語「自然言語処理」の一部分であるが，「自然言語」自体一塊で複合語を形成し「自然言語処理」の構成要素となると考えられる．
訓練コーパスから取得した共起情報をそのまま利用する手法ではノイズの問題が防げない．
この問題を解決するため，本章では共起情報をそのまま利用するのではなく，共起情報を利用して辞書登録候補文字列を抽出しこれを事前に辞書に登録する手法を提案する．
図[REF_fig:flo-d]に本章で提案する辞書作成システムを利用して事前に作成した有繋文字列辞書を茶筌の辞書に組み込んだ場合による形態素解析の流れを示す．
基本的な流れは図[REF_fig:flo-o]と同じだが，利用する辞書は茶筌の基本辞書に有繋文字列辞書を組み込んだものとなっている．
この有繋文字列辞書は訓練コーパスから作成したものであり，この辞書を組み込むことによってドメイン固有の文字列を形態素解析処理で利用する．
本章で作成する辞書は茶筌の辞書の補完という位置付けであり，辞書既登録語は登録しない．
また，茶筌が元々持つ辞書の改変を行なうこともない．
辞書登録文字列の属性は以下のように決定する．
登録文字列それぞれに対して適切な品詞を人手で設定することは多大な労力を必要とするだけでなく，その適切さの評価や曖昧性の問題などが存在するため，本稿では登録文字列はすべて同じ品詞とした．
登録文字列に割り振る品詞として「有繋文字列」を新設した．
品詞「有繋文字列」と他の品詞との接続コストの設定は茶筌の既存の品詞「名詞」中の「一般」カテゴリに準拠することとした．
個々の登録文字列の形態素コストはその文字列の頻度情報などの情報に基づいて個々に設定することとする．
この関数で利用するパラメータは，本稿で提案する複数の有繋文字列辞書作成手法に依存するものとする．
形態素コスト[MATH]の算出式を式([REF_exp:morphcost])に示す．
ここで[MATH]は文字列，[MATH]は文字列[MATH]の情報を示す値であり，[MATH]に適用する値を変化させることで各辞書の特徴を形態素コストに反映させる．
形態素コストは「コーパス内に1回出現する文字列の形態素コストを4,000とする」とする茶筌の定義に基づき，下限[MATH]を0，上限[MATH]を4,000または8,000とする．
[MATH]および[MATH]は各辞書で利用する[MATH]によって決まる．
本稿では辞書に登録する文字列の選択手法を4種類用意し，4つの辞書を作成した(表[REF_tab:jisho])．
本稿では辞書登録の対象を名詞に準じる文字列に絞る．
訓練コーパスから抽出された有繋文字列を一人の手によってすべての候補をチェックし，そのままで辞書登録可能な有繋文字列，過接合有繋文字列から適切な部分を切り出した文字列の2種類の文字列を選択した．
過分割有繋文字列については，分割され削除されていた部分が容易に推測できる場合であっても，登録文字列としなかった．
登録文字列の切り出しの対象は，名詞，複合名詞，数式，数値(単位も含む)，意味のある記号の羅列，英単語の羅列とし，茶筌既登録語は登録文字列から除外した．
各登録文字列[MATH]の形態素コスト[MATH]は，式([REF_exp:morphcost])に[MATH]として[MATH]の候補文字列としての出現頻度[MATH]を適用して算出した．
他の選択手法と異なり完全に人手で確認しているためノイズの心配がないことから，[MATH]は4,000とした．
ユーザ個人が一人で選択する場合，登録語とする基準をユーザ個人で設定できるため，複数人で選別を行なう場合のようなばらつきや基準の統一といった問題がない．
しかし，ドメインが大きくなれば登録候補語も増加するため，一個人がこの選別を行なうことは大きな労力となる．
11名の被験者に登録候補文字列のリストを提示し，登録すべきもの，登録すべきか迷うもの，登録すべきでないもの，判断できないものの4段階に分類してもらい，それぞれ2，1，0点として集計を行なった．
「判断不能」は評価から外すものとした．
評価得点が0となった文字列は，被験者全員が登録すべきでないと判断したものであるため，登録候補としない．
各登録文字列[MATH]の形態素コスト[MATH]は，式([REF_exp:morphcost])に[MATH]として[MATH]の評価得点[MATH]，[MATH]を適用して算出した．
対象ドメインに詳しい複数の人間が選別を行なうことで，一人一人の労力の軽減が図れるだけでなく，適切な候補語選択がなされると考えられる．
しかし選択を行なう人の専門分野や考え方などの相違から，候補語の絞り込みが難しくなる場合もあり得る．
自動的に選別を行なう場合の最もシンプルな手法は，登録候補になんらかの順位付けを行ないそれに従って登録文字列を決定するものである．
評価値は共起情報を基に算出するため，出現頻度の高い文字列は評価値も高くなる傾向があり，この二点は独立ではない．
従って，本稿では出現頻度のみを基準として辞書登録文字列の選択および形態素コストの設定を行なう．
各登録文字列[MATH]の形態素コスト[MATH]は，式([REF_exp:morphcost])に[MATH]として[MATH]の候補文字列としての出現頻度[MATH]，[MATH]を適用して算出した．
出現頻度が1の文字列はノイズである可能性があるため登録文字列から外し，出現頻度2の時形態素コストは最大の8,000を採るように設定した．
登録候補文字列を茶筌に掛けて形態素解析を施し，得られた品詞情報を利用して登録文字列を決定する．
各登録文字列[MATH]の形態素コスト[MATH]は，式([REF_exp:morphcost])に[MATH]として[MATH]に対応する品詞列の候補文字列としての出現頻度[MATH]，[MATH]を適用して算出した．
処理の段階で動的に有繋文字列を認識し利用するシステムMでは，ノイズを完全に防ぐことは不可能である．
ノイズを抑えるためには，動的な処理でなく，事前に必要な有繋文字列を辞書登録してしまう方法が有効である．
辞書登録を行なうことでドメイン固有の文字列を辞書に反映させることが可能となるが，完全な辞書の作成は不可能であるという辞書ベースの手法の問題点の完全な解決にはならない．
また本稿で利用するd-bigram確率モデルはbigram情報の積み重ねであるため特に複合語やこれに類するものの認識において間に入る語句を柔軟に扱えるという利点があるが，辞書登録ではd-bigramの持つ柔軟性が失われる．
これらの問題を解決するために，辞書登録と切り分け処理の併用が考えられる．
事前にドメイン固有文字列の辞書登録を行ない，さらに補助として組み込みの切り分けシステムを利用することで，頻出語句の認識が可能な上，ノイズの減少を図ることが可能となる．
図[REF_fig:flo-md]に本稿で提案する統計情報利用システムと辞書作成システムを利用した有繋文字列辞書の両方を茶筌に組み込んだ場合による形態素解析の流れを示す．
表[REF_tab:threshold]にシステムMを組み込んだ実験での閾値ごとの形態素コストを示す．
実験M+DではシステムMをシステムDの補完の立場で利用するため実験Mに比べて形態素コストを大きく設定している．
