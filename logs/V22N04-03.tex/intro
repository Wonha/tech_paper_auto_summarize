\section{はじめに}
\label{sect:intro}

対訳文中の単語の対応関係を解析する単語アラインメントは，統計的機械翻訳に欠かせない重要な処理の一つであり，研究が盛んに行われている．
その中で，生成モデルであるIBMモデル1-5 \cite{brown93}やHMMに基づくモデル\cite{vogel96}は最も有名な手法であり，それらを拡張した手法が数多く提案されている\cite{och03,taylor10}．
近年では，Yangらが，フィードフォワードニューラルネットワーク (FFNN) の一種である「Context-Dependent Deep Neural Network for HMM (CD-DNN-HMM)」\cite{dahl12}をHMMに基づくモデルに適用した手法を提案し，中英アラインメントタスクにおいてIBMモデル4やHMMに基づくモデルよりも高い精度を達成している\cite{yang13}．
このFFNN-HMMアラインメントモデルは，単語アラインメントに単純マルコフ性を仮定したモデルであり，アラインメント履歴として，一つ前の単語アラインメント結果を考慮する．

一方で，ニューラルネットワーク (NN) の一種にフィードバック結合を持つリカレントニューラルネットワーク (RNN) がある．
RNNの隠れ層は再帰的な構造を持ち，自身の信号を次のステップの隠れ層へと伝達する．
この再帰的な構造により，過去の入力データの情報を隠れ層で保持できるため，入力データに内在する長距離の依存関係を捉えることができる．
このような特長を持つRNNに基づくモデルは，近年，多くのタスクで成果をあげており，FFNNに基づくモデルの性能を凌駕している．
例えば，言語モデル\cite{mikolov10,mikolov12,sundermeyer13}や翻訳モデル\cite{auli13,nal13}の構築で効果を発揮している．
一方で，単語アラインメントタスクにおいてRNNを活用したモデルは提案されていない．
本論文では，単語アラインメントにおいて，過去のアラインメントの情報を保持して活用することは有効であると考え，RNNに基づく単語アラインメントモデルを提案する．
前述の通り，従来のFFNNに基づくモデルは，直前のアラインメント履歴しか考慮しない．
一方で，RNNに基づくモデルは，隠れ層の再帰的な構造としてアラインメントの情報を埋め込むことで，FFNNに基づくモデルよりも長い，文頭から直前の単語アラインメントの情報，つまり過去のアラインメント履歴全体を考慮できる．

NNに基づくモデルの学習には，通常，教師データが必要である．
しかし，単語単位の対応関係が付与された対訳文を大量に用意することは容易ではない．
この状況に対して，Yangらは，従来の教師なし単語アラインメントモデル（IBMモデル，HMMに基づくモデル）により生成した単語アラインメントを疑似の正解データとして使い，モデルを学習した\cite{yang13}．
しかし，この方法では，疑似正解データの作成段階で生み出された，誤った単語アラインメントが正しいアラインメントとして学習されてしまう可能性がある．
これらの状況を踏まえて，本論文では，正解の単語アラインメントや疑似の正解データを用意せずにRNNに基づくモデルを学習する教師なし学習法を提案する．
本学習法では，Dyerらの教師なし単語アラインメント\cite{dyer11}を拡張し，正しい対訳文における単語対と語彙空間全体における単語対を識別するようにモデルを学習する．
具体的には，まず，語彙空間全体からのサンプリングにより偽の対訳文を人工的に生成する．
その後，正しい対訳文におけるアラインメントスコアの期待値が，偽の対訳文におけるアラインメントスコアの期待値より高くなるようにモデルを学習する．


RNNに基づくモデルは，多くのアラインメントモデルと同様に，方向性（「原言語$\boldsymbol{f}\rightarrow$目的言語$\boldsymbol{e}$」又は「$\boldsymbol{e}\rightarrow\boldsymbol{f}$」）を持ち，各方向のモデルは独立に学習，使用される．
ここで，学習される特徴は方向毎に異なり，それらは相補的であるとの考えに基づき，各方向の合意を取るようにモデルを学習することによりアラインメント精度が向上することが
    示されている (Matusov, Zens, and Ney 2004; Liang, Taskar, and Klein 2006; Gra\c{c}a, Ganchev, and Taskar 2008; Ganchev, Gra\c{c}a, and Taskar 2008)．\nocite{matusov04,liang06,graca08,gancev08}
そこで，提案手法においても，「$\boldsymbol{f}\rightarrow\boldsymbol{e}$」と「$\boldsymbol{e}\rightarrow\boldsymbol{f}$」の2つのRNNに基づくモデルの合意を取るようにそれらのモデルを同時に学習する．
両方向の合意は，各方向のモデルのword embeddingが一致するようにモデルを学習することで実現する．
具体的には，各方向のword embeddingの差を表すペナルティ項を目的関数に導入し，その目的関数にしたがってモデルを学習する．
この制約により，それぞれのモデルの特定方向への過学習を防ぎ，双方で大域的な最適化が可能となる．

提案手法の評価は，日英及び仏英単語アラインメント実験と日英及び中英翻訳実験で行う．
評価実験を通じて，前記提案全てを含む「合意制約付き教師なし学習法で学習したRNNに基づくモデル」は，FFNNに基づくモデルやIBMモデル4よりも単語アラインメント精度が高いことを示す．
また，機械翻訳実験を通じて，学習データ量が同じ場合には，FFNNに基づくモデルやIBMモデル4を用いた場合よりも高い翻訳精度を実現できることを示す\footnote{実験では，NNに基づくモデルの学習時の計算量を削減するため，学習データの一部を用いた．全学習データから学習したIBMモデル4を用いた場合とは同等の翻訳能であった．}．
具体的には，アラインメント精度はFFNNに基づくモデルより最大0.0792（F1値），IBMモデル4より最大0.0703（F1値），翻訳精度はFFNNに基づくモデルより最大0.74\% (BLEU)，IBMモデル4より最大0.58\% (BLEU) 上回った．
また，各提案（RNNの利用，教師なし学習法，合意制約）個別の有効性も検証し，機械翻訳においては一部の設定における精度改善にとどまるが，単語アラインメントにおいては各提案により精度が改善できることを示す．

以降，\ref{sect:related}節で従来の単語アラインメントモデルを説明し，\ref{sect:RNN}節でRNNに基づく単語アラインメントモデルを提案する．
そして，\ref{sect:learning}節でRNNに基づくモデルの学習法を提案する．
\ref{sect:experiment}節では提案手法の評価実験を行い，\ref{sect:discuss}節で提案手法の効果や性質についての考察を行う．
最後に，\ref{sect:conclusion}節で本論文のまとめを行う．


