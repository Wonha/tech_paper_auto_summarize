実験と考察

 前章で述べた各要素の評価のために2種類の実験を行った．1つは文順序に関
 するタスクであり，もう1つは自動要約で生成された要約テキストの
 ランキングのタスクである．

 実験において性能を評価するモデルと各モデルから生成される文書ベクトル
 の次元数を表\ref{features}に示す．各モデルに対して接続関係毎に遷移確率を計算してベクトルを作
 成した場合(+CONJ)と，
 それを行わなかった場合(no CONJ)の両方で実験を行った．
 全て
 のモデルにおいて頻出する要素とそうでない要素に分けてgridを作成する．そ
 の閾値はBarzilayらの手法と同様に2とした．BaselineはBarzilayら
 \cite{barzilay2008}の設定に従ったモデルである．但し，本論文ではBaselineにおいても共参照解析は行わない．

   \begin{table}[t]
     \caption{検討するモデル}\label{features}
\input{09table07.txt}
   \end{table}

   実験では，形態素解析には
   MeCab\footnote{http://mecab.sourceforge.net/}を，係り受け解析には
   CaboCha\footnote{http://www.chasen.org/\~{}taku/software/cabocha/}を
   使用した．



 \subsection{予備実験}\label{pilot}

 語彙的結束性の考慮において，日本語語彙大系を利用したクラスタリングの際
 に使用する意味属性の段と語彙的連鎖によるクラスタリングの際の閾値をあら
 かじめ決定する必要がある．本論文ではこれらの値を予備実験によって決定した．

 予備実験は表\ref{features}のSC(1st)とLC(1st)のモデルに
 対して，\ref{sentenceordering}節と同じタスクを行った．使用したデータは
 朝日新聞コーパスの2003年の記事のうち，``人もの''というカテゴリに分類
 されている記事100件である．この記事の順番を無作為に並べ替えたものと元の
 記事を比べて元の記事の方が一貫性があると判定したペアを正解と見なし，そ
 の精度が最も良かった値を使用するパラメータとした．

 日本語語彙大系で用いられている意味属性体系は最大で12段あり1が最も抽象的な概
 念である．このうち3〜9の段に対して実験を行った．結果を
 表\ref{goitaikeipilot}に示す．

 同様に，語彙的連鎖のクラスタのマージに関する閾値について，0.05か
 ら0.5まで0.05刻みで値を変えて行った実験の結果を表\ref{lcpilot}に示す．

 これらの結果から，本実験では語彙的連鎖のクラスタのマージの閾値には0.35
 を使用した．また，日本語語彙大系を用いたクラスタリングは語彙的連鎖のク
 ラスタリングに比べて，精度が低いことが判明したため，以降の実験では
 文中の要素のクラスタリングでは語彙的連鎖によるクラスタリングについての
 み行う．

 


 \subsection{実験1: テキストの並べ替え}\label{sentenceordering}

 文順序に関するタスクでは，3章で述べた評価方法と同様にオリジナ
 ルのテキストと文の順番を並べ替えたテキストとを比較し，どちらが一貫性があ
 るかを正しく判定できた精度でモデルの評価を行った．

 \begin{table}[b]
   \caption{予備実験結果（日本語語彙大系）}\label{goitaikeipilot}
\input{09table08.txt}
 \end{table}
 \begin{table}[b]
   \caption{予備実験結果（語彙的連鎖）}\label{lcpilot}
\input{09table09.txt}
 \end{table}
 \begin{table}[b]
   \caption{各モデルのコストパラメータの値}\label{costparameter}
\input{09table10.txt}
 \end{table}


 学習にはSVM$^{light}$\footnote{http://www.cs.cornell.edu/people/tj/svm\_light/}
 のranking SVMモードを使用した．
 コストパラメータ$c$の値については各モデル毎に予備実験で使用
 したデータを使って10分割交差検定を行い，最も精度が高いものを使用してい
 る．各モデルに対する$c$の値を表\ref{costparameter}に示す．
 
 また，並べ替えテキスト中の文の順番がオリジナルのテキストの文の順番と大きく異なってい
 れば，一貫性の判定は容易になると考えられる．このことを
 検証するために，表\ref{permutation}に示す5種類の並べ替えの比較を行う．
 swap1はオリジナルのテキストとの差が最も小さく，randomはその差が最も大き
 くなる．mixはswap1，swap2，swap3の混合であり，swap3よりは差は小さい．

 本実験では朝日新聞コーパスの2003年分の記事から，``行政改
 革''，``医療''，``教育''というカテゴリに該当するもので，1記事あたり10文
 以上で構成されているものを使用した．データセット中に含まれる記事のカテゴリの割合は
 均等になるように調整している．
 オリジナルのテキストと比較する並べ替えテキストに関して，表\ref{permutation}に
 示す各並べ替えの種類のそれぞれにおいて，1つの記事に対して20個の並べ替え
 テキストを生成した．
 
 この比較する並べ替えのテキストの数に関して，Karamanis \cite{karamanis2006}はセンタリング理論
 に基づいた手法に対して，信頼できる結果を得るために100,000個の並べ替えテ
 キストを生成して評価を行っているが，我々は
 entity gridに基づいたモデルを用いており，このモデルでの精度向上を目的と
 しているため，使用する並べ替えテキストの数はBarzilayら
 \cite{barzilay2008}の実験の設定にあわせている．

 \begin{table}[b]
   \caption{並べ替えの種類}\label{permutation}
\input{09table11.txt}
 \end{table}

 実験はデータセットに対して10分割交差検定を行い，テストデータ中の各ペアにおいてオリジナルのテキストの
 方が一貫性があると判定されたペアの割合で評価した．
 
 表\ref{exp1_model}に100記事，300記事に対してmixの並べ替えを行ったデータ
 を用いた各モデルの結果を示す．``no CONJ''は各モデルにおいて接続関係毎の
 遷移確率の計算を行わなかった場合，``+CONJ''は接続関係毎の遷移確率の計算
 を行った場合を示す．
 

 表中の太字の
 数値は使用したデータにおいて最も良かったものを表し，斜字は
 ベースライン（接続関係を未考慮のBaseline）
 を下回ったものを表す．また，右肩の記号$ ^{**} (p < 0.01)$，$ ^{*} (p
 < 0.05)$は符号検定においてベースラインの精度と有意な差があることを示す．

 全体としては，いくつかベースラインを下回っているものがあるものの，多く
 のモデルにおいてベースラインを有意に上回る結果を得ることができた．
 接続関係毎に遷移確率を計算したモデル（``+CONJ''の列）の方がそうでないモデル
 （``no CONJ''の列）に比べて良い結果を示している．特に各データセッ
 トにおいて接続関係のタイプを考慮したモデルが最も良い精度を得ており，文
 脈の展開を明示的に示す接続表現から得られる接続関係が一貫性の判定に有用
 であることを示している．

\begin{table}[t]
  \caption{モデル別の結果（実験1）}\label{exp1_model}
\input{09table12.txt}
\end{table}

 gridから作成される文書ベクトルの素性は構文役割の遷移の組合せの数だけ存
 在する．従って，構文役割を拡張したモデル(SR(H))はベースラインのモデルに
 比べて素性の数が多くなり，
 データが少なかった時の影響が顕著に表れると考えられる．

 また，参照表現については少数の明示的な指示形容詞のみに限定したため，参照
 表現を考慮したモデル(REF)でもそれほど差が出なかったと考えられる．

 語彙的結束性に基づいたクラスタリングを行ったモデルでは良好な結果を得る
 ことができた．

 本論文で提案した要素の全てを考慮したモデルは，全ての場合で最良の結果を
 得ることはなく語彙的結束性のみを考慮したモデルを下回った場合も
 あった．これは構文役割を拡張したモデルと同様に，全ての要素を考慮したモデルと語彙的結束性のみを考慮したモ
 デルとでは文書ベクトルの次元の数が異なることが影響していると考えられる．

 本論文で用いている一貫性モデルではモデルの学習に必要なデータは人間が書
 いたテキストのみであり，学習のために特別な情報を付与する必要はない．従っ
 て，学習データの作成に必要なコストはほとんどない．そこで，データを
 更に増やして実験を行った．この実験ではベースラインのモデル(Baseline)，構文役割を
 拡張したモデル(SR(H))，全ての要素を考慮したモデル(ALL-LC(comb))，接続関係毎に遷移確率を計算
 したモデル(+CONJ, Baseline)と全ての要素を考慮した接続関係毎に遷移確率を
 計算したモデル(+CONJ, ALL-LC(comb))のみを使用した．
 結果を表\ref{exp1_add_tab}に，そのグラフを図\ref{exp1_add}に示す．

  接続関係毎の遷移確率を考慮したモデルはそうでないモデルに比べて，データ
  が増加するにつれて精度が向上することが明らかになった．一方，構文役割を
  拡張したモデルはデータを増やしていってもベースラインに比べてあまり向上
  は見られなかった．

 表\ref{exp1_data}に並べ替えの種類毎での結果を示す．
 比較するテキストの差と問題の難易度との関係については，差が一番小さい
 swap1では精度が最も低く，一番大きいrandomでは精度が最も高くなっており，
 仮説通りの結果が得られた．また，全てのデータセットにおいてBaselineと比べて本論文のモデルの方が良
 好な結果を得ることができた．


\begin{table}[t]
  \caption{データ増加時の精度（実験1）}\label{exp1_add_tab}
\input{09table13.txt}
\end{table}
  \begin{figure}[t]
   \begin{center}
\includegraphics{17-1ia9f6.eps}
   \end{center}
     \caption{データ増加時の精度（実験1）}
\label{exp1_add}
  \end{figure}
\begin{table}[t]
  \caption{データ別の結果（実験1）}\label{exp1_data}
\input{09table14.txt}
\end{table}



 \subsection{実験2: 要約文書の比較}

 \ref{sentenceordering}節で行った実験では，あるテキストとそのテキスト中
 の文の順番を並べ替えたものとを比較している．このため比較する2つのテキストの単語の
 出現頻度分布は等しいと言える．しかし，実際にはこのような状況は稀であると考えら
 れる．
 例えば，自動要約の評価においては同じ元文書から生成された要約を用いてシ
 ステムの評価を行う．元文書が同じであっても，システム毎に異なる要
 約が生成されることがあり，これらの単語の出現傾向は異なると考えられる．

 そこで，実際に自動要約システムによって生成された要約を比較し，どちらが
 一貫性があるかを判定するという実験を行った．
 実験に使用したデータは
 NTCIR-4\footnote{http://research.nii.ac.jp/ntcir/ntcir-ws4/ws-en.html}\cite{ntcir4}
 のサブタスクであったTSC3 (Text Summarization Challenge
3)\footnote{http://www.lr.pi.titech.ac.jp/tsc/index-en.html}に提出された
 要約である．TSC3では11のシステムが30件の元文書に対してそれぞれ長短2種類
 の要約を生成している．このうち実際に要約文が出力されている657件の要約を
 使用した．

 それぞれの要約には被験者による評価結果が付与されている．この
 評価では，被験者は15個のQuality questionと呼ばれるテキストの質に関するチェック項
 目\cite{hirao2004}が示され，各項目毎にスコアを付ける．このQuality questionは主に要約の
 読みやすさに対する質問で構成されている．本実験では特に一貫性に関係する項目のスコ
 アのみに着目し，これらから要約の一貫性に関するスコアを計算し比較を行った．

 要約のスコアの決定について述べる．TSC3で用いられた15個のチェック項目の
 うち，付録\ref{qq}に示す8個の項目のスコアを利用した．

 それぞれチェック項目のスコアは各項目の内容に該当する箇所の個数であり，$qq_2$〜$qq_8$については$qq_1$の項目に当てはまった重複文は除外して数え
 られている．以上より，要約$S$のスコア$score(S)$を以下の式によって求める．
 \begin{equation}
  score(S)=\frac{N(qq_2)+\dots +N(qq_8)}{length(S)-N(qq_1)}+\frac{N(qq_1)}{length(S)}
 \end{equation}
 ここで，$N(qq_i)$はチェック項目$qq_i$のスコアであり，$length(S)$は要約
 $S$の文数である．$qq_8$の回答は``矛盾している''，
 ``どちらともいえない''，``矛盾していない''のいずれかであり，これらのス
 コアは順に1，0.5，0として$N(qq_8)$の値とした．各スコアは文章中のおかし
 な箇所の個数であることから，$score(S)$は小さい方が良いテキスト，即ち，本実験
 においては一貫性が高いと考える．

 この$score(S)$を用いて，本実験では同じテキストから生成された異なるシステムによる同じ長さの要約のペアに対し，どちら
 の要約が一貫性があるかを判定する．従って，タスクとしては実験1と同じもの
 となる．比較する要約のスコアが等しいペアは除外する．

 テキスト一貫性の判定を実際に利用する状況では，判定が必要なデータを訓練データ
 に用いることはできず，別に訓練データを用意する必要がある．
 このことを考慮して，
 本実験では交差検定ではなく\ref{sentenceordering}節の実験
 において作成した300記事とそのmixの並べ替えを訓練データとして用い，それによって
 得られたモデルを用いて判定を行った．実験に使用した学習器や各パラメータの
 値は\ref{sentenceordering}節での実験と同じである．

 また，\ref{sentenceordering}節の実験と同様に，比較するテキストの差によ
 る精度の違いの検証も行った．本実験ではそれぞれの要約のスコアの差が大き
 ければ，一貫性の判定は容易になると考えられる．そこで比較する要約のペア
 のスコアの差を0から2.0まで0.5刻みでの範囲で分割し，それぞれでの精度を計
 算した．


 用いたテストデータ全てに対する，各モデルの精度を表\ref{exp2_model}に示す．表
 中の記号，字体の意味については前節の実験と同様である．

\begin{table}[b]
  \caption{モデル別の結果（実験2）}\label{exp2_model}
\input{09table15.txt}
\end{table}


 学習データとテストデータのドメインが異なるために，前節の実験に比べて全
 体的な精度は低くなっているが，提案したほぼ全てのモデルにおいてベースラインよりも良い精度を得る
 ことができた．

 接続関係を考慮したモデル（表中の``+CONJ''の列）とそうでないモデル
 （表中の``no CONJ''の列）では，最も良い精度を得られたモデルは接続関係を考
 慮しない場合でのものであったが，それぞれのモデルにおいての接続関係の考慮
 の有無による違いでは考慮した方が良い精度を示しているものが多くなってい
 る．本実験においても，構文役割の拡張(SR(H))や参照表現の考慮(REF)を組み
 込んだモデルは，ほとんど改善が見られなかった．これは前節の実験結果と同
 様であった．


 語彙的結束性に基づくクラスタリングにおいても，同様にベースラインを上回
 る結果を得ることができた．



 比較した要約のスコアの差が小さければ，それらの一貫性を判定することが困
 難になると考えられる．そこでテストデータの各ペアのスコアの差毎の精度を
 求めた．その結果を表\ref{exp2_data}に示す．1行目のラベルの括弧の中の数
 字はその範囲に該当する要約ペアの数である．差が2.0より大きいペアについて
 は，該当するペアの数が多くなかったため省略している．

 要約のスコアの差と精度の関係については，こちらも仮説通りスコアの差が小
 さければ判定は難しくなり，差が大きくなれば判定は容易であるという結果に
 なった．
 

 本実験で用いたTSCのデータは1つの元文書に対して複数のシステム要約が存在
 しており，上述の計算式で求められたスコアに基づいて同じ元文書から
 生成された要約を順位付けすることができる．そこで要約のペアの比較ではな
 く，同一文書から生成された要約の順位を推定するという実験を行った．
 使用したモデルは前述の実験と同じ300記事とそのmixの並べ替えのデータで学
 習したものである．評価にはSpearmanの順位相関係数の平均を使用した．結果
 を表\ref{rankcor}に示す．

\begin{table}[t]
  \caption{スコアの差毎の結果（実験2）}\label{exp2_data}
\input{09table16.txt}
\end{table}
\begin{table}[t]
  \caption{順位相関（実験2）}\label{rankcor}
\input{09table17.txt}
\end{table}

ベースラインに比べて提案したモデルに高い相関を示すものがあったが，全体的に相関は低いという結果が得られた．
これは特にスコアの差が小さい場合での判定精度が影響していると考えられる．


 本実験結果から，ベースラインと比べると，ある程度実際にテキスト一貫性の
 判定を行うような設定においても本論文で提案したモデルの方が有効であるこ
 とが明らかになった．しかし，その精度は高いとは言えず，改良の余地がある．



 