関連研究
\label{sec:related-work}

言語の多様化にともなって，自動的に言語の識別を行うことの重要性が増してきている．
その一つの背景には，ウェブの急速な成長にともなう英語以外の文書の増加がある．
Internet World Stats による近年のウェブユーザ数の言語別集計の結果\footnote{http://www.internetworldstats.com/stats7.htm} によると，2008 年現在では，依然として英語を利用するユーザが最も多く，それに続いて，中国語，スペイン語，日本語，フランス語を利用するユーザが多い．
2000 年からの言語別ユーザ数の増加の割合は，アラビア語，ポルトガル語，中国語，フランス語，スペイン語が大きな伸びをみせている．
この調査結果は，用いられる言語の多様性が増していることを示しているといえる．
また，ウェブ上の文書の内の半数以上は英語以外の言語で書かれているものであると同時に，一つの文書の中で複数の言語が使われていることもある．
多種多様な情報元からの情報検索や質問応答，機械翻訳等，ウェブ上の膨大なデータを対象とした自動処理の実現においては，
文書の使用言語の自動推定だけでなく，文書中に出現する固有名詞等の外国語の語句の的確な解析も，自然言語処理の応用分野における精度向上に大きな影響を与える要因となりうる．


\subsection{統計情報を用いた言語識別に関する研究}
\label{sec:language-identification}

言語識別の研究は，文書を対象としたものに限らず，音声認識 \cite{matrouf98,berkling94} や，文書イメージを対象にしたものもある．
Sibun らは，文書イメージから抽出された文字の形状の統計的な分布を利用して言語識別を行った\cite{Sibun94}．
彼らは，アルファベットの文字の形状を，ベースライン，ボトム，トップ，X ハイトの情報を使って分類し，文書中の文字を Linear Discriminate Analysis (LDA) を用いて分類した．
2,000 から 3,000 文字を含む 23 の言語の文書イメージを用いて文書を構成する言語を識別する実験を行った結果，90\% 以上の精度を達成している．

Dunning は，ドイツ語の `\"u' やフランス語の `\^e' 等のアクセント記号を用いずに，5,000 バイトのトレーニングコーパスと 500 バイトのテスト用テキストを用いた言語識別の実験で 97\% の精度を実現している \cite{dunning94}．
Dunning らは 20 バイトのテスト用テキストでも 92\% の精度での言語識別を実現し，短い文書や数単語で構成される句であっても，言語識別が可能であることを示した．

Lins らは，文書中に含まれる複数の単語に対して各言語の辞書中での出現の有無を調べる手法で言語識別を行った~\cite{Lins04}．
Lins らは言語内で比較的種類が少ないとされている副詞，冠詞，接続詞，感嘆詞，数詞，前置詞，代名詞のみを辞書引きの対象とすることで，高速かつ汎用性の高い手法を提案している．
Lins らはこの手法を用いて，ポルトガル語，スペイン語，フランス語，英語の文書（約 1,000 単語で構成される 600 の文書）を対象とした評価実験を行い，ウェブの文書でも 80\% 以上，通常の文書では 90\% 以上の精度を達成している．

Martins らは，ウェブページに特化した言語識別手法を提案した\cite{Martins05}．
Martins らの手法は，$n$-gram（1 から 5）情報のプロファイル間の距離と，ウェブページ固有のヒューリスティクスを用いるものである．
12 の言語で各 500 ウェブページを用いた実験では，全ての言語で 84\% 以上の正解を出したが，スウェーデン語とデンマーク語等の北欧の言語の類似性が若干の精度の低下をもたらしたことを今後の課題として挙げている．

また，地名以外の固有名詞として人名に着目し統計情報を用いた所属国の推定を行う研究もある．
Nobesawa らは，言語識別の手法を人名に対して適用することで，人名用の言語識別のためのシンプルなシステムを提案し，人名を属する国で分類することが可能であることを示した\cite{nobesawa0512paclic}．
この手法は，人名文字列の長さや，人名の文字単位の $n$-gram の情報を活用したものであり，9 種類の言語圏の 12 の国に対して 90\% 以上の精度を実現することに成功している．
また，Nobesawa らは，英語の人名に対して SVM の分類器を用いた手法も提案している\cite{nobesawa0605ieee}．


\subsection{エリア推定に関する関連研究}
\label{sec:toponym-resolution}

地名のエリア推定の最終的な目標は，その地名が地球上のどの場所を示しているのかを判断することである．
文章中の地名のエリア推定タスクは，一般に， (1) 地名文字列の認識， (2) 地名文字列の国推定， (3) 地名文字列と場所との対応付け，の三段階の処理からなる．
(1) の地名文字列の認識は固有名詞の自動抽出タスクに相当する．
(2) および (3) は，地名文字列と地球上の場所との対応付けを行う処理である．
本稿ではこのうち，研究のあまり進んでいなかった (2) の所属国推定を目的とし，その実現手法を提案するものである．

(3) の地名文字列と場所との対応付けの研究では，あらかじめ対象ドメインや対象言語を制限することで，(2) の所属国の推定のステップを省略することが多い．
したがって，本稿が対象とする所属国の推定の研究は，この地名文字列と場所との対応付けの研究を助け，その精度の向上に寄与するものと考えている．

本節では，関連する研究として (3) の地名文字列と場所との対応付けを行っているものについて述べる．
これらは，対象の地名が辞書に登録されていることを前提として辞書引きによって場所の候補を探しだし，複数の場所が候補として挙がる等の曖昧性がある場合には文脈情報等を用いてその判別を行うアプローチが一般的である．

Hauptmann と Olligschlaeger は，音声データを対象とした場所の判別を行う手法を提案している~\cite{Hauptmann99,Olligschlaeger99}．
基本的には地名辞書に含まれる地名のみを対象としているが，同じ地名であっても複数の異なる場所を示す曖昧性がある場合には，同一の会話内に現れる他の地名の情報を活用することによって，その場所の相違を判断している．
Hauptmann らの手法では，200 のニュース記事に出現した 357 の地名のうち 269 の地名を正しく判別することができ，75\% の精度を達成している．
Hauptmann らは，正しく判別することができなかったものは，地名辞書に載っていなかったもの，曖昧性によるエラー，音声認識誤り等が原因であると報告している．

また，Smith らは地名の曖昧性の解消に焦点を当て，文書中の地名の出現頻度の重心を利用した手法を提案した \cite{Smith2001}．
これは，地名の出現頻度によって重みが付けられた地図上での重心を計算し，ある閾値よりも離れているものを枝刈りした上で重心を再度計算しなおすことで候補の曖昧性を解消しようとするものである．
大きな地名辞書を使うことによって，再現率を高く保てるようにした上で，F 値が 0.81 から 0.96 という結果を出している．
しかし，この手法はその重心の付近を示すだけであり，重心のみを使用しただけでは頑健性に欠ける場合があると結論づけている．

地名の曖昧性を解消するための手法として，Li らは，地名表現のパターンマッチングと重み付き地名の類似度グラフの探索，サーチエンジンを用いた地名辞書の補間の三つのアプローチを組み合わせる方法を提案した \cite{Li2002}．
地名表現のパターンマッチングでは，`city of' + ``地名''（``city of Vancouver'' 等）や ``地名1'' + `,' + ``地名2'' + `,' + ``地名3''（``Boston, Massachusetts, USA'' 等）といった地名の周辺の表現のパターンを利用している．
Li らは大きな地名辞書と地名表現のパターンマッチングを用いることで，93.8\% の精度を実現した．

Pouliquen らは，ヨーロッパのエリアに限定したマルチリンガルテキストを対象として，場所の判別の精度の向上を目指す手法を提案した~\cite{Pouliquen06}．
この手法では，``And'', ``To'', ``Be'' 等の瀕出する単語と同形異義語であるような地名を geo-stop list として抽出し，このような地名を候補から排除することで，精度の向上を図っている．
また，それ以外にも場所の重要度，人名との区別，地名同士の物理的な距離の情報等を用いて曖昧性の解消を行っている．
geo-stop list に登録されている地名を候補から排除することで再現率は低下するが，F 値で 0.77 という結果を示している．

Clough は，複数の地名辞書を用いた場所の判別手法を提案している \cite{Clough05}．
複数の地名辞書を優先順位を付けて検索し，stop-list を使って，各候補に対してスコアを与えている．
このスコアは，地名表現の周辺の出現パターン，オントロジにおける階層の深さ，地名辞書の優先順位，ユーザのプリファレンスによって計算される．
Clough はイギリス，フランス，ドイツ，スイスを対象とした実験で 89\% の精度を達成している．

Zong らは，ウェブページに対してそのページが記述しているエリアを判別する実験を行った~\cite{Zong05}．
アメリカに関する文書のみを対象とし，地名の周辺の出現パターンと地名同士の物理的な距離を利用することで，地名が 32 個以上 199 個以下だけ含まれるウェブページを対象に 760 の地名について実験を行い 88.9\% の精度を達成している．

これらの関連研究のほとんどは，文書中に出現する地名を対象としており，文脈の情報を用いて地名の場所の判別を行っている．
これらは，その地名がどんな文脈で出現し，同時に出現するその他の地名とどんな関連があるのかといった情報を積極的に利用する方法である．
これらの手法の大きな特徴として，地名の認識および場所の判別に地名辞書を利用していることが挙げられる．
地名を表記するときによく用いられるパターンや会話における局所性等の自然言語処理でよく用いられるヒューリスティクスだけではなく，都市の人口数，実際の二地点間の距離等の地理的な情報を活用しているものもある．
このような辞書ベースのアプローチは，特定のドメインを対象とした処理の場合には高い精度で場所の判別を行うことが可能である．
このように一般的な自然言語処理のヒューリスティクスが適用可能な情報元を用い，かつ，そのドメインに出現しうる地名が変化するスピードが遅く，辞書や地理的なデータの整備を十分に行える場合には，これらの手法は十分に適用可能である．

しかし，全世界のすべての地名を網羅した地名辞書を整備することは現実的でない上に，情報元の多様化のスピードがますます加速している現在では，より頑健性の高い柔軟な手法が必要と考えられる．
Rauch らは，知らない地名であっても人間はその所属地域をある程度推測可能であるという事実を背景として，表層的な統計情報をベースとしたシステムが有効であると主張している~\cite{Rauch03aconfidence-based}．
本稿は，Rauch らと同じ主張を共有し，具体的な実現手法を提案するものである．


