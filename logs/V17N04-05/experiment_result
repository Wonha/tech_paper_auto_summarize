提案手法では従来手法と比べ，下記の2点が異なる．
各感情コーパスにおける``入力文の形態素N-gram''の出現回数によって決まるペナルティFPの導入
形態素N-gramの適合率の相加平均による類似度計算
評価実験では上記の2点によって感情推定の成功率がどの程度変化するのかを調べる．
実験には，\resp{基本的な感情であり，収集したコーパス中に比較的頻出した}``喜び''，``怒り''，``嫌悪''，``希望''の4種類の感情カテゴリを用いた．
各感情コーパスには838文の発話文が含まれる．
発話文はWeb上の掲示板から8名の作業者によって収集した．
発話文の分類先となる感情コーパスは，作業者の主観によって決定した．
また発話文の分類先は複数選ぶことを許容した．
入力となる文とその感情は次の手順で決定した．
まず，感情コーパスに含まれない，別途掲示板から収集した文を無作為な順番で被験者4名に提示し，被験者に文の感情を判定させる．
このとき，判定結果としての感情を``喜び''，``怒り''，``嫌悪''，``希望''の中から0個以上を選ばせる．
被験者4名のうち3名以上の判定結果が一致した文を各感情ごとに51文ずつ用意し，入力文として用いる．
なお，この予備実験で入力文に割り振られた感情の数はすべて1つとなった．
感情推定の成功条件として，出力として得られる4つの感情類似度のうち，最も値が大きい類似度の感情と，入力文の感情が一致すれば成功とした．
上記の([REF_item:new1])，([REF_item:new2])の効果を確かめるために，実験に用いた類似度計算式は\resp{三品らの方法（式({[REF_eq:bleu]})）}と\resp{RECARE（式({[REF_eq:recare]})）}に加え，BLEUにFPのみを導入した\resp{{[MATH]}（式({[REF_eq:bleu_with_fp_pi]})）}と，\respeqn{RECARE}からFPを除いた[MATH]を用いた．
[MATH]を次のとおりに定義する．
類似度の計算に用いる[MATH]の[MATH]の値を変化させながら，推定成功率を計算した．
図[REF_fig:success_ratios]に感情推定の成功率を示す．
三品らの方法で最も推定成功率が良好だったのは[MATH]を用いたときの60.3%であり，提案手法では[MATH]を用いたときの\resp{81.8%}であった．
ここでは，まず([REF_item:new1])のFPを導入したことによる成功率の影響について考察する．
図[REF_fig:success_ratios]より，FP無しの[MATH]で最も良好だった成功率57.8%(N=3)と比べて，FP有りの提案手法[MATH]の成功率\resp{81.8%}が大きく上回っていることがわかる．
同様に，FP無しの従来手法[MATH]の成功率60.3%と比べて，FP有りの[MATH]で最も良好だった成功率\resp{77.9%(N=1)}が大きく上回っている．
これらのことから，形態素N-gramの適合率の平均の求め方に関わらず，FPの導入が成功率の向上に寄与していることがわかる．
次に([REF_item:new2])の，類似度計算に形態素N-gramの適合率の相加平均を用いたことによる成功率の影響を考察する．
[MATH]以降で[MATH]が増加するにつれて，三品らの方法の成功率は減少して\resp{いるが，}提案手法においては\resp{成功率の減少は認められなかった．
}このことから，形態素N-gramの適合率の相加平均を類似度計算に用いることは，高次の[MATH]を用いたときの成功率の改善に効果があることがわかる．
\resp{なお，類似度計算に相乗平均を用い，FPを導入した方法({[MATH]})は，Nを増加させると急激にその性能を落としていた．
この原因は，Nが大きくなると共通の形態素N-gramがコーパス中に存在しなくなる割合が増加するため，式({[REF_eq:wn]})において，1文中のすべての{[MATH]}で{[MATH]}が0になる，という場合が増加したためであった．
提案方法においては，類似度計算に相加平均を用いることでこの問題を解決し，Nが大きい場合においても高い性能を維持していることがわかった．
}
\resp{このような場合は，共通形態素N-gramが存在しないため，FPと同様に形態素N-gram適合率({[MATH]})も0になる．
そこで，{[MATH]}だけではなく，同じように相乗平均を利用している従来方法({[MATH]})も影響を受けると考えられる．
本実験においては，従来方法における類似度計算に式({[REF_eq:bleu]})を用いている．
この式では{[MATH]}の対数をとっているために，実装上，もし{[MATH]}であった場合は，非常に小さな正の値にフロアリングした上で対数を求めていた．
そのため，{[MATH]}のように急激に性能を落とすことはなかったと考えられる．
このことを確認するため，式({[REF_eq:bleu]})を式({[REF_eq:bleu_with_fp_pi]})と同様，対数を用いない形に変形した上で，フロアリングをせずに実験を行ったところ，{[MATH]}も{[MATH]}と同様，Nが大きくなるとその性能を急激に落とす結果となった．
}
\resp{提案方法（や三品らの方法）においては，各感情コーパス中に含まれる形態素N-gramのうち，感情ごとに出現頻度に偏りがあるものが，感情推定において重要な意味を持つ．
そこで，どのような形態素N-gramが提案方法にとって有効に働いたのか，といったことについて調査を行った．
}
\resp{感情推定実験において，三品らの方法では感情判定に失敗し，提案方法で成功した入力サンプルを抽出し，その中に含まれるすべての形態素N-gramについて，各感情コーパス内での出現頻度を調べた．
特に正解感情のコーパスに偏って頻出している形態素N-gramを表{[REF_tbl:freq1]}〜{[REF_tbl:freq4]}に示す．
これを見ると，「喜び」の``ありがとう／感動詞''や``♪／名詞''，「怒り」の``むかつく／動詞''，「希望」の``たい／助動詞，です／助動詞''等，感情を表現するであろうと思われる形態素N-gramが感情推定に寄与していたことがわかる．
}
\resp{一方，「喜び」の``でし／助動詞，た／助動詞''や「嫌悪」の``顔／名詞''等，一見すると感情とは無関係と思われる形態素N-gramも存在した．
これらについては，今後別の角度からの検証（例えば，Web掲示板において喜びを表現する時は，「〜でした」のような丁寧な表現が用いられることが多い，といった仮説を立て，統計的に検証する）を行う必要があるが，今まで発見されていなかった事実を暗示するものである可能性がある．
また，「怒り」に``職場／名詞''，``仕事／名詞''，「嫌悪」に``上司／名詞''，``会社／名詞''がはいっていることも，「Webの掲示板においては，仕事に対する不満や愚痴等が多い」といった事実を暗示しているのかもしれない．
}
\resp{FPはコーパス中に含まれる以下のふたつの文の影響を低減する目的で提案された．
}
\resp{感情が異なっていても，たまたま表現や文型が類似している文}
\resp{コーパスを構築する際に誤って分類された文}
\resp{FPがこうした文に対して，どの程度頑健性を持っているかを検証するため，感情コーパス内に存在するこうした文を増減させ，その時の性能を評価した．
また，FPのかわりにTF-IDFを用いた時の性能についても評価を行った．
}
\resp{``入力文と感情が異なっているが，たまたま表現や文型が類似している文''の影響について，こうした文を各感情コーパスから削除した時の性能を評価することで検証を行った．
}
\resp{ ``たまたま表現や文型が類似している文''として``入力文とは感情が異なるコーパス中において，BLEUスコアが高い文''と定義し，各感情コーパスにおいて，このような文をBLEUスコア順に上位から{[MATH]}文削除した．
また条件を揃えるため，正解となる感情コーパスからは乱数で{[MATH]}文削除した．
}
\resp{感情の推定成功率を図{[REF_fig:similar]}に示す．
ここで横軸は，コーパス1つあたりの除去した文数を表す．
この結果を見ると，三品らの方法，提案方法どちらも``たまたま表現や文型が類似している文''を削除することで性能が向上していることから，これらの文の影響を受けていたことがわかる．
しかし，提案方法では文を一切削除しなかった場合においても比較的性能の低下が抑えられていることから，その目的である``たまたま表現や文型が類似している文''による影響を抑えることができていることがわかった．
}
\resp{次に，各感情コーパス中で感情分類を誤った文を意図的に増加させ，その時の性能を評価した．
具体的には，各感情コーパスから{[MATH]}文を乱数で抽出し，それらを他の感情コーパスへと均等に混入させることで，感情分類を誤った文を増加させた．
}
\resp{感情推定成功率を図{[REF_fig:shuffle]}に示す．
ここで横軸は，コーパス1つあたりの混入させた文数を表す．
この結果を見ると，混入する文が増加するに従って両方法ともに性能が低下し，感情分類を誤った文の影響を大きく受けていることがわかる．
しかし，両者の性能差（表{[REF_tbl:shuffle-diff]}）はわずかではあるが拡がっており，提案方法の方が若干ではあるが，こうした文の影響を低減できていることがわかった．
}
\resp{FPの有効性を確認するため，FPのかわりにTF-IDFを用いた実験を行った．
ここでは，TF-IDFをFPと同様，0から1の範囲の値とするため，tf値として拡大正規化索引語頻度{[CITE]}を用い，またidf値も通常のidf値{[CITE]}を式({[REF_eq:idf-norm]})を用いて正規化し，下記のような計算式によるTF-IDF値を用いた．
}
\respeqn{ sim_TFIDF(x,s)} & \respeqn{=} \respeqn{BP \cdot\frac{1}N \sum^N_{n=1}{tfidf}_{n} \cdot p_{n}(x,s)}
\respeqn{ tfidf_{n} } & \respeqn{=} \respeqn{1}|G_{n}(x)\right|\sum_\in G_{n}(x){tf_{n} \cdot idf_{n}}
\respeqn{ tf_{n}} & \respeqn{=} \respeqn{ \left{
{lll} 0.5 + 0.5 \cdot\fracfreq_{C_{e}(w_{n})}\max_{c \in C}freq_{c}(w_{n}) & if & freq_{C_{e}}(w_{n}) > 0
0 & if & freq_{C_{e}}(w_{n}) = 0
\right. }
\respeqn{ idf_{n} } & \respeqn{=} \respeqn{ \left{
{lll} \frac\frac{|C|}{D(w_{n})} + 1|C| + 1 & if & D(w_{n}) > 0
0 & if & D(w_{n}) = 0
\right .
}
\resp{ここで{[MATH]}は{[MATH]}を含むコーパスの数を返す関数である．
}
\resp{TF-IDFを用いた感情推定実験の結果を図{[REF_fig:tf-idf]}に示す．
この結果を見ると，すべてのNにおいてFPのほうが2.5ポイント〜8ポイント程度上まわっており，FPの有効性が認められた．
}
三品らの方法とは異なる従来手法の一つとして，良好なクラス分類が可能なSVM (Support Vector Machine)による感情推定を行い，提案手法との比較を行った．
本稿では学習，分類を行うプログラムとして，[MATH]を用いた．
SVMを用いて感情推定を行うために，まず感情コーパスの各発話文から特徴ベクトルを生成する必要がある．
今回は特徴ベクトルとして，1文中に出現する[MATH]の出現回数をベクトルして表現したものを用いた．
特徴ベクトルを生成するために，まず考慮する[MATH]の最高次数Nを決め，それ以下の各[MATH]について，感情コーパスから得られる\NGRAMすべてに通し番号を振った（このとき，\NGRAMが低次であるほど若い番号を振ることとした）．
次に，感情コーパスから取り出した一つの発話文[MATH]の\NGRAMを[MATH]，[MATH]における[MATH]の出現回数を[MATH]，それ以外の次元の値を0とする特徴ベクトルを生成した．
例えば最高次数を[MATH]とした場合，形態素unigramと形態素bigramを用いて発話文から特徴ベクトルを生成する．
この時，特徴ベクトルの次元数は，コーパス中に出現するすべてのunigramとbigramの種類数となる．
[MATH]とした特徴ベクトルの例として，``ありがとうございました！！''という発話文の場合，各形態素unigramに割り振られる番号を表[REF_table:mresult]のとおりとすると，``ありがとうございました！！''の特徴ベクトル[MATH]は以下のように表現される．
クラス分類モデルは感情コーパスの数と同じ数だけ構築する．
例えば，``怒り''の分類モデルを構築する場合，ポジティブデータを``怒り''のコーパスに含まれる発話文から生成した特徴ベクトル，ネガティブデータを``怒り''以外の感情コーパスに含まれる発話文から生成した特徴ベクトルと定義し，学習を行う．
本稿で学習に用いるポジティブデータの量はネガティブデータの量に比べて少なく，デフォルト値では良好な分類性能が得られない．
そこでcost factor ([MATH])の計算には，Morikらが定義した次の式を用いた[CITE]．
cost factor以外の学習パラメータはデフォルト値を用いた．
また学習パラメータで与えるカーネルのタイプもデフォルトである線形カーネルを用いた．
実験に用いた感情コーパスは，[REF_sec:comp_es_bleu]節と同様に``喜び''，``怒り''，``嫌悪''，``希望''の各838文であり，入力文も[REF_sec:comp_es_bleu]節と同様の文を用いた．
成功条件は，入力文の感情と出力感情が一致すれば推定成功とした．
表[REF_table:svm_success_ratios]に推定成功率を示す．
SVMで最も高かった推定成功率が[MATH]を用いたときの80.4%であり，提案手法の中で最も高かった\resp{成功率81.8%と比べると1.4ポイント程度の差になっている．
このことから，発話文の感情推定において，適切なNを選択すれば，SVMと提案手法は同程度の感情推定成功率が得られることが示された．
}
\resp{しかし，表{[REF_table:svm_success_ratios]}を見ると，{[MATH]}を用いた場合のSVMによる推定精度が急激に減少していることがわかる．
}これは``感情コーパス中で出現回数が少ない高次の\NGRAM ''を素性として利用している事例に対する過学習が原因であると考えられる．
\resp{一方提案方法においては，Nを増加させていってもその性能にはほとんど差がなく，``出現回数が少ない高次''の影響をほとんど受けていないことがわかる．
これは，RECAREの計算を相加平均で行ったことの効果であると考えられる．
}
\resp{一般に感情コーパス中の文数などによって最適なNは異なることが考えられ，SVMの場合は，実際の応用に際し評価実験を通して最適値を探索することが必要である．
一方RECAREであれば，十分大きなNを設定しておくことで，（計算量や記憶容量等の問題を除けば）常に最適な推定精度を得ることが可能となる．
}
\resp{更に，例えば6形態素からなるある特定の文末表現がある特定の感情に数多く出現する，といったことがあった場合，SVMであれば，全体として考慮すべき形態素の長さを決定する必要があるが，RECAREならば，その特定の文末表現を利用するためだけに[MATH]と設定してしまっても，悪影響をほとんど及ぼさない．
こうしたことから，SVMに比べてRECAREが有効であることが示された．
}
