Nグラム一致率に基づく自動評価法の問題点

Nグラム一致率を用いてシステム翻訳を評価する際の問題点を以下に定義する
BLEUを例として説明する．

システム翻訳文集合を${\mathcal H}$，それに対応する参照翻訳文集合
\footnote{
BLEUでは，原文に対して複数の参照翻文があることを想定している．}を
${\mathcal R}$とする．システム翻訳文$h_i \in \mathcal H$には，対応す
る参照翻訳文の集合$R_i \in \mathcal R$が割り当てられており，
$R_i$の$j$番目の参照翻訳文を$r_j$とする．
なお，$S=|\mathcal H|=|\mathcal R|$とする．ここで，BLEUは，以下の式で定義される．
\begin{equation}
 \text{BLEU}(\mathcal{H},\mathcal{R})=\text{BP}\cdot\exp\left(\frac{1}{N}\sum_{n=1}^N \log P_n\right)
\end{equation}

$N$はNグラムの長さパラメタであり，一般的には$N=4$である．$P_n$は，Nグラム適
合率であり，以下の式で定義される．
\begin{equation}
P_n=\frac{\displaystyle \sum_{i=1}^S\displaystyle\sum_{t_n \in h_i} \min(\text{count}(h_i,t_n),
	\max\_\text{count}(R_i,t_n))}{\displaystyle \sum_{i=1}^S \displaystyle\sum_{t_n \in h_i}\text{count}(h_i,t_n)}
\end{equation}

count($h_i$,$t_n$)は，任意のNグラム ($t_n$) のシステム翻訳文$h_i$における出現頻
度，
max\_count($R_i$,$t_n$)は，$t_n$の参照翻訳文集合$R_i$における
出現頻度の最大値，$\max_{r_j \in R_i}\text{count}(r_j,t_n)$
である．BP (Brevity Penalty) は，短
いシステム翻訳に対するペナルティであり，以下の式で定義される．
\begin{equation}
 \text{BP}=\min \left( 1,\exp \left( 1-\frac{\text{closest}\_\text{len}(\mathcal{R})}{\text{len}(\mathcal{H})}\right)\right)
\end{equation}

closest\_len($\mathcal R$)は，各$h_i \in {\cal H}$に対し，
最も近い単語数の参照翻訳文 $r_j \in R_i$を決定した後，それらの単語数を全て
の$i$で合計したもの，
len($\mathcal{H}$)は，$h_i$単語数を全ての$i$で合計したものを表す．

いま，原文 ($s$)，参照翻訳 ($r$)，システム翻訳 ($h_1$, $h_2$) が以下の通り与えられた
としよう．

\begin{description}
 \item[{\mdseries $s$:}] 雨に濡れたので，彼は風邪をひいた．
 \item[{\mdseries $r$:}] He caught a cold because he got soaked in the rain.
 \item[{\mdseries $h_1$:}] He caught a cold because he had gotten wet in the rain.
 \item[{\mdseries $h_2$:}] He got soaked in the rain because he caught a cold.
\end{description}

$r$は原文の直訳であり，$h_1$ はほぼそれと等しい訳であるが，$h_2$ は
「風邪をひいたので，彼は雨に濡れた」という意味であり，原文が表す因果関係
が逆転している．$h_1$ と $h_2$ を比較すると，
翻訳としての流暢さ (fluency)，いわゆる言語
モデル的な確からしさは同程度であ
るが，内容の適切性 (adequacy)は，$h_1$ が $h_2$ よりも高くならねばならない．

ここで，この2つのシステム翻訳を先に示したBLEUで評価してみよう\footnote{式(1)の定
義から明らかなようにBLEUは文集合を引数として評価スコアを計算する．
通常，1文を対象としてそのスコアを計算することはないが，ここでは
説明のため1文でのBLEUスコアを計算する．}．
$h_1$，$h_2$ とも$r$よりも長いため，ともに BPは1となる．$h_1$ の$P_1$〜$P_4$はそれぞれ，
9/12，7/11，5/10，3/9なので，BLEUスコアは0.53となる．一方，$h_2$ の$P_1$〜$P_4$
はそれぞれ，11/11，9/10，6/9，4/8なので，BLEUスコアは0.74となる．
この結果は，我々の直感に反しており，BLEUを最大化するようにシステムを最適
化することが，良い翻訳システムの開発に結びつくかどうかは疑問である．

こうした問題が起こる原因はNグラムという局所的な語の
並びにのみに着目してスコアを計算することにある．
短い単語列のみを評価対象とすると，先の例のように，
参照翻訳の節中のNグラムを保持していれば，節の順番が入れ替わったとして
も十分高いスコアを獲得する．


もちろん，$h_2$ のような翻訳をシステムが出力するようなことはほとんどあり得ない
のではないかという疑問もあろう．
確かに語順が似た言語対を対象とする場合や翻訳システムがルールベースで構築
されている場合には起こりにくい問題であるが，
語順が大きく異なる言語対を対象とした統計翻訳 (Statistical Machine
Translation: SMT)
システムでは十分起こり得る問
題である．
以下に Web 上の SMT による翻訳サービスの出力例を示す．

\begin{description}
 \item[原文：]ボブはメアリに指輪を買うためにジョンの店に行った．
 \item[参照翻訳：]Bob went to John's store to buy a ring for Mary.
 \item[SMT出力：]Bob to buy rings, Mary went to John shop.
\end{description}

SMT出力をみると，訳語という観点では参照翻訳と良く合致しており，バイグラム，
トライグラムでもある程度の数が一致している．しかし，原文の「店に行く」
の主体が「ボブ」であるという構造を捉えることができず，
その主体が「メアリ」となってしまっている．
SMTシステムでは，大
きな語順の入れ替えを許すと探索空間は膨大になる．
よって，現実的な時間で
翻訳文を生成するため，
語順の入れ替えにある程度の制限を設けざるを
得ない．その結果，Nグラムでは参照翻訳と良く合致す
るものの原文の意味とはかけ離れた翻訳を出力することがある．

このような状況のもと，
BLEUスコアで翻訳システムを比較すると，
正しい評価ができない可能性が高い．なお，この問題は BLEU に限ったことでは
なく，その変種であるNISTスコア，METEOR など Nグラム一致率を利用した自動
評価法すべてに当てはまる問題である．


LCSに基づく自動評価法の問題点

ROUGE-L \cite{ROUGEL}，IMPACT \cite{impact}は，参照翻訳とシステム翻訳と
の間の最長共通部分単語列 (LCS) に基づき評価スコアを決定する．先に挙げた
例で説明する．

\begin{description}
 \item[{\mdseries $s$:}] 雨に濡れたので，彼は風邪をひいた．
 \item[{\mdseries $r$:}] He caught a cold because he got soaked in the rain.
 \item[{\mdseries $h_1$:}] He caught a cold because he had gotten wet in the rain.
 \item[{\mdseries $h_2$:}] He got soaked in the rain because he caught a cold.
\end{description}

$r$と$h_1$との間のLCSは，``He caught a cold because he in the rain''であり，
その長さ（単語数）は9である．$r$の長さは11，$h_1$の長さは12であることか
ら，LCS の適合率は9/12，再現率は9/11となる．一方，$r$と$h_2$との間の
LCS は，``he got soaked in the rain''であり，その長さは6である．$h_2$の
長さは11なので，LCS の適合率は6/11，再現率は6/11 となる．ROUGE-L ス
コアは LCS 適合率と再現率の調和平均，F値なので BLEU とは違い，$h_1$を
$h_2$より高く評価することができる．

IMPACT は ROUGE-L を改良したものであり，上述の LCS を一度見つけただけで
やめるのではなく，見つかった LCS を削除した単語列に対し，再度 LCS を探すとい
うことを繰り返す．つまり，$h_1$の例では，$r$と$h_1$から，
``He caught a cold because he in the rain''を削除し，
\begin{description}
 \item[{\mdseries $r$:}] got soaked
 \item[{\mdseries $h_1$:}] had gottten wet 
\end{description}
から，$h_2$の例では，``he got soaked in the rain''を削除し，
\begin{description}
 \item[{\mdseries $r$:}] caught a cold because he
 \item[{\mdseries $h_2$:}] because he caught a cold
\end{description}
から，再度 LCS を探し出すという手順を繰り返す．

これらの手法の問題点は，参照翻訳とシステム翻訳との間の LCS 適合率，再
現率を計算するため，それらの間で一致しなかった単語を評価の対象に含めてい
る点にある．例えば，以下のシステム翻訳$h_3$を考えると，$r$と$h_3$との間
のLCSは，``he caught a cold the rain''となるので，LCS適合率，再現率はそれぞ
れ，6/13，6/11 となり，適合率が$h_2$の場合より低い値をとってしまい， ROUGE-Lスコ
アは$h_2$の場合よりも低くなる．

\begin{description}
 \item[{\mdseries $h_3$:}] He caught a cold as a result of getting hit by the rain
\end{description}

このように適合率，再現率といった参照翻訳とシステム翻訳との間で一致しない単語を評価
に含めてしまう尺度を用いると訳語の違いに敏感になり過ぎ，システムを過
小評価することがある．


語順の相関に基づく自動評価法

本稿では，Nグラム一致率に基づく自動評価法の問題点を解決するため，文内の
局所的な語の並びに着目するのではなく，大局的な語の並びに着目する．
つまり，参照翻訳とシステム翻訳との間で一致して出現する単語の出現順
の近さに基づき評価する．さらに，訳語の違いに寛大な評価をするため，システ
ム翻訳の単語適合率の重みを調整できるようにして別途ペナルティとして用いる．


\subsection{単語アラインメント}


参照翻訳とシステム翻訳の語順との間の相関を計算するため，双方の翻訳に一致
して出現する単語を同定しなければならない．これは，参照翻訳とシステム翻訳
との間の単語アラインメントを決定する問題となる．本稿では，単語の表層での
一致に基づくアラインメント法を採用した．Algorithm \ref{wordalign}にその
疑似コードを示す．

\begin{algorithm}[b]
 \caption{Word Alignment Algorithm}
 \label{wordalign}
 \footnotesize
 \begin{algorithmic}[1]
  \STATE Read hypothesis sentence $h=w_1^h,w_2^h,\ldots,w_m^h$
  \STATE Read reference sentence $r=w_1^r,w_2^r,\ldots,w_n^r$
  \STATE Initialize {\tt worder} with an empty list.

  \FOR{each word $w_i^h$ in $h$}

  \IF {$w_i^h$ appears only once each in $h$ and $r$}
  \STATE append $j$ s.t. $w_i^h=w_j^r$ to {\tt worder}
  \ELSIF


  \FOR{$\ell$=2 to $m-i$}
  \IF {$w_i^h,\ldots,w_{i+(\ell-1)}^h$ appears only once each in $h$ and $r$}
  \STATE append $j$ s.t. $w_i^h,\ldots,w_{i+(\ell-1)}^h=w_j^r,\ldots,w_{j+(\ell-1)}^r$ to {\tt worder}
  \STATE break the loop
  \ENDIF
  \ENDFOR
  \ELSE
  \FOR{$\ell$=2 to $i$}
  \IF {$w_{i-(\ell-1)}^h,\ldots,w_i^h$ appears only once each in $h$ and $r$}
  \STATE append $j$ s.t. $w_{i-(\ell-1)}^h,\ldots,w_i^h=w_{j-(\ell-1)}^r,\ldots,w_{j}^r$ to {\tt worder}
  \STATE break the loop
  \ENDIF

  \ENDFOR

  \ENDIF
  \ENDFOR
  \STATE Return {\tt worder}
 \end{algorithmic}
\end{algorithm}

システム翻訳を長さ$m$，参照翻訳を長さ$n$の単語リストして読み込み，アライ
ンメントを格納する配列{\tt worder}を初期化する（1〜3行目）．システム翻訳の
単語リストの先頭から順に単語$w_i^h$を取り出し，その単語がシステム翻訳，参照翻
訳の双方にただ1度のみ出現している場合，$i$と単語$w_i^h$の参照翻訳における出現
位置$j$を対応づける（5, 6行目）．それ以外の場合，$w_i^h$を基準として右側にN
グラムを伸長させ，システム翻訳と参照翻訳の双方における出現頻度が1となっ
た時点で$i$と$j$を対応づける（8〜13行目）．それでも対応がつかない場合，
$w_i^h$を基準として左側にNグラムを伸長させ，システム翻訳
と参照翻訳の双方における出現頻度が1となった時点で$i$と$j$を対応づける（15〜
20行目）．これでも曖昧性が残る（システム翻訳と参照翻訳での頻度が1にならな
い）場合，あるいは対応先が見つからない場合は単語対応付けを行わない．
図\ref{figalign}に2章の例文に対する単語アラインメントを示す．上段の例か
ら，\texttt{worder}の1番目の要素，つまり，$h_1$の1単語目
が$r$の1番目の要素（単語）に対応することがわかる．
下段の例から，$h_2$の1単語目が$r$の6番目の単語と対応していることがわかる．

 \begin{figure}[t]
   \begin{center}
\includegraphics{21-3ia985f1.eps}
   \end{center}
    \caption{単語アラインメントの例}
    \label{figalign}
\end{figure}


\subsection{単語出現順の相関}

1対1の単語アラインメントを決定することができれば，参照翻訳とシステム翻訳
から単語出現位置IDを要素とするリストを得ることができる．図\ref{figalign}の例では，
$r$:[1,2,3,4,5,6,9,10,11]，$h_1$:[1,2,3,4,5,6,9,10,11]および
$r$:[1,2,3,4,5,6,7,8,9,10,11]，$h_2$:[6,7,8,9,10,11,5,1,2,3,4]という2つ
のリストペアを得る．こうした順序列間の順位相関係数を計算することで参照翻訳とシ
ステム翻訳との間で一致して出現する単語の出現順の近さを測ることができる．
本稿では以下に示すKendallの順位相関係数 ($\tau$) \cite{kendall}を採用した．
順位相関係数としては，Spearman の順位相関係数 ($\rho$) もよく知られ
ている．しかし，
$\tau$と比べて$\rho$は，順位の小さな入れ替わりには寛容すぎ，大きな入れ替
わりには厳しすぎる．
予備実験の結果では，人間の評価との間の相関が$\tau$よりも低い傾向
を示したため，
\pagebreak
本稿では$\tau$を採用した．
\begin{equation}
 \tau =\frac{\displaystyle \sum_{i=1}^{n-1} T_i - \displaystyle \sum_{i=1}^{n-1}U_i}{\frac{n(n-1)}{2}}
\end{equation}

$T_i$は，アラインメント手続きを用いてシステム翻訳から得た単語出現位置の
IDリスト (\texttt{worder}) につ
いて，$i$番目の要素の値よりも大きな要素が$i+1$番目から$n$番目の要
素までの間に出現
する数，$U_i$はその逆に，
$i$番目の要素の値よりも小さな要素が$i+1$番目から$n$番目の要素まで
の間に出現する数を表す．表\ref{tau}に図\ref{figalign}の$h_2$から得た{\tt
worder}と$T_i$，$U_i$をそれぞれ示す．この表より，$r$ と $h_2$ との間の語順の
相関をKendallの$\tau$で計算すると，
$\tau(r,{h_2})=(21-34)/((11\cdot10)/2)=-0.236$となる．
同様に図\ref{figalign}の$h_1$から得た \texttt{worder}を用いて$\tau(r,{h_1})$を
計算すると
$\tau(r,{h_1})=(36-0)/((9\cdot8)/2)=1$となる．
$\tau$は参照翻訳と
システム翻訳との語順が完全一致する場合に1，逆順の場合に$-1$をとる．

\begin{table}[t]
  \caption{Kendall の順位相関係数の計算例}
  \label{tau}
\input{0985table01.txt}
\end{table}

BLEUでは，$h_2$が$h_1$よりも高いスコアを獲得したが，文全体での語順に着目
し，システム翻訳と参
照翻訳との間の語順の順位相関を計算すると，$h_1$が$h_2$よりも高いスコアを獲得
でき，我々の直感に合致した結果を得ることができた．

ただし，$\tau$は負の値をとり得るため，従来の自動評価法が出力するスコアレ
ンジと同様[0,1]の値をとるよう以下の式で正規化する．
\begin{equation}
 \text{Normalized Kendall's $\tau$: NKT} =\frac{\tau+1}{2}
\end{equation}


\subsection{ペナルティ}

参照翻訳とシステム翻訳との間の語順の相関を
計算するためには，単語アラインメントを決定し，双方に一致して出現する単語
のみを評価の対象としなければならない．しかし，
参照翻訳とシステム翻訳との間で一致する単語のみを評価対象とすることには以
下の2つ問題がある．

\begin{enumerate}
 \item システム翻訳の単語数に対し，参照翻訳との間で一致する単語の割合が
       少ない場合，過剰に高いスコアを与える可能性がある．
 \item システム翻訳の単語数が少ない場合，過剰に高いスコアを与える可能性
       がある，
\end{enumerate}

(1) に関して，以下の例を考えよう．

\begin{description}
 \item[{\mdseries $r$:}] John went to a restaurant yesterday
 \item[{\mdseries $h$:}] John read a book yesterday
\end{description}

$h$は5単語からなる訳であり，そのうち``John''，``a''，``yesterday''
のみしか参照翻訳と一致していない．しかし，その出現順が参照翻訳と一致していることから
NKT は1となる． つまり，システムが出力した単語数に関係なく順位相関だけを
みていると不当に高いスコアを獲得する可能性がある．

次に (2) に関して，以下の例を考えよう．

\begin{description}
 \item[{\mdseries $r$:}] John went to a restaurant yesterday
 \item[{\mdseries $h$:}] to a
\end{description}

システム翻訳は2単語しかない意味の無い訳であるにもかかわらず，
単語正解率は1であり，2単語の出現順序も参照翻訳と一致しているこ
とから，NKTも1となる．
つまり，単語数が少
ない場合，順位相関と単語正解率だけでは不当に高いスコアを獲得する可能性
がある．

このように，順位相関係数を用いると，システム翻訳の2単語のみが参照翻訳と
出現順まで一致すると，不当に高いスコアを獲得する可能性がある．
よって，
本稿では，前者に対して単語正解率 ($P$)，後者に対してはBLEUの
BPをペナルティとして導入する．それぞれの定義を以下に示す．
\begin{gather}
 P(h_i,r_i)=\frac{\text{len (\texttt{worder})}}{\text{len}(h_i)} \\
 \text{BP}_s(h_i,r_i)=\min\left(1,\exp \left(1-\frac{\text{len}(r_i)}{\text{len}(h_i)}\right)\right)
\end{gather}

単語正解率は，システム翻訳の単語のうちアラインメントをとることが
できた単語数 (len({\tt worder})) の割合であり，
len($r$)は，参照翻訳の単語数，len($h$)はシステム翻訳の単語数で
ある．
BLEU の BP は文集合全体で計算していたが，ここでは，文単位で
計算することに注意されたい．
これらを用いて最終的な自動評価スコアを以下の式(\ref{ribes})で定義する．
なお，{\bf この手法を RIBES (Rank-based Intuitive Bilingual Evaluation Score) と名付け，
\url{http://www.kecl.ntt.co.jp/icl/lirg/ribes/}にてオープンソースソフトウェ
アとして公開している．}
\begin{equation}
\mbox{RIBES}(\mathcal{H},\mathcal{R})=\frac{\displaystyle\mathop\sum_{h_i \in \mathcal{H}}
\max_{r_j \in R_i} \{ \mbox{NKT}(h_i,r_j) \cdot P(h_i,r_j)^{\alpha}\cdot
\mbox{BP}_s(h_i,r_j)^{\beta}\}}{|\mathcal{H}|}
\label{ribes}
\end{equation}

$\alpha (\ge 0)$は単語適合率の重みであり，$\alpha$が大きいほど訳語の違い
に敏感になる．

\begin{itemize}
 \item 参照翻訳が1つしかない場合，参照翻訳にはない訳語をシステムが出力す
       る可能性が高いため，$\alpha$は小さめに設定した方がよいだろう．
 \item 参照翻訳が複数の場合，参照翻訳のいずれかに出現する単語をシステム
       が出力する可能性が高くなる．そこで，不適切な訳語を厳しく採点するため
       $\alpha$は高めに設定した方がよいだろう．
\end{itemize}

$\beta (\ge 0)$は BP の重みであり，$\beta$が大きいほど訳文の長さに敏感に
なる．

\begin{itemize}
 \item 参照翻訳が1つしかない場合，それよりも短い翻訳があり得る可能性が高
       いので，$\beta$は小さめに設定してよいだろう．
 \item 参照翻訳が複数ある場合，一番短い翻訳を基準にして考えれば，$\beta$
       を高めに設定してよいだろう．
\end{itemize}


実験の設定


\subsection{実験データ}

RIBES の有効性を示すため，NTCIR-7，NTCIR-9の特許翻訳タスク (PATMT) の
データを用いて評価実験（評価指標の評価なので，以降メタ評価と呼ぶ）を行った．
言語対は英日 (EJ)，日英 (JE)とした．
それぞれのデータセットの文数，1文あたりの参照翻訳の数，評価者の数，参加
システム数
を表\ref{data}に示す．なお，カッコ内の数字はルールベースシステムの数を示
す．

\begin{table}[b]
  \caption{実験データの詳細}
  \label{data}
\input{0985table02.txt}
\end{table}

NTCIRワークショップの事務局から公開されてい
るデータには，EJ，JEタスクとも1つの参照翻訳しか含まれていない．
そこで，NTCIR-7のデータに対してのみ，特許翻訳の専門家に依頼し，参照翻訳を
独自に追加した．
また，NTCIR-7のEJタスクに関しては，5システムだけにしか人間の主観
評価の結果が与えられていなかったため，特許に精通した被験者5名で再度 JE
タスクと同様，5段階評価で主観評価を行った．
さらに，評価対象とする翻訳システムに著者のグループの英日翻訳システム\cite{headfinal}を追加
し，計14システムで実験を行った．

全てのデータに対し，メタ評価の対象は翻訳の内容としての適切性 (adequacy) のみ
とした．これは，翻訳の流暢さよりも内容の適切性を
自動評価できた方がより良い翻訳システムの開発に貢献できると考えたからである．

なお，各システム翻訳文に対し複数の人間の評価スコアが与えられている場合に
は，その平均値を文に対する評価スコアとした．このように各システム翻訳
文に対して評価値を決定し，これを文集合全体での平均したものを人
間がシステムに与えた評価スコアとした．


\subsection{比較した自動評価手法}

比較評価には，Nグラム一致率に基づく評価手法として先に説明したBLEU，
大局的な単語列を考慮した評価法として同じく先に説明した
ROUGE-L \cite{ROUGEL}，
その改良版である IMPACT \cite{impact}を用いた．
IMPACTには，LCS の長さに応じた重みパラメタ，語順の入れ替えに応じた
重みパラメタがある．詳細については文献 \cite{impact} を参照されたい．
なお，ROUGE-L，IMPACTとも参
照翻訳
が複数ある場合には個々の参照翻訳を用いて求めたスコアの最大値を評価スコア
として採用した．BLEU の計算には {\tt mteval-v13a}，ROUGE-L 
には，\texttt{ROUGE-1.5.5}，IMPACT には \texttt{IMPACT version 4}を利用した．

また，LRscore \cite{birch,birch-wmt2010,birch-acl}も比較評価の対象とした．
LRscore は，参照翻訳とシステム翻訳との間の語順の近さを表すスコアとBLEUス
コアとの間の線形補間で評価スコアを決定する．
語順の近さを表す尺度としては，ハミング距離$d_h(h,r)$を利用する
ものと Kendall の$\tau$に基づく$d_k(h,r)$を利用するものがあるが，以降で
は，本稿との
関連が深い後者について述べる．
LRscore の定義を以下に示す．
\begin{equation}
 \mbox{LRscore}(\mathcal{H},\mathcal{R})=\gamma R(\mathcal{H},\mathcal{R})+(1-\gamma)\mbox{BLEU}(\mathcal{H},\mathcal{R})
\end{equation}

$R({\mathcal H},{\mathcal R})$は以下の式で定義される．
\begin{equation}
 R(\mathcal{H},\mathcal{R})=\frac{\displaystyle\mathop\sum_{h_i \in \mathcal{H}} d(h_i,r_i)\mbox{BP}_s(h_i,r_i)}{|\mathcal{H}|}
\end{equation}

$d_k(h,r)$は，文献\cite{birch-acl} に従うと
$d_k(h,r)=1-\sqrt{1-\mbox{NKT}(h,r)}$で定義されるが，それ以前の文献
\cite{birch,birch-wmt2010} では，$d_k(h,r)=\mbox{NKT}(h,r)$も用いられて
いる．以降，前者を${d_k}_1$，後者を${d_k}_2$とよぶ．
RIBES で $\alpha=0$, $\beta=1$と設定したときと，LRscore に ${d_k}_2$ を採用，
$\gamma=1$と設定したとき，これら2つの手法は一致する．しかし，LRscore
は日本語，英語のような大きな語順の入れ替えがある言語対を対象として考案
された手法ではなく，ヨーロッパ言語間，中英\footnote{
もちろん，英語，フランス語ほど語順が近くはないが，英語も中国語も SVO
型の言語であり，日本語，英語ほどの語順の違いはない．}翻訳という比較的語順が似た言語を対象として考案されたため，最終的には
${d_k}_1$を採用することで順位相関の低レンジスコアの感度を下げ，さらに語順の近い言語対を
対象としたときに実績のある BLEU \footnote{
実際，NTCIR-9 の中英翻訳タスクにおいて，BLEUは人間の評価結果との相関が
0.9以上の非常に高い値を記録している \cite{ntcir9}．} の恩恵を受けるため，それとの間の線形補間という定式化に至ったのであろう．
後述するが，英日，日英翻訳の評価では BLEU を利用するメリットは期待できな
い．さらに，NKT を${d_k}_1$によって非線形変換することで低レンジスコアの
感度をさらに下げるメリットも元々高いNKTを得ることが難しい英日，日英翻訳タ
スクでは期待できない．
以上より，LRscore は確かに RIBES と良く似た手法といえるが，
 BLEU を補うために派生した評価指標と捉えた方が自然であり， 
RIBES とはその根底にある研究の動機に大きな違いがある．
なお，LRscore には，参照翻訳とシステム翻訳との間の単語
アラインメントを決定する手段が提供されないため，
以降の実験では本稿での単語アラインメントを利用した．


\subsection{メタ評価の指標}

本稿では，メタ評価の指標として広く用いられている Pearson の積率相関係数，
Spearman の順位相関係数，Kendall の順位相関係数を用いた．Pearson の積率
相関係数は人間の評価と自動評価の結果がどの程度線形の関係にあるかを評価し，
Spearman，Kendall の相関係数は人間の評価と自動評価の結果の順位がどの程度
近いかを評価する．
Spearman と Kendall の違いは，先にも説明したように
順位の差に対して重みをどのように与えるかという点にある．


\subsection{実験の手順}

RIBES に対してはシステム翻訳の長さに対する重みパラメタと単語正解率に対
する重みパラメタ，IMPACT に対してはLCSに対する重みパラメタと語順の違いに
対する重みパラメタ，LRscore には順位相関係数とBLEUスコアの重みを調整する
パラメタがある．
これらの手法に対しては，以下の手順でパラメタの最適化を行い，メタ評価を行った．

\begin{enumerate}
 \item 文のIDをランダムに10個選択する．
 \item 選択したIDによる10文の集合を用いて，文集合全体での人間の評価スコアと自動
       評価スコアとの間の Spearman の順位相関係数が最大となるようパラメタを決定する．
 \item (2) で決定したパラメタを用いて (1) の残りの文集合全体を用いてメタ評
       価を行い，相関係数を記録する．
 \item (1) から (3) を100回繰り返し，相関係数の平均値を求める．
\end{enumerate}

なお，パラメタが存在しないBLEUとROUGE-Lに対しては，(2) をスキップし，同様
の手順でメタ評価を行った．


実験結果と考察

\subsection{NTCIR-7データ}

表 \ref{NTCIR7-single}にオーガナイザから配布された参照翻訳のみを用いた時
の相関係数の平均値，表 \ref{NTCIR7-multi}に複数参照翻訳を用いた時の相関係
数の平均値を示す．
平均値の差の
検定には，ペアワイズの比較に Wilcoxon の符号順位検定\cite{wilcox}を採用し，Holm
法 \cite{holm} による多重比較を用いた．

\begin{table}[t]
  \caption{メタ評価の結果（NTCIR-7データ，単一参照翻訳）}
  \label{NTCIR7-single}
\input{0985table03.txt}
\vspace{4pt}\small
表中，1はRIBES，2は
  LRscore(${d_k}_1$)，3はLRscore(${d_k}_2$)，4はROUGE-L，5はIMPACT，6はBLEUに対し，
  有意水準5\%で有意差があることを示す．
\par
\end{table}
\begin{table}[t]
  \caption{メタ評価の結果（NTCIR-7データ，複数参照翻訳）}
  \label{NTCIR7-multi}
\input{0985table04.txt}
\vspace{4pt}\small
表中，1はRIBES，2は
  LRscore(${d_k}_1$)，3はLRscore(${d_k}_2$)，4はROUGE-L，5はIMPACT，6はBLEUに対し，
  有意水準5\%で有意差があることを示す．
\par
\end{table}

表 \ref{NTCIR7-single}，\ref{NTCIR7-multi} より，
どの手法に対しても Spearman の順
位相関係数の方が Kendall の順位相関係数よりも高い．
Kendall の順位相関係数は，2つの順序列の間で一致する半順序関係の
数に基づき決定されるため，細かな順位の間違いに敏感である．
一方，Spearman の順位相関係数は順序列の間の
順位の差に基づき決定されるため，細かな順位の間違いには鈍感である．
本稿で用いたデータでは，
自動評価法にとって，明らかに良いシステムと悪いシステムの区別が容易であっ
たため，大きな差での順位の不一致が減少し，
Spearman の順位相関係数が
Kendall の順位相関係数よりも相対的に高い値を記録
したのであろう．ただし，全体の傾向としては両者の間に大きな違いはない．
Pearson の積率相関係数は順位相関係数より高い値を示しており，
BLEUではその差が特に大きい．
たとえば，表 \ref{NTCIR7-multi}の英日
タスクでは，Pearsonが0.9以上であることに対し，Spearmanは0.7程度でしかない．
これは，人間の評価との順位付けはやや強い程度相関でしか示していないにも関
わらず，
線形の相関は非常に強いことを意味する．
Pearson の積率相関係数は外れ値がある
場合，その値が過剰に高く見積もられるということが知られているが，その影響が
強く出ているのではないかと考える．よって，以降では主に順位相関に焦点をあ
てて議論する．

表\ref{NTCIR7-single}より，
日英翻訳に関しては，RIBES が他のすべての手法に対して統計的有意に優れてい
る．
英日翻訳に関しては，RIBES とLRscore(${d_k}_1$)が同程度で優れて
おり，十分強い相関である．
ROUGE-L，IMPACTが，RIBES ほどではないものの比較的よい相関を得ているこ
とに対し，
BLEUは双方の言語対において，相関係数の平均値が他のほぼ全ての手法に対し統
計的に有意な差で劣っ
ている．さらに，順位相関係数は弱い相関程度でしかない．
この結果は，英日，日英翻訳という大きな語順の入れ替えを必要とする言語対を対象
とした場合，Nグラム一致率で自動評価を行うことが不適切であることを示唆し
ている．

表 \ref{NTCIR7-multi}より，
参照翻訳の数が増えると相関係数の平均値は上昇する傾向にある．
BLEUの相関係数が他の手法よりも有意に劣っていることは単一参照翻訳の場合と同
様であるが，順位相関係数はやや強い相関程度にまで上昇している．
ROUGE-L，IMPACTの相関係数も上昇しており，ROUGE-Lは日英翻訳に関しては，
他のすべての手法に対して，英日翻訳に関しては，RIBES 以外の手法に対して
統計的有意に優れている．
RIBES は，日英翻訳ではROUGE-Lに次いでIMPACTと同程度，英日翻訳では
ROUGE-Lと同程度であるが，十分強い相関を示している．

BLEU，ROUGE-L，IMPACT の相関係数が複数参照翻訳が与えられた場合に顕著に改
善される理由は，語彙のバリエーションが増えたことであろう．
これらの手法は，N グラ
ム一致率，LCS 適合率，再現率を利用しているため，参照翻訳とシステム翻訳と
の間で一致しない単語も評価対象となる．よって，
語彙の一致判定を単に文字列としての一致だけで判定すると，意味的には一致す
るはずのものが一致せずに不当に低いスコアを得るという問題が起こる．
しかし，複数参照翻訳の場
合には，語彙のバリエーションが増えるためこうした問題は軽減される
のであろう．
一方，RIBES でもシステム翻訳と参照翻訳との間の単語一致率は利用するため，
一致しない単語を評価対象として用いているが，パラメタによりその影響を小さ
く抑えることができる．よって，単一参照翻訳でも複数参照翻訳でも安定して高い相関を示
すことができた．


\subsection{NTCIR-9データ}

表 \ref{NTCIR9}に相関係数の平均値を示す．
NTCIR-7 のデータとは異なり，どの手法も相関係数の平均値は大幅に低下してい
る．特にROUGE-L，IMPACT，BLEUは，非常に弱い相関，あるいは無相関と言える
ほどである．この原因は先の実験結果と同様，参照翻訳の数が1つであること
に加え，評価対象となる翻訳システムの中でのルールベースの翻訳システム（SMT
とのハイブリッドも含む）の占める
割合が増したことにある．NTCIR-7と比較すると，日英翻訳に関しては
ルールベースシステムは2システムから6システムへ，英日翻訳に関しては1シス
テムから5システムへと増えた．

\begin{table}[t]
  \caption{メタ評価の結果（NTCIR-9データ，単一参照翻訳）}
  \label{NTCIR9}
\input{0985table05.txt}
\vspace{4pt}\small
表中，1はRIBES，2は
  LRscore(${d_k}_1$)，3はLRscore(${d_k}_2$)，4はROUGE-L，5はIMPACT，6はBLEUに対し，
  有意水準5\%で有意差があることを示す．
\par
\end{table}
\begin{figure}[t]
   \begin{center}
\includegraphics{21-3ia985f2.eps}
   \end{center}
    \caption{ユニグラム適合率と人間のスコアの関係}
    \label{uniprec}
\end{figure}

図\ref{uniprec}にユニグラム適合率と人間のスコア
の関係を示す．図中四角のマーカ(RBMT-1〜6，JAPIO，TORI，EIWA)がルールベース
の翻訳システムである．図か
ら明らかなように，ルールベースの翻訳システムはユニグラム適合率が低いにも
関わらず人間の評価では高いスコアを獲得している．つまり，これらのシステム
の翻訳
は，参照翻訳と一致する単語の割合が少ないにも関わらず人間の評価では高いスコアを獲
得している．
ルールベースの翻訳システムは，
訓練データから訳語を推定するSMTシステムほど語彙の統制がとれていない．
よって，翻訳対象のドメインに合致した語彙，すなわち，特許特有の語彙を用い
て翻訳できるとは限らない．
しかし，SMTシステムにとって大きな問題となる語順の入れ替えに関しては，記述され
たルールに当てはまる限りは問題となり得ない．
よって，特許の訳語として多少おかしくとも文全体で意味が通る翻訳となり，そ
の結果，人間が高いスコアを与えたのであろう．
先にも説明した通り，
BLEUは，Nグラムの適合率，ROUGE-LとIMPACTはLCSの適合率・再現率に基づきスコア
を決定する．つまり，
参照翻訳と一致する単語の割合が大きいシステム翻訳にしか
高いスコアを与えることができない．よって，ルールベースシステム
に高いスコアを与えることはできず，それらの性能を低く見積もってしまったこ
とにより，相関が著しく低下したと考える．

一方，RIBES とLRscoreはこれらの手法とは異なり，単語正解率，BLEUスコアをパラメタで
軽減することで単語一致率の低いシステムであっても高いスコアを与
えること
ができるという特徴がある．
実際，日英，英日の双方においてROUGE-L，IMPACT，BLEUといった従来の自動評価法に
対し，統計的有意に高い相関係数を獲得していることがそれの有効性を示唆してい
る．

ユニグラム適合率が低いところでの自動評価法の性能をより詳細に調べるため，
ルール
ベースの翻訳システムのみを取り出し，同様の実験を行い相関係数の平均値を
求めたところ，表\ref{rbmt}を得た．翻訳システム数は日英翻訳タスクで6，英
日翻訳タスク
で5である．サンプル数が少ないため，相関係数の値に対する信頼性がこれまで
の実験よりも劣ることに注意されたい．表\ref{rbmt}より，RIBES は日英翻訳タス
クで ROUGE-L，IMPACT に劣るものの全体を通してみれば他の手法より良い相関
を得ている．参照翻訳が1つしかないという影響もあるが，英日翻訳タスクでは
ROUGE-L，IMPACT，BLEU は負の相関でしかない．さらに，先に示したとおり，
表\ref{NTCIR9}において RIBES がルールベースシステム，SMT システム双方
を含む場合でも良い相
関を得たこともふまえると，参照翻訳とシステム翻訳との間で一致す
る単語が少ない場合でも RIBES は有効であると考える．

\begin{table}[b]
  \caption{RBMTだけを用いたメタ評価の結果（NTCIR-9データ，単一参照翻訳）}
  \label{rbmt}
\input{0985table06.txt}
\vspace{4pt}\small
  表中，1はRIBES，2は
  LRscore(${d_k}_1$)，3はLRscore(${d_k}_2$)，4はROUGE-L，5はIMPACT，6はBLEUに対し，
  有意水準5\%で有意差があることを示す．
\par
\end{table}

以上より，BLEU，ROUGE-L，IMPACTといった単語の一致に強く依存する従来の自
動評価法は，単一参照翻訳時，評価対象としてルールベースシステムが多く混在
する場合には著しく信頼性が低下することを示した．特に，参照翻訳は常に
複数与えられるとは限らないため，自動評価法としては単一参照翻訳でも人間の
評価結果との間の相関が高いことが望ましい．
RIBES は，参照翻訳数，ルールベースシステムの数が変化した場合でも
安定して高い相関であることから，従来の自動評価法よりも優れていると考える．
ただし，RIBES には単語正解率と短い翻訳に対するペナルティを調整するため
の重みパラメタがある．
これらパラメタを最適化するためには，いわゆる教師データが必要となることか
ら，
それを必要としない BLEU，ROUGE-Lよりもコストのかかる手法とも言える．
しかし，実験結果より，
各システムに対し10文を人間が評価した結果を教師データとしてパラメタを最適
化できることを示した．よって，十分低いコストでパラメタの最適が可能である．


\subsection{獲得されたパラメタに関する考察}

 \begin{figure}[b]
   \begin{center}
\includegraphics{21-3ia985f3.eps}
   \end{center}
    \caption{獲得されたパラメタの分布(RIBES)}
    \label{parm}
\end{figure}

最後に RIBES と LRscore について獲得されたパラメタ，
$\alpha$, $\beta$, $\gamma$の違いから考察する．図 \ref{parm}に$\alpha$, $\beta$の
分布，図 \ref{lr-parm}に$\gamma$の分布を示す．

 \begin{figure}[t]
   \begin{center}
\includegraphics{21-3ia985f4.eps}
   \end{center}
    \caption{獲得されたパラメタの分布(LRscore)}
    \label{lr-parm}
\end{figure}

図 \ref{parm}より，パラメタ最適化のための訓練事例が10文と少ないことも影響
してか，獲得されたパラメタにばらつきがあるが，単一参照翻訳の場合には小
さな$\alpha$が選択されている割合が多く，4.3 節の仮説と一致する．また，
NTCIR-9 のように単一参照翻訳かつルールベースシステムの数が多い場合には，
$\alpha=0$の場合が非常に多い．一方，複数参照翻訳がある場合，比較的大きな
$\alpha$が選択されている．$\beta$に関しては，複数参照翻訳時には高い値が
選ばれている割合が高く，4.3 節の仮説と一致するが，単一参照翻訳時には必ず
しも低い値が選ばれておらず先の仮説と一致しない．
しかし，人間の
評価との間の相関をみる限り，RIBES は従来法よりも比較的高い相関を得てい
ることから，これは大きな問題ではないと考える．

図 \ref{lr-parm}より，LRscore の NTCIR-9 では，$\gamma=1$付近が多く選択
されており，
語順の相関に対する重みを上げ，
語彙の一致（BLEUスコア）に対する重みを下げるようにパラメタを選択しており，
RIBES と同様の傾向を示している．しかし，NTCIR-7 では，単一参照翻訳，複数参
照翻訳に関わらず0.3から0.6までの値が多く選ばれており，NTCIR-9 の場合ほど
BLEU スコアに対する重みを下げるようなパラメタが選択されていない．
NTCIR-7
では RIBES の相関が概ね LRscore の相関を上回っていたが，その原因はこうした語彙の
一致に対する重みの違いによると考える．
さらに RIBES は2つのパラメタ$\alpha$, $\beta$があることに対し，LRscoreは1つの
パラメタ$\gamma$しかない．よって，RIBES はLRscoreよりもより柔軟に
データにフィットできる点が双方の手法のパラメタ選択，相関係数の差に影響を
与えたとも考える．

5.2 節でも述べたが，$\alpha=0$, $\beta=1$，$\gamma=1$とした場合，RIBES も
LRscoreも語彙の一致スコアを考慮せず順位相関と短い翻訳に対するペナルティ
だけを考慮することになり，両者はほぼ一致する．図 \ref{lr-parm}より，
LRscore では$\gamma=1$が試行の半数近くで選択されているが，
図 \ref{parm}
の NTCIR-9 でそうした例がみられるものの全体に占める割合は決して多
くはない．
また，LRscore の語順相関の計算法として${d_k}_1$と${d_k}_2$の双方を試した
が，一貫して，${d_k}_1$の方がよい相関を示した．
こうしたことから，基本的には RIBES と LRscore は違うものと捉えて差し支え
ないだろう．


