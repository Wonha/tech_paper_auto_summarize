 
 \section{はじめに}

 機械翻訳システムの開発過程では，システムの評価と改良を幾度も繰り返さね
 ばならない．
 信頼性の高い評価を行うためには，人間による評価を採用することが理想ではあるが，
 時間的な制約を考えるとこれは困難である．
 よって，人間と同程度の質を持つ自動評価法
 ，つまり，人間の評価と高い相関を
 持つ自動評価法を利用して人間の評価を代替することが実用上求められる\footnote{
 本稿では，100 文規模程度のコーパスを用いて翻訳システムの性能を評価すること，
 つまり，システム間の優劣を比較することを目的とした自動評価法について
 議論する．}．

 こうした背景のもと，
 様々な自動評価法が提案されてきた．BLEU \cite{bleu}, NIST \cite{nist},
 METEOR \cite{meteor}，Word Error Rate (WER) \cite{WER} などが広く利用されて
 いるが，
 そのなかでも BLEU \cite{bleu} は，
 数多くの論文でシステム評価の指標として採用されているだけでなく，
 評価型ワー
 クショップにおける公式指標としても用いられており，自動評価のデファクトス
 タンダードとなっている．
 その理由は，人間による評価との相関が高いと言われていること，計算法
 がシステム翻訳と参照翻訳（正解翻訳）との間で一致するNグラム（一般的に
 $\mathrm{N}=4$が用いられる）を数えあげるだけで実装も簡単なことにある．
score of this paragraph is 5
