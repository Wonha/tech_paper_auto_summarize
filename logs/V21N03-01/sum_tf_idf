================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:1, score:0.66713] これまでに様々な自動評価法が提案されてきたが，参照翻訳とシステム翻訳との間で一致するNグラムの割合に基づきスコアを決定するBLEUや最大共通部分単語列の割合に基づきスコアを決定するROUGE-Lなどがよく用いられてきた．
[i:9, score:0.47352] 大局的な語順は順位相関係数で測定し，訳語の違いは，単語適合率で測定するがパラメタでその重みを調整できるようにする．
[i:10, score:0.46285] NTCIR-7，NTCIR-9の特許翻訳タスクにおける英日，日英翻訳のデータを用いてメタ評価を行ったところ，提案手法が従来の自動評価法よりも優れていることを確認した．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[i:24, score:0.58272] LCSという文全体での大局的な語順を考慮していることから，英日，日英翻訳システムの評価において，Nグラム一致率に基づく自動評価法よりもより良い評価ができるだろう．
[i:29, score:0.65506] より具体的には，システム翻訳と参照翻訳との間の語順の近さを測るため，両者に一致して出現する単語を同定した後，それらの出現順序の近さを順位相関係数を用いて計算し，これに重み付き単語正解率と短い翻訳に対するペナルティを乗じたものを最終的なスコアとする．
[i:31, score:0.70273] LRscoreは，参照翻訳とシステム翻訳との間で一致する単語の語順の近さをKendall距離で表し，それをさらに低レンジでのスコアを下げるために非線形変換した後，短い翻訳に対するペナルティを乗じ，さらにBLEUスコアとの線形補間で評価スコアを決定する．

================================================================
[section type  : experiment_result]
[section title : Nグラム一致率に基づく自動評価法の問題点]
================================================================
[i:38, score:0.45694] Nグラム一致率を用いてシステム翻訳を評価する際の問題点を以下に定義するBLEUを例として説明する．
[i:61, score:0.44049] 短い単語列のみを評価対象とすると，先の例のように，参照翻訳の節中のNグラムを保持していれば，節の順番が入れ替わったとしても十分高いスコアを獲得する．
[i:74, score:0.40626] なお，この問題はBLEUに限ったことではなく，その変種であるNISTスコア，METEORなどNグラム一致率を利用した自動評価法すべてに当てはまる問題である．

================================================================
[section type  : experiment_result]
[section title : LCSに基づく自動評価法の問題点]
================================================================
[i:75, score:0.59893] ROUGE-L [CITE]，IMPACT [CITE]は，参照翻訳とシステム翻訳との間の最長共通部分単語列(LCS)に基づき評価スコアを決定する．
[i:85, score:0.46252] ROUGE-LスコアはLCS適合率と再現率の調和平均，F値なのでBLEUとは違い，[MATH]を[MATH]より高く評価することができる．
[i:95, score:0.67360] 例えば，以下のシステム翻訳[MATH]を考えると，[MATH]と[MATH]との間のLCSは，``he caught a cold the rain''となるので，LCS適合率，再現率はそれぞれ，6/13，6/11となり，適合率が[MATH]の場合より低い値をとってしまい，ROUGE-Lスコアは[MATH]の場合よりも低くなる．

================================================================
[section type  : experiment_result]
[section title : 語順の相関に基づく自動評価法]
================================================================
[i:98, score:0.24517] 本稿では，Nグラム一致率に基づく自動評価法の問題点を解決するため，文内の局所的な語の並びに着目するのではなく，大局的な語の並びに着目する．
[i:99, score:0.30964] つまり，参照翻訳とシステム翻訳との間で一致して出現する単語の出現順の近さに基づき評価する．
[i:100, score:0.34502] さらに，訳語の違いに寛大な評価をするため，システム翻訳の単語適合率の重みを調整できるようにして別途ペナルティとして用いる．
-----------------------------------------------------
  [subsection title : 単語アラインメント]
-----------------------------------------------------
  [i:lead, score:0.46703] 参照翻訳とシステム翻訳の語順との間の相関を計算するため，双方の翻訳に一致して出現する単語を同定しなければならない．
.....
  [i:101, score:0.46703] 参照翻訳とシステム翻訳の語順との間の相関を計算するため，双方の翻訳に一致して出現する単語を同定しなければならない．
  [i:109, score:0.38786] それ以外の場合，[MATH]を基準として右側にNグラムを伸長させ，システム翻訳と参照翻訳の双方における出現頻度が1となった時点で[MATH]と[MATH]を対応づける（8〜13行目）．
  [i:110, score:0.38954] それでも対応がつかない場合，[MATH]を基準として左側にNグラムを伸長させ，システム翻訳と参照翻訳の双方における出現頻度が1となった時点で[MATH]と[MATH]を対応づける（15〜20行目）．
-----------------------------------------------------
  [subsection title : 単語出現順の相関]
-----------------------------------------------------
  [i:lead, score:0.31381] 1対1の単語アラインメントを決定することができれば，参照翻訳とシステム翻訳から単語出現位置IDを要素とするリストを得ることができる．
.....
  [i:117, score:0.52102] こうした順序列間の順位相関係数を計算することで参照翻訳とシステム翻訳との間で一致して出現する単語の出現順の近さを測ることができる．
  [i:122, score:0.35948] [MATH]は，アラインメント手続きを用いてシステム翻訳から得た単語出現位置のIDリスト(worder)について，[MATH]番目の要素の値よりも大きな要素が[MATH]番目から[MATH]番目の要素までの間に出現する数，[MATH]はその逆に，[MATH]番目の要素の値よりも小さな要素が[MATH]番目から[MATH]番目の要素までの間に出現する数を表す．
  [i:127, score:0.63513] BLEUでは，[MATH]が[MATH]よりも高いスコアを獲得したが，文全体での語順に着目し，システム翻訳と参照翻訳との間の語順の順位相関を計算すると，[MATH]が[MATH]よりも高いスコアを獲得でき，我々の直感に合致した結果を得ることができた．
-----------------------------------------------------
  [subsection title : ペナルティ]
-----------------------------------------------------
  [i:lead, score:0.51007] 参照翻訳とシステム翻訳との間の語順の相関を計算するためには，単語アラインメントを決定し，双方に一致して出現する単語のみを評価の対象としなければならない．
.....
  [i:129, score:0.51007] 参照翻訳とシステム翻訳との間の語順の相関を計算するためには，単語アラインメントを決定し，双方に一致して出現する単語のみを評価の対象としなければならない．
  [i:144, score:0.55243] このように，順位相関係数を用いると，システム翻訳の2単語のみが参照翻訳と出現順まで一致すると，不当に高いスコアを獲得する可能性がある．
  [i:149, score:0.39255] 単語正解率は，システム翻訳の単語のうちアラインメントをとることができた単語数(len(worder))の割合であり，len([MATH])は，参照翻訳の単語数，len([MATH])はシステム翻訳の単語数である．

================================================================
[section type  : experiment_result]
[section title : 実験の設定]
================================================================
[i:160, score:0.00000] 
-----------------------------------------------------
  [subsection title : 実験データ]
-----------------------------------------------------
  [i:lead, score:0.48092] RIBESの有効性を示すため，NTCIR-7，NTCIR-9の特許翻訳タスク(PATMT)のデータを用いて評価実験（評価指標の評価なので，以降メタ評価と呼ぶ）を行った．
.....
  [i:161, score:0.48092] RIBESの有効性を示すため，NTCIR-7，NTCIR-9の特許翻訳タスク(PATMT)のデータを用いて評価実験（評価指標の評価なので，以降メタ評価と呼ぶ）を行った．
  [i:165, score:0.34448] NTCIRワークショップの事務局から公開されているデータには，EJ，JEタスクとも1つの参照翻訳しか含まれていない．
  [i:166, score:0.32112] そこで，NTCIR-7のデータに対してのみ，特許翻訳の専門家に依頼し，参照翻訳を独自に追加した．
-----------------------------------------------------
  [subsection title : 比較した自動評価手法]
-----------------------------------------------------
  [i:lead, score:0.56136] 比較評価には，Nグラム一致率に基づく評価手法として先に説明したBLEU，大局的な単語列を考慮した評価法として同じく先に説明したROUGE-L [CITE]，その改良版であるIMPACT [CITE]を用いた．
.....
  [i:173, score:0.56136] 比較評価には，Nグラム一致率に基づく評価手法として先に説明したBLEU，大局的な単語列を考慮した評価法として同じく先に説明したROUGE-L [CITE]，その改良版であるIMPACT [CITE]を用いた．
  [i:179, score:0.59523] LRscoreは，参照翻訳とシステム翻訳との間の語順の近さを表すスコアとBLEUスコアとの間の線形補間で評価スコアを決定する．
  [i:186, score:0.76219] しかし，LRscoreは日本語，英語のような大きな語順の入れ替えがある言語対を対象として考案された手法ではなく，ヨーロッパ言語間，中英翻訳という比較的語順が似た言語を対象として考案されたため，最終的には[MATH]を採用することで順位相関の低レンジスコアの感度を下げ，さらに語順の近い言語対を対象としたときに実績のあるBLEU の恩恵を受けるため，それとの間の線形補間という定式化に至ったのであろう．
-----------------------------------------------------
  [subsection title : メタ評価の指標]
-----------------------------------------------------
  [i:lead, score:0.41669] 本稿では，メタ評価の指標として広く用いられているPearsonの積率相関係数，Spearmanの順位相関係数，Kendallの順位相関係数を用いた．
.....
  [i:191, score:0.41669] 本稿では，メタ評価の指標として広く用いられているPearsonの積率相関係数，Spearmanの順位相関係数，Kendallの順位相関係数を用いた．
  [i:192, score:0.42896] Pearsonの積率相関係数は人間の評価と自動評価の結果がどの程度線形の関係にあるかを評価し，Spearman，Kendallの相関係数は人間の評価と自動評価の結果の順位がどの程度近いかを評価する．
  [i:193, score:0.18299] SpearmanとKendallの違いは，先にも説明したように順位の差に対して重みをどのように与えるかという点にある．
-----------------------------------------------------
  [subsection title : 実験の手順]
-----------------------------------------------------
  [i:lead, score:0.86702] RIBESに対してはシステム翻訳の長さに対する重みパラメタと単語正解率に対する重みパラメタ，IMPACTに対してはLCSに対する重みパラメタと語順の違いに対する重みパラメタ，LRscoreには順位相関係数とBLEUスコアの重みを調整するパラメタがある．
.....
  [i:194, score:0.86702] RIBESに対してはシステム翻訳の長さに対する重みパラメタと単語正解率に対する重みパラメタ，IMPACTに対してはLCSに対する重みパラメタと語順の違いに対する重みパラメタ，LRscoreには順位相関係数とBLEUスコアの重みを調整するパラメタがある．
  [i:197, score:0.47282] 選択したIDによる10文の集合を用いて，文集合全体での人間の評価スコアと自動評価スコアとの間のSpearmanの順位相関係数が最大となるようパラメタを決定する．
  [i:200, score:0.37789] なお，パラメタが存在しないBLEUとROUGE-Lに対しては，(2)をスキップし，同様の手順でメタ評価を行った．

================================================================
[section type  : experiment_result]
[section title : 実験結果と考察]
================================================================
[i:201, score:0.00000] 
-----------------------------------------------------
  [subsection title : NTCIR-7データ]
-----------------------------------------------------
  [i:lead, score:0.50198] 表[REF_NTCIR7-single]にオーガナイザから配布された参照翻訳のみを用いた時の相関係数の平均値，表[REF_NTCIR7-multi]に複数参照翻訳を用いた時の相関係数の平均値を示す．
.....
  [i:221, score:0.72576] ROUGE-L，IMPACTの相関係数も上昇しており，ROUGE-Lは日英翻訳に関しては，他のすべての手法に対して，英日翻訳に関しては，RIBES以外の手法に対して統計的有意に優れている．
  [i:222, score:0.68621] RIBESは，日英翻訳ではROUGE-Lに次いでIMPACTと同程度，英日翻訳ではROUGE-Lと同程度であるが，十分強い相関を示している．
  [i:223, score:0.68825] BLEU，ROUGE-L，IMPACTの相関係数が複数参照翻訳が与えられた場合に顕著に改善される理由は，語彙のバリエーションが増えたことであろう．
-----------------------------------------------------
  [subsection title : NTCIR-9データ]
-----------------------------------------------------
  [i:lead, score:0.30023] 表[REF_NTCIR9]に相関係数の平均値を示す．
.....
  [i:246, score:0.66060] 実際，日英，英日の双方においてROUGE-L，IMPACT，BLEUといった従来の自動評価法に対し，統計的有意に高い相関係数を獲得していることがそれの有効性を示唆している．
  [i:250, score:0.70669] 表[REF_rbmt]より，RIBESは日英翻訳タスクでROUGE-L，IMPACTに劣るものの全体を通してみれば他の手法より良い相関を得ている．
  [i:251, score:0.70286] 参照翻訳が1つしかないという影響もあるが，英日翻訳タスクではROUGE-L，IMPACT，BLEUは負の相関でしかない．
-----------------------------------------------------
  [subsection title : 獲得されたパラメタに関する考察]
-----------------------------------------------------
  [i:lead, score:0.35501] 最後にRIBESとLRscoreについて獲得されたパラメタ，[MATH], [MATH], [MATH]の違いから考察する．
.....
  [i:267, score:0.75334] 図[REF_lr-parm]より，LRscoreのNTCIR-9では，[MATH]付近が多く選択されており，語順の相関に対する重みを上げ，語彙の一致（BLEUスコア）に対する重みを下げるようにパラメタを選択しており，RIBESと同様の傾向を示している．
  [i:268, score:0.57487] しかし，NTCIR-7では，単一参照翻訳，複数参照翻訳に関わらず0.3から0.6までの値が多く選ばれており，NTCIR-9の場合ほどBLEUスコアに対する重みを下げるようなパラメタが選択されていない．
  [i:272, score:0.65791] 5.2節でも述べたが，[MATH], [MATH]，[MATH]とした場合，RIBESもLRscoreも語彙の一致スコアを考慮せず順位相関と短い翻訳に対するペナルティだけを考慮することになり，両者はほぼ一致する．

================================================================
[section type  : conclusion]
[section title : まとめと今後の課題]
================================================================
[i:276, score:0.80713] 本稿では，翻訳時に大きな語順の入れ替えが必要となる英日，日英翻訳システムを対象として，文全体での大局的な語順の相関をKendallの順位相関係数に基づき決定し，これと単語適合率，短い翻訳に対するペナルティを重み付きで乗じた自動評価法であるRIBES (Rank-based Intuitive Bilingual Evaluation Score)を提案した．
[i:277, score:0.83934] NTCIR-7，NTCIR-9の特許翻訳タスクのデータを用いてメタ評価を行ったところ，BLEU，ROUGE-L，IMPACTといった従来の自動評価法は，参照翻訳の数が少ない場合，評価対象システムにおけるルールベースシステムの割合が大きい場合に相関が低下することに対し，RIBESは，こうした状況でも安定して高い相関を示すことを確認した．
[i:282, score:0.68725] ROUGE-L，IMPACTと同様，RIBESは文単位でも自動評価スコアを計算できるので，これらの手法の文単位での自動評価スコアと人間の評価スコアとの間のSpearmanの順位相関係数を計算した．

