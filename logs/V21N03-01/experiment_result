Nグラム一致率を用いてシステム翻訳を評価する際の問題点を以下に定義するBLEUを例として説明する．
システム翻訳文集合を[MATH]，それに対応する参照翻訳文集合を[MATH]とする．
システム翻訳文[MATH]には，対応する参照翻訳文の集合[MATH]が割り当てられており，[MATH]の[MATH]番目の参照翻訳文を[MATH]とする．
なお，[MATH]とする．
ここで，BLEUは，以下の式で定義される．
[MATH]はNグラムの長さパラメタであり，一般的には[MATH]である．
[MATH]は，Nグラム適合率であり，以下の式で定義される．
count([MATH],[MATH])は，任意のNグラム([MATH])のシステム翻訳文[MATH]における出現頻度，max_count([MATH],[MATH])は，[MATH]の参照翻訳文集合[MATH]における出現頻度の最大値，[MATH]である．
BP (Brevity Penalty)は，短いシステム翻訳に対するペナルティであり，以下の式で定義される．
closest_len([MATH])は，各[MATH]に対し，最も近い単語数の参照翻訳文[MATH]を決定した後，それらの単語数を全ての[MATH]で合計したもの，len([MATH])は，[MATH]単語数を全ての[MATH]で合計したものを表す．
いま，原文([MATH])，参照翻訳([MATH])，システム翻訳([MATH], [MATH])が以下の通り与えられたとしよう．
雨に濡れたので，彼は風邪をひいた．
He caught a cold because he got soaked in the rain.
He caught a cold because he had gotten wet in the rain.
He got soaked in the rain because he caught a cold.
[MATH]は原文の直訳であり，[MATH]はほぼそれと等しい訳であるが，[MATH]は「風邪をひいたので，彼は雨に濡れた」という意味であり，原文が表す因果関係が逆転している．
[MATH]と[MATH]を比較すると，翻訳としての流暢さ(fluency)，いわゆる言語モデル的な確からしさは同程度であるが，内容の適切性(adequacy)は，[MATH]が[MATH]よりも高くならねばならない．
ここで，この2つのシステム翻訳を先に示したBLEUで評価してみよう．
[MATH]，[MATH]とも[MATH]よりも長いため，ともにBPは1となる．
[MATH]の[MATH]〜[MATH]はそれぞれ，9/12，7/11，5/10，3/9なので，BLEUスコアは0.53となる．
一方，[MATH]の[MATH]〜[MATH]はそれぞれ，11/11，9/10，6/9，4/8なので，BLEUスコアは0.74となる．
この結果は，我々の直感に反しており，BLEUを最大化するようにシステムを最適化することが，良い翻訳システムの開発に結びつくかどうかは疑問である．
こうした問題が起こる原因はNグラムという局所的な語の並びにのみに着目してスコアを計算することにある．
短い単語列のみを評価対象とすると，先の例のように，参照翻訳の節中のNグラムを保持していれば，節の順番が入れ替わったとしても十分高いスコアを獲得する．
もちろん，[MATH]のような翻訳をシステムが出力するようなことはほとんどあり得ないのではないかという疑問もあろう．
確かに語順が似た言語対を対象とする場合や翻訳システムがルールベースで構築されている場合には起こりにくい問題であるが，語順が大きく異なる言語対を対象とした統計翻訳(Statistical Machine Translation: SMT)システムでは十分起こり得る問題である．
以下にWeb上のSMTによる翻訳サービスの出力例を示す．
ボブはメアリに指輪を買うためにジョンの店に行った．
Bob went to John's store to buy a ring for Mary.
Bob to buy rings, Mary went to John shop.
SMT出力をみると，訳語という観点では参照翻訳と良く合致しており，バイグラム，トライグラムでもある程度の数が一致している．
しかし，原文の「店に行く」の主体が「ボブ」であるという構造を捉えることができず，その主体が「メアリ」となってしまっている．
SMTシステムでは，大きな語順の入れ替えを許すと探索空間は膨大になる．
よって，現実的な時間で翻訳文を生成するため，語順の入れ替えにある程度の制限を設けざるを得ない．
その結果，Nグラムでは参照翻訳と良く合致するものの原文の意味とはかけ離れた翻訳を出力することがある．
このような状況のもと，BLEUスコアで翻訳システムを比較すると，正しい評価ができない可能性が高い．
なお，この問題はBLEUに限ったことではなく，その変種であるNISTスコア，METEORなどNグラム一致率を利用した自動評価法すべてに当てはまる問題である．
ROUGE-L [CITE]，IMPACT [CITE]は，参照翻訳とシステム翻訳との間の最長共通部分単語列(LCS)に基づき評価スコアを決定する．
先に挙げた例で説明する．
雨に濡れたので，彼は風邪をひいた．
He caught a cold because he got soaked in the rain.
He caught a cold because he had gotten wet in the rain.
He got soaked in the rain because he caught a cold.
[MATH]と[MATH]との間のLCSは，``He caught a cold because he in the rain''であり，その長さ（単語数）は9である．
[MATH]の長さは11，[MATH]の長さは12であることから，LCSの適合率は9/12，再現率は9/11となる．
一方，[MATH]と[MATH]との間のLCSは，``he got soaked in the rain''であり，その長さは6である．
[MATH]の長さは11なので，LCSの適合率は6/11，再現率は6/11となる．
ROUGE-LスコアはLCS適合率と再現率の調和平均，F値なのでBLEUとは違い，[MATH]を[MATH]より高く評価することができる．
IMPACTはROUGE-Lを改良したものであり，上述のLCSを一度見つけただけでやめるのではなく，見つかったLCSを削除した単語列に対し，再度LCSを探すということを繰り返す．
つまり，[MATH]の例では，[MATH]と[MATH]から，``He caught a cold because he in the rain''を削除し，
got soaked
had gottten wet
から，[MATH]の例では，``he got soaked in the rain''を削除し，
caught a cold because he
because he caught a cold
から，再度LCSを探し出すという手順を繰り返す．
これらの手法の問題点は，参照翻訳とシステム翻訳との間のLCS適合率，再現率を計算するため，それらの間で一致しなかった単語を評価の対象に含めている点にある．
例えば，以下のシステム翻訳[MATH]を考えると，[MATH]と[MATH]との間のLCSは，``he caught a cold the rain''となるので，LCS適合率，再現率はそれぞれ，6/13，6/11となり，適合率が[MATH]の場合より低い値をとってしまい，ROUGE-Lスコアは[MATH]の場合よりも低くなる．
He caught a cold as a result of getting hit by the rain
このように適合率，再現率といった参照翻訳とシステム翻訳との間で一致しない単語を評価に含めてしまう尺度を用いると訳語の違いに敏感になり過ぎ，システムを過小評価することがある．
本稿では，Nグラム一致率に基づく自動評価法の問題点を解決するため，文内の局所的な語の並びに着目するのではなく，大局的な語の並びに着目する．
つまり，参照翻訳とシステム翻訳との間で一致して出現する単語の出現順の近さに基づき評価する．
さらに，訳語の違いに寛大な評価をするため，システム翻訳の単語適合率の重みを調整できるようにして別途ペナルティとして用いる．
参照翻訳とシステム翻訳の語順との間の相関を計算するため，双方の翻訳に一致して出現する単語を同定しなければならない．
これは，参照翻訳とシステム翻訳との間の単語アラインメントを決定する問題となる．
本稿では，単語の表層での一致に基づくアラインメント法を採用した．
Algorithm [REF_wordalign]にその疑似コードを示す．
[b]
[1] \STATE Read hypothesis sentence [MATH] \STATE Read reference sentence [MATH] \STATE Initialize worder with an empty list. \FOR{each word [MATH] in [MATH]} \IF{[MATH] appears only once each in [MATH] and [MATH]} \STATE append [MATH] s.t. [MATH] to worder \ELSIF\FOR{[MATH]=2 to [MATH]} \IF{[MATH] appears only once each in [MATH] and [MATH]} \STATE append [MATH] s.t. [MATH] to worder \STATE break the loop \ENDIF\ENDFOR\ELSE\FOR{[MATH]=2 to [MATH]} \IF{[MATH] appears only once each in [MATH] and [MATH]} \STATE append [MATH] s.t. [MATH] to worder \STATE break the loop \ENDIF\ENDFOR\ENDIF\ENDFOR\STATE Return worder
システム翻訳を長さ[MATH]，参照翻訳を長さ[MATH]の単語リストして読み込み，アラインメントを格納する配列worderを初期化する（1〜3行目）．
システム翻訳の単語リストの先頭から順に単語[MATH]を取り出し，その単語がシステム翻訳，参照翻訳の双方にただ1度のみ出現している場合，[MATH]と単語[MATH]の参照翻訳における出現位置[MATH]を対応づける（5, 6行目）．
それ以外の場合，[MATH]を基準として右側にNグラムを伸長させ，システム翻訳と参照翻訳の双方における出現頻度が1となった時点で[MATH]と[MATH]を対応づける（8〜13行目）．
それでも対応がつかない場合，[MATH]を基準として左側にNグラムを伸長させ，システム翻訳と参照翻訳の双方における出現頻度が1となった時点で[MATH]と[MATH]を対応づける（15〜20行目）．
これでも曖昧性が残る（システム翻訳と参照翻訳での頻度が1にならない）場合，あるいは対応先が見つからない場合は単語対応付けを行わない．
図[REF_figalign]に2章の例文に対する単語アラインメントを示す．
上段の例から，worderの1番目の要素，つまり，[MATH]の1単語目が[MATH]の1番目の要素（単語）に対応することがわかる．
下段の例から，[MATH]の1単語目が[MATH]の6番目の単語と対応していることがわかる．
1対1の単語アラインメントを決定することができれば，参照翻訳とシステム翻訳から単語出現位置IDを要素とするリストを得ることができる．
図[REF_figalign]の例では，[MATH]:[1,2,3,4,5,6,9,10,11]，[MATH]:[1,2,3,4,5,6,9,10,11]および[MATH]:[1,2,3,4,5,6,7,8,9,10,11]，[MATH]:[6,7,8,9,10,11,5,1,2,3,4]という2つのリストペアを得る．
こうした順序列間の順位相関係数を計算することで参照翻訳とシステム翻訳との間で一致して出現する単語の出現順の近さを測ることができる．
本稿では以下に示すKendallの順位相関係数([MATH]) [CITE]を採用した．
順位相関係数としては，Spearmanの順位相関係数([MATH])もよく知られている．
しかし，[MATH]と比べて[MATH]は，順位の小さな入れ替わりには寛容すぎ，大きな入れ替わりには厳しすぎる．
予備実験の結果では，人間の評価との間の相関が[MATH]よりも低い傾向を示したため，本稿では[MATH]を採用した．
[MATH]は，アラインメント手続きを用いてシステム翻訳から得た単語出現位置のIDリスト(worder)について，[MATH]番目の要素の値よりも大きな要素が[MATH]番目から[MATH]番目の要素までの間に出現する数，[MATH]はその逆に，[MATH]番目の要素の値よりも小さな要素が[MATH]番目から[MATH]番目の要素までの間に出現する数を表す．
表[REF_tau]に図[REF_figalign]の[MATH]から得たworderと[MATH]，[MATH]をそれぞれ示す．
この表より，[MATH]と[MATH]との間の語順の相関をKendallの[MATH]で計算すると，[MATH]となる．
同様に図[REF_figalign]の[MATH]から得たworderを用いて[MATH]を計算すると[MATH]となる．
[MATH]は参照翻訳とシステム翻訳との語順が完全一致する場合に1，逆順の場合に[MATH]をとる．
BLEUでは，[MATH]が[MATH]よりも高いスコアを獲得したが，文全体での語順に着目し，システム翻訳と参照翻訳との間の語順の順位相関を計算すると，[MATH]が[MATH]よりも高いスコアを獲得でき，我々の直感に合致した結果を得ることができた．
ただし，[MATH]は負の値をとり得るため，従来の自動評価法が出力するスコアレンジと同様[0,1]の値をとるよう以下の式で正規化する．
参照翻訳とシステム翻訳との間の語順の相関を計算するためには，単語アラインメントを決定し，双方に一致して出現する単語のみを評価の対象としなければならない．
しかし，参照翻訳とシステム翻訳との間で一致する単語のみを評価対象とすることには以下の2つ問題がある．
システム翻訳の単語数に対し，参照翻訳との間で一致する単語の割合が少ない場合，過剰に高いスコアを与える可能性がある．
システム翻訳の単語数が少ない場合，過剰に高いスコアを与える可能性がある，
(1)に関して，以下の例を考えよう．
John went to a restaurant yesterday
John read a book yesterday
[MATH]は5単語からなる訳であり，そのうち``John''，``a''，``yesterday''のみしか参照翻訳と一致していない．
しかし，その出現順が参照翻訳と一致していることからNKTは1となる．
つまり，システムが出力した単語数に関係なく順位相関だけをみていると不当に高いスコアを獲得する可能性がある．
次に(2)に関して，以下の例を考えよう．
John went to a restaurant yesterday
to a
システム翻訳は2単語しかない意味の無い訳であるにもかかわらず，単語正解率は1であり，2単語の出現順序も参照翻訳と一致していることから，NKTも1となる．
つまり，単語数が少ない場合，順位相関と単語正解率だけでは不当に高いスコアを獲得する可能性がある．
このように，順位相関係数を用いると，システム翻訳の2単語のみが参照翻訳と出現順まで一致すると，不当に高いスコアを獲得する可能性がある．
よって，本稿では，前者に対して単語正解率([MATH])，後者に対してはBLEUのBPをペナルティとして導入する．
それぞれの定義を以下に示す．
P(h_i,r_i)=\frac{len (worder)}{len}(h_i)
\text{BP}_s(h_i,r_i)=\min\left(1,\exp\left(1-\frac{len}(r_i){len}(h_i)\right)\right)
単語正解率は，システム翻訳の単語のうちアラインメントをとることができた単語数(len(worder))の割合であり，len([MATH])は，参照翻訳の単語数，len([MATH])はシステム翻訳の単語数である．
BLEUのBPは文集合全体で計算していたが，ここでは，文単位で計算することに注意されたい．
これらを用いて最終的な自動評価スコアを以下の式([REF_ribes])で定義する．
なお，この手法をRIBES(Rank-based Intuitive Bilingual Evaluation Score)と名付け，\url{http://www.kecl.ntt.co.jp/icl/lirg/ribes/}にてオープンソースソフトウェアとして公開している．
[MATH]は単語適合率の重みであり，[MATH]が大きいほど訳語の違いに敏感になる．
参照翻訳が1つしかない場合，参照翻訳にはない訳語をシステムが出力する可能性が高いため，[MATH]は小さめに設定した方がよいだろう．
参照翻訳が複数の場合，参照翻訳のいずれかに出現する単語をシステムが出力する可能性が高くなる．
そこで，不適切な訳語を厳しく採点するため[MATH]は高めに設定した方がよいだろう．
[MATH]はBPの重みであり，[MATH]が大きいほど訳文の長さに敏感になる．
参照翻訳が1つしかない場合，それよりも短い翻訳があり得る可能性が高いので，[MATH]は小さめに設定してよいだろう．
参照翻訳が複数ある場合，一番短い翻訳を基準にして考えれば，[MATH]を高めに設定してよいだろう．

RIBESの有効性を示すため，NTCIR-7，NTCIR-9の特許翻訳タスク(PATMT)のデータを用いて評価実験（評価指標の評価なので，以降メタ評価と呼ぶ）を行った．
言語対は英日(EJ)，日英(JE)とした．
それぞれのデータセットの文数，1文あたりの参照翻訳の数，評価者の数，参加システム数を表[REF_data]に示す．
なお，カッコ内の数字はルールベースシステムの数を示す．
NTCIRワークショップの事務局から公開されているデータには，EJ，JEタスクとも1つの参照翻訳しか含まれていない．
そこで，NTCIR-7のデータに対してのみ，特許翻訳の専門家に依頼し，参照翻訳を独自に追加した．
また，NTCIR-7のEJタスクに関しては，5システムだけにしか人間の主観評価の結果が与えられていなかったため，特許に精通した被験者5名で再度JEタスクと同様，5段階評価で主観評価を行った．
さらに，評価対象とする翻訳システムに著者のグループの英日翻訳システム[CITE]を追加し，計14システムで実験を行った．
全てのデータに対し，メタ評価の対象は翻訳の内容としての適切性(adequacy)のみとした．
これは，翻訳の流暢さよりも内容の適切性を自動評価できた方がより良い翻訳システムの開発に貢献できると考えたからである．
なお，各システム翻訳文に対し複数の人間の評価スコアが与えられている場合には，その平均値を文に対する評価スコアとした．
このように各システム翻訳文に対して評価値を決定し，これを文集合全体での平均したものを人間がシステムに与えた評価スコアとした．
比較評価には，Nグラム一致率に基づく評価手法として先に説明したBLEU，大局的な単語列を考慮した評価法として同じく先に説明したROUGE-L [CITE]，その改良版であるIMPACT [CITE]を用いた．
IMPACTには，LCSの長さに応じた重みパラメタ，語順の入れ替えに応じた重みパラメタがある．
詳細については文献[CITE]を参照されたい．
なお，ROUGE-L，IMPACTとも参照翻訳が複数ある場合には個々の参照翻訳を用いて求めたスコアの最大値を評価スコアとして採用した．
BLEUの計算にはmteval-v13a，ROUGE-Lには，ROUGE-1.5.5，IMPACTにはIMPACT version 4を利用した．
また，LRscore [CITE]も比較評価の対象とした．
LRscoreは，参照翻訳とシステム翻訳との間の語順の近さを表すスコアとBLEUスコアとの間の線形補間で評価スコアを決定する．
語順の近さを表す尺度としては，ハミング距離[MATH]を利用するものとKendallの[MATH]に基づく[MATH]を利用するものがあるが，以降では，本稿との関連が深い後者について述べる．
LRscoreの定義を以下に示す．
[MATH]は以下の式で定義される．
[MATH]は，文献[CITE]に従うと[MATH]で定義されるが，それ以前の文献[CITE]では，[MATH]も用いられている．
以降，前者を[MATH]，後者を[MATH]とよぶ．
RIBESで[MATH], [MATH]と設定したときと，LRscoreに[MATH]を採用，[MATH]と設定したとき，これら2つの手法は一致する．
しかし，LRscoreは日本語，英語のような大きな語順の入れ替えがある言語対を対象として考案された手法ではなく，ヨーロッパ言語間，中英翻訳という比較的語順が似た言語を対象として考案されたため，最終的には[MATH]を採用することで順位相関の低レンジスコアの感度を下げ，さらに語順の近い言語対を対象としたときに実績のあるBLEU の恩恵を受けるため，それとの間の線形補間という定式化に至ったのであろう．
後述するが，英日，日英翻訳の評価ではBLEUを利用するメリットは期待できない．
さらに，NKTを[MATH]によって非線形変換することで低レンジスコアの感度をさらに下げるメリットも元々高いNKTを得ることが難しい英日，日英翻訳タスクでは期待できない．
以上より，LRscoreは確かにRIBESと良く似た手法といえるが，BLEUを補うために派生した評価指標と捉えた方が自然であり，RIBESとはその根底にある研究の動機に大きな違いがある．
なお，LRscoreには，参照翻訳とシステム翻訳との間の単語アラインメントを決定する手段が提供されないため，以降の実験では本稿での単語アラインメントを利用した．
本稿では，メタ評価の指標として広く用いられているPearsonの積率相関係数，Spearmanの順位相関係数，Kendallの順位相関係数を用いた．
Pearsonの積率相関係数は人間の評価と自動評価の結果がどの程度線形の関係にあるかを評価し，Spearman，Kendallの相関係数は人間の評価と自動評価の結果の順位がどの程度近いかを評価する．
SpearmanとKendallの違いは，先にも説明したように順位の差に対して重みをどのように与えるかという点にある．
RIBESに対してはシステム翻訳の長さに対する重みパラメタと単語正解率に対する重みパラメタ，IMPACTに対してはLCSに対する重みパラメタと語順の違いに対する重みパラメタ，LRscoreには順位相関係数とBLEUスコアの重みを調整するパラメタがある．
これらの手法に対しては，以下の手順でパラメタの最適化を行い，メタ評価を行った．
文のIDをランダムに10個選択する．
選択したIDによる10文の集合を用いて，文集合全体での人間の評価スコアと自動評価スコアとの間のSpearmanの順位相関係数が最大となるようパラメタを決定する．
(2)で決定したパラメタを用いて(1)の残りの文集合全体を用いてメタ評価を行い，相関係数を記録する．
(1)から(3)を100回繰り返し，相関係数の平均値を求める．
なお，パラメタが存在しないBLEUとROUGE-Lに対しては，(2)をスキップし，同様の手順でメタ評価を行った．

表[REF_NTCIR7-single]にオーガナイザから配布された参照翻訳のみを用いた時の相関係数の平均値，表[REF_NTCIR7-multi]に複数参照翻訳を用いた時の相関係数の平均値を示す．
平均値の差の検定には，ペアワイズの比較にWilcoxonの符号順位検定[CITE]を採用し，Holm法[CITE]による多重比較を用いた．
表[REF_NTCIR7-single]，[REF_NTCIR7-multi]より，どの手法に対してもSpearmanの順位相関係数の方がKendallの順位相関係数よりも高い．
Kendallの順位相関係数は，2つの順序列の間で一致する半順序関係の数に基づき決定されるため，細かな順位の間違いに敏感である．
一方，Spearmanの順位相関係数は順序列の間の順位の差に基づき決定されるため，細かな順位の間違いには鈍感である．
本稿で用いたデータでは，自動評価法にとって，明らかに良いシステムと悪いシステムの区別が容易であったため，大きな差での順位の不一致が減少し，Spearmanの順位相関係数がKendallの順位相関係数よりも相対的に高い値を記録したのであろう．
ただし，全体の傾向としては両者の間に大きな違いはない．
Pearsonの積率相関係数は順位相関係数より高い値を示しており，BLEUではその差が特に大きい．
たとえば，表[REF_NTCIR7-multi]の英日タスクでは，Pearsonが0.9以上であることに対し，Spearmanは0.7程度でしかない．
これは，人間の評価との順位付けはやや強い程度相関でしか示していないにも関わらず，線形の相関は非常に強いことを意味する．
Pearsonの積率相関係数は外れ値がある場合，その値が過剰に高く見積もられるということが知られているが，その影響が強く出ているのではないかと考える．
よって，以降では主に順位相関に焦点をあてて議論する．
表[REF_NTCIR7-single]より，日英翻訳に関しては，RIBESが他のすべての手法に対して統計的有意に優れている．
英日翻訳に関しては，RIBESとLRscore([MATH])が同程度で優れており，十分強い相関である．
ROUGE-L，IMPACTが，RIBESほどではないものの比較的よい相関を得ていることに対し，BLEUは双方の言語対において，相関係数の平均値が他のほぼ全ての手法に対し統計的に有意な差で劣っている．
さらに，順位相関係数は弱い相関程度でしかない．
この結果は，英日，日英翻訳という大きな語順の入れ替えを必要とする言語対を対象とした場合，Nグラム一致率で自動評価を行うことが不適切であることを示唆している．
表[REF_NTCIR7-multi]より，参照翻訳の数が増えると相関係数の平均値は上昇する傾向にある．
BLEUの相関係数が他の手法よりも有意に劣っていることは単一参照翻訳の場合と同様であるが，順位相関係数はやや強い相関程度にまで上昇している．
ROUGE-L，IMPACTの相関係数も上昇しており，ROUGE-Lは日英翻訳に関しては，他のすべての手法に対して，英日翻訳に関しては，RIBES以外の手法に対して統計的有意に優れている．
RIBESは，日英翻訳ではROUGE-Lに次いでIMPACTと同程度，英日翻訳ではROUGE-Lと同程度であるが，十分強い相関を示している．
BLEU，ROUGE-L，IMPACTの相関係数が複数参照翻訳が与えられた場合に顕著に改善される理由は，語彙のバリエーションが増えたことであろう．
これらの手法は，Nグラム一致率，LCS適合率，再現率を利用しているため，参照翻訳とシステム翻訳との間で一致しない単語も評価対象となる．
よって，語彙の一致判定を単に文字列としての一致だけで判定すると，意味的には一致するはずのものが一致せずに不当に低いスコアを得るという問題が起こる．
しかし，複数参照翻訳の場合には，語彙のバリエーションが増えるためこうした問題は軽減されるのであろう．
一方，RIBESでもシステム翻訳と参照翻訳との間の単語一致率は利用するため，一致しない単語を評価対象として用いているが，パラメタによりその影響を小さく抑えることができる．
よって，単一参照翻訳でも複数参照翻訳でも安定して高い相関を示すことができた．
表[REF_NTCIR9]に相関係数の平均値を示す．
NTCIR-7のデータとは異なり，どの手法も相関係数の平均値は大幅に低下している．
特にROUGE-L，IMPACT，BLEUは，非常に弱い相関，あるいは無相関と言えるほどである．
この原因は先の実験結果と同様，参照翻訳の数が1つであることに加え，評価対象となる翻訳システムの中でのルールベースの翻訳システム（SMTとのハイブリッドも含む）の占める割合が増したことにある．
NTCIR-7と比較すると，日英翻訳に関してはルールベースシステムは2システムから6システムへ，英日翻訳に関しては1システムから5システムへと増えた．
図[REF_uniprec]にユニグラム適合率と人間のスコアの関係を示す．
図中四角のマーカ(RBMT-1〜6，JAPIO，TORI，EIWA)がルールベースの翻訳システムである．
図から明らかなように，ルールベースの翻訳システムはユニグラム適合率が低いにも関わらず人間の評価では高いスコアを獲得している．
つまり，これらのシステムの翻訳は，参照翻訳と一致する単語の割合が少ないにも関わらず人間の評価では高いスコアを獲得している．
ルールベースの翻訳システムは，訓練データから訳語を推定するSMTシステムほど語彙の統制がとれていない．
よって，翻訳対象のドメインに合致した語彙，すなわち，特許特有の語彙を用いて翻訳できるとは限らない．
しかし，SMTシステムにとって大きな問題となる語順の入れ替えに関しては，記述されたルールに当てはまる限りは問題となり得ない．
よって，特許の訳語として多少おかしくとも文全体で意味が通る翻訳となり，その結果，人間が高いスコアを与えたのであろう．
先にも説明した通り，BLEUは，Nグラムの適合率，ROUGE-LとIMPACTはLCSの適合率・再現率に基づきスコアを決定する．
つまり，参照翻訳と一致する単語の割合が大きいシステム翻訳にしか高いスコアを与えることができない．
よって，ルールベースシステムに高いスコアを与えることはできず，それらの性能を低く見積もってしまったことにより，相関が著しく低下したと考える．
一方，RIBESとLRscoreはこれらの手法とは異なり，単語正解率，BLEUスコアをパラメタで軽減することで単語一致率の低いシステムであっても高いスコアを与えることができるという特徴がある．
実際，日英，英日の双方においてROUGE-L，IMPACT，BLEUといった従来の自動評価法に対し，統計的有意に高い相関係数を獲得していることがそれの有効性を示唆している．
ユニグラム適合率が低いところでの自動評価法の性能をより詳細に調べるため，ルールベースの翻訳システムのみを取り出し，同様の実験を行い相関係数の平均値を求めたところ，表[REF_rbmt]を得た．
翻訳システム数は日英翻訳タスクで6，英日翻訳タスクで5である．
サンプル数が少ないため，相関係数の値に対する信頼性がこれまでの実験よりも劣ることに注意されたい．
表[REF_rbmt]より，RIBESは日英翻訳タスクでROUGE-L，IMPACTに劣るものの全体を通してみれば他の手法より良い相関を得ている．
参照翻訳が1つしかないという影響もあるが，英日翻訳タスクではROUGE-L，IMPACT，BLEUは負の相関でしかない．
さらに，先に示したとおり，表[REF_NTCIR9]においてRIBESがルールベースシステム，SMTシステム双方を含む場合でも良い相関を得たこともふまえると，参照翻訳とシステム翻訳との間で一致する単語が少ない場合でもRIBESは有効であると考える．
以上より，BLEU，ROUGE-L，IMPACTといった単語の一致に強く依存する従来の自動評価法は，単一参照翻訳時，評価対象としてルールベースシステムが多く混在する場合には著しく信頼性が低下することを示した．
特に，参照翻訳は常に複数与えられるとは限らないため，自動評価法としては単一参照翻訳でも人間の評価結果との間の相関が高いことが望ましい．
RIBESは，参照翻訳数，ルールベースシステムの数が変化した場合でも安定して高い相関であることから，従来の自動評価法よりも優れていると考える．
ただし，RIBESには単語正解率と短い翻訳に対するペナルティを調整するための重みパラメタがある．
これらパラメタを最適化するためには，いわゆる教師データが必要となることから，それを必要としないBLEU，ROUGE-Lよりもコストのかかる手法とも言える．
しかし，実験結果より，各システムに対し10文を人間が評価した結果を教師データとしてパラメタを最適化できることを示した．
よって，十分低いコストでパラメタの最適が可能である．
最後にRIBESとLRscoreについて獲得されたパラメタ，[MATH], [MATH], [MATH]の違いから考察する．
図[REF_parm]に[MATH], [MATH]の分布，図[REF_lr-parm]に[MATH]の分布を示す．
図[REF_parm]より，パラメタ最適化のための訓練事例が10文と少ないことも影響してか，獲得されたパラメタにばらつきがあるが，単一参照翻訳の場合には小さな[MATH]が選択されている割合が多く，4.3節の仮説と一致する．
また，NTCIR-9のように単一参照翻訳かつルールベースシステムの数が多い場合には，[MATH]の場合が非常に多い．
一方，複数参照翻訳がある場合，比較的大きな[MATH]が選択されている．
[MATH]に関しては，複数参照翻訳時には高い値が選ばれている割合が高く，4.3節の仮説と一致するが，単一参照翻訳時には必ずしも低い値が選ばれておらず先の仮説と一致しない．
しかし，人間の評価との間の相関をみる限り，RIBESは従来法よりも比較的高い相関を得ていることから，これは大きな問題ではないと考える．
図[REF_lr-parm]より，LRscoreのNTCIR-9では，[MATH]付近が多く選択されており，語順の相関に対する重みを上げ，語彙の一致（BLEUスコア）に対する重みを下げるようにパラメタを選択しており，RIBESと同様の傾向を示している．
しかし，NTCIR-7では，単一参照翻訳，複数参照翻訳に関わらず0.3から0.6までの値が多く選ばれており，NTCIR-9の場合ほどBLEUスコアに対する重みを下げるようなパラメタが選択されていない．
NTCIR-7ではRIBESの相関が概ねLRscoreの相関を上回っていたが，その原因はこうした語彙の一致に対する重みの違いによると考える．
さらにRIBESは2つのパラメタ[MATH], [MATH]があることに対し，LRscoreは1つのパラメタ[MATH]しかない．
よって，RIBESはLRscoreよりもより柔軟にデータにフィットできる点が双方の手法のパラメタ選択，相関係数の差に影響を与えたとも考える．
5.2節でも述べたが，[MATH], [MATH]，[MATH]とした場合，RIBESもLRscoreも語彙の一致スコアを考慮せず順位相関と短い翻訳に対するペナルティだけを考慮することになり，両者はほぼ一致する．
図[REF_lr-parm]より，LRscoreでは[MATH]が試行の半数近くで選択されているが，図[REF_parm]のNTCIR-9でそうした例がみられるものの全体に占める割合は決して多くはない．
また，LRscoreの語順相関の計算法として[MATH]と[MATH]の双方を試したが，一貫して，[MATH]の方がよい相関を示した．
こうしたことから，基本的にはRIBESとLRscoreは違うものと捉えて差し支えないだろう．
