本節では，IREXワークショップの固有表現抽出タスクの訓練データおよび試験データを用いて，複数の固有表現抽出結果の混合法の実験的評価を行なった結果について述べる．
以下では，訓練データとして用いているCRL固有表現データの一般ドメインのものを[MATH]，評価データとして用いている本試験データのうちの一般ドメインのものを[MATH]と記す．
ただし，いずれも，表[REF_tab:MnNE]の「その他」のものは除いている．
本節では，[REF_subsec:context]節で述べた各モデル単独の性能について述べ，各モデルの出力を比較する．
実験に用いたモデルは，[REF_subsubsec:3gram]節の固定長モデルとしては，5グラムモデル，7グラムモデル，9グラムモデル，および，[REF_subsubsec:vgram]節の可変長モデルである．
また，7グラムモデル，9グラムモデル，および，可変長モデルについては，[REF_subsubsec:ftr34]節の三種類の素性の設定も区別して実験を行なった．
まず，表[REF_tab:indivi_res]に，個々の固有表現抽出モデルを学習するための訓練データセット[MATH]を[MATH]とした場合の，本試験データ[MATH]に対する各モデルのF値([MATH])を示す．
この結果からわかるように，単独のモデルでは，5グラムモデルが最も高い性能を示す．
また，7グラムモデルおよび9グラムモデルは，素性の設定に関わらず，ほぼ同等の性能を示している．
次に，最も性能のよい5グラムモデルの出力と，他のモデルの出力との違いを調べるために，5グラムモデル以外の各モデルの出力について，5グラムモデルの出力との和集合を求め，本試験データ[MATH]の正解データに対する再現率を算出した．
また，5グラムモデル以外の各モデルの誤出力と5グラムモデルの誤出力の間の重複率
を求めた．
これらの結果を表[REF_tab:dif_indivi]に示す．
特に，和の再現率が最も高く，誤出力の重複率が最も低い結果(この場合は，可変長モデル(形態素[MATH]，[MATH]の素性[MATH]全て)との差分)を太字で示す．
表[REF_tab:indivi_res]および表[REF_tab:dif_indivi]の結果から分かるように，7グラムモデルおよび9グラムモデルは，5グラムモデルと比べて出力の和集合の再現率が低く，かつ誤出力の重複率も高いことから，相対的に5グラムモデルと似通ったモデルであると言える．
一方，可変長モデルは，7グラムモデルおよび9グラムモデルと比べて，相対的に5グラムモデルとの類似性が小さいことがわかる．
特に，誤出力の重複率が比較的小さい点が目立つ．
次に，7グラムモデル，9グラムモデル，可変長モデルについて，それぞれ，[REF_subsubsec:ftr34]節の三種類の素性の設定を区別して，合計9種類のモデルを考え，その各々について，5グラムモデルの出力との間で混合を行ない，その性能を評価した．
ただし，個々の固有表現抽出モデルを学習するための訓練データセット[MATH]，複数システムの出力の正誤判別規則を学習するための訓練データセット[MATH]，[REF_subsec:DL]節の([REF_eqn:lbdF])式の頻度閾値[MATH]の設定の組合わせとしては，以下の二通りについて評価を行なった．
なお，複数システムの出力の正誤判別規則を評価するための評価データセット[MATH]については，いずれも，本試験データ[MATH]を用いた．
このうち，設定(a)は，二つの訓練データセット[MATH]と[MATH]について，重複のないデータセットを用いたものに相当する．
ただし，利用可能なデータ量に限界があることから，混合のための正誤判別規則学習の訓練データセット[MATH]のサイズが小さくなっている．
一方，設定(b)の方は，個々の固有表現抽出モデルを訓練データ[MATH]自身に適用したインサイド適用の結果を利用した混合となるが，混合のための正誤判別規則学習の訓練データセット[MATH]のサイズは設定(a)よりもずっと大きい．
評価結果を表[REF_tab:res-comb]に示す．
この結果から分かるように，設定(a)と(b)を比べると，一律に，設定(b)の方が高い性能が得られている．
このことから，正誤判別規則の学習において，たとえ，インサイド適用の結果しか利用できなかったとしても，混合のための正誤判別規則学習の訓練データセット[MATH]のサイズはできるだけ大きい方がよいことがわかる．
特に，設定(b)においては，どの混合結果においても5グラムモデル単独の性能を上回っていることから，混合規則学習のための十分な訓練データがあれば，混合により多少なりとも個々のモデルの出力の性能を向上できることが予想される．
また，設定(b)の場合，7グラムモデル，9グラムモデルといった固定長モデルの出力と5グラムモデルの出力を混合した場合よりも，可変長モデルの出力と5グラムモデルの出力を混合した場合の方が圧倒的に高い性能向上を達成している．
この結果は，表[REF_tab:dif_indivi]の差分の傾向と合致しており，5グラムモデルとの類似性が相対的に小さい可変長モデルの出力との混合において，より高い性能向上が得られている．
また，可変長モデル同士の間で，形態素[MATH]，[MATH]の素性の設定が異なる場合を比較しても，この傾向が成り立っており，5グラムモデルとの類似性が小さいほど混合結果における性能向上は大きい．
これらの結果から，出力の和の再現率が高く，誤出力の重複率が小さくなるような，なるべく類似性の小さい複数の日本語固有表現抽出モデルの出力を用意して，本論文の手法により出力の混合を行なえば，単独のモデルの出力の性能向上が期待できることがわかる．
次に，5グラムモデルの出力と可変長モデルの出力の混合の場合について，固有表現を構成する形態素数ごと，および，固有表現の種類ごとに，単独モデルの出力および混合結果の性能(F値，再現率，適合率)を列挙したものを，それぞれ，表[REF_tab:res-len]，および，表[REF_tab:res-netag]に示す．
なお，表中で，固有表現を構成する形態素数ごと，あるいは，固有表現の種類ごとに，最も高いF値を達成した結果をそれぞれ太字で示す．
表[REF_tab:res-len]から分かるように，どの可変長モデルの出力との混合においても，ほぼ全ての形態素長の固有表現において，5グラムモデル単独の出力の再現率・適合率をともに上回っている．
特に，最高の性能を示している「5グラムモデル+可変長モデル(全て)」の結果においては，5グラムモデルからの性能向上の度合は，形態素長が長くなるほど大きいことから，可変長モデルでしか出力されなかった長い固有表現を，混合によってうまく抽出できていることがわかる．
また，表[REF_tab:res-netag]からは，どの可変長モデルの出力との混合においても，ほぼ全ての種類の固有表現において，5グラムモデルの出力の再現率・適合率とほぼ同等かそれ以上の性能が得られている．
そのうち，TIME，MONEY，PERCENTの三種類については，他の種類と比較して，訓練データ・評価データともその頻度が小さく，また，5グラムモデルにおける性能もかなり高いことから，改善の余地があまりなかったと考えられる．
ただし，その場合でも，混合結果においては，可変長モデルの低い性能の悪影響を受けることなく，5グラムモデルの高い性能が反映されている．
{|c||c|c|c|c|c|c|c|c|} \hline & ORGANI- & PER- & LOCA- & ARTI- & DATE & TIME & MONEY & PER-
& ZATION & SON & TION & FACT & & & & CENT
\hline\hline & 67.74 & 81.82 & 77.04 & 30.43 & 91.49 & 93.20 & 92.86 & 87.18
5グラムモデル& (58.45) & (79.88) & (71.91) & (29.17) & (88.85) & (88.89) & (86.67) & (80.95)
& (80.53) & (83.85) & (82.96) & (31.82) & (94.29) & (97.96) & (100.00) & (94.44)
\hline
& 35.48 & 48.45 & 38.47 & 5.80 & 78.60 & 56.90 & 60.61 & 87.18
可変長モデル& (37.40) & (48.52) & (32.93) & (22.92) & (81.92) & (61.11) & (66.67) & (80.95)
(全て) & (33.75) & (48.38) & (46.26) & (3.32) & (75.53) & (53.23) & (55.56) & (94.44)
\hline
& 65.30 & 78.56 & 72.46 & 26.92 & 88.51 & 77.36 & 80.00 & 89.47
可変長モデル& (57.34) & (77.51) &(66.59) & (29.17) & (88.85) & (75.93) & (80.00) & (80.95)
(語彙+品詞) & (75.82) & (79.64) &(79.48) & (25.00) & (88.17) & (78.85) & (80.00) & (100.00)
\hline
& 63.96 & 76.81 & 72.29 & 25.00 & 86.96 & 54.21 & 73.33 & 81.08
可変長モデル& (54.57) & (78.40) &(68.52) & (20.83) & (84.62) & (53.70) & (73.33) & (71.43)
(語彙) & (77.25) & (75.28) &(76.49) & (31.25) & (89.43) & (54.72) & (73.33) & (93.75)
\hline\hline
5グラムモデル& 72.18 & 84.15 & 79.58 & 38.71 & 92.86 & 93.20 & 92.86 & 87.18
+可変長モデル& (62.88) & (81.66) & (73.61) & (37.50) & (90.00) & (88.89) & (86.67) & (80.95)
(全て) & (84.70) & (86.79) & (86.61) & (40.00) & (95.90) & (97.96) & (100.00) & (94.44)
\hline
5グラムモデル& 70.19 & 83.41 & 78.22 & 35.29 & 92.64 & 92.16 & 92.86 & 87.18
+可変長モデル& (60.66) & (81.07) & (72.15) & (31.25) & (89.62) & (87.04) & (86.67) & (80.95)
(語彙+品詞) & (83.27) & (85.89) & (85.39) & (40.54) & (95.88) & (97.92) & (100.00) & (94.44)
\hline
5グラムモデル& 68.82 & 84.46 & 77.50 & 31.46 & 91.85 & 93.20 & 92.86 & 89.47
+可変長モデル& (59.28) & (82.84) & (72.15) & (29.17) & (88.85) & (88.89) & (86.67) & (80.95)
(語彙) & (81.99) & (86.15) & (83.71) & (34.15) & (95.06) & (97.96) & (100.00) & (100.00)
\hline
5グラムモデルの出力と可変長モデルの出力の混合の場合について，各単独モデルの出力における固有表現の有無，および，混合結果における固有表現の有無と，正解データにおける固有表現の有無のパターンの割合を調査した結果を表[REF_tab:res-syspat]に示す．
表中で，「有」「無」は，それぞれ，単独モデルの出力，混合結果，正解データに固有表現が存在する場合，および，存在しない場合を表す．
例えば，「有」「有」「有」「有」のパターンは，両方の単独モデルの出力にその固有表現が存在し，混合結果においてもその固有表現が出力され，かつ，それが正解データにも存在する正解の固有表現である場合に相当する．
また，割合(%)の計算においては，両方の単独モデルの出力の和における固有表現数を分母，それぞれのパターンに該当する固有表現数を分子として，割合(%)を計算している．
さらに，混合における正誤判別結果が正解であるか否かについては，混合結果および正解データにおける出力の有無が一致する場合は正誤判別が正解，一致しない場合は正誤判別が誤りであるので，「正誤判別率」の欄にそれぞれの率を示した．
形態素[MATH]，[MATH]の素性の設定が異なる場合についてこの結果を比較すると，「5グラムモデル+可変長モデル(全て)」において判別正解率が高くなっているが，これは，「可変長モデル(全て)」の性能が極端に悪く，「可変長モデル(全て)」のみが出力した固有表現の多くが誤りであり，その判別が比較的容易であったからである．
全体では，どの可変長モデルの出力との混合においても，5グラムモデルの出力を覆すことで正解となった場合(「無」「有」「有」「有」および「有」「無」「無」「無」)が数%あり，これが，5グラムモデルからの性能向上に寄与している．
その一方で，判別誤りの内訳をみると，その多くは，誤出力の検出が十分できなかった場合で，ほとんどの場合，少なくとも5グラムモデルはその誤りの固有表現を出力している．
このことから，より効果的な素性を用いる，あるいは，より高性能な学習器を用いるなどして，誤出力検出の精度を向上させることにより，適合率を向上できる余地があることがわかる．
最後に，正誤判別規則学習の学習法の比較のために，最大エントロピー法を用いて正誤判別規則学習を行なった．
まず，最大エントロピー法を適用するために，[REF_subsubsec:event]節の([REF_eqn:segev])式の事象表現[MATH]を，以下のように変換する．
ここで，各事象表現[MATH]は，システムの指標のリストごとに固有表現をまとめたもので，固有表現のリストの事象表現に相当する．
[REF_subsubsec:event]節の場合と同様に，以下の二種類のどちらかに対応し，それぞれ異なったデータ構造を持つ．
そのセグメント中で少なくとも一つのシステムにより出力された固有表現のリストの事象表現．
そのセグメント中で一つも固有表現を出力しなかった一つのシステムに関する情報を表す事象表現．
i)のタイプの事象表現[MATH]は以下のようなデータ構造を持つ．
このデータ構造は，[REF_subsubsec:event]節の([REF_eqn:NEnon-emp])式のデータ構造とほぼ同じであるが，固有表現のリストを表現するために，各素性に相当する情報が全てリスト表現になっている点が異なる．
一方，ii)のタイプの事象表現[MATH]は，[REF_subsubsec:event]節の([REF_eqn:NEemp])式と同じく，以下のデータ構造で表現される．
このような事象表現を用いて正誤判別規則の学習および適用を行なう際には，上述の([REF_eqn:segev-ME])式の事象表現を事象の単位とし，[REF_subsubsec:class]節の場合と同様に，各システム[MATH]ごとにまとめた以下のクラス表現を設定し，各システム[MATH]ごとにクラスの判別を行なうための正誤判別規則の学習および適用を行なう．
その際には，([REF_eqn:NEnon-emp-ME])式の固有表現のリストの事象表現[MATH]の[MATH]，[MATH]，[MATH]，および，([REF_eqn:NEemp-ME])式の固有表現の事象表現[MATH]の[MATH]を，それぞれ文脈[MATH]とし，上式の，各システムごとにまとめた正誤のクラスのリストを付与するための条件付確率モデルを，最大エントロピーモデルとして学習する．
この最大エントロピーモデルは，各システム[MATH]ごとに個別にモデルの学習・適用を行なう．
このような方法で，7グラムモデル，9グラムモデル，可変長モデルについて，それぞれ，[REF_subsubsec:ftr34]節の三種類の素性の設定を区別して，合計9種類のモデルを考え，その各々について，5グラムモデルの出力との間で混合を行ない，その性能を評価した．
ただし，[MATH]とし，評価データセット[MATH]は本試験データ[MATH]とした．
最大エントロピーモデルの素性関数の頻度に下限を設け，評価データセット[MATH]に対して最も高い性能が得られた場合の結果を表[REF_tab:res-comb-ME](a)に示す．
また，決定リスト学習との間で条件を揃えるために，[REF_subsubsec:ftr]節の([REF_eqn:ftr])式の形式の決定リスト学習の素性のうち，上述の実験結果(a)では用いていなかった結合素性を追加して最大エントロピーモデルの学習および適用を行なった結果を表[REF_tab:res-comb-ME](b)に示す．
この場合は，決定リスト学習における各規則の条件付確率[MATH]に下限を設け，評価データセット[MATH]に対して最も高い性能が得られた場合の結果を示している．
表[REF_tab:res-comb-ME]の(a)と(b)の結果を比較すると，結合素性を用いた場合の方が性能が悪くなっている．
また，いくかの結果を除いて，5グラムモデルの性能からの向上はみられるものの，決定リスト学習による可変長モデルの出力との混合の場合のような高い性能向上は達成できていない．
この理由の一つとしては，最大エントロピーモデルと決定リスト学習の間のモデルの形式の違いの影響が挙げられる．
最大エントロピーモデルは，あらゆる素性とクラスとの相関をそれぞれ別個のパラメータとし，モデル内では全パラメータを考慮する形式のモデルになっている．
一方，決定リスト学習は，各々のクラス決定において最も寄与する素性の組合わせのみを考慮し，他の素性は全く考慮しない．
したがって，素性間で寄与する度合の差がわずかしかない場合でも，決定リスト学習では，最も寄与する素性の組合わせのみが考慮されるのに対して，最大エントロピーモデルでは，全素性の寄与を総合的に考慮する．
本論文の正誤判別規則学習による混合の問題では，素性の種類が比較的少なく，特に高頻度な素性は，実際にクラス判別に寄与する度合に関係なく，どの事象においても常に一定の値以上の重みを持つと考えられる．
そのような問題の場合には，最大エントロピーモデルのように全素性の寄与を総合的に考慮する学習法でなく，決定リスト学習のように各々のクラス決定に最も寄与する素性の組合わせのみを考慮する学習法が適していると考えられる．
逆に，正誤判別規則学習による混合の前段階である，形態素への固有表現まとめ上げ状態付与の問題の場合には，[CITE]に示されるように，決定リスト学習よりも最大エントロピーモデルの方が高い性能を示している．
この問題の場合には，素性の種類が比較的多く，極端に高頻度な素性も少ないことから，最大エントロピーモデルのように全素性の寄与を総合的に考慮する学習法が適していると考えられる．
