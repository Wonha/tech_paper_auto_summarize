これまで，機械学習などの分野を中心として，複数のモデル・システムの出力を混合する手法がいくつか提案され，その効果が報告されている．
それらの成果を背景として，近年，統計的手法に基づく自然言語処理においても，複数のモデル・システムの出力を混合する手法を様々な問題に適用することが試みられ，品詞付け[CITE]，名詞句等の句のまとめ上げ[CITE]，構文解析(前置詞句付加含む) [CITE]などへの適用事例が報告されている．
一般に，複数のモデル・システムの出力を混合することの利点は，単一のモデル・システムでは，全ての現象に対して網羅的かつ高精度に対処できない場合でも，個々のモデル・システムがそれぞれ得意とする部分を選択的に組み合わせることで，全体として網羅的かつ高精度なモデル・システムを実現できるという点にある．
本論文では，日本語固有表現抽出の問題に対して，複数のモデルの出力を混合する手法を適用し，個々の固有表現抽出モデルがそれぞれ得意とする部分を選択的に組み合わせることで，全体として網羅的かつ高精度なモデルを実現し，その効果を実験的に検証する．
一般に，日本語固有表現抽出においては，前処理として形態素解析を行ない，形態素解析結果の形態素列に対して，人手で構築されたパターンマッチング規則や統計的学習によって得られた固有表現抽出規則を適用することにより，固有表現が抽出される[CITE]．
特に，統計的学習によって得られた固有表現抽出規則を用いる場合には，形態素解析結果の形態素列に対して，一つもしくは複数の形態素をまとめ上げる処理を行ない，同時にまとめ上げられた形態素列がどの種類の固有表現を構成しているかを同定するという手順が一般的である[CITE]．
このとき，実際のまとめ上げの処理は，現在注目している位置にある形態素およびその周囲の形態素の語彙・品詞・文字種などの属性を考慮しながら，現在位置の形態素が固有表現の一部となりうるかどうかを判定することの組合わせによって行なわれる．
一方，一般に，複数のモデル・システムの出力を混合する過程は，大きく以下の二つの部分に分けて考えることができる．
できるだけ振る舞いの異なる複数のモデル・システムを用意する．
(通常，振る舞いの酷似した複数のモデル・システムを用意しても，複数のモデル・システムの出力を混合することによる精度向上は望めないことが予測される．
)
用意された複数のモデル・システムの出力を混合する方式を選択・設計し，必要であれば学習等を行ない，与えられた現象に対して，用意された複数のモデル・システムの出力を混合することを実現する．
複数の日本語固有表現抽出モデルの出力を混合するにあたっても，これらの([REF_enum:sub1])および([REF_enum:sub2])の過程をどう実現するかを決める必要がある．
本論文では，まず，([REF_enum:sub1])については，統計的学習を用いる固有表現抽出モデルをとりあげ，まとめ上げの処理を行なう際に，現在位置の周囲の形態素を何個まで考慮するかを区別することにより，振る舞いの異なる複数のモデルを学習する．
そして，複数のモデルの振る舞いの違いを調査し，なるべく振る舞いが異なり，かつ，適度な性能を保った複数のモデルの混合を行なう．
特に，これまでの研究事例[CITE]でやられたように，現在位置の形態素がどれだけの長さの固有表現を構成するのかを全く考慮せずに，常に現在位置の形態素の前後二形態素(または一形態素)ずつまでを考慮して学習を行なうモデル(固定長モデル，[REF_subsubsec:3gram]節参照)だけではなく，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるかを考慮して学習を行なうモデル(可変長モデル[CITE]，[REF_subsubsec:vgram]節参照)も用いて複数モデルの出力の混合を行なう．
次に，([REF_enum:sub2])については，重み付多数決やモデルの切り替えなど，これまで自然言語処理の問題によく適用されてきた混合手法を原理的に包含し得る方法として，stacking法[CITE]と呼ばれる方法を用いる．
stacking法とは，何らかの学習を用いた複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段の学習器を用いて，複数のシステム・モデルの出力の混合を行なう規則を学習するという混合法である．
本論文では，具体的には，複数のモデルによる固有表現抽出結果，およびそれぞれの固有表現がどのモデルにより抽出されたか，固有表現のタイプ，固有表現を構成する形態素の数と品詞などを素性として，各固有表現が正しいか誤っているかを判定する第二段の判定規則を学習し，この正誤判定規則を用いることにより複数モデルの出力の混合を行なう．
以下では，まず，[REF_sec:JNE]節で，本論文の実験で使用したIREX(Information Retrieval and Extraction Exercise)ワークショップ[CITE]の日本語固有表現抽出タスクの固有表現データについて簡単に説明する．
次に，[REF_sec:NEchunk]節では，個々の固有表現抽出モデルのベースとなる統計的固有表現抽出モデルについて述べる．
本論文では，統計的固有表現抽出モデルとして，最大エントロピー法を用いた日本語固有表現抽出モデル[CITE]を採用する．
最大エントロピー法は，自然言語処理の様々な問題に適用されその性能が実証されているが，日本語固有表現抽出においても高い性能を示しており，IREXワークショップの日本語固有表現抽出タスクにおいても，統計的手法に基づくシステムの中で最も高い成績を達成している[CITE]．
[REF_sec:combi]節では，複数のモデルの出力の正誤判別を行なう規則を学習することにより，複数モデル出力の混合を行なう手法を説明する．
本論文では，正誤判別規則の学習モデルとしては，決定リスト学習を用い，その性能を実験的に評価する．
以上の手法を用いて，[REF_sec:experi]節で，複数の固有表現抽出結果の混合法の実験的評価を行ない，提案手法の有効性を示す．
[CITE]にも示されているように，固定長モデルに基づく単一の日本語固有表現抽出モデルの場合は，現在位置の形態素の前後二形態素ずつを考慮して学習を行なう場合が最も性能がよい．
また，[REF_sec:experi]節の結果からわかるように，この，常に前後二形態素ずつを考慮する固定長モデルの性能は，可変長モデルに基づく単一のモデルの性能をも上回っている(なお，[CITE]では，最大エントロピー法を学習モデルとして可変長モデルを用いた場合には，常に前後二形態素ずつを考慮する固定長モデルよりも高い性能が得られると報告しているが，この実験結果には誤りがあり，本論文で示す実験結果の方が正しい．
)．
ところが，可変長モデルと，現在位置の形態素の前後二形態素ずつを考慮する固定長モデルとを比較すると，モデルが出力する固有表現の分布がある程度異なっており，実際，これらの二つのモデルの出力を用いて複数モデル出力の混合を行なうと，個々のモデルを上回る性能が達成された．
[REF_sec:experi]節では，これらの実験について詳細に述べ，本論文で提案する混合法が有効であることを示す．
