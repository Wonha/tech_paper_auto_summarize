0	これ 機械学習 分野 中心 複数 モデルシステム 出力 混合 する 手法 いくつ 提案 さ れ 効果 報告 さ れ いる 
0	それら 成果 背景 近年統計的手法 基づく 自然言語処理 複数 モデルシステム 出力 混合 する 手法 様々 問題 適用 する こと 試み られ 品詞付けCITE名詞句等 句 まとめ上げCITE構文解析前置詞句付加 含む CITE 適用事例 報告 さ れ いる 
1	一般 複数 モデルシステム 出力 混合 する こと 利点 単一 モデルシステム 全て 現象 網羅的 精度 対処 でき 場合 個々 モデルシステム それぞれ得意 する 部分 選択的 組み合わせる こと 全体 網羅的 精度 モデルシステム 実現 できる 点 ある 
1	論文 日本語固有表現抽出 問題 複数 モデル 出力 混合 する 手法 適用 し 個々 固有表現抽出モデル それぞれ得意 する 部分 選択的 組み合わせる こと 全体 網羅的 精度 モデル 実現 し 効果 実験的 検証 する 
0	一般 日本語固有表現抽出 処理 形態素解析 行ない 形態素解析結果 形態素列 人手 構築 さ れ パターンマッチング規則 統計的学習 得 られ 固有表現抽出規則 適用 する こと 固有表現 抽出 さ れる 
0	特に 統計的学習 得 られ 固有表現抽出規則 用いる 場合 形態素解析結果 形態素列 一つ 複数 形態素 まとめ 上げる 処理 行ない 同時に まとめ 上げ られ 形態素列 種類 固有表現 構成 し いる 同定 する 手順 一般的 
0	とき 実際 まとめ上げ 処理 現在注目 し いる 位置 ある 形態素 周囲 形態素 語彙品詞文字種 属性 考慮 し 現在位置 形態素 固有表現 一部 なり うる どう 判定 する こと 組合わせ 行なわ れる 
1	一般 複数 モデルシステム 出力 混合 する 過程 大きく 以下 二つ 部分 分け 考える こと できる 
1	できるだけ 振る舞い 異なる 複数 モデルシステム 用意 する 
1	通常振る舞い 酷似 し 複数 モデルシステム 用意 し 複数 モデルシステム 出力 混合 する こと 精度向上 望め こと 予測 さ れる 
1	用意 さ れ 複数 モデルシステム 出力 混合 する 方式 選択設計 し 必要 学習等 行ない 与え られ 現象 用意 さ れ 複数 モデルシステム 出力 混合 する こと 実現 する 
0	複数 日本語固有表現抽出モデル 出力 混合 する これら REF_enumsub1 REF_enumsub2 過程 どう 実現 する 決める 必要 ある 
0	論文 まず REF_enumsub1 統計的学習 用いる 固有表現抽出モデル とりあげ まとめ上げ 処理 行なう 際 現在位置 周囲 形態素 何個 考慮 する 区別 する こと 振る舞い 異なる 複数 モデル 学習 する 
1	複数 モデル 振る舞い 違い 調査 し なるべく 振る舞い 異なり 適度 性能 保っ 複数 モデル 混合 行なう 
1	特に これ 研究事例CITE やら れ よう 現在位置 形態素 どれ 長 さ 固有表現 構成 する の 全く 考慮 せ 常に 現在位置 形態素 前後二形態素 一形態素 考慮 し 学習 行なう モデル固定長モデルREF_subsubsec3gram節参照 現在位置 形態素 いくつ 形態素 構成 さ れる 固有表現 一部 考慮 し 学習 行なう モデル可変長モデルCITEREF_subsubsecvgram節参照 用い 複数モデル 出力 混合 行なう 
0	REF_enumsub2 重み付多数決 モデル 切り替え これ 自然言語処理 問題 よく 適用 さ れ き 混合手法 原理的 包含 し 得る 方法 stacking法CITE 呼ば れる 方法 用いる 
1	stacking法 学習 用い 複数 システムモデル 出力 訓練データそのもの 入力 する 二段 学習器 用い 複数 システムモデル 出力 混合 行なう 規則 学習 する 混合法 
0	論文 具体的 複数 モデル 固有表現抽出結果 それぞれ 固有表現 モデル 抽出 さ れ 固有表現 タイプ固有表現 構成 する 形態素 数 品詞 素性 固有表現 正しい 誤っ いる 判定 する 二段 判定規則 学習 し 正誤判定規則 用いる こと 複数モデル 出力 混合 行なう 
0	以下 まず REF_secJNE節 論文 実験 使用 し IREXInformationRetrievalandExtractionExerciseワークショップCITE 日本語固有表現抽出タスク 固有表現データ 簡単 説明 する 
0	REF_secNEchunk節 個々 固有表現抽出モデル ベース なる 統計的固有表現抽出モデル 述べる 
0	論文 統計的固有表現抽出モデル 最大エントロピー法 用い 日本語固有表現抽出モデルCITE 採用 する 
0	最大エントロピー法 自然言語処理 様々 問題 適用 さ れ 性能 実証 さ れ いる 日本語固有表現抽出 高い 性能 示し おり IREXワークショップ 日本語固有表現抽出タスク 統計的手法 基づく システム 中 最も 高い 成績 達成 し いる 
0	REF_seccombi節 複数 モデル 出力 正誤判別 行なう 規則 学習 する こと 複数モデル出力 混合 行なう 手法 説明 する 
0	論文 正誤判別規則 学習モデル 決定リスト学習 用い 性能 実験的 評価 する 
0	以上 手法 用い REF_secexperi節 複数 固有表現抽出結果 混合法 実験的評価 行ない 提案手法 有効性 示す 
0	CITE 示さ れ いる よう 固定長モデル 基づく 単一 日本語固有表現抽出モデル 場合 現在位置 形態素 前後二形態素 考慮 し 学習 行なう 場合 最も 性能 よい 
0	REF_secexperi節 結果 わかる よう 常に 前後二形態素 考慮 する 固定長モデル 性能 可変長モデル 基づく 単一 モデル 性能 上回っ いる CITE 最大エントロピー法 学習モデル 可変長モデル 用い 場合 常に 前後二形態素 考慮 する 固定長モデル 高い 性能 得 られる 報告 し いる 実験結果 誤り あり 論文 示す 実験結果 方 正しい 
0	可変長モデル 現在位置 形態素 前後二形態素 考慮 する 固定長モデル 比較 する モデル 出力 する 固有表現 分布 ある程度 異なっ おり 実際 これら 二つ モデル 出力 用い 複数モデル出力 混合 行なう 個々 モデル 上回る 性能 達成 さ れ 
