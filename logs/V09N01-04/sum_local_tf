================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[5614] 本論文では，最大エントロピー法に基づく統計的学習による固有表現抽出モデルにおいて，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるかを考慮して学習を行なう可変(文脈)長モデルと，常に現在位置の形態素の前後数形態素ずつまでを考慮して学習を行なう固定(文脈)長モデルとの間のモデルの挙動の違いに注目する．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[6299] 本論文では，具体的には，複数のモデルによる固有表現抽出結果，およびそれぞれの固有表現がどのモデルにより抽出されたか，固有表現のタイプ，固有表現を構成する形態素の数と品詞などを素性として，各固有表現が正しいか誤っているかを判定する第二段の判定規則を学習し，この正誤判定規則を用いることにより複数モデルの出力の混合を行なう．

================================================================
[section type  : proposed_method]
[section title : 日本語固有表現抽出]
================================================================
[3352] また，最近では，日本語においても，MET (Multilingual Entity Task,例えば，MET-1 [CITE]，MET-2 [CITE])やIREXワークショップ[CITE]などのコンテストにおいて，固有表現抽出が課題の一つに取り上げられている．
-----------------------------------------------------
  [subsection title : IREXワークショップの固有表現抽出タスク]
-----------------------------------------------------
  [4060] 表[REF_tab:irex_tag]には，主催者側から提供された訓練データの主要部分を占めるCRL(郵政省通信総合研究所---現，独立行政法人通信総合研究所)固有表現データ(毎日新聞1,174記事の固有表現をタグ付け)，および本試験データのうちの一般ドメインのもの(毎日新聞71記事の固有表現をタグ付け)について，八種類の固有表現数を調査した結果を示す．
-----------------------------------------------------
  [subsection title : 形態素と固有表現の対応パターン]
-----------------------------------------------------
  [4465] また，そのうち，一つの固有表現が複数の形態素から構成されている場合は90%近く(7175/ (7175+1022) = 87.5%)を占めており，これらの固有表現については，各固有表現の区切り位置はいずれかの形態素の区切り位置と一致している，すなわち，固有表現の開始位置は，先頭の構成要素となる形態素の開始位置と，また，固有表現の終了位置は，末尾の構成要素となる形態素の終了位置と，それぞれ一致する．

================================================================
[section type  : proposed_method]
[section title : 最大エントロピー法を用いた固有表現抽出]
================================================================
[3956] 本節では，まず，ベースモデルとなる，最大エントロピー法を用いた日本語固有表現抽出の手法[CITE]を定式化する．
-----------------------------------------------------
  [subsection title : 問題設定]
-----------------------------------------------------
  [5440] なお，通常，学習された確率モデルを適用して，形態素に固有表現まとめ上げ状態および固有表現タイプを付与することにより，固有表現の抽出を行なう場合は，一文全体で，固有表現まとめ上げ状態および固有表現タイプの確率を最大とする固有表現の組合わせを求める必要がある．
-----------------------------------------------------
  [subsection title : 最大エントロピー法]
-----------------------------------------------------
  [4262] 次に，実際に確率モデル学習を行う際には，活性化された素性集合[MATH]中の各素性[MATH]について，学習すべき確率分布[MATH]による素性[MATH]の期待値(左辺)と経験的確率分布[MATH]による素性[MATH]の期待値(右辺)が等しいとする以下の制約等式を課す．
-----------------------------------------------------
  [subsection title : 固有表現まとめ上げ状態の表現法]
-----------------------------------------------------
  [3851] 本論文では，固有表現まとめ上げの際のまとめ上げ状態の表現法として，日本語固有表現抽出の既存の手法[CITE]において用いられた\sekine_encoding法を採用する．
-----------------------------------------------------
  [subsection title : 各形態素の素性]
-----------------------------------------------------
  [2887] 語彙---訓練コーパス中で，固有表現の位置および周囲二形態素以内に5回以上出現した2,052語彙．
-----------------------------------------------------
  [subsection title : 周囲の形態素のモデル化]
-----------------------------------------------------
  [5556] 一方，もう一つのモデルは，学習時において，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるか(式([REF_eqn:NE-len])参照)を考慮して学習を行なうモデルで，これを可変長モデルと呼ぶことにする[CITE]．

================================================================
[section type  : proposed_method]
[section title : 正誤判別規則学習を用いた複数システム出力の混合]
================================================================
[0] 
-----------------------------------------------------
  [subsection title : 訓練・評価データセット]
-----------------------------------------------------
  [3321] [MATH]:個々の固有表現抽出モデルを学習するための訓練データセット．
-----------------------------------------------------
  [subsection title : 訓練および評価手続きの概要]
-----------------------------------------------------
  [3998] 訓練データセット(テキスト)[MATH]中での各固有表現の出現位置の情報を用いて，抽出結果の固有表現リスト[MATH] [MATH]を，複数システム間[MATH]で整列し，訓練データセット[MATH]の事象表現[MATH]を作成する．
-----------------------------------------------------
  [subsection title : データ構造]
-----------------------------------------------------
  [4940] この正誤判別規則の学習の際には，式([REF_eqn:segev])で定義されるセグメントの事象表現[MATH]から，次節で説明する素性を抽出し，この素性を用いて各システム[MATH]ごとのクラス[MATH]を判別する規則を学習する([REF_subsec:DL]節)．
-----------------------------------------------------
  [subsection title : 学習アルゴリズム]
-----------------------------------------------------
  [3706] 本論文では，各規則の優先度として，素性[MATH]の条件のもとでの，システム[MATH]のクラス[MATH]の条件付確率[MATH]を用い，この条件付確率順に決定リストを構成する．
-----------------------------------------------------
  [subsection title : 正誤判別規則の適用による複数システム出力の混合]
-----------------------------------------------------
  [4455] という二つの制約のもとで，全システムについての条件付確率[MATH]の積を最大化するクラス割当ての組合わせが求められ，これが，セグメント中で各システム[MATH] [MATH]が出力した固有表現への正誤クラスの判別結果[MATH]となる．

================================================================
[section type  : experiment_result]
[section title : 実験および評価]
================================================================
[3859] 本節では，IREXワークショップの固有表現抽出タスクの訓練データおよび試験データを用いて，複数の固有表現抽出結果の混合法の実験的評価を行なった結果について述べる．
-----------------------------------------------------
  [subsection title : 各モデル単独の出力の比較]
-----------------------------------------------------
  [4521] まず，表[REF_tab:indivi_res]に，個々の固有表現抽出モデルを学習するための訓練データセット[MATH]を[MATH]とした場合の，本試験データ[MATH]に対する各モデルのF値([MATH])を示す．
-----------------------------------------------------
  [subsection title : 複数システムの出力の混合の性能評価]
-----------------------------------------------------
  [5868] 形態素[MATH]，[MATH]の素性の設定が異なる場合についてこの結果を比較すると，「5グラムモデル+可変長モデル(全て)」において判別正解率が高くなっているが，これは，「可変長モデル(全て)」の性能が極端に悪く，「可変長モデル(全て)」のみが出力した固有表現の多くが誤りであり，その判別が比較的容易であったからである．
-----------------------------------------------------
  [subsection title : 最大エントロピー法による正誤判別規則学習]
-----------------------------------------------------
  [5103] その際には，([REF_eqn:NEnon-emp-ME])式の固有表現のリストの事象表現[MATH]の[MATH]，[MATH]，[MATH]，および，([REF_eqn:NEemp-ME])式の固有表現の事象表現[MATH]の[MATH]を，それぞれ文脈[MATH]とし，上式の，各システムごとにまとめた正誤のクラスのリストを付与するための条件付確率モデルを，最大エントロピーモデルとして学習する．

================================================================
[section type  : related_study]
[section title : 関連研究]
================================================================
[0] 
-----------------------------------------------------
  [subsection title : 複数モデルの出力の混合法]
-----------------------------------------------------
  [4691] i)と似ているが，学習モデルは単一のものを用い，データの表現法(具体的には，まとめ上げ問題におけるまとめ上げ状態の表現法)として複数のものを設定することにより，複数の出力を得る[CITE]．
-----------------------------------------------------
  [subsection title : Stacking法]
-----------------------------------------------------
  [4968] これらの事例と比較すると，本論文の日本語固有表現抽出の問題においては，第一段の学習器は，個々の形態素に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を行なっているのに対して，第二段の学習器は，個々のシステムの固有表現抽出結果，および，第一段の学習器の入力となった素性(の一部)を入力として，個々のシステムの固有表現抽出結果の正誤を判定するための分類器の学習を行なっている．
-----------------------------------------------------
  [subsection title : 統計的手法に基づく日本語固有表現抽出]
-----------------------------------------------------
  [5310] その他には，[CITE]で報告されているように，解析の方向を文頭から文末と文末から文頭の二通り設定し，解析済の固有表現のタグを素性として利用する方法により，振る舞いの異なった出力が得られる可能性があり，stacking法でその出力を利用することで，精度の向上が期待できる可能性がある．

================================================================
[section type  : conclusion]
[section title : おわりに]
================================================================
[5653] まず，最大エントロピー法に基づく統計的学習による固有表現抽出モデルにおいて，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるかを考慮して学習を行なう可変長モデルと，常に現在位置の形態素の前後数形態素ずつまでを考慮して学習を行なう固定長モデルとの間のモデルの挙動の違いに注目し，なるべく挙動が異なり，かつ，適度な性能を保った複数のモデルの出力の混合を行なった．

