================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:2, score:410] 本論文では，最大エントロピー法に基づく統計的学習による固有表現抽出モデルにおいて，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるかを考慮して学習を行なう可変(文脈)長モデルと，常に現在位置の形態素の前後数形態素ずつまでを考慮して学習を行なう固定(文脈)長モデルとの間のモデルの挙動の違いに注目する．
[i:4, score:424] 次に，混合の方式としては，複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段目の学習器を用いて，複数のシステム・モデルの出力の混合を行なう規則を学習するという混合法(stacking法)を採用する．
[i:5, score:595] 第二段目の学習器として決定リスト学習を用いて，固定長モデルおよび可変長モデルの出力を混合する実験を行なった結果，最大エントロピー法に基づく固有表現抽出モデルにおいてこれまで得られていた最高の性能を上回る性能が達成された．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[i:21, score:590] 特に，これまでの研究事例[CITE]でやられたように，現在位置の形態素がどれだけの長さの固有表現を構成するのかを全く考慮せずに，常に現在位置の形態素の前後二形態素(または一形態素)ずつまでを考慮して学習を行なうモデル(固定長モデル，[REF_subsubsec:3gram]節参照)だけではなく，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるかを考慮して学習を行なうモデル(可変長モデル[CITE]，[REF_subsubsec:vgram]節参照)も用いて複数モデルの出力の混合を行なう．
[i:24, score:595] 本論文では，具体的には，複数のモデルによる固有表現抽出結果，およびそれぞれの固有表現がどのモデルにより抽出されたか，固有表現のタイプ，固有表現を構成する形態素の数と品詞などを素性として，各固有表現が正しいか誤っているかを判定する第二段の判定規則を学習し，この正誤判定規則を用いることにより複数モデルの出力の混合を行なう．
[i:35, score:527] ところが，可変長モデルと，現在位置の形態素の前後二形態素ずつを考慮する固定長モデルとを比較すると，モデルが出力する固有表現の分布がある程度異なっており，実際，これらの二つのモデルの出力を用いて複数モデル出力の混合を行なうと，個々のモデルを上回る性能が達成された．

================================================================
[section type  : proposed_method]
[section title : 日本語固有表現抽出]
================================================================
[i:37, score:154] 固有表現抽出は，情報検索・抽出，機械翻訳，自然言語理解など自然言語処理の応用的局面における基礎技術として重要な技術の一つである．
[i:38, score:172] 英語においては，特に米国において，MUC(Message Understanding Conference，例えば，MUC-7 [CITE])コンテストにおける課題の一つとして固有表現抽出がとりあげられ，集中的に研究が行なわれてきた．
[i:39, score:183] また，最近では，日本語においても，MET (Multilingual Entity Task,例えば，MET-1 [CITE]，MET-2 [CITE])やIREXワークショップ[CITE]などのコンテストにおいて，固有表現抽出が課題の一つに取り上げられている．
-----------------------------------------------------
  [subsection title : IREXワークショップの固有表現抽出タスク]
-----------------------------------------------------
  [i:lead, 191] IREXワークショップの固有表現抽出タスクでは，表[REF_tab:irex_tag]に示す八種類の固有表現の抽出が課題とされた
.....
  [i:40, score:191] IREXワークショップの固有表現抽出タスクでは，表[REF_tab:irex_tag]に示す八種類の固有表現の抽出が課題とされた
  [i:41, score:1] [CITE]．
  [i:42, score:251] 表[REF_tab:irex_tag]には，主催者側から提供された訓練データの主要部分を占めるCRL(郵政省通信総合研究所---現，独立行政法人通信総合研究所)固有表現データ(毎日新聞1,174記事の固有表現をタグ付け)，および本試験データのうちの一般ドメインのもの(毎日新聞71記事の固有表現をタグ付け)について，八種類の固有表現数を調査した結果を示す．
-----------------------------------------------------
  [subsection title : 形態素と固有表現の対応パターン]
-----------------------------------------------------
  [i:lead, 262] 次に，上記のIREXワークショップの固有表現抽出タスクの訓練データを形態素解析システムbreakfast[CITE] で形態素解析し，その結果の形態素と固有表現の対応パターンを調査した結果を表[REF_tab:MnNE]に示す．
.....
  [i:43, score:262] 次に，上記のIREXワークショップの固有表現抽出タスクの訓練データを形態素解析システムbreakfast[CITE] で形態素解析し，その結果の形態素と固有表現の対応パターンを調査した結果を表[REF_tab:MnNE]に示す．
  [i:45, score:213] また，そのうち，一つの固有表現が複数の形態素から構成されている場合は90%近く(7175/ (7175+1022) = 87.5%)を占めており，これらの固有表現については，各固有表現の区切り位置はいずれかの形態素の区切り位置と一致している，すなわち，固有表現の開始位置は，先頭の構成要素となる形態素の開始位置と，また，固有表現の終了位置は，末尾の構成要素となる形態素の終了位置と，それぞれ一致する．
  [i:47, score:195] また，表[REF_tab:MnNE]の「その他」の場合の多くは，一つ以上の固有表現が一つの形態素の一部となる場合である．

================================================================
[section type  : proposed_method]
[section title : 最大エントロピー法を用いた固有表現抽出]
================================================================
[i:50, score:245] 本節では，まず，ベースモデルとなる，最大エントロピー法を用いた日本語固有表現抽出の手法[CITE]を定式化する．
-----------------------------------------------------
  [subsection title : 問題設定]
-----------------------------------------------------
  [i:lead, 195] ここでの固有表現抽出の問題は，固有表現まとめ上げおよび固有表現タイプ分類の問題ととらえることができる．
.....
  [i:53, score:277] ここで，現在の位置が形態素[MATH]のところであるとすると，日本語固有表現まとめ上げおよび固有表現タイプ分類の問題とは，この現在位置の形態素[MATH]に，まとめ上げ状態および固有表現タイプ(詳細は[REF_subsec:tagck]節で述べる)を付与することである．
  [i:58, score:521] }また，次節で述べる最大エントロピー法を用いて固有表現抽出モデルを学習する際には，現在位置および周囲の形態素の素性([REF_subsec:ftr]節)を条件として，現在位置の形態素に固有表現まとめ上げ状態およびタイプ([REF_subsec:tagck]節)をクラスとして付与するための条件付確率モデルを最大エントロピー法により学習する．
  [i:59, score:381] なお，通常，学習された確率モデルを適用して，形態素に固有表現まとめ上げ状態および固有表現タイプを付与することにより，固有表現の抽出を行なう場合は，一文全体で，固有表現まとめ上げ状態および固有表現タイプの確率を最大とする固有表現の組合わせを求める必要がある．
-----------------------------------------------------
  [subsection title : 最大エントロピー法]
-----------------------------------------------------
  [i:lead, 157] 最大エントロピー法は，文脈を規定する制約を素性として与え，与えられた素性のもとでエントロピーを最大化するという条件によって求められる確率モデルである．
.....
  [i:63, score:164] ここでは，与えられた訓練集合から，文脈[MATH]においてクラス[MATH]を出力するプロセスの確率的振舞い，すなわち条件付確率分布[MATH]を最大エントロピー法に基づいて推定する方法の概略を説明する．
  [i:67, score:211] また，一般に確率モデル学習の際には，大量の素性からなる素性の候補集合[MATH]から，活性化された素性の部分集合[MATH]が選択され，これらによって事象[MATH]および確率分布[MATH]が記述される．
  [i:68, score:175] 次に，実際に確率モデル学習を行う際には，活性化された素性集合[MATH]中の各素性[MATH]について，学習すべき確率分布[MATH]による素性[MATH]の期待値(左辺)と経験的確率分布[MATH]による素性[MATH]の期待値(右辺)が等しいとする以下の制約等式を課す．
-----------------------------------------------------
  [subsection title : 固有表現まとめ上げ状態の表現法]
-----------------------------------------------------
  [i:lead, 213] 本論文では，固有表現まとめ上げの際のまとめ上げ状態の表現法として，日本語固有表現抽出の既存の手法[CITE]において用いられた\sekine_encoding法を採用する．
.....
  [i:73, score:213] 本論文では，固有表現まとめ上げの際のまとめ上げ状態の表現法として，日本語固有表現抽出の既存の手法[CITE]において用いられた\sekine_encoding法を採用する．
  [i:78, score:196] --現在位置の形態素は単独で一つの固有表現を構成する．
  [i:82, score:214] この方法により日本語固有表現のまとめ上げを行なう様子を表[REF_tab:NEcode]に示す．
-----------------------------------------------------
  [subsection title : 各形態素の素性]
-----------------------------------------------------
  [i:lead, 72] 各形態素の素性としては，以下の三種類のものを用いる．
.....
  [i:83, score:72] 各形態素の素性としては，以下の三種類のものを用いる．
  [i:84, score:230] 語彙---訓練コーパス中で，固有表現の位置および周囲二形態素以内に5回以上出現した2,052語彙．
  [i:86, score:45] 文字種---平仮名・片仮名・漢字・数字・英語アルファベット・記号，およびそれらの組合わせ．
-----------------------------------------------------
  [subsection title : 周囲の形態素のモデル化]
-----------------------------------------------------
  [i:lead, 312] 次に，本論文では，現在位置の形態素に対して固有表現のまとめ上げ状態を付与する際に，周囲のどれだけの形態素を考慮するか，つまり周囲の形態素をどのようにモデル化するかについて，以下の二種類のモデルを用いる．
.....
  [i:96, score:413] 一方，もう一つのモデルは，学習時において，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるか(式([REF_eqn:NE-len])参照)を考慮して学習を行なうモデルで，これを可変長モデルと呼ぶことにする[CITE]．
  [i:97, score:397] 学習時には，現在位置の形態素が固有表現を構成しない場合には，5グラムモデルと同じく，現在位置およびその左右の二個ずつの形態素を考慮して学習を行なう．
  [i:106, score:424] モデルの適用時には，現在位置の形態素がどのような固有表現を構成するかという情報が利用できないので，固定長の9グラムモデルの場合と同様に，現在位置の形態素，および，左右四形態素ずつの素性を考慮してモデルの適用を行なう．

================================================================
[section type  : proposed_method]
[section title : 正誤判別規則学習を用いた複数システム出力の混合]
================================================================
[i:112, score:0] 
-----------------------------------------------------
  [subsection title : 訓練・評価データセット]
-----------------------------------------------------
  [i:lead, 224] 本論文の複数システム出力の混合法では，以下の三種類の訓練・評価データセットを用いる．
.....
  [i:113, score:224] 本論文の複数システム出力の混合法では，以下の三種類の訓練・評価データセットを用いる．
  [i:114, score:301] [MATH]:個々の固有表現抽出モデルを学習するための訓練データセット．
  [i:115, score:253] [MATH]:複数システムの出力の正誤判別規則を学習するための訓練データセット．
-----------------------------------------------------
  [subsection title : 訓練および評価手続きの概要]
-----------------------------------------------------
  [i:lead, 269] まず，以下に，訓練データセット[MATH]および[MATH]を用いて，複数システムの出力の正誤判別規則を学習するため手続きの概要を示す．
.....
  [i:119, score:309] 個々の固有表現抽出モデル[MATH] [MATH]を，それぞれ，訓練データセット[MATH]に適用し，各固有表現抽出モデル[MATH]につき，抽出結果の固有表現リスト[MATH]をそれぞれ一つずつ得る．
  [i:120, score:307] 訓練データセット(テキスト)[MATH]中での各固有表現の出現位置の情報を用いて，抽出結果の固有表現リスト[MATH] [MATH]を，複数システム間[MATH]で整列し，訓練データセット[MATH]の事象表現[MATH]を作成する．
  [i:121, score:310] 訓練データセット[MATH]の事象表現[MATH]を教師あり訓練データとして，複数システムの出力の正誤判別規則[MATH]を学習する．
-----------------------------------------------------
  [subsection title : データ構造]
-----------------------------------------------------
  [i:lead, 370] 本節では，訓練データセット[MATH]の事象表現[MATH]，あるいは，評価データセット[MATH]の事象表現[MATH]のデータ構造を説明し，複数システムの出力の正誤判別規則を学習する際の素性・クラスについて述べる．
.....
  [i:139, score:406] 複数システムの出力の正誤判別を行なう規則は，式([REF_eqn:segev])で定義されるセグメントの事象表現[MATH]を一つの事象単位として，学習および適用が行なわれる．
  [i:145, score:409] この正誤判別規則の学習の際には，式([REF_eqn:segev])で定義されるセグメントの事象表現[MATH]から，次節で説明する素性を抽出し，この素性を用いて各システム[MATH]ごとのクラス[MATH]を判別する規則を学習する([REF_subsec:DL]節)．
  [i:150, score:402] 正誤判別規則の学習時には，式([REF_eqn:segev])で定義されるセグメントの事象表現[MATH]から，式([REF_eqn:ftr])の形式のあらゆる可能な素性[MATH]のうち，以下の制約を含むいくつかの制約を満たすものだけが抽出される．
-----------------------------------------------------
  [subsection title : 学習アルゴリズム]
-----------------------------------------------------
  [i:lead, 67] 教師あり学習法としては，決定リスト学習を用いる．
.....
  [i:179, score:154] 本論文では，各規則の優先度として，素性[MATH]の条件のもとでの，システム[MATH]のクラス[MATH]の条件付確率[MATH]を用い，この条件付確率順に決定リストを構成する．
  [i:180, score:117] ただし，決定リストを構成する際には，素性[MATH]の条件のもとでの，システム[MATH]のクラス[MATH]の頻度[MATH]に下限[MATH]を設け，
  [i:182, score:210] 頻度の下限[MATH]は，各規則の条件付確率[MATH]を推定する際に使用したデータセット以外のデータセットに対して，正誤判別規則の性能を最大にする値を用いる．
-----------------------------------------------------
  [subsection title : 正誤判別規則の適用による複数システム出力の混合]
-----------------------------------------------------
  [i:lead, 476] 学習された正誤判別規則を適用することにより複数システムの出力の混合を行なう場合は，式([REF_eqn:segev])と同じ形式のセグメントの事象表現
.....
  [i:183, score:476] 学習された正誤判別規則を適用することにより複数システムの出力の混合を行なう場合は，式([REF_eqn:segev])と同じ形式のセグメントの事象表現
  [i:186, score:264] 複数のシステムによって出力された単一の固有表現は，同一の正誤クラスを持つ．
  [i:188, score:414] という二つの制約のもとで，全システムについての条件付確率[MATH]の積を最大化するクラス割当ての組合わせが求められ，これが，セグメント中で各システム[MATH] [MATH]が出力した固有表現への正誤クラスの判別結果[MATH]となる．

================================================================
[section type  : experiment_result]
[section title : 実験および評価]
================================================================
[i:189, score:323] 本節では，IREXワークショップの固有表現抽出タスクの訓練データおよび試験データを用いて，複数の固有表現抽出結果の混合法の実験的評価を行なった結果について述べる．
[i:190, score:190] 以下では，訓練データとして用いているCRL固有表現データの一般ドメインのものを[MATH]，評価データとして用いている本試験データのうちの一般ドメインのものを[MATH]と記す．
[i:191, score:35] ただし，いずれも，表[REF_tab:MnNE]の「その他」のものは除いている．
-----------------------------------------------------
  [subsection title : 各モデル単独の出力の比較]
-----------------------------------------------------
  [i:lead, 144] 本節では，[REF_subsec:context]節で述べた各モデル単独の性能について述べ，各モデルの出力を比較する．
.....
  [i:194, score:323] また，7グラムモデル，9グラムモデル，および，可変長モデルについては，[REF_subsubsec:ftr34]節の三種類の素性の設定も区別して実験を行なった．
  [i:195, score:367] まず，表[REF_tab:indivi_res]に，個々の固有表現抽出モデルを学習するための訓練データセット[MATH]を[MATH]とした場合の，本試験データ[MATH]に対する各モデルのF値([MATH])を示す．
  [i:203, score:264] 表[REF_tab:indivi_res]および表[REF_tab:dif_indivi]の結果から分かるように，7グラムモデルおよび9グラムモデルは，5グラムモデルと比べて出力の和集合の再現率が低く，かつ誤出力の重複率も高いことから，相対的に5グラムモデルと似通ったモデルであると言える．
-----------------------------------------------------
  [subsection title : 複数システムの出力の混合の性能評価]
-----------------------------------------------------
  [i:lead, 463] 次に，7グラムモデル，9グラムモデル，可変長モデルについて，それぞれ，[REF_subsubsec:ftr34]節の三種類の素性の設定を区別して，合計9種類のモデルを考え，その各々について，5グラムモデルの出力との間で混合を行ない，その性能を評価した．
.....
  [i:207, score:565] ただし，個々の固有表現抽出モデルを学習するための訓練データセット[MATH]，複数システムの出力の正誤判別規則を学習するための訓練データセット[MATH]，[REF_subsec:DL]節の([REF_eqn:lbdF])式の頻度閾値[MATH]の設定の組合わせとしては，以下の二通りについて評価を行なった．
  [i:220, score:572] 次に，5グラムモデルの出力と可変長モデルの出力の混合の場合について，固有表現を構成する形態素数ごと，および，固有表現の種類ごとに，単独モデルの出力および混合結果の性能(F値，再現率，適合率)を列挙したものを，それぞれ，表[REF_tab:res-len]，および，表[REF_tab:res-netag]に示す．
  [i:222, score:539] 表[REF_tab:res-len]から分かるように，どの可変長モデルの出力との混合においても，ほぼ全ての形態素長の固有表現において，5グラムモデル単独の出力の再現率・適合率をともに上回っている．
-----------------------------------------------------
  [subsection title : 最大エントロピー法による正誤判別規則学習]
-----------------------------------------------------
  [i:lead, 190] 最後に，正誤判別規則学習の学習法の比較のために，最大エントロピー法を用いて正誤判別規則学習を行なった．
.....
  [i:246, score:524] その際には，([REF_eqn:NEnon-emp-ME])式の固有表現のリストの事象表現[MATH]の[MATH]，[MATH]，[MATH]，および，([REF_eqn:NEemp-ME])式の固有表現の事象表現[MATH]の[MATH]を，それぞれ文脈[MATH]とし，上式の，各システムごとにまとめた正誤のクラスのリストを付与するための条件付確率モデルを，最大エントロピーモデルとして学習する．
  [i:248, score:462] このような方法で，7グラムモデル，9グラムモデル，可変長モデルについて，それぞれ，[REF_subsubsec:ftr34]節の三種類の素性の設定を区別して，合計9種類のモデルを考え，その各々について，5グラムモデルの出力との間で混合を行ない，その性能を評価した．
  [i:261, score:567] 逆に，正誤判別規則学習による混合の前段階である，形態素への固有表現まとめ上げ状態付与の問題の場合には，[CITE]に示されるように，決定リスト学習よりも最大エントロピーモデルの方が高い性能を示している．

================================================================
[section type  : related_study]
[section title : 関連研究]
================================================================
[i:263, score:0] 
-----------------------------------------------------
  [subsection title : 複数モデルの出力の混合法]
-----------------------------------------------------
  [i:lead, 226] [REF_sec:intro]節で述べたように，一般に，複数のモデル・システムの出力を混合する過程は，大きく以下の二つの部分に分けて考えることができる．
.....
  [i:272, score:331] これに対して，本論文においては，振る舞いの異なる複数のモデルを得る方法として，学習モデルは単一のものを用い，固有表現まとめ上げの際に考慮する周囲の形態素の個数を区別することで複数のモデルを得るという方法をとった．
  [i:277, score:429] 原理的に，上記のi)およびii)を包含し得る方法として，複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段の学習器を用いて，複数のシステム・モデルの出力の混合を行なうstacking法[CITE]，あるいは，それと同等の方法に基づくもの[CITE]．
  [i:281, score:272] また，通常のbagging法やboosting法を適用する場合でも，第一段としては何らかの学習モデルを採用する必要があるが，本論文の混合法にはそのような制約はないので，原理的には，第一段として任意のシステムを採用することが可能である．
-----------------------------------------------------
  [subsection title : Stacking法]
-----------------------------------------------------
  [i:lead, 190] 次に，本節では，stacking法についての関連研究，および，stacking法と同等の手法を自然言語処理におけるシステム混合の問題に適用している研究事例について述べる．
.....
  [i:288, score:406] 一方，自然言語処理におけるシステム混合の問題にstacking法と同等の手法を適用している研究事例としては，英語品詞付けにおいて，最大エントロピー法，変形に基づく学習，トライグラムモデル，メモリベース学習を第一段の学習器とし，決定木学習，メモリベース学習法などを第二段の学習器としてstackingを行なうもの[CITE]，英語名詞句まとめ上げにおいて，七種類の学習器を第一段に用い，決定木学習，メモリベース学習法を第二段の学習器としてstackingを行なうもの[CITE]などがある．
  [i:290, score:444] また，[CITE]は，英語の固有表現抽出において，単一の最大エントロピーモデルの素性として，通常の固有表現まとめ上げ・タイプ分類に用いる素性とあわせて，他の既存のシステムの出力を素性として用いて，個々の単語に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を行なっている．
  [i:293, score:483] これらの事例と比較すると，本論文の日本語固有表現抽出の問題においては，第一段の学習器は，個々の形態素に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を行なっているのに対して，第二段の学習器は，個々のシステムの固有表現抽出結果，および，第一段の学習器の入力となった素性(の一部)を入力として，個々のシステムの固有表現抽出結果の正誤を判定するための分類器の学習を行なっている．
-----------------------------------------------------
  [subsection title : 統計的手法に基づく日本語固有表現抽出]
-----------------------------------------------------
  [i:lead, 266] 統計的手法に基づく日本語固有表現抽出の研究事例としては，我々がベースとした，最大エントロピー法を用いるもの[CITE]の他に，決定木学習を用いるもの[CITE]，最大エントロピー法を用いるもの[CITE]，決定リスト学習を用いるもの[CITE]，SVM(support vector machines)を用いるもの[CITE]などがある．
.....
  [i:297, score:266] 統計的手法に基づく日本語固有表現抽出の研究事例としては，我々がベースとした，最大エントロピー法を用いるもの[CITE]の他に，決定木学習を用いるもの[CITE]，最大エントロピー法を用いるもの[CITE]，決定リスト学習を用いるもの[CITE]，SVM(support vector machines)を用いるもの[CITE]などがある．
  [i:299, score:300] 決定リスト学習を用いる事例[CITE]では，可変長文脈素性を用いることにより，固定長モデルの性能の上回る結果が得られているが，ベースとなる決定リスト学習の性能は最大エントロピー法の性能よりも劣っている．
  [i:302, score:299] その他には，[CITE]で報告されているように，解析の方向を文頭から文末と文末から文頭の二通り設定し，解析済の固有表現のタグを素性として利用する方法により，振る舞いの異なった出力が得られる可能性があり，stacking法でその出力を利用することで，精度の向上が期待できる可能性がある．

================================================================
[section type  : conclusion]
[section title : おわりに]
================================================================
[i:306, score:622] まず，最大エントロピー法に基づく統計的学習による固有表現抽出モデルにおいて，現在位置の形態素が，いくつの形態素から構成される固有表現の一部であるかを考慮して学習を行なう可変長モデルと，常に現在位置の形態素の前後数形態素ずつまでを考慮して学習を行なう固定長モデルとの間のモデルの挙動の違いに注目し，なるべく挙動が異なり，かつ，適度な性能を保った複数のモデルの出力の混合を行なった．
[i:307, score:422] 混合の方式としては，複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段の学習器を用いて，複数のシステム・モデルの出力の混合を行なう規則を学習するという混合法(stacking法)を採用した．
[i:308, score:594] 第二段の学習器として決定リスト学習を用いて，固定長モデルおよび可変長モデルの出力を混合する実験を行なった結果，最大エントロピー法に基づく固有表現抽出モデルにおいてこれまで得られていた最高の性能を上回る性能が達成された．

