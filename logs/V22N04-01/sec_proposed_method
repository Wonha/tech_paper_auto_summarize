機械学習を用いて関連語・周辺語から検索用語を予測・提示する場合，その学習データとして，入力（関連語・周辺語）と正解となるレスポンス（検索用語）のペアからなるコーパスが必要となる．
本稿ではこのようなコーパスを「関連語・周辺語コーパス」と呼ぶ．
また，教師あり機械学習では，レスポンスをラベルと呼ぶ場合が多いので，本稿では検索用語をラベルと呼ぶ．
表[REF_tab:example]はコーパスの入力（関連語・周辺語とその元となる説明文書）とラベルのペアの例を示す．
本章では，コーパスデータの収集・作成方法について述べる．
また，収集・作成したデータからの関連語・周辺語の抽出方法と特徴ベクトルの構成方法について述べる．
本研究では，ラベルを説明している文書には関連語・周辺語が多く含まれると考え，インターネットからこのようなWebページを手動と自動の2通りの方法で収集した．
手動収集では人手でラベルを説明するWebページを選別し収集する．
一方，自動収集では，ラベルの後に「とは」「は」「というものは」「については」「の意味は」の5語を付けて（たとえば，ラベルが「グラフィックボード」であれば「グラフィックボードとは」「グラフィックボードというものは」などで）Googleで検索したものを説明文書として収集する．
手動収集データは規模が小さい代わりに精度が高く，自動収集データは精度が低い代わりに規模が大きい．
機械学習の汎化能力を向上させるために，学習データとして，精度は高いが規模が小さい手動収集データに加え，精度はそれほど高くない（つまり，ノイズはある）が相対的に規模の大きい自動収集データを用いることにした．
しかし，自動収集したデータには説明文書とラベルがそもそも一致しない，つまり説明文書へのラベルが履き違えられている可能性も考えられる．
そのために，手動で収集した説明文書をオリジナルのデータとしてとらえ，それらに適度なノイズを加えて作成した疑似データも用いることにした．
このようなデータは自動収集したデータに比べノイズが少なくラベルの履き違いもないと考えることができる．
疑似データの具体的な生成手順は以下の通りである．
オリジナルの説明文書からすべての異なり単語を抽出する．
個々のオリジナルの説明文書に対し，追加，削除，または追加&削除の処理を加える．
具体的には，手順(1)で抽出した単語のうち，説明文書にない単語を説明文書の単語数の10%個ランダムに選んで加える，説明文書から単語を説明文書の単語数の10%個ランダムに選んで削除する，または上記の（10%ずつの）追加と削除を同時に施す，という処理を等確率（つまり，それぞれを1/3の確率）で行う．
手順(2)で得られたデータを疑似データとする．
なお，この生成方法においては，1つのオリジナルの説明文書に対し，疑似データを複数生成することが可能である．
評価データは学習データとは別に自動収集したものを用いる．
ただし，自動収集データは，ラベルが正確とは限らないため，評価データとして用いても適切な評価とならない可能性がある．
そのため，評価データとして自動収集データの中からラベルの正しいものを人手で選別して用いることにした．
以下の手順(1)〜(4)で説明文書から関連語・周辺語を抽出する．
それに手順(5)(6)を加えることにより，機械学習に必要な特徴ベクトルへの変換を行う．
手動収集のデータを形態素解析し，名詞（固有名詞，サ変接続，一般）を抽出する．
名詞が連続しているならば，日本語同士なら結合し，英語同士なら空白を間に入れて結合し，1つの単語と見なす．
各ラベルから出現頻度がトップ50以内の単語を抽出する．
ラベル間で重複している単語を除外する．
本研究では，以下に述べる考えに基づき2ラベル間で重複する単語を除外する，または，3ラベル以上で共通する単語を除外するという2通りの方法を採用した．
まず，各ラベルにできるだけ特徴的な単語のみを素性にするためには重複単語をできるだけ除外するのが効果的と考える．
また，今回は実験規模が小さくあまり問題にならないが，予測用語の数の増加に伴う特徴ベクトル次元の大幅な増加を抑える1つの方法として重複単語を除外することが考えられる．
特徴ベクトル次元の抑制はまた一般的に，学習におけるデータスパースネス問題の緩和にもつながる．
しかし一方，ラベル間の単語重複をまったく認めないと，たとえば「USBメモリ」のような，「USB」や「メモリ」に共通する重要な単語を除外してしまう問題も考えられる．
そのため，本研究では2ラベル間の重複を許容し3ラベル以上で共通する単語を除外する方法も用いる．
上記手順で得られた単語をベクトルの要素とし，個々の要素はその単語が出現していれば1，出現していなければ0の2値を取る．
2.1, 2.2, 2.3節で述べたすべてのデータに対し形態素解析を行い，手順(5)にしたがって特徴ベクトルに変換する．
深層学習とは従来の機械学習より深い層構造をしている機械学習手法全般のことを指す．
その代表的な手法としてDeep Belief Network (DBN) [CITE]とStacked Denoising Autoencoder (SdA) [CITE]が提案されている．
数多くの課題において，その両者の性能がほぼ同じと言われているが，本研究ではよりスマートなアーキテクチャを有するDBNを用いることにした．
深層学習は，本来経験則で行っていた特徴抽出を機械学習に組み込もうとしてできたものである．
そのため，DBNは，Restricted Boltzmann Machine (RBM)を複数並べ教師なし学習の特徴抽出器として利用する多層のニューラルネットと，ラベルを出力する教師あり学習の最終層から構成される．
特徴抽出器の教師なし学習はPre-training，最終層の教師あり学習はFine-tuningと呼ばれる．
RBMは制限付きボルツマンマシンとも呼ばれ，学習データの確率分布を教師なし学習で表現する（言い換えれば，学習データの生成モデルを統計的な機械学習の方法で構築する），一種の確率的なグラフィカルモデルである．
本来のボルツマンマシンの可視層と隠れ層のユニット間の結合を制限することにより，効率的な教師なし学習を実現している．
RBMの構造は図[REF_fig_rbm]に示しているように可視層と隠れ層の2層から構成され，層内ユニット間に結合がなく，層間のユニット，すなわち可視ユニット([MATH])と隠れユニット([MATH])，は結合されている．
以下，その学習アルゴリズム[CITE]を簡潔に述べておく．
学習データ[MATH]が可視層に与えられたとき，まず，式(1)，(2)，そして再度(1)の順で条件付確率に基づくサンプリングを行う．
P(h_i^{(k)}=1|\bm{v}^{(k)})=sigmoid\bigg(\sum_{j=1}^m w_{ij}v_j^{(k)}+c_i\bigg)
P(v_j^{(k+1)}=1|\bm{h}^{(k)})=sigmoid\bigg(\sum_{i=1}^n w_{ij}h_i^{(k)}+b_j\bigg)
ただし，[MATH] ([MATH])はサンプリングの繰り返し回数，[MATH]，[MATH]はユニット[MATH]と[MATH]間の結合の重み，そして，[MATH]と[MATH]は可視層と隠れ層のユニット[MATH]と[MATH]のオフセット（バイアス）である．
サンプリングを[MATH]回行った後，重みとオフセットは以下のように更新される．
\bm{W} \leftarrow\bm{W} + \epsilon(\bm{h}^{(1)}\bm{v}^{T}- P(\bm{h}^{(k+1)}=1|\bm{v}^{(k+1)})\bm{v}^{(k+1)T})
\bm{b} \leftarrow\bm{b} + \epsilon(\bm{v}-\bm{v}^{(k+1)})
\bm{c} \leftarrow\bm{c} + \epsilon(\bm{h}^{(1)}-P(\bm{h}^{(k+1)}=1|\bm{v}^{(k+1)}))
ただし，[MATH]は学習率である．
[MATH]は微小な乱数，[MATH], [MATH]は[MATH]で初期化する．
サンプリングの繰り返し回数が十分多いときはGibbs samplingと呼ばれており計算コストが非常に高い．
そのため，通常，サンプリングを[MATH]回のみ行う[MATH]-Contrastive Divergence（略してCD-[MATH]）と呼ばれる方法が採用される．
実際，[MATH] (CD-1)でも結果が十分よいことが経験的に知られており[CITE]，本研究も[MATH]に設定して学習を行う．
ここで[MATH]個の学習データに対しCD-[MATH]と呼ばれるサンプリング方法で[MATH]回繰り返し学習を行う手順を図[REF_fig:procedure-RBM]にまとめる．
学習が進むにつれ，可視層のサンプル[MATH]が学習データ[MATH]に近づいていく．
図[REF_fig_dbn]は一例として，三つのRBMと教師あり学習器から構成されるDBNを示す．
ただし実際，DBNを構成するRBMの数は可変である．
それらRBMはPre-trainingとも呼ばれ，教師なしの特徴抽出器として機能する．
一方，教師あり学習器はFine-tuningとも呼ばれ，入力（図[REF_fig_dbn]の場合はその入力から得られたRBM 3の出力）とラベルのペア（つまり正解付学習データ）を学習することにより未知の入力に対しても適切なラベルを出力できるようになる．
図に示しているように前方のRBMの隠れ層は後方のRBMの可視層となっている．
ここでは簡便化のために，RBMの層（ただし入力層を除く）をDBNの隠れ層と見なす．
つまり，図の例は三層の隠れ層のDBNである（隠れ層の数とRBMの数は同じであることに注意されたい）．
なお，教師あり学習はいろいろな方法で実現できるが，本稿ではロジスティク回帰を用いることにした．
三つのRBMを持つDBNの学習手順を図[REF_fig:procedure-DBN]にまとめる．
本研究では特定の分野の関連語・周辺語または説明文書を入力としたときの検索用語の予測・提示を行う検索支援を想定している．
まず，説明文書による支援の意義は，たとえばThe 5th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Accessのようなワークショップ型共同研究[CITE]における長い文書を検索課題としたタスクからも類推できる．
つまり，たとえばユーザが関連語・周辺語もはっきりわからないときはその支援要求を文書の形で伝える（入力する）ニーズはあると考える．
また一方，当然のことではあるが，本研究では，少数キーワード（関連語・周辺語）による検索用語の予測も期待している．
実際，表[REF_tab:keyword-prec]は，DBNについて，各学習データセットを用いた場合の，表[REF_tab:keyword]に示す3関連語・周辺語（[MATH]ノイズ語）による全検索用語の平均予測精度を示している．
実験はまだ小規模ではあるが，この結果は提案手法が少数キーワードによる支援も可能であることを示唆していると思われる．
機械学習を用いて関連語・周辺語から検索用語を予測・提示する場合，その学習データとして，入力（関連語・周辺語）と正解となるレスポンス（検索用語）のペアからなるコーパスが必要となる．
本稿ではこのようなコーパスを「関連語・周辺語コーパス」と呼ぶ．
また，教師あり機械学習では，レスポンスをラベルと呼ぶ場合が多いので，本稿では検索用語をラベルと呼ぶ．
表[REF_tab:example]はコーパスの入力（関連語・周辺語とその元となる説明文書）とラベルのペアの例を示す．
本章では，コーパスデータの収集・作成方法について述べる．
また，収集・作成したデータからの関連語・周辺語の抽出方法と特徴ベクトルの構成方法について述べる．
本研究では，ラベルを説明している文書には関連語・周辺語が多く含まれると考え，インターネットからこのようなWebページを手動と自動の2通りの方法で収集した．
手動収集では人手でラベルを説明するWebページを選別し収集する．
一方，自動収集では，ラベルの後に「とは」「は」「というものは」「については」「の意味は」の5語を付けて（たとえば，ラベルが「グラフィックボード」であれば「グラフィックボードとは」「グラフィックボードというものは」などで）Googleで検索したものを説明文書として収集する．
手動収集データは規模が小さい代わりに精度が高く，自動収集データは精度が低い代わりに規模が大きい．
機械学習の汎化能力を向上させるために，学習データとして，精度は高いが規模が小さい手動収集データに加え，精度はそれほど高くない（つまり，ノイズはある）が相対的に規模の大きい自動収集データを用いることにした．
しかし，自動収集したデータには説明文書とラベルがそもそも一致しない，つまり説明文書へのラベルが履き違えられている可能性も考えられる．
そのために，手動で収集した説明文書をオリジナルのデータとしてとらえ，それらに適度なノイズを加えて作成した疑似データも用いることにした．
このようなデータは自動収集したデータに比べノイズが少なくラベルの履き違いもないと考えることができる．
疑似データの具体的な生成手順は以下の通りである．
オリジナルの説明文書からすべての異なり単語を抽出する．
個々のオリジナルの説明文書に対し，追加，削除，または追加&削除の処理を加える．
具体的には，手順(1)で抽出した単語のうち，説明文書にない単語を説明文書の単語数の10%個ランダムに選んで加える，説明文書から単語を説明文書の単語数の10%個ランダムに選んで削除する，または上記の（10%ずつの）追加と削除を同時に施す，という処理を等確率（つまり，それぞれを1/3の確率）で行う．
手順(2)で得られたデータを疑似データとする．
なお，この生成方法においては，1つのオリジナルの説明文書に対し，疑似データを複数生成することが可能である．
評価データは学習データとは別に自動収集したものを用いる．
ただし，自動収集データは，ラベルが正確とは限らないため，評価データとして用いても適切な評価とならない可能性がある．
そのため，評価データとして自動収集データの中からラベルの正しいものを人手で選別して用いることにした．
以下の手順(1)〜(4)で説明文書から関連語・周辺語を抽出する．
それに手順(5)(6)を加えることにより，機械学習に必要な特徴ベクトルへの変換を行う．
手動収集のデータを形態素解析し，名詞（固有名詞，サ変接続，一般）を抽出する．
名詞が連続しているならば，日本語同士なら結合し，英語同士なら空白を間に入れて結合し，1つの単語と見なす．
各ラベルから出現頻度がトップ50以内の単語を抽出する．
ラベル間で重複している単語を除外する．
本研究では，以下に述べる考えに基づき2ラベル間で重複する単語を除外する，または，3ラベル以上で共通する単語を除外するという2通りの方法を採用した．
まず，各ラベルにできるだけ特徴的な単語のみを素性にするためには重複単語をできるだけ除外するのが効果的と考える．
また，今回は実験規模が小さくあまり問題にならないが，予測用語の数の増加に伴う特徴ベクトル次元の大幅な増加を抑える1つの方法として重複単語を除外することが考えられる．
特徴ベクトル次元の抑制はまた一般的に，学習におけるデータスパースネス問題の緩和にもつながる．
しかし一方，ラベル間の単語重複をまったく認めないと，たとえば「USBメモリ」のような，「USB」や「メモリ」に共通する重要な単語を除外してしまう問題も考えられる．
そのため，本研究では2ラベル間の重複を許容し3ラベル以上で共通する単語を除外する方法も用いる．
上記手順で得られた単語をベクトルの要素とし，個々の要素はその単語が出現していれば1，出現していなければ0の2値を取る．
2.1, 2.2, 2.3節で述べたすべてのデータに対し形態素解析を行い，手順(5)にしたがって特徴ベクトルに変換する．
深層学習とは従来の機械学習より深い層構造をしている機械学習手法全般のことを指す．
その代表的な手法としてDeep Belief Network (DBN) [CITE]とStacked Denoising Autoencoder (SdA) [CITE]が提案されている．
数多くの課題において，その両者の性能がほぼ同じと言われているが，本研究ではよりスマートなアーキテクチャを有するDBNを用いることにした．
深層学習は，本来経験則で行っていた特徴抽出を機械学習に組み込もうとしてできたものである．
そのため，DBNは，Restricted Boltzmann Machine (RBM)を複数並べ教師なし学習の特徴抽出器として利用する多層のニューラルネットと，ラベルを出力する教師あり学習の最終層から構成される．
特徴抽出器の教師なし学習はPre-training，最終層の教師あり学習はFine-tuningと呼ばれる．
RBMは制限付きボルツマンマシンとも呼ばれ，学習データの確率分布を教師なし学習で表現する（言い換えれば，学習データの生成モデルを統計的な機械学習の方法で構築する），一種の確率的なグラフィカルモデルである．
本来のボルツマンマシンの可視層と隠れ層のユニット間の結合を制限することにより，効率的な教師なし学習を実現している．
RBMの構造は図[REF_fig_rbm]に示しているように可視層と隠れ層の2層から構成され，層内ユニット間に結合がなく，層間のユニット，すなわち可視ユニット([MATH])と隠れユニット([MATH])，は結合されている．
以下，その学習アルゴリズム[CITE]を簡潔に述べておく．
学習データ[MATH]が可視層に与えられたとき，まず，式(1)，(2)，そして再度(1)の順で条件付確率に基づくサンプリングを行う．
P(h_i^{(k)}=1|\bm{v}^{(k)})=sigmoid\bigg(\sum_{j=1}^m w_{ij}v_j^{(k)}+c_i\bigg)
P(v_j^{(k+1)}=1|\bm{h}^{(k)})=sigmoid\bigg(\sum_{i=1}^n w_{ij}h_i^{(k)}+b_j\bigg)
ただし，[MATH] ([MATH])はサンプリングの繰り返し回数，[MATH]，[MATH]はユニット[MATH]と[MATH]間の結合の重み，そして，[MATH]と[MATH]は可視層と隠れ層のユニット[MATH]と[MATH]のオフセット（バイアス）である．
サンプリングを[MATH]回行った後，重みとオフセットは以下のように更新される．
\bm{W} \leftarrow\bm{W} + \epsilon(\bm{h}^{(1)}\bm{v}^{T}- P(\bm{h}^{(k+1)}=1|\bm{v}^{(k+1)})\bm{v}^{(k+1)T})
\bm{b} \leftarrow\bm{b} + \epsilon(\bm{v}-\bm{v}^{(k+1)})
\bm{c} \leftarrow\bm{c} + \epsilon(\bm{h}^{(1)}-P(\bm{h}^{(k+1)}=1|\bm{v}^{(k+1)}))
ただし，[MATH]は学習率である．
[MATH]は微小な乱数，[MATH], [MATH]は[MATH]で初期化する．
サンプリングの繰り返し回数が十分多いときはGibbs samplingと呼ばれており計算コストが非常に高い．
そのため，通常，サンプリングを[MATH]回のみ行う[MATH]-Contrastive Divergence（略してCD-[MATH]）と呼ばれる方法が採用される．
実際，[MATH] (CD-1)でも結果が十分よいことが経験的に知られており[CITE]，本研究も[MATH]に設定して学習を行う．
ここで[MATH]個の学習データに対しCD-[MATH]と呼ばれるサンプリング方法で[MATH]回繰り返し学習を行う手順を図[REF_fig:procedure-RBM]にまとめる．
学習が進むにつれ，可視層のサンプル[MATH]が学習データ[MATH]に近づいていく．
図[REF_fig_dbn]は一例として，三つのRBMと教師あり学習器から構成されるDBNを示す．
ただし実際，DBNを構成するRBMの数は可変である．
それらRBMはPre-trainingとも呼ばれ，教師なしの特徴抽出器として機能する．
一方，教師あり学習器はFine-tuningとも呼ばれ，入力（図[REF_fig_dbn]の場合はその入力から得られたRBM 3の出力）とラベルのペア（つまり正解付学習データ）を学習することにより未知の入力に対しても適切なラベルを出力できるようになる．
図に示しているように前方のRBMの隠れ層は後方のRBMの可視層となっている．
ここでは簡便化のために，RBMの層（ただし入力層を除く）をDBNの隠れ層と見なす．
つまり，図の例は三層の隠れ層のDBNである（隠れ層の数とRBMの数は同じであることに注意されたい）．
なお，教師あり学習はいろいろな方法で実現できるが，本稿ではロジスティク回帰を用いることにした．
三つのRBMを持つDBNの学習手順を図[REF_fig:procedure-DBN]にまとめる．
本研究では特定の分野の関連語・周辺語または説明文書を入力としたときの検索用語の予測・提示を行う検索支援を想定している．
まず，説明文書による支援の意義は，たとえばThe 5th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Accessのようなワークショップ型共同研究[CITE]における長い文書を検索課題としたタスクからも類推できる．
つまり，たとえばユーザが関連語・周辺語もはっきりわからないときはその支援要求を文書の形で伝える（入力する）ニーズはあると考える．
また一方，当然のことではあるが，本研究では，少数キーワード（関連語・周辺語）による検索用語の予測も期待している．
実際，表[REF_tab:keyword-prec]は，DBNについて，各学習データセットを用いた場合の，表[REF_tab:keyword]に示す3関連語・周辺語（[MATH]ノイズ語）による全検索用語の平均予測精度を示している．
実験はまだ小規模ではあるが，この結果は提案手法が少数キーワードによる支援も可能であることを示唆していると思われる．
