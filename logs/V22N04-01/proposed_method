関連語・周辺語コーパス

\begin{table}[b]
\caption{コーパスの入力とラベルのペアの例}
\label{tab:example}
\input{01table01.txt}
\end{table}

機械学習を用いて関連語・周辺語から検索用語を予測・提示する場合，その学習データとして，入力（関連語・周辺語）と正解となるレスポンス（検索用語）のペアからなるコーパスが必要となる．本稿ではこのようなコーパスを「関連語・周辺語コーパス」と呼ぶ．また，教師あり機械学習では，レスポンスをラベルと呼ぶ場合が多いので，本稿では検索用語をラベルと呼ぶ．表~\ref{tab:example}はコーパスの入力（関連語・周辺語とその元となる説明文書）とラベルのペアの例を示す．
本章では，コーパスデータの収集・作成方法について述べる．また，収集・作成したデータからの関連語・周辺語の抽出方法と特徴ベクトルの構成方法について述べる．


\subsection{手動収集と自動収集}

本研究では，ラベルを説明している文書には関連語・周辺語が多く含まれると考え，インターネットからこのようなWebページを手動と自動の 2 通りの方法で収集した．手動収集では人手でラベルを説明するWebページを選別し収集する\footnote{人手でWebページを選別した後，そのWebページから説明文書として該当する箇所を人手で選別する処理を行っている．}．一方，自動収集では，ラベルの後に「とは」「は」「というものは」「については」「の意味は」の 5 語を付けて（たとえば，ラベルが「グラフィックボード」であれば「グラフィックボードとは」「グラフィックボードというものは」などで）Googleで検索したものを説明文書として収集する\footnote{収集したWebページ全体をそのラベルの説明文書として扱っている．}．手動収集データは規模が小さい代わりに精度が高く，自動収集データは精度が低い代わりに規模が大きい．


\subsection{疑似データ}

機械学習の汎化能力を向上させるために，学習データとして，精度は高いが規模が小さい手動収集データに加え，精度はそれほど高くない（つまり，ノイズはある）が相対的に規模の大きい自動収集データを用いることにした．しかし，自動収集したデータには説明文書とラベルがそもそも一致しない，つまり説明文書へのラベルが履き違えられている可能性も考えられる．そのために，手動で収集した説明文書をオリジナルのデータとしてとらえ，それらに適度なノイズを加えて作成した疑似データも用いることにした．このようなデータは自動収集したデータに比べノイズが少なくラベルの履き違いもないと考えることができる．疑似データの具体的な生成手順は以下の通りである．
\begin{enumerate}
\item オリジナルの説明文書からすべての異なり単語を抽出する．
\item 個々のオリジナルの説明文書に対し，追加，削除，または追加\&削除の処理を加える．具体的には，手順(1)で抽出した単語のうち，説明文書にない単語を説明文書の単語数の10\%個ランダムに選んで加える，説明文書から単語を説明文書の単語数の10\%個ランダムに選んで削除する，または上記の（10\%ずつの）追加と削除を同時に施す，という処理を等確率（つまり，それぞれを1/3の確率）で行う\footnote{10\%という値は，予備実験などで精査して決めたものではなく，著者らが適度なノイズとして主観で設定したものである．}．
\item 手順(2)で得られたデータを疑似データとする．
\end{enumerate}
なお，この生成方法においては，1 つのオリジナルの説明文書に対し，疑似データを複数生成することが可能である．


\subsection{評価データ}

評価データは学習データとは別に自動収集したものを用いる．ただし，自動収集データは，ラベルが正確とは限らないため，評価データとして用いても適切な評価とならない可能性がある．そのため，評価データとして自動収集データの中からラベルの正しいものを人手で選別して用いることにした．


\subsection{関連語・周辺語抽出とベクトル変換}

以下の手順(1)〜(4)で説明文書から関連語・周辺語を抽出する．それに手順(5)(6)を加えることにより，機械学習に必要な特徴ベクトルへの変換を行う．
\begin{enumerate}
\item
手動収集のデータを形態素解析し，名詞（固有名詞，サ変接続，一般）を抽出する\footnote{形態素解析にMeCab 0.98を使用した．未知語と表記ゆれについては特別な処理を施しておらず今後の課題となる． ただし，未知語としての複合語については，たとえば「木村製作所」や「株式会社ウエーブ」など大半の日本企業名の場合は（中小企業で社名としては未知語であっても）「木村」と「製作所」などがそれぞれ名詞として解析されているので，手順(2)にしたがって問題なく1つの既知単語として扱われる．一方，たとえば「騰迅公司」のような表現において，最初の漢字が名詞以外の品詞と判断された場合は1つの既知単語として正しく扱うことができない．また，表記ゆれについては，「サーバ」と「サーバー」，「神経回路」と「ニューラルネット」のような形態素解析ツールの辞書に登録されているものはそれぞれ異なる単語として扱われてしまい，予測性能を落とす可能性がある．}．
\item
名詞が連続しているならば，日本語同士なら結合し，英語同士なら空白を間に入れて結合し，1 つの単語と見なす．
\item
各ラベルから出現頻度がトップ50以内の単語を抽出する\footnote{50という値は，手動で収集したデータにおいて各ラベルの関連語・周辺語の数は確実にそれ以下であることを確認した上で，提案手法の拡張性（つまり多少大きい目に）と機械学習の素性選択能力（つまり多少大きい目にしても問題がないこと）も考慮にいれて設定した．なお，この値は多少大きく設定されても手順(4)で絞られるので値をある程度大きい目に設定しておけば40 がよいか60 がよいかといった細かい選択はほとんど意味をなさないと思われる．}．
\item
ラベル間で重複している単語を除外する．本研究では，以下に述べる考えに基づき 2 ラベル間で重複する単語を除外する，または，3 ラベル以上で共通する単語を除外するという 2 通りの方法を採用した．まず，各ラベルにできるだけ特徴的な単語のみを素性にするためには重複単語をできるだけ除外するのが効果的と考える．また，今回は実験規模が小さくあまり問題にならないが，予測用語の数の増加に伴う特徴ベクトル次元の大幅な増加を抑える 1 つの方法として重複単語を除外することが考えられる．特徴ベクトル次元の抑制はまた一般的に，学習におけるデータスパースネス問題の緩和にもつながる．しかし一方，ラベル間の単語重複をまったく認めないと，たとえば「USBメモリ」のような，「USB」や「メモリ」に共通する重要な単語を除外してしまう問題も考えられる．そのため，本研究では 2 ラベル間の重複を許容し 3 ラベル以上で共通する単語を除外する方法も用いる．
\item 上記手順で得られた単語をベクトルの要素とし，個々の要素はその単語が出現していれば1，出現していなければ0の2値を取る．
\item 2.1, 2.2, 2.3節で述べたすべてのデータに対し形態素解析を行い，手順(5)にしたがって特徴ベクトルに変換する．
\end{enumerate}


深層学習

深層学習とは従来の機械学習より深い層構造をしている機械学習手法全般のことを指す．
その代表的な手法としてDeep Belief Network (DBN)~\cite{Hinton,Lee,Bengio:09,Bengio:13}とStacked Denoising Autoencoder (SdA)~\cite{Bengio:07,Bengio:09,Bengio:13,Vincent:08,Vincent:10}が提案されている．数多くの課題において，その両者の性能がほぼ同じと言われているが，本研究ではよりスマートなアーキテクチャを有するDBNを用いることにした．

深層学習は，本来経験則で行っていた特徴抽出を機械学習に組み込もうとしてできたものである．そのため，DBNは，Restricted Boltzmann Machine (RBM) を複数並べ教師なし学習の特徴抽出器として利用する多層のニューラルネットと，ラベルを出力する教師あり学習の最終層から構成される．特徴抽出器の教師なし学習はPre-training，最終層の教師あり学習はFine-tuningと呼ばれる．


\subsection{Restricted Boltzmann Machine (RBM)}

RBMは制限付きボルツマンマシンとも呼ばれ，学習データの確率分布を教師なし学習で表現する（言い換えれば，学習データの生成モデルを統計的な機械学習の方法で構築する），一種の確率的なグラフィカルモデルである．本来のボルツマンマシンの可視層と隠れ層のユニット間の結合を制限することにより，効率的な教師なし学習を実現している．

RBMの構造は図~\ref{fig_rbm}に示しているように可視層と隠れ層の  2層から構成され，層内ユニット間に結合がなく，層間のユニット，すなわち可視ユニット($v_1, v_2, \cdots, v_m$)と隠れユニット ($h_1, h_2, \cdots, h_n$)， は結合されている． 

\begin{figure}[t]
\begin{center}
\includegraphics{22-4ia1f1.eps}
\end{center}
\caption{Restricted Boltzmann Machineの構造}
\label{fig_rbm}
\vspace{-1\Cvs}
\end{figure}

以下，その学習アルゴリズム~\cite{Bengio:09}を簡潔に述べておく．

学習データ$\bm{v}$が可視層に与えられたとき，まず，式(1)，(2)，そして再度(1)の順で条件付確率に基づくサンプリングを行う．
{\allowdisplaybreaks
\begin{gather}
P(h_i^{(k)}=1|\bm{v}^{(k)})={\rm sigmoid}\bigg(\sum_{j=1}^m w_{ij}v_j^{(k)}+c_i\bigg) \\
P(v_j^{(k+1)}=1|\bm{h}^{(k)})={\rm sigmoid}\bigg(\sum_{i=1}^n w_{ij}h_i^{(k)}+b_j\bigg)
\end{gather}
}
ただし，$k$ ($\geq 1$)はサンプリングの繰り返し回数，$\bm{v}^{(1)}=\bm{v}$，$w_{ij}$はユニット$v_j$と$h_i$間の結合の重み，そして， $b_j$と$c_i$は可視層と隠れ層のユニット$v_j$と$h_i$のオフセット（バイアス）である． 
サンプリングを$k$回行った後，重みとオフセットは以下のように更新される．
\begin{gather}
\bm{W} \leftarrow \bm{W} + \epsilon (\bm{h}^{(1)}\bm{v}^{T}- P(\bm{h}^{(k+1)}=1|\bm{v}^{(k+1)})\bm{v}^{(k+1)T}) \\
\bm{b} \leftarrow \bm{b} + \epsilon(\bm{v}-\bm{v}^{(k+1)}) \\
\bm{c} \leftarrow \bm{c} + \epsilon(\bm{h}^{(1)}-P(\bm{h}^{(k+1)}=1|\bm{v}^{(k+1)}))
\end{gather}
ただし，$\epsilon$は学習率である．$\bm{W}$は微小な乱数\footnote{本研究では，http://deeplearning.net/tutorial/mlp.htmlのチュートリアルに従って，区間[$-4\frac{\sqrt{6}}{\sqrt{m+n}}$, $4\frac{\sqrt{6}}{\sqrt{m+n}}$]内の一様乱数を用いる（ただし，$m$と$n$はそれぞれ可視層と隠れ層のユニット数である）
．その数学的な考えについては\cite{Glorot:10}を参照されたい．}， $\bm{b}$, $\bm{c}$は$\bm{0}$で初期化する． 
サンプリングの繰り返し回数が十分多いときはGibbs samplingと呼ばれており計算コストが非常に高い．そのため，通常，サンプリングを$k$回のみ行う$k$-Contrastive Divergence （略してCD-$k$）と呼ばれる方法が採用される．実際，$k=1$ (CD-1)でも結果が十分よいことが経験的に知られており\cite{Bengio:09}，本研究も$k=1$に設定して学習を行う．

ここで$N$個の学習データに対しCD-$k$と呼ばれるサンプリング方法で$e$回繰り返し学習を行う手順を図~\ref{fig:procedure-RBM}にまとめる．学習が進むにつれ，可視層のサンプル\footnote{ここでは条件付確率の式(1)(2)に基づき生成されたデータをサンプルと呼んでいる．} $\bm{v}^{(k+1)}$が学習データ$\bm{v}$に近づいていく．


\subsection{Deep Belief Network (DBN)}

図~\ref{fig_dbn}は一例として，三つのRBMと教師あり学習器から構成されるDBNを示す．ただし実際，DBNを構成するRBMの数は可変である．それらRBMはPre-trainingとも呼ばれ，教師なしの特徴抽出器として機能する．一方，教師あり学習器はFine-tuningとも呼ばれ，入力（図~\ref{fig_dbn}の場合はその入力から得られたRBM 3の出力）とラベルのペア（つまり正解付学習データ）を学習することにより未知の入力に対しても適切なラベルを出力できるようになる．図に示しているように前方のRBMの隠れ層は後方のRBMの可視層となっている．ここでは簡便化のために，RBMの層（ただし入力層を除く）をDBNの隠れ層と見なす．つまり，図の例は三層の隠れ層のDBNである（隠れ層の数とRBMの数は同じであることに注意されたい）．なお，教師あり学習はいろいろな方法で実現できるが，本稿ではロジスティク回帰を用いることにした．

\begin{figure}[b]
\vspace{-0.5\Cvs}
\begin{center}
\includegraphics{22-4ia1f2.eps}
\end{center}
\caption{RBMの学習手順}
\label{fig:procedure-RBM}
\end{figure}
\begin{figure}[b]
\begin{center}
\includegraphics{22-4ia1f3.eps}
\end{center}
\caption{Deep Belief Networkの例}
\label{fig_dbn}
\end{figure}
\begin{figure}[b]
\begin{center}
\includegraphics{22-4ia1f4.eps}
\end{center}
\caption{三つのRBMを持つDBNの学習手順}
\label{fig:procedure-DBN}
\end{figure}


三つのRBMを持つDBNの学習手順を図~\ref{fig:procedure-DBN}にまとめる． 


