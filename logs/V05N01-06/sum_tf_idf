================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:5, score:0.36392] ところが，従来のクラスタ検索では，検索時に使う距離尺度とクラスタリング時に使う距離尺度が直接関係ないため，単純な二分木検索では十分な検索精度が得られなかった．
[i:7, score:0.35583] 実験では，「現代用語の基礎知識」を用いて，HBCを用いたクラスタ検索がWard法を用いた従来のクラスタ検索よりも優れていることを実証する．
[i:8, score:0.37510] また，「Wall Street Journal」を用いて，HBCを用いたクラスタ検索が網羅検索に比べノイズ頑健性に優れていることを実証する．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[i:21, score:0.34376] クラスタ検索では，通常，クラスタリングによりクラスタの二分木をあらかじめ構築しておき，その上でトップダウンに二分木検索を行う．
[i:25, score:0.39503] ほとんどの研究では，クラスタリングの手法として単一リンク法，Ward法などを用いていたが，これらの手法は，後の検索で使われる尺度(例えば，TF[MATH]IDF法や確率)とは直接関係のない尺度でクラスタの二分木を構築していく．
[i:31, score:0.34457] 通常のキーワード検索では，検索要求と単一文書を厳密なキーワード符合に基づいて比較するため，キーワードの表記の異なりにより関連する文書をとり逃すこともあるが，クラスタ検索では，検索要求を意味的にまとまった文書集合(クラスタ仮説で言うところの「密接に関連した文書群」)と比較するため，この問題も起りにくくなる．

================================================================
[section type  : proposed_method]
[section title : クラスタ検索]
================================================================
[i:45, score:0.42483] 本論文で扱うクラスタ検索では，文書集合を二分木として自動的に構成し(このステップをクラスタリングまたは訓練と呼ぶ)，検索要求を各クラスタ(ノード)と比較することによって，検索要求と類似する文書を指定した数だけとりだす(このステップを検索またはテストと呼ぶ)．
[i:46, score:0.38147] 最も単純な検索法は二分木検索であり(図[REF_fig:search_strategies] (b)参照)，クラスタ木の根からトップダウンに木をたどり，指定した数の文書を含むクラスタを探す．
[i:55, score:0.30675] クラスタ木上をトップダウンに二分木検索する方法とは逆に，葉からボトムアップにクラスタ木を検索する方法もある．

================================================================
[section type  : proposed_method]
[section title : 確率的クラスタリング(HBC)]
================================================================
[i:63, score:0.35000] 特に，クラスタ木の根に近い部分は，与えられたほとんどの文書集合を含むため漠然性が高く，二分木検索もこの部分での比較で誤りを起しやすい．
[i:86, score:0.34139] 従来の実験では，単一リンク法やWard法をクラスタリングの方法として用いていたが，これらの方法は，検索に使う尺度とは直接関係のない尺度を使いクラスタ木を構築している．
[i:94, score:0.39503] それに対しHBCは，文書集合が与えられると，それらを自己検索した時の精度(具体的には自己再現率)を最大化するようなクラスタ木を構築する．

================================================================
[section type  : experiment_result]
[section title : 実験]
================================================================
-----------------------------------------------------
  [subsection title : 実験方式とデータについて]
-----------------------------------------------------
  [i:lead, score:0.30432] 実験では，連想検索の精度を評価するためにトピック割り付けを行った．
.....
  [i:98, score:0.30432] 実験では，連想検索の精度を評価するためにトピック割り付けを行った．
  [i:106, score:0.40659] [MATH]-NN法では，ある文書[MATH]にトピックを割り付ける際，あらかじめ専門家によりトピックが割り付けられている文書集合(訓練データ)の中から[MATH]に近いものを[MATH]個検索する．
  [i:107, score:0.31356] この検索法に，文書連想検索の手法(網羅検索，クラスタ検索)を用い比較した．
-----------------------------------------------------
  [subsection title : 従来のクラスタ検索との比較]
-----------------------------------------------------
  [i:lead, score:0.36620] まず，比較的ノイズの少ないGKを用いて，HBCを用いたクラスタ検索と従来から行われていたクラスタ検索を比較する．
.....
  [i:141, score:0.36620] まず，比較的ノイズの少ないGKを用いて，HBCを用いたクラスタ検索と従来から行われていたクラスタ検索を比較する．
  [i:158, score:0.43200] ただし，HBCを用いたクラスタ検索，網羅検索が，[MATH]を適当に設定するとトピック検索法を上回るのに対し，Ward法を用いたクラスタ検索は，常にトピック検索法を下回る．
  [i:160, score:0.38226] 次節では，ノイズを含むより実データに近いWSJを用いて，HBCを用いたクラスタ検索と網羅検索との違いを詳しく調べる．
-----------------------------------------------------
  [subsection title : クラスタ検索のノイズ頑健性]
-----------------------------------------------------
  [i:lead, score:0.25693] クラスタ検索は，網羅検索と比べると汎化能力という点で優れている．
.....
  [i:165, score:0.36479] 本節では，WSJを使って，データセット中に存在するノイズがトピック割り付け(すなわち連想検索)に及ぼす影響を調べる．
  [i:186, score:0.46170] また，比例配分割り付けでは，網羅検索，クラスタ検索共に，トピック検索と同程度の精度である．
  [i:190, score:0.43586] 定数割り付け，確率的割り付けでは，網羅検索，クラスタ検索共に，ベースラインのトピック検索を大きく上回っている．

================================================================
[section type  : conclusion]
[section title : おわりに]
================================================================
[i:199, score:0.29590] 本論文では，文書連想検索のための新しいクラスタ検索法を提案した．
[i:200, score:0.35141] 提案したクラスタ検索では，与えられた文書集合を自己検索した時の精度を最大化する確率的クラスタリングを用いている．
[i:201, score:0.29501] よって，本クラスタリング手法は，従来のクラスタ検索で用いられていたクラスタリング手法に比べると，検索に密接に関連した手法であると言える．

