提案モデル

以降の説明においては言語対として日本語と英語を用いるが，提案モデルはこの
言語対に特別に設計されたものではなく，言語対によらないロバストなものであ
る．

提案モデルは依存構造木上で定義されるものであるので，まず対訳文を両言語と
も依存構造解析し，単語の依存構造木に変換する．図
\ref{fig:word-based-alignment}の一番右に依存構造木の例を示す．単語は上か
ら下に順に並んでおり，文のヘッドとなる単語は最も左側に位置している．アラ
イメントの最小単位はこれら各単語であるが，モデル推定時に複数単語のかたま
りを句として自動的に獲得する．これについては\ref{expand_step}章で詳しく
述べる．

\begin{figure}[t]
 \begin{center}
  \includegraphics{17-1ia6f1.eps}
 \end{center}
  \caption{単語列アライメントモデルと提案手法との比較}
  \label{fig:word-based-alignment}
\end{figure}


\subsection{提案モデル概観}

本章では，広く知られており，かつ一般的に用いられている統計的なアライメン
ト手法であるIBMモデルと比較しながら，我々が提案するモデルについて説明す
る．IBMモデル\cite{Brown93}では，与えられた日本語文$\mathbf{f}$と英語文
$\mathbf{e}$からなる対訳文間の最も良いアライメント$\mathbf{\hat{a}}$ は
以下の式により獲得される：
\begin{equation}
 \label{eq:best}
\begin{aligned}[b]
 \hat{\mathbf{a}} & = \argmax_{\mathbf{a}} p(\mathbf{a} | \mathbf{f}, \mathbf{e})\\
   & = \argmax_{\mathbf{a}} \frac{p(\mathbf{a}, \mathbf{f} | \mathbf{e})}{p(\mathbf{f} | \mathbf{e})}\\
   & = \argmax_{\mathbf{a}} \frac{p(\mathbf{a}, \mathbf{f} | \mathbf{e})}{\sum_\mathbf{a} p(\mathbf{a}, \mathbf{f} | \mathbf{e})} \\
   & = \argmax_{\mathbf{a}} p(\mathbf{a}, \mathbf{f} | \mathbf{e}) \\
   & = \argmax_{\mathbf{a}} p(\mathbf{f}|\mathbf{a}, \mathbf{e}) \cdot p(\mathbf{a}|\mathbf{e})
\end{aligned}
\end{equation}
ここで，$p(\mathbf{f}|\mathbf{a}, \mathbf{e})$は{\bf 語彙確率}
(\textit{lexicon probability})と呼ばれ，$p(\mathbf{a}|\mathbf{e})$は{\bf 
アライメント確率}(\textit{alignment probability})と呼ばれている．


$\mathbf{f}$が$n$語($f_1,f_2,...,f_n$)からなり，$\mathbf{e}$が$m$語
($e_1,e_2,...,e_m$)とNULL($e_0$)からなるとする．またアライメント
$\mathbf{a}$は$\mathbf{f}$の各単語から$\mathbf{e}$の単語への対応を表し，
$a_j = i$ は$f_j$が$e_i$に対応していることを示すとする．このような条件の
下，上記二つの確率は以下のように展開される：
{\allowdisplaybreaks
\begin{gather}
 p(\mathbf{f}|\mathbf{a}, \mathbf{e}) = \prod_{j=1}^{J} p(f_j|e_{a_j}) 
 \label{eq:lex} \\
 p(\mathbf{a}|\mathbf{e}) = \prod_{i=1}^{I} p(\Delta j|e_i)
 \label{eq:align}
\end{gather}
}
ここで$\Delta j$は$e_i$に対応する$\mathbf{f}$の単語の相対位置である．式
\ref{eq:lex}は単語翻訳確率の積であり，式\ref{eq:align}は相対位置確率の積
となっている．ただし，ここで示した式は正確にIBMモデルを記述しているわけ
ではなく，その意図を簡単に示したものである．また図
\ref{fig:word-based-alignment}の左側にIBMモデルによるアライメントの例を
示す．IBMモデルは方向性があるため，アライメントに制限がある．これを解消
するため，両方向による結果を最後に統合して最終的なアライメントとすること
が多い\cite{koehn-och-marcu:2003:HLTNAACL}．しかし日英のような言語構造の
違いの大きい言語対においては，このような方法では十分な精度でのアライメン
トは行えない．

提案モデルはIBMモデルを3つの点で改善する．一つ目は式\ref{eq:lex}において，
単語ではなく句を考慮する．二つ目は式\ref{eq:align}において，文中での単語
の位置ではなく，依存関係を考慮する．最後に，提案モデルでは最も良いアライ
メント$\hat{\mathbf{a}}$を求める際に，片方向のモデルだけでなく，両方向の
モデルを同時に利用する．つまり，式\ref{eq:best}を以下のように変更する：
\begin{equation}
 \label{eq:best_proposed}
\begin{aligned}[b]
  \hat{\mathbf{a}} & = \argmax_{\mathbf{a}} p(\mathbf{a} | \mathbf{e}, \mathbf{f})^2 \\
     & = \argmax_{\mathbf{a}} p(\mathbf{a}, \mathbf{f} | \mathbf{e}) \cdot p(\mathbf{a}, \mathbf{e} | \mathbf{f})\\
     & = \argmax_{\mathbf{a}} p(\mathbf{f}|\mathbf{a}, \mathbf{e}) \cdot p(\mathbf{a}|\mathbf{e}) \cdot p(\mathbf{e}|\mathbf{a}, \mathbf{f}) \cdot p(\mathbf{a}|\mathbf{f})
\end{aligned}
\end{equation}


我々のモデルでは句を扱っているため，上式を素直に計算できる．また式の上で
はIBMモデルと同じ解が得られるはずであるが，それぞれの確率を近似するため，
両方向を考慮した方がよりよい解が得られる．図
\ref{fig:word-based-alignment}の一番右に提案モデルによるアライメント例を
示す．従来手法のアライメントと比べると，多対多対応が自然と獲得されている
ことがわかる．


提案モデルはEMアルゴリズムにより学習される
\cite{liang-taskar-klein:2006:HLT-NAACL06-Main}．目的関数として，与えら
れたデータに対する尤度を考える：
\begin{equation}
\begin{aligned}[b]
 \sum_{(\mathbf{e},\mathbf{f})} \log p(\mathbf{e}, \mathbf{f})
   & = \sum_{(\mathbf{e},\mathbf{f})}\left(\log \sum_{\mathbf{a}} p(\mathbf{a}, \mathbf{e}, \mathbf{f})\right)\\
   & = \sum_{(\mathbf{e},\mathbf{f})}\left(\log \sum_{\mathbf{a}} \sqrt{p(\mathbf{a}, \mathbf{e}, \mathbf{f})^2}\right)\\
   & = \sum_{(\mathbf{e},\mathbf{f})}\left(\log \sum_{\mathbf{a}} \sqrt{p(\mathbf{a},\mathbf{e}|\mathbf{f})\cdot p(\mathbf{f})\cdot p(\mathbf{a},\mathbf{f}|\mathbf{e})\cdot p(\mathbf{e})}\right)
\end{aligned}
\end{equation}
この尤度を最大化するようなパラメータ$\theta$を求める．$\theta$は各方向の
モデルにおけるパラメータをまとめたものとする．E-stepでは現在のパラメータ
$\theta$の下でのアライメントの事後確率を以下のように計算する：
\begin{equation}
\begin{aligned}[b]
  q(\mathbf{a}; \mathbf{e},\mathbf{f}) &:= p(\mathbf{a}|\mathbf{e},\mathbf{f};\theta) \\
    & = \frac{p(\mathbf{a}, \mathbf{e}, \mathbf{f}; \theta)}{\sum_{\mathbf{a}}p(\mathbf{a}, \mathbf{e}, \mathbf{f}; \theta)} \\[0.5em]
    & = \frac{\sqrt{p(\mathbf{a},\mathbf{e}|\mathbf{f};\theta) \cdot p(\mathbf{f}) \cdot p(\mathbf{a},\mathbf{f}|\mathbf{e};\theta)\cdot p(\mathbf{e})}}{\sum_{\mathbf{a}}\sqrt{p(\mathbf{a},\mathbf{e}|\mathbf{f};\theta) \cdot p(\mathbf{f}) \cdot p(\mathbf{a},\mathbf{f}|\mathbf{e};\theta)\cdot p(\mathbf{e})}}
\end{aligned}
\end{equation}
M-stepではパラメータの更新を行う：
\begin{equation}
 \theta' := \argmax_{\theta} \sum_{\mathbf{a},\mathbf{e},\mathbf{f}} q(\mathbf{a};\mathbf{e},\mathbf{f}) \log p(\mathbf{a},\mathbf{e},\mathbf{f};\theta)
\end{equation}


次節以降では，lexicon probabilitiyとalignment probabilitiyを定義する．


\subsection{句翻訳確率}

$\mathbf{f}$が$N$個の句($F_1,F_2,...,F_N$)からなり，$\mathbf{e}$が$M$個
の句($E_1,E_2,...,E_M$)とNULL($E_0$)からなるとする．またアライメント
$\mathbf{A^{fe}}$は$f$の各句から$e$の単句への対応を表し，$A_j^{fe} = i$は句$F_j$が
句$E_i$に対応していることを示すとする．


提案モデルでは，IBMモデルにおける単語翻訳確率$p(f_j|e_i)$の代わりに，
{\bf 句翻訳確率}$p(F_j|E_i)$を考える．ただし，2語以上からなる句はNULL対
応にはならないという制限を加える（その句に含まれる各単語がNULL対応になる
ものとする）．句翻訳確率を用いて，式\ref{eq:lex}を以下のように変更する：
\begin{equation}
 \label{eq:lex_mod}
 p(\mathbf{f}|\mathbf{a},\mathbf{e}) = \prod_{j=1}^{N} p(F_j|E_{A_j^{fe}})
\end{equation}


ここで，句$F_j$と句$E_i$が対応付いたと仮定すると，この句の対応に寄与する
句翻訳確率は，双方向分の句翻訳確率を掛け合わせるため以下のようになる：
\begin{equation}
 \label{eq:phrase_alignment_prob}
p(F_j|E_i) \cdot p(E_i|F_j)
\end{equation}
この確率の積を{\bf 句対応確率}と呼ぶことにする．表\ref{tab:sample_prob} 
の上部に図\ref{fig:word-based-alignment}の例における句対応確率を示す．




\subsection{依存関係確率}

IBMモデルにおいて，単語の移動，すなわちreorderingモデルは，
\pagebreak
式\ref{eq:align}に示したように，一つ前の単語のアライメントとの相対位置によっ
て定義されている．これに対し提案モデルでは，単語の文内での位置ではなく，
依存関係を考慮する．

\begin{table}[t]
  \caption{各確率の計算例}
  \label{tab:sample_prob}
\input{06table01.txt}
\end{table}


まず$\mathbf{e}$のある単語$e_p$と，$e_p$に係る単語$e_c$について考え，そ
れらの可能なアライメントのうち，$e_p$が句$E_P$に属し，$e_c$が句$E_C$に属
しており，$E_C$が$E_P$に係っているものを考える．このような状況において，
$E_P$と$E_C$の$\mathbf{f}$での対応句$F_{A_P^{ef}}$と$F_{A_C^{ef}}$の関係
をモデル化したものが依存関係確率である．図\ref{fig:dpnd_prob}に例を示す．
日英などのように語順の大きく異なる言語対であっても，文内の単語や句の依存
関係は多くの場合保存され，$F_{A_C^{ef}}$が直接$F_{A_P^{ef}}$に係ることが
多い．提案モデルはこのような傾向を考慮したものである．直接の親子関係にあ
る2単語が属する2句の対応先の句の関係は$rel(e_p, e_c)$のように記述するこ
とにし，これは$e_p$が属する句の対応先の句$F_{A_P^{ef}}$から，$e_c$が属す
る句の対応先の句$F_{A_C^{ef}}$への経路として定義される．経路は以下のよう
な表記に従って示される：
\begin{itemize}
 \item 子ノードへ行く場合は `c'({\it c}hild node) 
 \item 親ノードへ行く場合は `p'({\it p}arent node) 
 \item 2ノード以上離れている場合は，上記二つを並べて表記する 
\end{itemize}
例えば図\ref{fig:word-based-alignment}において，``for''から
``photodetector''への経路は`c'となり，``the''から``for''への経路は，2ノー
ド離れているため`p;p' となる．句同士の依存関係を記述する際には，経路上に
ある全ての句は，2つ以上の単語からなる句も含めて，すべて1つのノードとして
扱う．このため，図\ref{fig:word-based-alignment}において``photogate''か
ら``the''への経路は`p;c;c;c'となる．

この$rel$を用いて，式\ref{eq:align}を以下のように改善する：
\begin{equation}
 \label{relation_probability}
 p(\mathbf{a}|\mathbf{e}) = \prod_{(e_p,e_c) \in D_{\mathbf{e}\mathchar`-pc}} p_{\mathbf{ef}}(rel(e_p, e_c))
\end{equation}
ここで$D_{\mathbf{e}\mathchar`-pc}$は$\mathbf{e}$の木構造において直接の
親子関係にある全ての単語の組み合わせである．また
$p_{\mathbf{ef}}(rel(e_p, e_c))$を$\mathbf{e}\rightarrow\mathbf{f}$方向
の{\bf 依存関係確率}と呼ぶ．$p_{\mathbf{ef}}$は木構造上でのreorderingモ
デルと考えることができる．

\begin{figure}[t]
 \begin{center}
  \includegraphics{17-1ia6f2.eps}
 \end{center}
  \hangcaption{依存関係の例（親子関係にある$e_p$と$e_c$が属する句$E_P$と$E_C$
  の対応先の句$F_{A_P^{ef}}$と$F_{A_C^{ef}}$の関係をモデル化する）}
  \label{fig:dpnd_prob}
\end{figure}

$rel$にはいくつか特別な値がある．まず$E_C$と$E_P$が同じ場合，つまり，
$e_c$と$e_p$が同じ句に属する場合，$rel = \mbox{`SAME'}$となる．次にNULL 
アライメントに関してだが，これには$e_p$ がNULL対応の場合，$e_c$ がNULL対
応の場合，両方ともNULL対応の場合の3通りがあり，それぞれ$rel$の値は`NULL\_p'，`NULL\_c'，`NULL\_b'となる．例として表\ref{tab:sample_prob} の
下部に図\ref{fig:word-based-alignment}の例における依存関係確率を各方向そ
れぞれ示す．

一般的に，構文解析などにおいても，親ノードとの関係だけでなく，さらにその
親のノードとの関係を考慮することは自然であり，精度の向上につながる．提案
モデルにおいても，直接の親子関係だけでなく，さらにその親ノードとの関係も
考慮し，以下のように定式化する：
\begin{equation}
 p(\mathbf{a}|\mathbf{e}) = \prod_{(e_p,e_c) \in D_{\mathbf{e}\mathchar`-pc}} p_{\mathbf{ef}\mathchar`-pc}(rel(e_p, e_c)) \cdot \prod_{(e_g,e_c) \in D_{\mathbf{e}\mathchar`-gc}} p_{\mathbf{ef}\mathchar`-gc}(rel(e_g, e_c))
\end{equation}
ここで$D_{\mathbf{e}\mathchar`-gc}$は$\mathbf{e}$の木構造において祖父と
子の関係にある全ての単語の組み合わせである．
$p_{\mathbf{ef}\mathchar`-pc}$は直接の親子関係にある2単語を見たときの依
存関係確率であり，$p_{\mathbf{ef}\mathchar`-gc}$は親の親と子の関係にある
2単語の場合の依存関係確率である．

なお，逆方向（$\mathbf{f}$から$\mathbf{e}$）のモデル$p(\mathbf{a}|\mathbf{f})$も全く同様に定義される．
\pagebreak
\begin{equation}
 p(\mathbf{a}|\mathbf{f}) = \prod_{(f_p,f_c) \in D_{\mathbf{f}\mathchar`-pc}} p_{\mathbf{fe}\mathchar`-pc}(rel(f_p, f_c)) \cdot \prod_{(f_g,f_c) \in D_{\mathbf{f}\mathchar`-gc}} p_{\mathbf{fe}\mathchar`-gc}(rel(f_g, f_c))
\end{equation}



トレーニング
\label{training}

提案モデルは2つのステップに分けて学習される．これはIBMモデルにおいて，完
全に最適解が求まる簡単なモデルからスタートし，徐々により複雑なモデルに移
行することに対応する．Step 1では単語翻訳確率の推定が行われ，Step 2では句
翻訳確率と依存関係確率の推定が行われる．どちらのステップにおいてもモデル
はEMアルゴリズムにより学習される．またステップ1においては句は扱わず，全
て単語単位での学習となる．複数単語の塊＝句はStep 2において自動的に獲得さ
れる．


\subsection{Step 1}

Step 1では各方向独立に，単語翻訳確率を推定する．これはIBM Model 1と全く
同様の方法により行われる．Step 1の推定の際には対応の単位は各ノード単体，
つまり単語のみであり，句は考慮しない．句はStep 2の推定から考慮し，句とな
るべき候補を動的に作り出すことにより実現する．これはStep 1の段階で可能な
句の候補全てを考慮すると，アライメント候補数が爆発し，扱えなくなるためで
ある．


$\mathbf{f}$から$\mathbf{e}$へのアライメントを考えると，$\mathbf{f}$の各
単語は，他の単語に関係なく，$\mathbf{e}$の任意の単語，またはNULLに対応す
ることができる．このことから，あるひとつの可能なアライメント$\mathbf{a}$
の確率は以下のように計算できる：
\begin{align}
 \label{eq:trans_prob}
p(\mathbf{a}, \mathbf{f}|\mathbf{e}) & = p(\mathbf{f}|\mathbf{a}, \mathbf{e}) \cdot p(\mathbf{a}|\mathbf{e}) \\
 & = \prod_{j=1}^{J} p(f_j|e_{a_j}) \cdot C(n, m)
\end{align}
ここで$p(\mathbf{a}|\mathbf{e})$は全てのアライメントにおいて一定
(uniform)であるとし，各文の単語数による関数$C(n, m)$と置く．さらに，全て
の可能なアライメントを考慮すると，確率$p(\mathbf{f}|\mathbf{e})$は以下の
ように計算できる．
\begin{equation}
 \label{eq:all_align_prob}
 p(\mathbf{f}|\mathbf{e}) = \sum_{\mathbf{a}} p(\mathbf{a}, \mathbf{f}|\mathbf{e})
\end{equation}

単語翻訳確率の初期値として一様な確率を与えておき，式\ref{eq:trans_prob} 
と\ref{eq:all_align_prob}を計算して，正規化したアライメント回数
$\frac{p(\mathbf{a}, \mathbf{f}|\mathbf{e})}{p(\mathbf{f}|\mathbf{e})}$ 
をアライメント$\mathbf{a}$内の全ての単語対応に与える．次に単語翻訳確率を
最尤推定により求める．これを繰り返すことにより，単語翻訳確率を推定する．
なおこの計算は効率的に行うことができ，近似することなく最適なパラメータが
求められる．反対方向（$\mathbf{e}$ から$\mathbf{f}$へのモデル）も同様に求
めることができる．



\subsection{Step 2}

Step 2では句翻訳確率と依存関係確率の両方を推定する．また$\mathbf{f}$から
$\mathbf{e}$，$\mathbf{e}$から$\mathbf{f}$の二つのモデルを同時に用いて，
一つの方向性のないアライメントを得る．Step 1では計算を効率化することによ
り，近似を用いずにモデルの推定が完全に行えるが，Step 2では可能なアライメ
ントを全て考慮することは不可能である．そこで我々は最も良いアライメントを
探索するために，まず句翻訳確率のみから初期アライメントを生成し，その後依
存関係確率も考慮しつつ，山登り法によってアライメントを徐々に修正するとい
う方法をとる．

さらにStep 2において新たな句候補の生成を行う．新たな句候補は山登り法によっ
て求められた最も良いアライメントの状態から生成され，次のイタレーションか
ら考慮される．つまり，Step 2のイタレーションが進むに連れ，より大きな句の
対応を発見することができる．

全体として，Step 2の1回のイタレーションは，E-stepでの“初期アライメント”
の生成と“山登り法”により最適なアライメントの探索，E-stepとM-stepの間で
の新たな句候補の生成，M-stepでのパラメータの更新の4つの要素からなる．

Step 2での一回目のイタレーションでは，パラメータの初期値を以下のようにす
る．一回目のイタレーションにおいては全ての句は1単語からなるため（2単語以
上からなる句候補が獲得されていないため）句翻訳確率については，Step 1で求
めた単語翻訳確率をそのまま用いる．依存関係確率は，Step 1の最後のイタレー
ションで得られた最も良いアライメント結果において依存関係の生起回数を計数
し，そこから求めた確率を用いる．


\subsection*{初期アライメント(E-step)}
\label{initial_align}

依存関係確率は用いず，句翻訳確率のみから初期アライメントを生成する．全て
の句候補同士の対応（もしくはNULL対応）に対して，句対応確率を式
\ref{eq:phrase_alignment_prob}により計算する．これらの中から，句対応確率の
相乗平均が高いものから順に，対応として採用する．この際，各単語は1度しか
対応付かないようにする．つまりすでに採用されている対応と重なるような対応
は採用しない．なお句候補の生成については後で述べる．

初期アライメントが生成されたら，その状態でのアライメント確率を計算する．
このときから依存関係確率も用い，式\ref{eq:best_proposed}のように計算する．



\subsection*{山登り法(E-step)}

初期アライメントの状態から，依存関係確率を考慮しながらアライメントを修正
し，徐々に確率の高いアライメントを探索していく．修正手段としては以下の4
種類を考える．

\begin{figure}[t]
 \begin{center}
  \includegraphics{17-1ia6f3.eps}
 \end{center}
  \caption{山登り法によるアライメントの修正例}
  \label{fig:hillclimb}
\end{figure}

\begin{description}
 \item[Swap:] 任意の2つの対応に注目し，それらの対応を入れ替える．例えば
	    図\ref{fig:hillclimb}の最初の操作では，``光 
	    $\leftrightarrow$ photogate''と``フォト ゲート 
	    $\leftrightarrow$ photodetector''の対応がそれぞれ``光 
	    $\leftrightarrow$ photodetector''と``フォト ゲート 
	    $\leftrightarrow$ photogate''というように対応が入れ替えられ
	    ている．
 \item[Extend:] 任意の1つの対応に注目し，そのいずれかの言語における句を，
	    親または子方向に1ノード分だけ拡大する．
 \item[Add:] NULL対応となっている原言語側及び目的言語側のノード間に，新
	    たに対応を追加する．
 \item[Reject:] すでにある対応を削除し，それぞれNULL対応とする．
\end{description}

図\ref{fig:hillclimb}に山登り法によるアライメント修正過程の例を示す．な
お図\ref{fig:hillclimb}は1回以上イタレーションを行ったあとの状態である．
修正後のアライメント確率が修正前よりも高くなる場合にのみ修正を実行し，修
正された状態から再度修正を行っていく．確率が高くなる修正箇所がなくなるま
で修正を繰り返し行い，最終的に得られたアライメントが，最も確率の高いアラ
イメントとなる．なお修正の途中で得られたアライメントの状態を，確率の高い
ものから$n$ 個保存しておき，仮想的な$n$-bestアライメントとし，パラメータ
推定の際に利用する．




\subsection*{新たな句候補の生成}
\label{expand_step}

山登り法により得られた最も良いアライメント結果のうち，NULL対応となった単
語に注目する．NULL対応となった語の親，または子の単語がNULL対応でなければ，
その単語とNULL対応の単語とをまとめたものを新たに句として獲得し，Step 2の
次のイタレーションから探索範囲に入れる．例えば図\ref{fig:hillclimb}の最
終状態においてNULL対応となっている“素子”は，その子の対応である“受 光 
$\leftrightarrow$ photodetector”に含まれ，新たに“受 光 素子”という句
を作りだし，“受 光 素子 $\leftrightarrow$ photodetector”という対応があ
るものと考えるさらに親の対応である“に $\leftrightarrow$ for”に含まれ，
“素子 に”という句もつくり出し，“素子 に $\leftrightarrow$ for”という
対応があるものと考える．これらの新たに考慮される対応には，元の対応の出現
期待値（正規化されたアライメントの確率）を分配する．例えば図
\ref{fig:hillclimb}のアライメントの正規化された確率を0.7とすると，“受 
光 $\leftrightarrow$ photodetector”と“受 光 素子 $\leftrightarrow$
photodetector”にそれぞれ0.35ずつ，“に $\leftrightarrow$ for”と“素子 
に $\leftrightarrow$ for”にも0.35ずつ出現期待値を与える．

このように，NULL対応に注目することにより動的に句となるべきかたまりを獲得
していき，モデルの構築を行う．



\subsection*{モデル推定(M-step)}

一般的なEMアルゴリズムにおいては，得られたn-best アライメントのそれぞれ
のアライメント確率を正規化し，各アライメントにおけるパラメータの出現回数
をこの正規化された確率値（出現期待値）を用いて計数する．我々もこの方法に従
い，全ての対訳文での全てのアライメント結果を集めてパラメータの推定を行う．
ただし，正確に全てのアライメントを数え上げることはできないため，山登り法
の途中で得られたアライメントのうち，アライメントの確率の高いもの上位$n$ 
個（山登りの回数が$n$に満たない場合はその全て）を用いる．パラメータ推定は
各パラメータの出現期待値の総和を全体の回数で正規化することにより行われる．
例えば句翻訳確率は以下のような式により推定する：
\begin{equation}
 \label{eq:normal}
 p(F_j|E_i) = \frac{C(F_j, E_i)}{\sum_{k}C(F_k, E_i)},~~~ p(E_i|F_j) = \frac{C(F_j, E_i)}{\sum_{k}C(E_k, F_j)}
\end{equation}
ここで$C(F_j, E_i)$は$F_j$と$E_i$がアライメントされた回数である．

ここまでの処理により，EMアルゴリズムのE-step，M-stepが終了し，再びE-step 
に戻る．これを複数回繰り返すことにより，モデルのトレーニングを行う．



