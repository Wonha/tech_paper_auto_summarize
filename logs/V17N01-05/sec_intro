日本語と英語のように言語構造が著しく異なり，語順変化が大きな言語対において，対訳文をアライメントする際に重要なことは二つある．
一つは構文解析や依存構造解析などの言語情報をアライメントに組み込み，語順変化を克服することであり，もう一つはアライメントの手法が1対1の単語対応だけでなく，1対多や多対多などの句対応を生成できることである．
これは一方の言語では1語で表現されているものが，他方では2語以上で表現されることが少なくないからである．
しかしながら，既存のアライメント手法の多くは文を単純に単語列としてしか扱っておらず[CITE]，句対応は単語対応を行った後にヒューリスティックなルールにより生成するといった方法を取っている[CITE]．
Quirkら[CITE]やCowanら[CITE]はアライメントに構造情報を統合しようとしたが，前述の単語列アライメントを行った後に用いるに留まっている．
単語列アライメント手法そのものの精度が高くないため，このような方法では十分な精度でアライメントが行えるとは言い難い．
一方で，アライメントの最初から構造情報を利用する手法もいくつか提案されている．
Wata\-nabeら[CITE]やMenezesとRichardson [CITE]は構文解析結果を利用したアライメント手法を提案しているが，対応の曖昧性解消の際にヒューリスティックなルールを用いている．
YamadaとKnight [CITE]やGildea [CITE]は木構造を利用した確率的なアライメント手法を提案している．
これらの手法は一方の文の木構造に対して葉の並べ替え，部分木の挿入・削除といった操作を行って，他方の文構造を再現するものであるが，構文情報の利用が逆に強い制約となってしまい，文構造の再現が難しいことが問題となっている．
YamadaとKnightはいったん木構造を崩すことによって，Gildeaは部分木を複製することによってこの問題に対処している．
我々はこのような木構造に対する操作は不要であり，依存構造木中の部分木をそのままアライメントすればよいと考えた．
またCherryとLin [CITE]は原言語側の依存構造木を利用した識別モデルを提案している．
しかしながらこの手法はアライメント単位が単語のみであり，一対一対応しか扱えないという欠点がある．
phrase-based SMTでいうところの“句”はただの単語列に過ぎないが，NakazawaとKurohashi [CITE]は言語的な句をアライメントの最小単位とし，句の依存関係に着目したモデルを提案しているが，そこでは内容語は内容語のみ，機能語は機能語のみにしか対応しないという制約があり，また複数の機能語をひとまとまりに扱っているという問題もあり，これらがしばしば誤ったアライメントを生成している．
本論文ではNakazawaとKurohashiの手法の問題点を改善し，単語や句の依存関係に注目した句アライメントモデルを提案する．
提案手法のポイントは以下の3つである．
両言語とも依存構造解析し，アライメントの最初から言語の構造情報を利用する
アライメントの最小単位は単語だが，モデル学習時に句となるべき部分を自動的に推定し，句アライメントを行う
各方向（原言語[MATH]目的言語と目的言語[MATH]原言語）の生成モデルを二つ同時に利用することにより，より高精度なアライメントを行う
本モデルは二つの依存構造木において，一方の依存構造木で直接の親子関係にある一組の対応について，他方のそれぞれの対応先の依存関係をモデル化しており，単語列アライメントで扱うのが困難な距離の大きな語順変化にも対応することができる．
言い替えれば，本モデルは木構造上でのreorderingモデルということができる．
また本モデルはヒューリスティックなルールを用いずに，句となるべき部分を自動的に推定することができる．
ここでいう句とは必ずしも言語的な句である必要はなく，任意の単語のまとまりである．
ただし，Phrase-based SMTにおける句の定義との重要な違いは，我々は木構造を扱っており，単語列としては連続でなくても，木構造上で連続ならば句として扱っているという点である．
また我々のモデルはIBMモデルのような各方向の生成モデルを両方向分同時に用いてアライメントを行う．
これはアライメントの良さを両方向から判断する方が自然であり，Liangら[CITE]による報告にもあるように，そうした方が精度よいアライメントが行えるからである．
ただし，Liangらの手法がIBMモデルと同様に単語列を扱うものであるのに対し，提案手法は木構造を扱っているという重要な違いがある．
またLiangらの手法では部分的に双方向のモデルを結合するに留まっており，アライメントの結果としては各方向それぞれ独立に生成されるが，我々の方法ではただ一つのアライメントを生成するという違いもある．
最近の報告では生成モデルよりも識別モデルを用いた方がより高精度なアライメントが行えるという報告がなされているが，学習用にアライメントの正解セットを用意するコストがかかってしまう．
そこで我々は教師なしでモデル学習が行える生成モデルを用いた．
モデルは2つのステップを経て学習される．
Step 1では単語翻訳確率を学習し，Step 2では句翻訳確率と依存関係確率が推定される．
さらにStep 2では単語対応が句対応に拡張される．
各StepはEMアルゴリズムにより反復的に実行される．
次章では我々の提案するアライメントモデルを，IBMモデルと比較しながら定義する．
[REF_training]章ではモデルのトレーニングについて説明し，[REF_result]章では提案手法の有効性を示すために行った実験の結果と結果の考察を述べ，最後に結論と今後の課題を述べる．
日本語と英語のように言語構造が著しく異なり，語順変化が大きな言語対において，対訳文をアライメントする際に重要なことは二つある．
一つは構文解析や依存構造解析などの言語情報をアライメントに組み込み，語順変化を克服することであり，もう一つはアライメントの手法が1対1の単語対応だけでなく，1対多や多対多などの句対応を生成できることである．
これは一方の言語では1語で表現されているものが，他方では2語以上で表現されることが少なくないからである．
しかしながら，既存のアライメント手法の多くは文を単純に単語列としてしか扱っておらず[CITE]，句対応は単語対応を行った後にヒューリスティックなルールにより生成するといった方法を取っている[CITE]．
Quirkら[CITE]やCowanら[CITE]はアライメントに構造情報を統合しようとしたが，前述の単語列アライメントを行った後に用いるに留まっている．
単語列アライメント手法そのものの精度が高くないため，このような方法では十分な精度でアライメントが行えるとは言い難い．
一方で，アライメントの最初から構造情報を利用する手法もいくつか提案されている．
Wata\-nabeら[CITE]やMenezesとRichardson [CITE]は構文解析結果を利用したアライメント手法を提案しているが，対応の曖昧性解消の際にヒューリスティックなルールを用いている．
YamadaとKnight [CITE]やGildea [CITE]は木構造を利用した確率的なアライメント手法を提案している．
これらの手法は一方の文の木構造に対して葉の並べ替え，部分木の挿入・削除といった操作を行って，他方の文構造を再現するものであるが，構文情報の利用が逆に強い制約となってしまい，文構造の再現が難しいことが問題となっている．
YamadaとKnightはいったん木構造を崩すことによって，Gildeaは部分木を複製することによってこの問題に対処している．
我々はこのような木構造に対する操作は不要であり，依存構造木中の部分木をそのままアライメントすればよいと考えた．
またCherryとLin [CITE]は原言語側の依存構造木を利用した識別モデルを提案している．
しかしながらこの手法はアライメント単位が単語のみであり，一対一対応しか扱えないという欠点がある．
phrase-based SMTでいうところの“句”はただの単語列に過ぎないが，NakazawaとKurohashi [CITE]は言語的な句をアライメントの最小単位とし，句の依存関係に着目したモデルを提案しているが，そこでは内容語は内容語のみ，機能語は機能語のみにしか対応しないという制約があり，また複数の機能語をひとまとまりに扱っているという問題もあり，これらがしばしば誤ったアライメントを生成している．
本論文ではNakazawaとKurohashiの手法の問題点を改善し，単語や句の依存関係に注目した句アライメントモデルを提案する．
提案手法のポイントは以下の3つである．
両言語とも依存構造解析し，アライメントの最初から言語の構造情報を利用する
アライメントの最小単位は単語だが，モデル学習時に句となるべき部分を自動的に推定し，句アライメントを行う
各方向（原言語[MATH]目的言語と目的言語[MATH]原言語）の生成モデルを二つ同時に利用することにより，より高精度なアライメントを行う
本モデルは二つの依存構造木において，一方の依存構造木で直接の親子関係にある一組の対応について，他方のそれぞれの対応先の依存関係をモデル化しており，単語列アライメントで扱うのが困難な距離の大きな語順変化にも対応することができる．
言い替えれば，本モデルは木構造上でのreorderingモデルということができる．
また本モデルはヒューリスティックなルールを用いずに，句となるべき部分を自動的に推定することができる．
ここでいう句とは必ずしも言語的な句である必要はなく，任意の単語のまとまりである．
ただし，Phrase-based SMTにおける句の定義との重要な違いは，我々は木構造を扱っており，単語列としては連続でなくても，木構造上で連続ならば句として扱っているという点である．
また我々のモデルはIBMモデルのような各方向の生成モデルを両方向分同時に用いてアライメントを行う．
これはアライメントの良さを両方向から判断する方が自然であり，Liangら[CITE]による報告にもあるように，そうした方が精度よいアライメントが行えるからである．
ただし，Liangらの手法がIBMモデルと同様に単語列を扱うものであるのに対し，提案手法は木構造を扱っているという重要な違いがある．
またLiangらの手法では部分的に双方向のモデルを結合するに留まっており，アライメントの結果としては各方向それぞれ独立に生成されるが，我々の方法ではただ一つのアライメントを生成するという違いもある．
最近の報告では生成モデルよりも識別モデルを用いた方がより高精度なアライメントが行えるという報告がなされているが，学習用にアライメントの正解セットを用意するコストがかかってしまう．
そこで我々は教師なしでモデル学習が行える生成モデルを用いた．
モデルは2つのステップを経て学習される．
Step 1では単語翻訳確率を学習し，Step 2では句翻訳確率と依存関係確率が推定される．
さらにStep 2では単語対応が句対応に拡張される．
各StepはEMアルゴリズムにより反復的に実行される．
次章では我々の提案するアライメントモデルを，IBMモデルと比較しながら定義する．
[REF_training]章ではモデルのトレーニングについて説明し，[REF_result]章では提案手法の有効性を示すために行った実験の結果と結果の考察を述べ，最後に結論と今後の課題を述べる．
