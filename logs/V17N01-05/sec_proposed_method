以降の説明においては言語対として日本語と英語を用いるが，提案モデルはこの言語対に特別に設計されたものではなく，言語対によらないロバストなものである．
提案モデルは依存構造木上で定義されるものであるので，まず対訳文を両言語とも依存構造解析し，単語の依存構造木に変換する．
図[REF_fig:word-based-alignment]の一番右に依存構造木の例を示す．
単語は上から下に順に並んでおり，文のヘッドとなる単語は最も左側に位置している．
アライメントの最小単位はこれら各単語であるが，モデル推定時に複数単語のかたまりを句として自動的に獲得する．
これについては[REF_expand_step]章で詳しく述べる．
本章では，広く知られており，かつ一般的に用いられている統計的なアライメント手法であるIBMモデルと比較しながら，我々が提案するモデルについて説明する．
IBMモデル[CITE]では，与えられた日本語文[MATH]と英語文[MATH]からなる対訳文間の最も良いアライメント[MATH]は以下の式により獲得される：
ここで，[MATH]は語彙確率(lexicon probability)と呼ばれ，[MATH]はアライメント確率(alignment probability)と呼ばれている．
[MATH]が[MATH]語([MATH])からなり，[MATH]が[MATH]語([MATH])とNULL([MATH])からなるとする．
またアライメント[MATH]は[MATH]の各単語から[MATH]の単語への対応を表し，[MATH]は[MATH]が[MATH]に対応していることを示すとする．
このような条件の下，上記二つの確率は以下のように展開される：
p(\mathbf{f}|\mathbf{a}, \mathbf{e}) = \prod_{j=1}^{J} p(f_j|e_{a_j})
p(\mathbf{a}|\mathbf{e}) = \prod_{i=1}^{I} p(\Delta j|e_i)
ここで[MATH]は[MATH]に対応する[MATH]の単語の相対位置である．
式[REF_eq:lex]は単語翻訳確率の積であり，式[REF_eq:align]は相対位置確率の積となっている．
ただし，ここで示した式は正確にIBMモデルを記述しているわけではなく，その意図を簡単に示したものである．
また図[REF_fig:word-based-alignment]の左側にIBMモデルによるアライメントの例を示す．
IBMモデルは方向性があるため，アライメントに制限がある．
これを解消するため，両方向による結果を最後に統合して最終的なアライメントとすることが多い[CITE]．
しかし日英のような言語構造の違いの大きい言語対においては，このような方法では十分な精度でのアライメントは行えない．
提案モデルはIBMモデルを3つの点で改善する．
一つ目は式[REF_eq:lex]において，単語ではなく句を考慮する．
二つ目は式[REF_eq:align]において，文中での単語の位置ではなく，依存関係を考慮する．
最後に，提案モデルでは最も良いアライメント[MATH]を求める際に，片方向のモデルだけでなく，両方向のモデルを同時に利用する．
つまり，式[REF_eq:best]を以下のように変更する：
我々のモデルでは句を扱っているため，上式を素直に計算できる．
また式の上ではIBMモデルと同じ解が得られるはずであるが，それぞれの確率を近似するため，両方向を考慮した方がよりよい解が得られる．
図[REF_fig:word-based-alignment]の一番右に提案モデルによるアライメント例を示す．
従来手法のアライメントと比べると，多対多対応が自然と獲得されていることがわかる．
提案モデルはEMアルゴリズムにより学習される[CITE]．
目的関数として，与えられたデータに対する尤度を考える：
この尤度を最大化するようなパラメータ[MATH]を求める．
[MATH]は各方向のモデルにおけるパラメータをまとめたものとする．
E-stepでは現在のパラメータ[MATH]の下でのアライメントの事後確率を以下のように計算する：
M-stepではパラメータの更新を行う：
次節以降では，lexicon probabilitiyとalignment probabilitiyを定義する．
[MATH]が[MATH]個の句([MATH])からなり，[MATH]が[MATH]個の句([MATH])とNULL([MATH])からなるとする．
またアライメント[MATH]は[MATH]の各句から[MATH]の単句への対応を表し，[MATH]は句[MATH]が句[MATH]に対応していることを示すとする．
提案モデルでは，IBMモデルにおける単語翻訳確率[MATH]の代わりに，句翻訳確率[MATH]を考える．
ただし，2語以上からなる句はNULL対応にはならないという制限を加える（その句に含まれる各単語がNULL対応になるものとする）．
句翻訳確率を用いて，式[REF_eq:lex]を以下のように変更する：
ここで，句[MATH]と句[MATH]が対応付いたと仮定すると，この句の対応に寄与する句翻訳確率は，双方向分の句翻訳確率を掛け合わせるため以下のようになる：
この確率の積を句対応確率と呼ぶことにする．
表[REF_tab:sample_prob]の上部に図[REF_fig:word-based-alignment]の例における句対応確率を示す．
IBMモデルにおいて，単語の移動，すなわちreorderingモデルは，式[REF_eq:align]に示したように，一つ前の単語のアライメントとの相対位置によって定義されている．
これに対し提案モデルでは，単語の文内での位置ではなく，依存関係を考慮する．
まず[MATH]のある単語[MATH]と，[MATH]に係る単語[MATH]について考え，それらの可能なアライメントのうち，[MATH]が句[MATH]に属し，[MATH]が句[MATH]に属しており，[MATH]が[MATH]に係っているものを考える．
このような状況において，[MATH]と[MATH]の[MATH]での対応句[MATH]と[MATH]の関係をモデル化したものが依存関係確率である．
図[REF_fig:dpnd_prob]に例を示す．
日英などのように語順の大きく異なる言語対であっても，文内の単語や句の依存関係は多くの場合保存され，[MATH]が直接[MATH]に係ることが多い．
提案モデルはこのような傾向を考慮したものである．
直接の親子関係にある2単語が属する2句の対応先の句の関係は[MATH]のように記述することにし，これは[MATH]が属する句の対応先の句[MATH]から，[MATH]が属する句の対応先の句[MATH]への経路として定義される．
経路は以下のような表記に従って示される：
子ノードへ行く場合は`c'(child node)
親ノードへ行く場合は`p'(parent node)
2ノード以上離れている場合は，上記二つを並べて表記する
例えば図[REF_fig:word-based-alignment]において，``for''から``photodetector''への経路は`c'となり，``the''から``for''への経路は，2ノード離れているため`p;p'となる．
句同士の依存関係を記述する際には，経路上にある全ての句は，2つ以上の単語からなる句も含めて，すべて1つのノードとして扱う．
このため，図[REF_fig:word-based-alignment]において``photogate''から``the''への経路は`p;c;c;c'となる．
この[MATH]を用いて，式[REF_eq:align]を以下のように改善する：
ここで[MATH]は[MATH]の木構造において直接の親子関係にある全ての単語の組み合わせである．
また[MATH]を[MATH]方向の依存関係確率と呼ぶ．
[MATH]は木構造上でのreorderingモデルと考えることができる．
[MATH]にはいくつか特別な値がある．
まず[MATH]と[MATH]が同じ場合，つまり，[MATH]と[MATH]が同じ句に属する場合，[MATH]となる．
次にNULLアライメントに関してだが，これには[MATH]がNULL対応の場合，[MATH]がNULL対応の場合，両方ともNULL対応の場合の3通りがあり，それぞれ[MATH]の値は`NULL_p'，`NULL_c'，`NULL_b'となる．
例として表[REF_tab:sample_prob]の下部に図[REF_fig:word-based-alignment]の例における依存関係確率を各方向それぞれ示す．
一般的に，構文解析などにおいても，親ノードとの関係だけでなく，さらにその親のノードとの関係を考慮することは自然であり，精度の向上につながる．
提案モデルにおいても，直接の親子関係だけでなく，さらにその親ノードとの関係も考慮し，以下のように定式化する：
ここで[MATH]は[MATH]の木構造において祖父と子の関係にある全ての単語の組み合わせである．
[MATH]は直接の親子関係にある2単語を見たときの依存関係確率であり，[MATH]は親の親と子の関係にある2単語の場合の依存関係確率である．
なお，逆方向（[MATH]から[MATH]）のモデル[MATH]も全く同様に定義される．
提案モデルは2つのステップに分けて学習される．
これはIBMモデルにおいて，完全に最適解が求まる簡単なモデルからスタートし，徐々により複雑なモデルに移行することに対応する．
Step 1では単語翻訳確率の推定が行われ，Step 2では句翻訳確率と依存関係確率の推定が行われる．
どちらのステップにおいてもモデルはEMアルゴリズムにより学習される．
またステップ1においては句は扱わず，全て単語単位での学習となる．
複数単語の塊＝句はStep 2において自動的に獲得される．
Step 1では各方向独立に，単語翻訳確率を推定する．
これはIBM Model 1と全く同様の方法により行われる．
Step 1の推定の際には対応の単位は各ノード単体，つまり単語のみであり，句は考慮しない．
句はStep 2の推定から考慮し，句となるべき候補を動的に作り出すことにより実現する．
これはStep 1の段階で可能な句の候補全てを考慮すると，アライメント候補数が爆発し，扱えなくなるためである．
[MATH]から[MATH]へのアライメントを考えると，[MATH]の各単語は，他の単語に関係なく，[MATH]の任意の単語，またはNULLに対応することができる．
このことから，あるひとつの可能なアライメント[MATH]の確率は以下のように計算できる：
p(\mathbf{a}, \mathbf{f}|\mathbf{e}) & = p(\mathbf{f}|\mathbf{a}, \mathbf{e}) \cdot p(\mathbf{a}|\mathbf{e})
& = \prod_{j=1}^{J} p(f_j|e_{a_j}) \cdot C(n, m)
ここで[MATH]は全てのアライメントにおいて一定(uniform)であるとし，各文の単語数による関数[MATH]と置く．
さらに，全ての可能なアライメントを考慮すると，確率[MATH]は以下のように計算できる．
単語翻訳確率の初期値として一様な確率を与えておき，式[REF_eq:trans_prob]と[REF_eq:all_align_prob]を計算して，正規化したアライメント回数[MATH]をアライメント[MATH]内の全ての単語対応に与える．
次に単語翻訳確率を最尤推定により求める．
これを繰り返すことにより，単語翻訳確率を推定する．
なおこの計算は効率的に行うことができ，近似することなく最適なパラメータが求められる．
反対方向（[MATH]から[MATH]へのモデル）も同様に求めることができる．
Step 2では句翻訳確率と依存関係確率の両方を推定する．
また[MATH]から[MATH]，[MATH]から[MATH]の二つのモデルを同時に用いて，一つの方向性のないアライメントを得る．
Step 1では計算を効率化することにより，近似を用いずにモデルの推定が完全に行えるが，Step 2では可能なアライメントを全て考慮することは不可能である．
そこで我々は最も良いアライメントを探索するために，まず句翻訳確率のみから初期アライメントを生成し，その後依存関係確率も考慮しつつ，山登り法によってアライメントを徐々に修正するという方法をとる．
さらにStep 2において新たな句候補の生成を行う．
新たな句候補は山登り法によって求められた最も良いアライメントの状態から生成され，次のイタレーションから考慮される．
つまり，Step 2のイタレーションが進むに連れ，より大きな句の対応を発見することができる．
全体として，Step 2の1回のイタレーションは，E-stepでの“初期アライメント”の生成と“山登り法”により最適なアライメントの探索，E-stepとM-stepの間での新たな句候補の生成，M-stepでのパラメータの更新の4つの要素からなる．
Step 2での一回目のイタレーションでは，パラメータの初期値を以下のようにする．
一回目のイタレーションにおいては全ての句は1単語からなるため（2単語以上からなる句候補が獲得されていないため）句翻訳確率については，Step 1で求めた単語翻訳確率をそのまま用いる．
依存関係確率は，Step 1の最後のイタレーションで得られた最も良いアライメント結果において依存関係の生起回数を計数し，そこから求めた確率を用いる．
依存関係確率は用いず，句翻訳確率のみから初期アライメントを生成する．
全ての句候補同士の対応（もしくはNULL対応）に対して，句対応確率を式[REF_eq:phrase_alignment_prob]により計算する．
これらの中から，句対応確率の相乗平均が高いものから順に，対応として採用する．
この際，各単語は1度しか対応付かないようにする．
つまりすでに採用されている対応と重なるような対応は採用しない．
なお句候補の生成については後で述べる．
初期アライメントが生成されたら，その状態でのアライメント確率を計算する．
このときから依存関係確率も用い，式[REF_eq:best_proposed]のように計算する．
初期アライメントの状態から，依存関係確率を考慮しながらアライメントを修正し，徐々に確率の高いアライメントを探索していく．
修正手段としては以下の4種類を考える．
任意の2つの対応に注目し，それらの対応を入れ替える．
例えば図[REF_fig:hillclimb]の最初の操作では，``光[MATH] photogate''と``フォトゲート[MATH] photodetector''の対応がそれぞれ``光[MATH] photodetector''と``フォトゲート[MATH] photogate''というように対応が入れ替えられている．
任意の1つの対応に注目し，そのいずれかの言語における句を，親または子方向に1ノード分だけ拡大する．
NULL対応となっている原言語側及び目的言語側のノード間に，新たに対応を追加する．
すでにある対応を削除し，それぞれNULL対応とする．
図[REF_fig:hillclimb]に山登り法によるアライメント修正過程の例を示す．
なお図[REF_fig:hillclimb]は1回以上イタレーションを行ったあとの状態である．
修正後のアライメント確率が修正前よりも高くなる場合にのみ修正を実行し，修正された状態から再度修正を行っていく．
確率が高くなる修正箇所がなくなるまで修正を繰り返し行い，最終的に得られたアライメントが，最も確率の高いアライメントとなる．
なお修正の途中で得られたアライメントの状態を，確率の高いものから[MATH]個保存しておき，仮想的な[MATH]-bestアライメントとし，パラメータ推定の際に利用する．
山登り法により得られた最も良いアライメント結果のうち，NULL対応となった単語に注目する．
NULL対応となった語の親，または子の単語がNULL対応でなければ，その単語とNULL対応の単語とをまとめたものを新たに句として獲得し，Step 2の次のイタレーションから探索範囲に入れる．
例えば図[REF_fig:hillclimb]の最終状態においてNULL対応となっている“素子”は，その子の対応である“受光[MATH] photodetector”に含まれ，新たに“受光素子”という句を作りだし，“受光素子[MATH] photodetector”という対応があるものと考えるさらに親の対応である“に[MATH] for”に含まれ，“素子に”という句もつくり出し，“素子に[MATH] for”という対応があるものと考える．
これらの新たに考慮される対応には，元の対応の出現期待値（正規化されたアライメントの確率）を分配する．
例えば図[REF_fig:hillclimb]のアライメントの正規化された確率を0.7とすると，“受光[MATH] photodetector”と“受光素子[MATH] photodetector”にそれぞれ0.35ずつ，“に[MATH] for”と“素子に[MATH] for”にも0.35ずつ出現期待値を与える．
このように，NULL対応に注目することにより動的に句となるべきかたまりを獲得していき，モデルの構築を行う．
一般的なEMアルゴリズムにおいては，得られたn-bestアライメントのそれぞれのアライメント確率を正規化し，各アライメントにおけるパラメータの出現回数をこの正規化された確率値（出現期待値）を用いて計数する．
我々もこの方法に従い，全ての対訳文での全てのアライメント結果を集めてパラメータの推定を行う．
ただし，正確に全てのアライメントを数え上げることはできないため，山登り法の途中で得られたアライメントのうち，アライメントの確率の高いもの上位[MATH]個（山登りの回数が[MATH]に満たない場合はその全て）を用いる．
パラメータ推定は各パラメータの出現期待値の総和を全体の回数で正規化することにより行われる．
例えば句翻訳確率は以下のような式により推定する：
ここで[MATH]は[MATH]と[MATH]がアライメントされた回数である．
ここまでの処理により，EMアルゴリズムのE-step，M-stepが終了し，再びE-stepに戻る．
これを複数回繰り返すことにより，モデルのトレーニングを行う．
