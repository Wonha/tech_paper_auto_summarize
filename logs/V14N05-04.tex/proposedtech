    \documentclass[japanese]{jnlp_1.3e}
\usepackage{jnlpbbl_1.1}
\usepackage{amsmath}

\usepackage{graphicx}

\Volume{14}
\Number{5}
\Month{Oct.}
\Year{2007}
\received{2007}{2}{26}
\revised{2007}{4}{7}
\accepted{2007}{5}{24}

\setcounter{page}{107}

\jtitle{NMFによる重み付きハイパーグラフを用いた\\
	アンサンブル文書クラスタリング}
\jauthor{新納　浩幸\affiref{Author_1} \and 佐々木　稔\affiref{Author_1}}
\jabstract{
	本論文では Non-negative Matrix Factorization (NMF) を利用したアンサン
ブル文書クラスタリングを提案する．

NMF は次元縮約を利用したクラスタリング手法であり，文書クラスタリングの
ようにデータが高次元かつスパースとなる場合に効果を発揮する．ただし NMF
は初期値によって得られるクラスタリング結果が異なるという問題がある．
そのために通常は初期値を様々に変えて，複数個得られたクラスタリング結果から，NMF の分解の精度
の最もよい結果を選択する．しかし NMF の分解の精度はクラスタリング結果の精度を直
接表しているわけではないので，最適な選択が行える保証はない．

ここでは NMF によるクラスタリングの精度を高めるために，
複数個得られたクラスタリング結果をアンサンブルすることを試みる．
アンサンブルは，複数個のクラスタリング結果からハイパーグラフを作成し，そのハイパーグラフ
で表現されたデータをクラスタリングすることで行える．従来，そのハイパーグラフ
は 0 か 1 のバイナリ値が用いられていたが，ここでは NMF の結果を用いて，適
切な実数値の重みを与えることで改良する．実験では k-means，NMF，通常のハイ
パーグラフを用いたアンサンブル手法および重み付きハイパーグラフを用いたア
ンサンブル手法（本手法）のクラスタリング結果を比較し，本手法の有効性を
示す．
}
\jkeywords{アンサンブルクラスタリング，NMF，ハイパーグラフ，局所解，アンサンブル学習}

\etitle{Ensemble Document Clustering Using Weighted Hypergraph Generated by NMF}
\eauthor{Hiroyuki Shinnou\affiref{Author_1} \and Minoru Sasaki\affiref{Author_1}} 
\eabstract{
	In this paper, we propose a new ensemble clustering method using
Non-negative Matrix Factorization (NMF).

NMF is a kind of the dimensional reduction method which is effective
for high dimensional and sparse data like document data.  
NMF has the problem that the result depends on the initial value of
the iteration.  The standard countermeasure for this problem is that
we generate multiple clustering results by changing the initial value,
and then select the best clustering result estimated by 
the NMF decomposition error.  However, this selection does not work well because 
the NMF decomposition error does not always measure the accuracy of the clustering.

To improve the clustering result of NMF, we propose a new ensemble clustering
method.  Our method generates multiple clustering results by using the
random initialization of NMF.  And they are integrated through the
weighted hypergraph, which can directly be constructed through the result
of NMF, instead of the traditional binary hypergraph.

In the experiment, we compared the k-means, NMF, the ensemble method
using the standard hypergraph and the ensemble method using the weighted
hypergraph (our method).  Our method achieved best.
}
\ekeywords{ensemble clustering, NMF, hypergraph, local optimum solution, ensemble learning}

\headauthor{新納，佐々木}
\headtitle{NMFによる重み付きハイパーグラフを用いたアンサンブル文書クラスタリング}

\affilabel{Author_1}{茨城大学工学部情報工学科}{Department of Computer and Information Sciences, Ibaraki University}

\begin{document}
\maketitle


\section{NMF と初期値の問題}


\subsection{NMF とその特徴}

NMF は\( m \times n \)の索引語文書行列\( X \)を，\( m \times k \)の行列\( U \)と
\( n \times k \)の行列\( V \)の転置行列\( V^{T} \)の積に分解する\cite{nmf}．
ただし\( k \)はクラスタ数である．
\[
X = U V^{T}   
\]

NMF はクラスタに対応したトピックの次元を\( k \)個想定し，その基底ベクトルの線形和によって，
文書ベクトル及び索引語ベクトルを表現することに対応する．
つまり基底ベクトルの係数が，そのトピックとの関連度を表しているので，
行列\( V \)自体がクラスタリング結果と見なせる．
具体的には，\( i \)番目の文書\( d_i \)は，
行列\( X \)の第\( i \)列のベクトルで表現され，
その次元圧縮された結果が，行列\( V \)の第\( i \)行のベクトルとなる．
このとき，\( V \)の第\( i \)行のベクトルは
\[
(v_{i1}, v_{i2}, \cdots, v_{ik})
\]
と表せ，文書\( d_i \)のクラスタの番号は
\[
\arg \max_{j \in 1:k} v_{ij}
\]
となる．

\subsection{NMF のアルゴリズム}

与えられた索引語文書行列\( X \)から，\( U \)と\( V \)は
以下の繰り返しで得ることができる\cite{lee00algorithms}． 
\begin{gather}
  \label{eq:1}
 u'_{ij} \leftarrow u_{ij} \frac{(XV)_{ij}}{(UV^{T}V)_{ij}}   \\
  \label{eq:2}
 v'_{ij} \leftarrow v_{ij} \frac{(X^{T}U)_{ij}}{(VU^{T}U)_{ij}} 
\end{gather}

ここで \( u_{ij} \)と\( v_{ij} \)はそれぞれ\( U \)と\( V \)の\( i \)行\( j \)列の
要素を表す．また \( (X)_{ij} \) により行列\( X \)の\( i \)行\( j \)列の要素を表す．
上記の式により，現在の\( U \)と\( V \)から，\( u'_{ij} \)と\( v'_{ij} \) が得られる，
つまり新たな\( U' \)と\( V' \)が得られるので，それを\( U \)と\( V \)と見なして，
上記の式を繰り返し適用する．

また各繰り返しの後に\( U \)を以下のように正規化する．
\begin{equation}
 u'_{ij} \leftarrow \frac{u_{ij}}{\sqrt{\sum_{i} u_{ij}^2}} 
\end{equation}

繰り返しの終了は，繰り返しの最大回数を決めておくか，
\( UV^{T} \)と\( X \)との距離\( J \)の変化量から判定する．
\begin{equation}
  \label{eq:3}
J = || X - UV^{T} ||_{F}           
\end{equation}

\( J \)の値は NMF の分解の精度を表現している．NMF ではこの分解の精度が
クラスタリングの目的関数となっており，この分解の精度が高い，つまり\( J \)の値が
小さいほど，良好なクラスタリングであると推定する．

また\( || \cdot ||_{F} \)は Frobenius ノルムを表し，\( m \times n \) の行列\( A \)の
Frobenius ノルムは以下で定義される．
\[
|| A ||_{F} = \sqrt{\sum_{i = 1}^{m} \sum_{j = 1}^{n} {a_{ij}}^2}
\]


\subsection{NMF の解の多様性}

通常，行列\( V \)と\( U \)の初期値にはランダムな値を与える．
しかし\mbox{式\ref{eq:1}と\ref{eq:2}}による繰り返しは局所最適解にしか収束しないために，
\( V \)と\( U \)の初期値の与え方によって，最終的に得られる\( V \)と\( U \)は大きく異なり，
結果としてクラスタリングの精度も大きく異なる．

例えば，\mbox{図\ref{tr45a}}は本論文の実験で用いた文書データセット tr45 に対して，
NMF によるクラスタリングの実験を20回行った結果である．
ただし各実験での NMF の初期値にはランダムな値を与えており，
各実験の初期値は異なる．
\mbox{図\ref{tr45a}}の横軸は実験の番号を示し，縦軸はクラスタリングの精度を表している．
\mbox{図\ref{tr45a}}から初期値によって得られる精度が大きく異なることが確認できる．

\begin{figure}[t]
\begin{center}
\includegraphics{14-5ia4f1.eps}
\caption{初期値とクラスタリングの精度}\label{tr45a}
\end{center}
\end{figure}

つまり，NMF は初期値によって得られるクラスタリング結果が異なる．
通常は適当な初期値を与える実験を複数回行い，
それらから得た複数個の解の中で\( X \)の分解の精度が最も高いものを選ぶ．
しかし分解の精度は，直接的にはクラスタリングの精度を意味していないため，
最も精度の高いクラスタリング結果を選択できる保証がない．

ここでは複数個のクラスタリング結果から1つを選択するのではなく，
それらをアンサンブルするアンサンブルクラスタリングを試みる．


\section{アンサンブルクラスタリング}


\subsection{ハイパーグラフによるデータの再表現}

本手法のアンサンブルクラスタリングでは，NMF の初期値を様々に変化させて，
複数個のクラスタリング結果を生成する．次に複数個得られたクラスタリング結果から
各データに対するベクトル表現を新たに作成し，その新たにベクトル表現されたデータに
対してクラスタリングを行うことで，アンサンブルクラスタリングを実現する．

ここでは複数個得られたクラスタリング結果からデータに対する新たなベクトル表現を
作る方法を説明する．基本的には論文\cite{strehl02}で提案された
ハイパーグラフを用いる．

クラスタの数が\( k \)個であり，得られているクラスタリング結果が\( m \)種類の場合，
各データは\( k m \)次元のベクトルで表現される．
データ\( d \)の\( k (i - 1) + c \)次元の値は，
\( i \)番目のクラスタリング結果として，データ\( d \)が
クラスタ番号\( c \)のクラスタに属していれば 1 を，属していなければ 0 を与える．
この結果，データ\( d \)の\( k m \)次元のベクトル表現が得られる．

例を示す．\( k = 3\)，\( m = 4\)とする．またデータは\( \{ d_1,d_2, \cdots, d_7 \} \) の
7つとする．4種類のクラスタリング結果が以下のようになっていたとする．

第1のクラスタリング結果：
\[
\{ d_1,d_2,d_5 \}, \{ d_3,d_4 \}, \{ d_6,d_7 \}
\]
この結果から目的の行列の1列目から3列目が得られる．
\[
                        \begin{array}{c}
                                 d_1\\
                                 d_2\\
                                 d_3\\
                                 d_4\\
                                 d_5\\
                                 d_6\\
                                 d_7\\
                        \end{array}
\left[
                        \begin{array}{rrr}
                                 1& 0& 0\\
                                 1& 0& 0\\
                                 0& 1& 0\\
                                 0& 1& 0\\
                                 1& 0& 0\\
                                 0& 0& 1\\
                                 0& 0& 1
                        \end{array}
\right]
\]

第2のクラスタリング結果：
\[
\{ d_1,d_5 \}, \{ d_2,d_3 \}, \{ d_4,d_6,d_7 \}
\]
この結果から目的の行列の4列目から6列目が得られる．
\[
                        \begin{array}{c}
                                 d_1\\
                                 d_2\\
                                 d_3\\
                                 d_4\\
                                 d_5\\
                                 d_6\\
                                 d_7\\
                        \end{array}
\left[
                        \begin{array}{rrr}
                                 1& 0& 0\\
                                 0& 1& 0\\
                                 0& 1& 0\\
                                 0& 0& 1\\
                                 1& 0& 0\\
                                 0& 0& 1\\
                                 0& 0& 1
                        \end{array}
\right]
\]


第3のクラスタリング結果：
\[
\{ d_2,d_5 \}, \{ d_1, d_4 \}, \{ d_3, d_6,d_7 \}
\]
この結果から目的の行列の7列目から9列目が得られる．
\[
                        \begin{array}{c}
                                 d_1\\
                                 d_2\\
                                 d_3\\
                                 d_4\\
                                 d_5\\
                                 d_6\\
                                 d_7\\
                        \end{array}
\left[
                        \begin{array}{rrr}
                                 0& 1& 0\\
                                 1& 0& 0\\
                                 0& 0& 1\\
                                 0& 1& 0\\
                                 1& 0& 0\\
                                 0& 0& 1\\
                                 0& 0& 1
                        \end{array}
\right]
\]


第4のクラスタリング結果：
\[
\{ d_1,d_5,d_7 \}, \{ d_3,d_4 \}, \{ d_2, d_6 \}
\]
この結果から目的の行列の10列目から12列目が得られる．
\[
                        \begin{array}{c}
                                 d_1\\
                                 d_2\\
                                 d_3\\
                                 d_4\\
                                 d_5\\
                                 d_6\\
                                 d_7\\
                        \end{array}
\left[
                        \begin{array}{rrr}
                                 1& 0& 0\\
                                 0& 0& 1\\
                                 0& 1& 0\\
                                 0& 1& 0\\
                                 1& 0& 0\\
                                 0& 0& 1\\
                                 1& 0& 0
                        \end{array}
\right]
\]

以上の4つの行列を結合させ，以下の\( 7 \times 12 \)の行列を得る．
これがハイパーグラフである．このハイパーグラフにおける行ベクトルが，
各データ（本論文の場合，文書）の新たなベクトル表現に対応している．
このベクトルの類似度に基づいて，データをクラスタリングする．
\[
                        \begin{array}{c}
                                 d_1\\
                                 d_2\\
                                 d_3\\
                                 d_4\\
                                 d_5\\
                                 d_6\\
                                 d_7\\
                        \end{array}
\left[
                        \begin{array}{rrrrrrrrrrrr}
 1& 0& 0 &  1& 0& 0 & 0& 1& 0 & 1& 0& 0 \\
 1& 0& 0 &  0& 1& 0 & 1& 0& 0 & 0& 0& 1 \\
 0& 1& 0 &  0& 1& 0 & 0& 0& 1 & 0& 1& 0 \\
 0& 1& 0 &  0& 0& 1 & 0& 1& 0 & 0& 1& 0 \\
 1& 0& 0 &  1& 0& 0 & 1& 0& 0 & 1& 0& 0 \\
 0& 0& 1 &  0& 0& 1 & 0& 0& 1 & 0& 0& 1 \\
 0& 0& 1 &  0& 0& 1 & 0& 0& 1 & 1& 0& 0
                        \end{array}
\right]
\]

\subsection{重み付きハイパーグラフ}

ハイパーグラフが表す行列の各要素の値は 0 か 1 のバイナリ値である．
しかし値の意味を考えれば，その次元に対応する
あるクラスタリング結果のあるクラスタに属する度合いと捉えられる．
そのため 0 か 1 のバイナリ値ではなく，非負の実数値を与える方が適切である．

しかも NMF の場合，各クラスタリング結果では各クラスタに属する度合いに
対応する値が行列\( V \)に記載されている．
そこでここではハイパーグラフの要素が 1 である部分を，
行列\( V \)の値から得ることで，非負の実数値を与えることにした．
このようにして作成したハイパーグラフを，ここでは重み付きハイパーグラフと呼ぶ．

\mbox{図\ref{ensemble}}に重み付きハイパーグラフの作成例を示す．これは先の
第1のクラスタリング結果に対応する部分である．
\( d_1 \) から \( d_7 \)の7個の文書データセットを
NMF により3グループにクラスタリングする．結果は行列\( V \)で表される．
次に行列\( V \)を正規化する．\( V \)の各行に注目し，最大値の部分を 1に，それ以外を
0 に変換したものが通常のハイパーグラフである．
\( V \)の各行に注目し，最大値の部分はそのままに，それ以外を
0 に変換したものが本論文で提案する重み付きハイパーグラフである．

\begin{figure}[tbp]
\begin{center}
\includegraphics{14-5ia4f2.eps}
\caption{行列 V から作られる重み付きハイパーグラフ}\label{ensemble}
\end{center}
\end{figure}



\section{考察と関連研究}



一般に複数の解をアンサンブルすると，複数の解の平均よりも良い値が得られると考えられる．
本実験でも 18 個のデータセット中 17 個でアンサンブルの効果が得られているが，
データセット tr23 に関しては，本手法のエントロピーの値の方が高い．
これは解の分散の影響と考えられる．

実験で得られた各データセットに対する NMF による 20 個のクラスタリング結果の
エントロピーの分散と，\mbox{表\ref{tab:result}}における
NMF mean と weighted hypergarph との差（つまりアンサンブルによる改善の度合い）を
プロットした図を\mbox{図\ref{kou}}に示す．図の横軸が分散を示し，
縦軸がweighted hypergarph と NMF mean との差（改善の度合い）を示している．

\begin{figure}[tbp]
\begin{center}
\includegraphics{14-5ia4f3.eps}
\caption{解の分散とアンサンブルによる改善}\label{kou}
\end{center}
\end{figure}

\mbox{図\ref{kou}}をみると，分散が大きい2つ（cranmad と reviews）は，
アンサンブルによる改善の度合いも大きいことが分かる．
そして3番目に分散が大きなデータセットが tr23 である．
つまり分散の大きな解をアンサンブルすると，非常に良い結果を
得ることもあるが，逆に悪い結果を得ることもあり得ると考えられる．

データセット tr23 に対する NMF の結果を見ると，1つだけ非常にエントロピーの
低いクラスタリング結果が得られていた．この解を取り除いて，
19個のクラスタリング結果で本手法によるアンサンブルを試したところ，
NMF mean のエントロピーは 0.493，weighted hypergarph のエントロピーは 0.492 となり，
アンサンブルの効果が現れた．

また，ここでは NMF で複数個のクラスタリング結果を生成する際に，
個々のクラスタリング結果のクラスタ数は，最終的な
クラスタ数と一致させている．
しかしハイパーグラフの考え方を用いれば，
生成される個々のクラスタリング結果のクラスタ数は任意でかまわない．
実際に k-means では少ないクラスタ数に直接クラスタリングするよりも，
多数のクラスタに分割してから，目的のクラスタ数にまとめた方が
効果があることが経験的にわかっている．
論文\cite{fred02data}ではこのヒューリスティクスを利用して，
多数のクラスタに分割してから，アンサンブルを行っている．
本手法においても，そのような工夫を取り入れることも可能である．


本手法ではハイパーグラフの値として，1 に当たる部分を行列\( V \)の
値を用いることで，実数値に変換した．
この効果は実験で確認できている．
この工夫を更に進めると，0 に当たる部分にも行列\( V \)
の値を用いることで，実数値に変換することが考えられる．
この場合，ハイパーグラフは単純に各クラスタリング結果に
対応する行列\( V \)を結合させたものになる．
実際にこのようにして作ったハイパーグラフに対して，クラスタリングを
行ってみた．結果を表\ref{tab:vresult}に示す．
ここで hypergraph V が行列\( V \)を結合させてハイパーグラフを作成する手法を示す．

\begin{table}[tbp]
\input{04t3.txt}
\end{table}

通常のハイパーグラフを使うよりも結果は良好であるが，
1 に当たる部分だけを精密化する方が効果があることがわかる．
また 0 の値はそのままにしている方が，ハイパーグラフがスパースになり，
データ間の類似度が 0 であるケースが生じやすくなる．
そのためグラフスペクトル理論を用いたクラスタリング手法\cite{graph-minmax-cut}なども使えるように
なるために好ましい．

最後にアンサンブル学習\cite{breiman96bagging}との関連について述べる．
アンサンブル学習とアンサンブルクラスタリングの違いは，
クラスタにラベルがつくかどうかである．
アンサンブル学習ではデータにラベルが付くので，
そのラベルをもつデータがラベル付きのクラスタと見なせる．
アンサンブルクラスタリングの場合は，クラスタにラベルがついていない．
もしもクラスタにラベルをつけることができれば，
アンサンブル学習の手法を直接利用できるために，
さらなる改良や発展が可能である．
クラスタにラベルをつける処理は，
クラスタ数が 2 や 3 などの小さい場合はそれほど大きな問題ではないので，
今後はクラスタにラベルをつけるという戦略で，アンサンブルを行う手法を開発したい．


\end{document}
