一般に複数の解をアンサンブルすると，複数の解の平均よりも良い値が得られると考えられる．
本実験でも18個のデータセット中17個でアンサンブルの効果が得られているが，データセットtr23に関しては，本手法のエントロピーの値の方が高い．
これは解の分散の影響と考えられる．
実験で得られた各データセットに対するNMFによる20個のクラスタリング結果のエントロピーの分散と，表[REF_tab:result]におけるNMF meanとweighted hypergarphとの差（つまりアンサンブルによる改善の度合い）をプロットした図を図[REF_kou]に示す．
図の横軸が分散を示し，縦軸がweighted hypergarphとNMF meanとの差（改善の度合い）を示している．
図[REF_kou]をみると，分散が大きい2つ（cranmadとreviews）は，アンサンブルによる改善の度合いも大きいことが分かる．
そして3番目に分散が大きなデータセットがtr23である．
つまり分散の大きな解をアンサンブルすると，非常に良い結果を得ることもあるが，逆に悪い結果を得ることもあり得ると考えられる．
データセットtr23に対するNMFの結果を見ると，1つだけ非常にエントロピーの低いクラスタリング結果が得られていた．
この解を取り除いて，19個のクラスタリング結果で本手法によるアンサンブルを試したところ，NMF meanのエントロピーは0.493，weighted hypergarphのエントロピーは0.492となり，アンサンブルの効果が現れた．
また，ここではNMFで複数個のクラスタリング結果を生成する際に，個々のクラスタリング結果のクラスタ数は，最終的なクラスタ数と一致させている．
しかしハイパーグラフの考え方を用いれば，生成される個々のクラスタリング結果のクラスタ数は任意でかまわない．
実際にk-meansでは少ないクラスタ数に直接クラスタリングするよりも，多数のクラスタに分割してから，目的のクラスタ数にまとめた方が効果があることが経験的にわかっている．
論文[CITE]ではこのヒューリスティクスを利用して，多数のクラスタに分割してから，アンサンブルを行っている．
本手法においても，そのような工夫を取り入れることも可能である．
本手法ではハイパーグラフの値として，1に当たる部分を行列[MATH]の値を用いることで，実数値に変換した．
この効果は実験で確認できている．
この工夫を更に進めると，0に当たる部分にも行列[MATH]の値を用いることで，実数値に変換することが考えられる．
この場合，ハイパーグラフは単純に各クラスタリング結果に対応する行列[MATH]を結合させたものになる．
実際にこのようにして作ったハイパーグラフに対して，クラスタリングを行ってみた．
結果を表[REF_tab:vresult]に示す．
ここでhypergraph Vが行列[MATH]を結合させてハイパーグラフを作成する手法を示す．
通常のハイパーグラフを使うよりも結果は良好であるが，1に当たる部分だけを精密化する方が効果があることがわかる．
また0の値はそのままにしている方が，ハイパーグラフがスパースになり，データ間の類似度が0であるケースが生じやすくなる．
そのためグラフスペクトル理論を用いたクラスタリング手法[CITE]なども使えるようになるために好ましい．
最後にアンサンブル学習[CITE]との関連について述べる．
アンサンブル学習とアンサンブルクラスタリングの違いは，クラスタにラベルがつくかどうかである．
アンサンブル学習ではデータにラベルが付くので，そのラベルをもつデータがラベル付きのクラスタと見なせる．
アンサンブルクラスタリングの場合は，クラスタにラベルがついていない．
もしもクラスタにラベルをつけることができれば，アンサンブル学習の手法を直接利用できるために，さらなる改良や発展が可能である．
クラスタにラベルをつける処理は，クラスタ数が2や3などの小さい場合はそれほど大きな問題ではないので，今後はクラスタにラベルをつけるという戦略で，アンサンブルを行う手法を開発したい．
