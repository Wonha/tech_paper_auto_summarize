================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[3686] NB，CNB，サポートベクターマシン(SVM)と比較したところ，特に一文書当たりの単語数が減り，クラスごとの文書数が偏る場合において，NNBが他のBayesianアプローチより勝る手法であること，また，時にはSVMを有意に上回り，比較手法中で最も良い分類正解率を示す手法であることが分かった．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[3490] その結果，クラスごとの単語数（トークン数）が少なく，なおかつクラス間の文書数に大きなばらつきがある場合には分類正解率がNB，CNBをカイ二乗検定で有意に上回ること，また，これらの条件が特に十分に当てはまる場合には，事前確率を無視したCNBも同検定で有意に上回ることを示す．

================================================================
[section type  : related_study]
[section title : 関連研究]
================================================================
[2661] 本研究では，多項モデルを用いたNBとCNBに注目し，その分類における特徴を考慮して，Bayesのアプローチを用いた新しい分類手法NNBを提案する．

================================================================
[section type  : proposed_method]
[section title : Negation Naive Bayesの導出]
================================================================
[2991] その結果，事前確率を数学的に正しく考慮することで，クラスごとの文書数が異なっているときにもより正確な文書分類を行えるようにした．
-----------------------------------------------------
  [subsection title : Naive Bayes分類器]
-----------------------------------------------------
  [2796] 一般に確率モデルによる文書分類では，分類対象となる文書を[MATH]，ある一つのクラスを[MATH]としたとき，事後確率[MATH]を最大化するクラス[MATH]を求める[CITE]．
-----------------------------------------------------
  [subsection title : Complement Naive Bayes分類器]
-----------------------------------------------------
  [2788] 多項モデルを用いたNB分類器では，クラス間の文書数に大きなばらつきがある場合に，文書数の小さいクラスで[MATH]が大きくなる傾向がある．
-----------------------------------------------------
  [subsection title : Negation Naive Bayes分類器]
-----------------------------------------------------
  [3152] なお，Rennieらの研究では，式([REF_eq:cnb])において事前確率の扱いについてあまり注意を払っていないが，我々はクラスごとに単語数の偏りが大きいデータセットについて分類を行う場合には，[MATH]を利用するか[MATH]を利用するかの影響は必ずしも無視して良いとは言えないと考える．

================================================================
[section type  : experiment_result]
[section title : 実験]
================================================================
[2545] また，NBBとCNBの差は事前確率[MATH]と[MATH]の部分であるため，CNBとNNBから第一項を省略した形の式（[MATH]なしのCNB）でも実験を行った．
-----------------------------------------------------
  [subsection title : オークションの商品分類の実験]
-----------------------------------------------------
  [2865] \tabref{Tab:オークションのカテゴリ分類実験のデータ}のこの「訓練事例中の商品数の標準偏差／平均」を見てみると，「記念切手」の値が「デスクトップ」や「赤ちゃん用の玩具」より高いことから，\figref{Fig:pc}〜\figref{Fig:toy}から読みとったように，「記念切手」が最も偏っていることが読みとれる．
-----------------------------------------------------
  [subsection title : ニュースグループの文書分類の実験]
-----------------------------------------------------
  [3213] NNBは事前確率を数学的に正しく考慮しているため，文書分類ではクラスごとの文書数が不均一である際に効果を発揮すると考えられる．

================================================================
[section type  : proposed_method]
[section title : 結果]
================================================================
[3609] また，\tabref{Tab:一文書あたりの単語数を減らした実験の分類正解率}と\tabref{Tab:クラスごとの文書数を不均一にした実験の分類正解率}から，ニュースグループの分類実験においても，マイクロ平均で比較した場合，常にNBBがNBとCNBを上回ることが分かる．

================================================================
[section type  : conclusion]
[section title : 考察]
================================================================
[3884] まず，\tabref{Tab:全ての品詞を使用したオークションのカテゴリ分類実験の分類正解率}〜\tabref{Tab:クラスごとの文書数を不均一にした実験の分類正解率（マクロ）}から，全ての実験を通して，NNBはNBとCNBを上回っていること，また[MATH]なしのCNBに有意に勝っていることはあっても有意に負けていることはないことが読みとれる．

================================================================
[section type  : conclusion]
[section title : まとめ]
================================================================
[3408] その結果，すべての実験においてNNBとCNBがNBの分類性能を上回ること，また，一文書当たりの単語数が減り，クラスごとの文書数が偏るときにマイクロ平均でNNBはCNBを上回ることが分かった．

