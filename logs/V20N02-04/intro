文書分類においてNaive Bayes (NB)を利用するのは極めて一般的である．
しかし，多項モデルを用いたNB分類器では，クラス間の文書数に大きなばらつきがある場合に，大きく性能が下がるという欠点があった．
そのため，[CITE]は「クラスに属する文書」ではなく「クラスに属さない文書」，つまり「補集合」を用いることによりNBの欠点を緩和したComplement Naive Bayes (CNB)を提唱した．
しかし，CNBはNBと同じ式，つまり事後確率最大化の式から導くことができない．
そこで我々は，事後確率最大化の式から導くことのできるNegation Naive Bayes (NNB)を提案し，その性質を他のBayesianアプローチと比較した．
その結果，クラスごとの単語数（トークン数）が少なく，なおかつクラス間の文書数に大きなばらつきがある場合には分類正解率がNB，CNBをカイ二乗検定で有意に上回ること，また，これらの条件が特に十分に当てはまる場合には，事前確率を無視したCNBも同検定で有意に上回ることを示す．
また，NNBは，Bayes手法以外の手法であるサポートベクターマシン(SVM)よりも，時に優れた結果を示した．
本稿の構成は以下のようになっている．
まず[REF_Sec:関連研究]節でBayes手法のテキスト分類の関連研究について紹介する．
[REF_Sec:Negation Naive Bayesの導出]節では提案手法であるNNBの導出について述べる．
[REF_Sec:実験]節では本研究で用いたデータと実験方法について述べ，[REF_Sec:結果]節に結果を，[REF_Sec:考察]節に考察を，[REF_Sec:まとめ]節にまとめを述べる．
