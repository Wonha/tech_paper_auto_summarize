多義性解消の重要性
\label{sec:imp}

本節では多義性解消の重要性を例を用いながら説明する．

例えば，最近重要視されつつある
質問応答\cite{murata2000_1_nl,qac_hp}の問題を考えてみる．
ここで，質問応答の質問として以下のようなものがあったとしよう．
\begin{quote}
  「トラは天王寺動物園にはどのくらいいますか。」
\end{quote}
ここで質問応答システムの知識源として，
\begin{quote}
  「天王寺動物園にはトラが11頭飼育されており…」
\end{quote}
があったとする．この場合システムは「どのくらい」
などの表現から数量表現が解とわかるのでこの知識源から
「11頭」を解として正しく抽出することができる．
しかし，質問が以下のようであったとしよう．
\begin{quote}
  「虎は天王寺動物園にはどのくらいいますか。」
\end{quote}
この質問では先の質問の「トラ」が「虎」に変わっている．
この場合，「トラ」と「虎」が同一であることを計算機に
認識させなければならないが，
これをするためには計算機に単語の意味に関する情報を与えて
おかなければならない．
ここで例えば EDR 辞書\cite{edr}を利用すると，
「虎」の同義語としては
「トラ」「酒酔」「酒酔い」「酔客」「酔狂人」「酔人」
が得られる．
これは「虎」には
「虎という動物」と「酒に酔った人」の二つの意味があり，
この後ろの「酒に酔った人」の意味の場合の同義語が
得られて「酒酔い」などの不要な同義語が得られるのである．
この不要な同義語を使って解の抽出を試みる場合，
例えば
\begin{quote}
  \mbox{「昨夜未明，天王寺動物園で5人の酒酔い客が暴れだし…」}
\end{quote}
という文が知識中にある場合，
誤って「５人」を解として取り出してしまう可能性がある．
ここで多義性の解消をし，ここでの「虎」の意味は
「虎という動物」と認識し同義語は「トラ」だけであると
してから，解の抽出をした方が誤る可能性は減るのである．
「虎」の場合はまだ意味が二つであったからよいが，
高頻度に用いられる平易な語ほど語義の数が多く，
この問題は深刻なものとなる．
このことから，多義性の解消の重要性がわかる．

ここでは質問応答の場合の例をあげたが
各解析システムにおいて多義性の解消は同様に重要なものと
なろう．例えば，
照応解析\cite{murata_deno_nlp}においても，ある語Aが「人間」と「物」の
二つの意味をもっていて，
物しか指示しない「それ」という指示詞が出現した場合，
語Ａの多義性を解消し，もし「人間」であるということが
わかれば，この「それ」の指示先は語Aではありえないと
わかり他の指示先の候補を探せばよいとわかる．
また，機械翻訳でも
訳し分けが必要な語は多義性を解消しなければ
正しい翻訳をすることができない．
このように多義性の解消は種々の場面で役に立つものである．

問題設定
\label{sec:mondai_settei}

本節では SENSEVAL2 の日本語辞書タスクの問題設定について説明する．

SENSEVAL2 の日本語辞書タスクでは，
評価用のデータとしては
100 単語(このうち50単語が名詞で50単語が動詞)について
それぞれ 100 事例が与えられ，
合計 10000 事例が与えられた．
学習用のデータとしては，
RWC コーパス\cite{shirai_nl2001}が与えられた．
このコーパスは
毎日新聞の 1994 年の 3,000 個の記事を用いたもので，
コーパス中の主要な名詞，動詞，形容詞(総数：約15万個)に対して，
岩波国語辞典に基づいて定義された語義がふられている．
このタスクの目的は，
この語義をその単語のまわりの情報などを用いて
推定することである．
また，精度の評価には，SENSEVAL2 のホームページ\footnote{http://www.sle.sharp.co.uk/senseval2}より
取得できる scorer2 
という評価用プログラムによって算出される， mixed-grained score という
値が用いられた\footnote{具体的には ``scorer2 結果のファイル 正解のファイル sense-map -g mixed'' という
コマンドを打ち込むことにより算出される．}．

機械学習手法
\label{sec:ml_method}

一般に，単語多義性解消問題の場合，各単語にふられる語義は，
単語ごとにかわるので，機械学習手法による実験は各単語ごとに
逐次的に行なわれる．つまり，学習器は
単語の異なり数の分だけ作成する．
しかし本タスクでは，あらかじめ 50個が名詞で，
 50個が動詞であるとわかっている．このため，
システムは，単語ごとだけでなく，単語と品詞の組に対して
個々に作成した．本コンテストの場合，50個の名詞と
50個の動詞が対象であったので，合計 100 個の学習器を作ることになる．

本稿では学習器のために用いる機械学習手法としては，以下の方法を利用した\footnote{機械学習
手法としては，他に C4.5\cite{c4.5j}などの決定木学習を利用する方法があるが，
本稿では，種々の問題で決定木学習手法が他の手法に比べて劣っている
こと\cite{murata:nlken98,murata_haihan_rule_ipsj,taira_svm}，
また，本稿で扱う問題は属性の種類の数が多く C4.5 が走るまで
属性の数を減らすと精度が落ちるであろうことの二つの理由により，
用いていない．また，最大エントロピー法も有力な手法であるが，
システムの都合で動かない単語があったこと，また先行研究\cite{murata_nlc2001_wsd}において
最大エントロピー法が他の手法よりも精度が低かったことにより本稿では
用いていない．}．
\begin{itemize}
\item 
  シンプルベイズ法
\item 
  決定リスト法
\item 
  サポートベクトルマシン法
\end{itemize}
本節ではこれらの個々の機械学習手法の説明と，
これらの機械学習手法のいくつかを組み合わせる融合手法について説明する．

\subsection{シンプルベイズ法}

シンプルベイズ法は，ベイズの定理に基づいて
各分類になる確率を推定し，
その確率値が最も大きい分類を求める分類とする方法であり，
多義性解消の研究における基本的な方法である．
文脈 $b$ で分類 $a$ を出力する確率は
以下の式で与えられる．
{
\begin{eqnarray}
  p(a|b)  & = & \frac{p(a)}{p(b)}p(b|a)\\ 
  \label{eq:simple_bayes}
  & \simeq & \frac{\tilde{p}(a)}{p(b)} \prod_i \tilde{p}(f_i|a)
\end{eqnarray}
}
ただし，ここで文脈 $b$ は，
あらかじめ設定しておいた素性 $f_j (\in F, 1\leq j\leq k)$ 
の集合である．
$p(b)$ は，文脈 $b$ の出現確率で，
今回の場合分類 a に非依存で定数のため，計算しない．
$\tilde{p}(a)$ と $\tilde{p}(f_i|a)$ は，
それぞれ学習データから推定された確率で，
分類 a の出現の確率，
分類 a のときに素性 $f_i$ を持つ確率を意味する．
$\tilde{p}(f_i|a)$ として最尤推定し求めた値を用いると，
しばしば値がゼロになり，
式(\ref{eq:simple_bayes}) の値がゼロになり
分類先を決めるのが難しい場合が多い．
このため，スムージングがなされるが，
本稿では以下のスムージングをしたものを用いる．
{
\begin{eqnarray}
  \label{eq:simple_bayes2}
  \tilde{p}(f_i|a) = \frac{freq(f_i,a)+\epsilon*freq(a)}{freq(a)+\epsilon*freq(a)}
\end{eqnarray}
}
ただし，$freq(f_i,a)$ と $freq(a)$ は，それぞれ，
素性 $f_i$ を持ちかつ分類が $a$ である事例の個数，
分類が $a$ である事例の個数を意味する．
$\epsilon$ は実験で定める定数である．
本稿では，$\epsilon$ としては 0.01 と 0.0001 を用いた\footnote{\label{fn:bayes_epsilon} SENSEVAL2 のコンテストでは，
われわれは $\epsilon$ としては 0.01 を用いていた．
コンテストの終了後，$\epsilon$ として 0.1 から 0.00000001 まで 1/10 の倍率でいくつか
試してみた．その結果，学習用データでの10分割のクロスバリデーションの結果では
$\epsilon = 0.0001$ のときの値がもっともよかった．}．

\subsection{決定リスト法}

これは，素性$f_i$と分類先$a$の組を規則とし，
それらをあらかじめ定めた優先順序でリストに蓄えておき，
リストで優先順位の高いところから，
入力と素性が一致する規則を利用して分類先を求める方法である\cite{Yarowsky:ACL94}．
本稿では優先順序としては以下のものを用いる\footnote{$\tilde{p}(a|f_i)$ の値が
等しい場合は，出現頻度の多い規則，
すなわち，素性$f_j$で分類$a$である事例の多い規則を優先するように
している．}$^,$\footnote{Yarowsky の研究など一般に用いられている
決定リストでは，対数尤度比，
さらには，その尤度比をスムージングしたものが用いられている．
ところで最近では，
本手法と同じ確率$\tilde{p}(a|f_i)$の式の上で
ベイズ推定に基づくスムージングをした式を用いることで，
従来の尤度比を用いる方法よりも高い精度を得たという
報告\cite{tsuruoka_nlp2002}がある．
本稿ではこのあたりはそれほど注意深く検討していない．
種々のスムージングをすることで今よりもよい精度を得る可能性はある．}．
{
\begin{eqnarray}
  \label{eq:decision_list_order}
  \tilde{p}(a|f_i)
\end{eqnarray}
}
この方法は，以下の式で与えられる，
ある文脈 $b$ での分類 $a$ を出力する確率 $p(a|b)$ がもっとも高い
分類 $a$ を解とする方法と等価であり，
本稿では実際にはこの方法を用いて分類先を特定する．
{
\begin{eqnarray}
  \label{eq:decision_list}
  p(a|b) = \tilde{p}(a|f_{max})
\end{eqnarray}
}
ただし，$f_{max}$ は以下の式によって与えられる．
{
\begin{eqnarray}
  \label{eq:decision_list2}
  f_{max} = argmax_{f_j\in F} \ max_{a_i\in A} \ \tilde{p}(a_i|f_j)
\end{eqnarray}
}
また，$\tilde{p}(a_i|f_j)$ は学習データで素性$f_j$を
文脈に持つ場合の分類$a_i$の出現の割合である．

\begin{figure}[t]
      \begin{center}
      \epsfile{file=margin.eps,height=4cm,width=8cm} 
      \end{center}
    \caption{マージン最大化}
    \label{fig:margin}
\end{figure}

\subsection{サポートベクトルマシン法}

サポートベクトルマシン法は，
空間を超平面で分割することにより
2つの分類からなるデータを分類する手法である．
このとき，2つの分類が正例と負例からなるものとすると，
学習データにおける正例と負例の間隔(マージン)が
大きいもの(図\ref{fig:margin}参照\footnote{図の
白丸，黒丸は，正例，負例を意味し，
実線は空間を分割する超平面を意味し，
破線はマージン領域の境界を表す面を意味する．})ほど
オープンデータで誤った分類をする可能性が低いと考えられ，
このマージンを最大にする超平面を求め
それを用いて分類を行なう．
基本的には上記のとおりであるが，通常，
学習データにおいてマージンの内部領域に
少数の事例が含まれてもよいとする手法の拡張や，
超平面の線形の部分を非線型にする拡張(カーネル関数の導入)がなされたものが
用いられる．
この拡張された方法は，以下の識別関数を用いて分類することと等価であり，
その識別関数の出力値が正か負かによって
二つの分類を判別することができる\cite{SVM,kudoh_svm}．
{
\begin{eqnarray}
  \label{eq:svm1}
  f({\bf x}) & = & sgn \left( \sum^{l}_{i=1} \alpha_i y_i K({\bf x}_i,{\bf x}) + b \right)\\
  b & = & -\frac{max_{i,y_i=-1}b_i + min_{i,y_i=1}b_i}{2}\nonumber\\
  b_i & = & \sum^l_{j=1} \alpha_j y_j K({\bf x}_j,{\bf x}_i) \nonumber
\end{eqnarray}
}
ただし，${\bf x}$ は識別したい事例の文脈(素性の集合)を，
${\bf x}_{i}$と$y_i(i=1,...,l, y_i\in\{1,-1\})$は
学習データの文脈と分類先を意味し，関数 $sgn$ は，
{
\begin{eqnarray}
  \label{eq:svm2}
  sgn(x) \, = & 1 & (x \geq 0)\\
          & -1 & (otherwise) \nonumber
\end{eqnarray}
}
であり，また，各$\alpha_i$は式(\ref{eq:svm5})と式(\ref{eq:svm6})の制約のもと
式(\ref{eq:svm4})の$L(\alpha )$を最大にする場合のものである．
{
\begin{eqnarray}
  \label{eq:svm4}
  L({\alpha}) & = & \sum^l_{i=1} \alpha_i - \frac{1}{2} \sum^l_{i,j=1} \alpha_i \alpha_j y_i y_j K({\bf x_i},{\bf x_j})
\end{eqnarray}
}
{
\begin{eqnarray}
  \label{eq:svm5}
  0 \leq \alpha_i \leq C \, \, (i=1,...,l)
\end{eqnarray}
}
{
\begin{eqnarray}
  \label{eq:svm6}
  \sum^l_{i=1} \alpha_i y_i = 0 
\end{eqnarray}
}
また，関数$K$ はカーネル関数と呼ばれ，様々なものが
用いられるが本稿では以下の多項式のものを用いる．
{
\begin{eqnarray}
  \label{eq:svm3}
  K({\bf x},{\bf y}) & = ({\bf x}\cdot{\bf y} + 1)^d
\end{eqnarray}
}
$C,d$は実験的に設定される定数である．
本稿ではすべての実験を通して$C$,$d$はそれぞれ1と2に固定した．
ここで，$\alpha_i > 0$ となる ${\bf x}_i$ は，
サポートベクトルと呼ばれ，通常，式(\ref{eq:svm1})の和をとっている部分は
この事例のみを用いて計算される．
つまり，実際の解析には学習データのうちサポートベクトルと
呼ばれる事例のみしか用いられない．

サポートベクトルマシン法は
分類の数が2個のデータを扱うもので，通常これに
ペアワイズ手法を組み合わせて用いることで，
分類の数が3個以上のデータを扱うことになる\cite{kudoh_chunk_nl2000}．

ペアワイズ手法とは，N個の分類を持つデータの場合，
異なる二つの分類先のあらゆるペア(N(N-1)/2 個)を作り，
各ペアごとにどちらがよいかを2値分類器(ここでは
サポートベクトルマシン法\footnote{本稿の2値分類器としての
サポートベクトルマシンは，工藤氏が作成した TinySVM\cite{kudoh_svm} を利用している．})で求め，
最終的にN(N-1)/2個の2値分類器の分類先の多数決により，
分類先を求める方法である．

本稿のサポートベクトルマシン法は，
上記のようにサポートベクトルマシン法とペアワイズ手法を
組み合わせることによって実現される．

\subsection{融合手法}
\label{sec:yuugou}

本節では，いくつかの機械学習を組み合わせて用いる融合手法について
説明する．
われわれの融合手法では，
それぞれの単語ごとに用いる機械学習手法を変更する．
(厳密には，本稿の場合は単語と名詞の組に対して学習器を作成しているので，
この融合手法は
それぞれの単語と名詞の組ごとに機械学習手法を変更することになる．)
各単語ごとに用いられる機械学習手法は，
融合する機械学習手法のうち
学習データでの10分割のクロスバリデーションの精度がもっともよかったものとする．

われわれは融合手法としては以下の五つの手法を試した．
\begin{itemize}
\item 
  融合手法1 

  シンプルベイズ($\epsilon$=0.01)とサポートベクトルマシンの組み合わせ

\item 
  融合手法2

  二種類のシンプルベイズ($\epsilon$=0.01)と
  二種類のサポートベクトルマシンの合計四種類のシステムの組み合わせ

  ただし，ここでいう二種類は\ref{sec:sosei}節で後述する素性(解析に用いる情報)を
  すべて用いた場合と，KNP構文素性のみを削除した場合の二つの場合を意味する\footnote{KNP構文素性のみを削除した場合の
    ものを融合の一つに用いたのは，先行研究\cite{murata_nlc2001_wsd}において
    KNP構文素性のみを削除した場合の方が精度が高かった場合があったことによる．}．

\item 
  融合手法3

  シンプルベイズ($\epsilon$=0.0001)とサポートベクトルマシンの組み合わせ

\item 
  融合手法4

  二種類のシンプルベイズ($\epsilon$=0.0001)と
  二種類のサポートベクトルマシンの合計四種類のシステムの組み合わせ

  ただし，ここでいう二種類は\ref{sec:sosei}節で後述する素性(解析に用いる情報)を
  すべて用いた場合と，KNP構文素性のみを削除した場合の二つの場合を意味する．

\item 
  融合手法5

  $\epsilon$=0.01のシンプルベイズと
  $\epsilon$=0.0001のシンプルベイズの組み合わせ

\end{itemize}

素性(解析に用いる情報)
\label{sec:sosei}

前節で種々の機械学習の説明を述べたが，
それぞれの手法ともに素性(解析に用いる情報)を定義しなければ，
その手法を用いることができない．
本節ではその素性の説明を行なう．

\ref{sec:mondai_settei}節の問題設定で述べたように，
本稿の問題設定では，
日本語文の入力を与えられたときに，
その入力中の語義タグがふられていた各形態素に対して，
その語義の分類を推定して出力することになっている．
このため，解析に用いる情報，すなわち，素性は
入力される日本語文から取り出すことになる．

本稿では素性としては以下のものを定義する．
\begin{itemize}
\item 
  {\bf 文字列素性}\footnote{日本語の研究で機械学習手法の素性に
    文字列素性を用いた研究としては文献\cite{NLP98_nakano,inui:nlken98,murata_nlc2001}などがある．}

  \begin{itemize}
  \item 
    解析する形態素自身の文字列 

  \item 
    解析する形態素の直前の 1 〜 3 gram の文字列

  \item 
    解析する形態素の直後の 1 〜 3 gram の文字列

  \end{itemize}

\item 
  {\bf RWC形態素素性}

  \begin{itemize}
  \item 
    解析する形態素自身のRWCコーパスの品詞情報，品詞細分類情報，品詞細細分類情報\footnote{ここで，
      品詞情報，品詞細分類情報，品詞細細分類情報は，
      RWC コーパスでの 3, 4, 5 カラム目のものを意味する．}

  \item 
    解析する形態素の直前の形態素の単語自身，
    その単語の分類語彙表\cite{bgh}の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞細細分類情報

  \item 
    解析する形態素の直後の形態素の単語自身，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞細細分類情報

  \end{itemize}

\item 
  {\bf JUMAN形態素素性}

  コーパスを JUMAN\cite{JUMAN3.6} で形態素解析し，
  その結果を素性として利用する．

  \begin{itemize}
  \item 
    解析する形態素自身の JUMAN の解析結果の品詞情報，品詞細分類情報，品詞活用形情報

  \item 
    解析する形態素の直前の形態素の単語自身，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞活用形情報

  \item 
    解析する形態素の直後の形態素の単語自身，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞活用形情報

  \end{itemize}

\item 
  {\bf 構文素性}

  コーパスを KNP\cite{KNP2.0b6} で構文解析し，
  その結果を素性として利用する．

  \begin{itemize}
  \item 
    解析する形態素を含む文節自身，また，
    その文節が体言かいなか，付属語の品詞，品詞細分類，活用情報

  \item 
    解析する形態素を含む文節の係り先の文節の自立語，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞活用形情報
    
  \item 
    解析する形態素を含む文節の係り元の文節の自立語，
    その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁，
    品詞情報，品詞細分類情報，品詞活用形情報．

    ただし，すべての場合において，付属語の情報を併用する．
    
  \end{itemize}

\item 
  {\bf 同一文内共起素性}

  コーパスを JUMAN で形態素解析し，
  その解析結果の形態素列を素性として利用する．

  \begin{itemize}
  \item 
    同一文中の各形態素，
    また，その単語の分類語彙表の 5桁，
    その単語の分類語彙表の 3桁．
    
  \end{itemize}

\item 
  {\bf UDC 素性}

  RWCコーパスには，各記事ごとに
  図書館などで書籍の分類に用いられる，
  国際十進分類(UDC)がふられている．
  この情報は各記事がどういう分野のものかを
  示している．

  \begin{itemize}
  \item 
    解析対象とする形態素を含む記事の UDC コード
    の最初の1桁，2桁，3桁．
    
  \end{itemize}
\end{itemize}

関連文献
\label{ref:kanren}

本節では関連文献について説明する．

英語単語の多義語の曖昧性解消に
機械学習手法を用いた研究は極めて
多数存在する\cite{Fuji98a,sense1,fukumoto_ipsj2001}が，
日本語単語の多義語の曖昧性解消に
機械学習手法を用いた研究は，
SENSEVAL2 以前はほとんどなかった\cite{shinou98}．
例えば，新納の研究では語の多義の解消ではなく，
同音異義語の判別を扱っていた．
その意味で，日本語多義解消の問題で「シンプルベイズ法」
「決定リスト法」「サポートベクトルマシン法」の三つの
機械学習手法，さらには，
素性を変化させた場合の実験結果を示している本稿は，
今後の日本語単語の多義語の曖昧性解消の問題を
考えるための資料として非常に役に立つものと思われる．

次にいくつかの関連研究を紹介したい．

まず，SENSEVAL2 で英語を含む数多くの言語で優秀な成績をとっていた
Yarowsky らのシステム\cite{yarowsky_s2}について説明する．
Yarowsky らのシステムは，シンプルベイス法と決定リストの組み合わせであり，
決定リストで求まる確信度が高いところでは決定リストの手法の解を
用い，それ以外の場合は種々の手法の多数決の結果を解とする手法である．
確信度を用い決定リストで確実に求まるところだけを
別個に扱っているところが興味深い．

次に，SENSEVAL2 の日本語辞書タスクに参加していた八木らの
システム\cite{yagi_nlc2001}について説明する．
八木らのシステムは決定リスト法を機械学習手法として
用いており，学習用のデータとして，
RWC コーパス以外に
岩波国語辞典の例文のデータを用いていることを特徴としている．
RWC コーパスでの語義の定義は
岩波国語辞典を用いているため，
岩波国語辞典の例文のデータも語義解析のための学習データとして
利用できるのである．
八木らの研究ではこの例文データも利用した場合の方が
利用しない場合よりも精度が高かったとしている．
この結果は，
われわれの研究でもこのデータを追加で用いることで精度を向上できる
可能性を示唆するものであり，興味深い．

次に，高村らの素性空間を再構成する手法\cite{takamura2001_NL}について説明する．
この研究は英語を対象に行なわれており，
機械学習手法としてはサポートベクトルマシン法を利用している．
この手法は学習に用いる素性を構成し直すところに特徴がある．
普通に抽出した素性だけでなく，その素性の分布に対して
独立成分分析や主成分分析を行ない，元々の素性よりも
一段抽象化したような素性を新たに作り出し，これも素性と
して追加で用いる方法である．
この素性を再構成する方法を含めた複数のシステムの多数決を
用いる方法で，単純なサポートベクトルマシン法の性能を
上回ったとしている．独立成分分析などにより素性の情報を
抽象化することでデータスパースネス対策などに役立っていると
思われ，興味深い．

最後に，中野らの AdaBoost を用いた手法\cite{nakano_ada}を説明する．
この研究では SENSEVAL2 日本語タスクのデータを対象としており，
AdaBoost を機械学習手法として利用している．
AdaBoost は正しく分類された事例の重みを下げ，
誤って分類された事例の重みを上げて，再学習をする手法である．
中野らの報告では，AdaBoost を利用することで
決定リスト法，シンプルベイズ法よりも高い精度を得たと
報告している．ただし，その最高精度は 79.1\,\% であり，
われわれの最高精度の 79.3\,\% より若干低い．
しかし，この結果はわれわれのシステムの素性の情報が豊かであるためで
ある可能性があり，
われわれのシステムの素性の情報で AdaBoost を利用すると
さらによい精度を得ることができる可能性がある．
しかし，中野らはわれわれのシステムでは用いていない
日本語語彙体系\cite{ntt}の辞書の情報を用いており，
中野らの方が素性の情報が少ないとは言い切れないので，
われわれのシステムの素性の情報で AdaBoost を利用すると
さらによい精度を得ることができるかどうかはわからない．
また，われわれのシステムの素性の情報の方が豊富であっても，
われわれのシステムの素性で AdaBoost が本当によい精度を出せるか
どうかはわからない．

次に，多義性の研究に直接は関係はないが，
複数の機械学習の方法を組み合わせるのに，スタッキングを
用いる手法\cite{Halteren_cl2001}について説明したい．
スタッキングを用いる手法とは，もともとの素性の他に，
複数の機械学習の結果を素性として追加し，その追加された素性を
用いて機械学習を行なう方法である．
従来は複数の機械学習の方法の融合には多数決が
多く用いられていたが，
スタッキングの方法ではどの機械学習の方法がよいかを学習することに
なっており
たいていの場合で多数決の方法よりも精度が高くなると思われる．
このスタッキングを利用する研究としては，
形態素解析のもの\cite{Halteren_cl2001}や固有名詞表現抽出のもの\cite{Utsuro_ne}などがある．本稿でのシステムの融合では，
各単語でもっとも精度の高い手法を利用していた．
このわれわれの融合手法は各単語ごとに用いる手法を
最尤推定で求めるものになっていて少々は
融合に学習を用いていることにはなっている．
しかし，手法の組み合わせにおいても強力な学習手法を用いた方が
精度はよいと思われるので，われわれの手法でもスタッキングを
利用することを考えた方がよい．

以上，種々の有力な関連研究を紹介した．
それぞれの手法ともに特徴的な要素を持っており，
\ref{sec:experiment2}節の最後に述べた考察と含めて
それらを総合的に考察し，
それぞれの手法の良い面を組み合わせることで，
さらによりよい多義解消を行なえると思われる．

