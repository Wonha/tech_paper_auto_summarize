    \documentstyle[epsf,jnlpbbl]{jnlp_j_b5_2e_new}

\setcounter{page}{3}
\setcounter{巻数}{13}
\setcounter{号数}{3}
\setcounter{年}{2006}
\setcounter{月}{7}
\受付{2005}{9}{27}
\再受付{2006}{1}{26}
\採録{2006}{1}{27}
\setcounter{secnumdepth}{2}
\def\theaffi@footnote{}

\title{用例ベース翻訳の確率的モデル化}
\authorC{荒牧 英治\affiref{TOKYO} \and 黒橋 禎夫\affiref{KYOTO} \and 柏
岡 秀紀\affiref{NICT}\affiref{ATR} \and 加藤 直人\affiref{NHK}}
\headauthor{荒牧，黒橋，柏岡，加藤}
\headtitle{用例ベース翻訳の確率的モデル化}
\affilabel{TOKYO}
	{東京大学附属病院企画情報運営部}
	{Department of Planning, Information and Management, 
	University of Tokyo Hospital}
\affilabel{KYOTO}
	{京都大学大学院情報学研究科}
	{Graduate School of Informatics, 
	Kyoto University}
\affilabel{NICT}
	{独立行政法人情報通信研究機構}
	{National Institute of Information and Communications Technology}
\affilabel{ATR}
	{ATR音声言語コミュニケーション研究所}
	{ATR Spoken Language Translation Research Laboratories}
\affilabel{NHK}
	{NHK放送技術研究所}
	{Science and Technical Research Laboratories of NHK}

\jabstract{
用例ベース翻訳は，これまで，経験則にもとづく指標／基準により用例を選択
してきた．
しかし，経験則に頼った場合，その修正を行うのが困難であり，また，アルゴ
リズムが不透明になる恐れがある．
そこで，本研究では用例ベース翻訳を定式化するための確率モデルを提案する．
提案するモデルは，翻訳確率の最も高い用例の組み合わせを探索することで，
翻訳文を生成する．
さらに，本モデルは用例と入力文のコンテキストの類似度を自然に翻訳確率に取り込
む拡張も可能である．
実験の結果，本モデルを用いたシステムは，従来の経験則によるシステムの精
度を僅かに上回り，用例ベース翻訳の透明性の高いモデル化を実現することに
成功した．
}
\jkeywords{用例ベース翻訳，機械翻訳，確率モデル，コンテキストの類似，依存構造}

\etitle{Probabilistic Formalization for Example-based \\Machine Translation}
\eauthor{
	Eiji Aramaki\affiref{TOKYO} \and 
	Sadao Kurohashi\affiref{KYOTO} \and 
	Hideki Kashioka\affiref{NICT}\affiref{ATR} \and 
	Naoto Kato\affiref{NHK}}
\eabstract{
Example-based machine translation (EBMT) systems, so far, rely on
heuristic
measures in retrieving translation examples.
Such a heuristic measure costs time to adjust, and might make its
algorithm unclear.
This paper presents a probabilistic model for EBMT.
Under the proposed model, the system searches the translation example
combination which has the highest probability.
The proposed model clearly formalizes EBMT process.
In addition, the model can naturally incorporate the context similarity of
translation examples.
The experimental results demonstrate that the proposed model has a
slightly better translation quality than state-of-the-art EBMT systems.
}
\ekeywords{Example-based Machine Translation, 
	Machine Translation, 
	Probabilistic Model, 
	Context Similarity, 
	Depedency Structure}

\begin{document}
\maketitle
\thispagestyle{empty}

\section{提案手法}

	
	
	
	\begin{figure}
	\begin{center}
	\epsfxsize=120mm\epsfbox{f_prob5.eps}
	\end{center}
	\caption{翻訳のながれ}
	\label{f_prob.eps}
	{\footnotesize * 本稿の図では，依存構造木の親を左に，子を右に描く．
	また，本研究では，ノードの単位は日本語は文節，英語はbase-NP,
	またはbase-VPとする．\par}
	\end{figure}
	
	
	

用例ベース翻訳の基本的な原則はできるだけ大きなサイズの用例を用いて翻訳
文を生成することである．
これを確率的に定式化するためには，大きな用例を用いた翻訳結果が大きな翻訳確率を持
たなくてはならない．
本章では，これを実現するための基本アイデアを述べる．


まず，提案手法は入力文を可能なかぎりの部分木の組合せに分解する：
\begin{equation}
D= \{d_{1},...,d_{N}\}.
\end{equation}
ここで，$d_{i}$は入力文の分解のパターン，$D$は$d_{i}$の集合である．

次に，$d_{i}$は入力文を$M_{i}$個の部分木に分解しているとする:
\begin{equation}
d_{i}=\{s_{i1},s_{i2},...,s_{iM_{i}} \},
\end{equation}
ここで，$s_{ij}$は入力文の部分木である．

例えば，図\ref{f_prob.eps}左の入力文の場合，$d_{1},...,d_{4}$の4通りの
部分木の組合せで表現できる．
この例では，$d_{1}$ は入力文を3つの部分木$s_{11}$, $s_{12}$，$s_{13}$に分
解している.
また，$d_{2}$，$d_{3}$は，入力文を2つの部分木に分解している．
また，$d_{4}$のように，文そのものも分解パターンとして取り扱う．

次に，各部分木$s_{ij}$それぞれについて,もっとも翻訳確率$P( t_{ij} \mid
s_{ij} )$(この確率の計算方法は次節にて述べる)の高い用例を選び，それら
の積を翻訳文の翻訳確率$T_{P}(d_{i})$とする:
\begin{equation}
T_{P}(d_{i})=\prod_{s_{ij} \in d_{i}}  P( t_{ij} \mid s_{ij} ).
\end{equation}
ここで，$t_{i1},...,t_{iM{i}}$を$d_{i}$の翻訳とみなし，$T(d_{i})$と表
記する.

最後に，もっとも高い翻訳確率を持つ分解パターン（$d_m$）を以下の式によって探索し，最終
的な翻訳を$T(d_m)$とする：
\begin{equation}
d_m = \arg\max_{d_{i} \in D} T_{P}(d_{i}).
\end{equation}
簡単に言うと，提案手法は，入力文のある単位をどう翻訳するかと，どういう
単位で翻訳するかという2つ問題を解いている．
前者は，最も確率の高い用例を選択することで解決しており，基本的
な統計的翻訳と同様の考え方である．
後者は，入力文の分解パターンを選択することで解決している．


ここで重要なことは，本モデルの枠組みでは，大きな用例を用いた翻訳文が
優先されることである．
この理由は，大きな用例は安定した翻訳先を持つ傾向にあるため，高い翻訳確率
を持ち，当然，その積である翻訳文の確率$ T_{P}(d_{i})$ も自然と高くなる
からである．

例えば，日本語``かける''は，翻訳の際には大きな曖昧性があり，
``bet'',``run''や``play''など様々な英語表現が考えられる．

ここで，もし，$T(d_{1})$のように，入力文を小さな部分木に分解した場合は，適切な
訳である$P(play \mid かける )$の翻訳確率は低く，適切な翻訳は行われない．

一方，$T(d_{2})$では，より大きな表現``CDをかける''を用いた用例を探索している．
この用例の英語表現としては，ほとんどが``play''となり，用例の翻訳確率は
高くなる．
その結果，用例群の翻訳確率の積である$P(d_{2})$も高くなり，この結果が翻
訳として採用される．

また，図\ref{f_prob.eps}の$T(d_{4})$のように，大きすぎる単位で検索した
場合は，コーパス中に存在せず，確率が定義されない場合がある．



\subsection{パラメータの推定}




\begin{figure}
\begin{center}
    \epsfxsize=75mm\epsfbox{f_cs.eps}
\end{center}
\caption{コンテストの定義}
\label{f_cs.eps}
\end{figure}


\begin{table}
\caption{``かける''を含んだ用例とそのコンテキスト (コンテキス
	  トは括弧で示されている)}
\label{tc0}
\begin{center}
\begin{tabular}{ccc}
\hline   用例原言語側 & 用例目的言語側 & context\_sim \\
\hline
	(テープを)かける& play & 0.8				\\
	(ＣＤを)かける  & play & 0.8				\\
	(ＭＤを)かける  & put  & 0.8				\\
	(目覚ましを)かける& set  & 0.6				\\
            :           &  :   &  : \\
	(お金を)かける  & bet  & 0.4				\\
	(1万円を)かける& bet  & 0.4				\\
	(名誉を)かける  & bet  & 0.3				\\
\hline
\end{tabular}
\end{center}
{\footnotesize
* 実際には用例は木構造の形で扱われているが，表記を簡潔にするため，この
図では用例の構造は記していない.
}
\end{table}

本節では，用例の翻訳確率の推定方法を述べる．
まず，英語部分木$t$と日本語部分木$s$からなる用例があるとする．
この翻訳確率$P(t \mid s)$は，
アライメントされたコーパス中での対応$(t,s)$の出現頻度を直接数えて求める:
\begin{equation}
       P(t \mid s) = \frac{ count (t,s) }{ count (*,s)  },
\end{equation}
ここで，$count(t,s)$は，アライメントされたコーパスにおける対応$(t,s)$
の出現頻度，$count(*,s)$は日本語
部分木($s$)の出現頻度である\footnote
{後述する実験では，データスパースネスの問題に対処するため，$s$と$t$は内容語に
  汎化して集計を行った．}．

ただし，この頻度の計算にあたっては，次節に述べるコンテキストの情報を利
用した拡張が可能である．
\clearpage

\subsection{入力文と用例のコンテキストの類似度を取り込んだ確率モデル}



用例の選択にあたって重要な手がかりは入力文と用例の一致するサイズであり，
それは，2.1節で提案された翻訳確率の枠組みで実現されている．
しかし，用例のサイズに加えて，入力文と用例のコンテキストの類似も重要な手がかりである．
提案するモデルは，このようなコンテキストの類似を取り込む拡張を自然に行
うことができる．

まず，提案手法を説明する前に，用例と入力文のコンテキストを定義する．
図\ref{f_cs.eps}に示されるように，用例の原言語側が
$i_{1..3}$という3つの句と接続しているとする．
これらとそれと対応する入力文の$j_{1..3}$をコンテキストと考える．

そして，用例と入力文のコンテキストの類似度を次の式で定める:
\begin{equation}
context\_sim(s) = \sum_{ i \in N} sim (i,j),
\end{equation}
ここで，$i$ は用例Aの日本語側で翻訳に使う部分の周辺の句，$j$は$i$と対
応する入力文の句，$N$は$i$の集合，$sim(i,j)$ はシソーラス\cite{NTT}を用いて計算する
$i$と$j$の類似度(max=1)であり，以下の式で定義される: 
\begin{eqnarray}
 sim(i, j) = \frac{2d_c}{d_{i}+d_{j}},
\end{eqnarray}
ここで，$d_{i}$と$d_{j}$は，それぞれ$i$と$j$のシソーラス上での深さ，
$d_c$ は，$d_{i}$と$d_{j}$の共通するパスの深さである．



提案手法のポイントは，
高い類似度を持つ用例は，同じく高い類似度を持つ用例のみを用いて翻訳確率を計算する
点である．
すなわち，式5によって，ある用例の翻訳確率を計算する際には，$context\_sim(s)$以上
の類似度を持つ用例だけを集計して翻訳確率を計算し，$context\_sim(s)$未満の
類似度の用例は，用例の翻訳確率の計算には用いない．
この操作を用例のフィルタリングと呼ぶ．

このフィルタリングの操作は，用例のサイズごとに翻訳確率を計算する手法を，
類似度にまで拡大したものであり，自然な拡張であるといえる．
この拡張の結果，高い$context\_sim$を持つ用例は，
それよりも低い$context\_sim$を持つ用例の影響を受けず，多くの場合，高い
翻訳確率を持つことになる．

例えば，``レコードをかける''がコーパスに存在しない（しかし，``レコード''
と``かける''それぞれ単独では出現している）場合に，表\ref{tc0}の用例群
を用いて翻訳することを考える．前節までの手法では，このように，大きな
サイズで一致するものがない場合，``かける''単独で翻訳確率を計算すること
になり，``bet''など不適切な訳語が選ばれる可能性がある．

本節の提案手法では，用例``CDをかける''と入力文``レコードをかける''の$context\_sim$
が0.8であるとすると，同じく0.8以上の$context\_sim$を持つ用例だけを用いて翻訳
確率を計算する．
この場合，用例の数は3つだけとなるとが，その英語表現は安定しており，
$P(play \mid かける)=\frac{2}{3}$,  $P(put \mid かける)=\frac{1}{3}$と
なる.
このように類似したコンテキストを持つ用例の翻訳確率は自然と高くなる傾向をもつ．


また，この枠組では，類似度がもっとも高い用例が一つしかない場合，その翻
訳確率は最大の1となる．
これは，より大きな用例が利用可能であった場合に，その大きな用例よりも，類似している小さな用例
を優先しているかのように見える．
この問題は次のように解決されている．

まず，提案手法は用例を構築する際に，大きな用例を分解した一部分も独立した用
例として扱い，データベースに蓄える(3.1節ステップ3)．
よって，ある大きな用例が利用できる状態で，それよりも小さい，もっとも類似した用例
が一つしかない場合における，その一つしかない用例とは，大きな用例の一部分
から作られた用例となる．
というのは，大きな用例が利用可能であるならば，その分解から得られた用例は，
その周辺が入力文と同一であり，最大の類似度とるためである．
よって，この場合，大きな用例ともっとも類似している用例のどちらを採
用すると考えても，作られる翻訳は同じとなる．


\section{翻訳システムの構成}

\begin{figure*}
\begin{center}
\epsfxsize=100mm\epsfbox{f_te_c6_te.eps}
\end{center}
\caption{用例データベースの構築}
\label{f_te_c6_te.eps}
\end{figure*}

提案するシステムは，次の2つのモジュールから構成される：

\begin{enumerate}
\item \textbf{アライメント・モジュール}：コーパスから用例を構築するモジュール，
\item \textbf{翻訳モジュール}：翻訳を行うモジュール．
\end{enumerate}

\subsection{アライメント・モジュール}

\begin{description}
  \item \textbf{ステップ1：対訳文の依存構造への変換}\\
  まず，対訳文を日本語パーサKNP \cite{Kurohashi1994}と英語パーサ
  nl-parser\cite{Charniak2000}によって統語解析する．

  日本語の句の単位は，KNPの出力する文節とし，KNPの出力する依存構造をそ
  のまま以降の処理に用いる．

  英語パーサは句構造を出力するので，句構造中の主辞を決定して，出力結果
  を依存構造に変換する．
  この際，主辞の決定には人手で作成した規則を用い，句の単位はbase-NP，base-VPとした．
  \\
  \item \textbf{ステップ2：アライメント}\\
  次に，翻訳辞書を用いたアライメントを行い，両言語の句の対応関係を得る．
  これには，\cite{Aramaki2001}の手法をそのまま用いた．
  
  
  
  
  
  
  
  この手法は，辞書を使用するが，後述する実験では次の辞書を用いた：EDR電子化辞書
  \footnote{http://www2.nict.go.jp/kk/e416/EDR/J\_index.html}，
  EDICT\footnote{http://www.csse.monash.edu.au/~jwb/j\_edict.html}, 英
  辞郎\footnote{http://www.eijiro.jp/}.
  これらの辞書はのべ二百万項目を持つ．
  \\
  このステップの結果，システムは，図\ref{f_te_c6_te.eps}左のようなアライメントされた対訳文を得る．
  \\
  \item \textbf{ステップ3：用例データベースの構築}\\
  最後に，アライメントされた対訳文（図\ref{f_te_c6_te.eps}左）から，用例
  データベースを構築する．
  この際，システムは，あらゆる対応の組み合わせを生成し，その周辺の句
  （これはコンテキストの類似度を計算する際に用いる）とともにデータベー
  スに登録する（図\ref{f_te_c6_te.eps}右）．
\end{description}

\subsection{翻訳モジュール}

  \begin{description}
  \item \textbf{ステップ1：入力文の解析}\\
  まず，入力文を日本語パーサKNP\cite{Kurohashi1994}を用いて統語解析す
  る．
  \\
  \item \textbf{ステップ2：用例の選択}\\
  入力文のあらゆる可能な部分木の組み合わせに分解し
  （前章の図\ref{f_prob.eps}の左），それらの部分木それぞれについて，
  用例データベース中を検索し，前章の手法にて，その翻訳確率を計算する．
  そして，最も翻訳確率の高くなる用例の組み合わせを採用する．
  \\
  \item \textbf{ステップ3：翻訳文の生成}\\
  前ステップで採用された用例群を結合し，出力文の依存構造にまとめ上げる．
  この操作は次の2つの規則によって行われる．
  \vspace{2mm}
  \begin{enumerate}
    \item
    用例内部の依存構造は，出力文にそのまま用いる．
    例えば，2つの翻訳用例（TE1，TE2）を結合して翻訳文を生成する場
    合を考える（図\ref{f_d1.eps}）．
    ここで，TE1に含まれる2つの句($t_1$,$t_3$)の依存関係（太線
    で描かれている）は，そのまま翻訳文に用いられる．
    \\
    \item
    用例間の依存関係は，その用例が対応する入力文句の依存構造と同じ親子
    関係とする．
    例えば，図\ref{f_d1.eps}のTE1とTE2の間の依存関係について考える．
    TE2は入力文の$i_{2}$と対応しており，$i_{2}$は$i_{3}$と親子関係にある．
    よって，翻訳文側でも，$i_{2}$と対応する$t_{2}$は，$i_{3}$と対応する$t_{3}$
    と親子関係にあるものと考え，点線で描かれている依存関係を得る．
  \end{enumerate}
  
  \vspace{3mm}
  最後に，出力文の依存構造の語順を決定する．
  これは，先の依存構造の場合と同様に，用例の内部の語順は保存し，用例の
  つなぎ目の語順は，単語$n$-gram言語モデル\footnote{単語$n$-gramの学習
  は，後述する実験のトレーニングセット2万文を用いて$n=3$にて行った．}にて
  優先される語順を採用する．
  \end{description}

\begin{figure}
\begin{center}
    \epsfxsize=85mm\epsfbox{f_d1.eps}
\end{center}
\caption{出力文の生成}
\label{f_d1.eps}
\end{figure}


\end{document}
