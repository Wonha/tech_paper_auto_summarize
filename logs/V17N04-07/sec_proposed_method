確率的言語モデルとは，任意の記号列に対して，その記号列がある自然言語から生成された確率を計算する枠組みを与えるためのモデルである[CITE]．
本節では，最も一般的な確率的言語モデルの1つである単語[MATH]-gramモデルとその応用について述べる．
本項では，確率的言語モデルとして広く用いられる単語[MATH]-gramモデルならびにモデルパラメータの推定について述べる．
単語[MATH]-gramモデルは，文を単語列[MATH]とみなし，単語の生起を([MATH])重マルコフ過程で近似したモデルである．
すなわち，単語[MATH]-gramモデルにおいて，ある時点での単語[MATH]の生起は直前の[MATH]単語に依存する．
ここで，単語列[MATH]の生成確率[MATH]は以下の式で与えられる．
この式で，[MATH]と[MATH]はそれぞれ文頭と文末を表す特別な記号である．
言語モデル構築の際には，学習コーパス内で観測されたデータの生じる確率を最大にするように最尤推定法でモデルパラメータを決定することが一般的である．
最尤推定で単語[MATH]-gramモデルのパラメータ推定を行う場合は，あらかじめ単語分割されているコーパス内に出現する単語[MATH]-gramの頻度を計数し，以下の式によって単語[MATH]-gramの確率を求める．
&　　　　& P(w_i) &= \cfrac{f(w_i)}{f( \cdot)} & (\text{if}　n = 1) &　　　　&
&& P(w_{i}| \Bdma{w}_{i-n+1}^{i-1}) & = \cfrac{f(\Bdma{w}_{i-n+1}^i)}{f(\Bdma{w}_{i-n+1}^{i-1})} & (\text{if}　n > 1) &&
式([REF_equation:para1])において，[MATH]はコーパス内の単語[MATH]の出現頻度（1-gram頻度）を表し，[MATH]はコーパス内における全ての単語の出現頻度（0-gram頻度）を表す．
式([REF_equation:para2])において，[MATH]はコーパス内における連続する[MATH]単語の組の出現頻度（[MATH]-gram頻度）を表す．
ここで，未知語を含む単語列の生成確率を単語[MATH]-gramモデルで計算する場合を考える．
未知語を含む単語列の生成確率が0となることを防ぐため，未知語を表す特別な記号[MATH]を用意して，モデル構築の際に他の語彙エントリと同様に0より大きい確率を与えておく．
未知語を予測するには，まず単語[MATH]-gramモデルにより[MATH]を予測し，さらにその表記（文字列）[MATH]を以下の文字[MATH]-gramモデルにより予測する．
ここで[MATH]と[MATH]は，それぞれ語頭と語末を表す特別な記号である．
本項で述べた[MATH]-gramモデルの応用として，文献[CITE]では日本語や中国語のように分かち書きされない言語に対する形態素解析器を提案している．
また，文献[CITE]では，文献[CITE]で提案された手法の拡張として，式([REF_equation:para1])([REF_equation:para2])における[MATH]を単語，読み，アクセント，品詞の4つ組に置き換えた[MATH]-gramモデルによってテキストの読みとアクセントの推定を行うシステムを提案している．
本項では，[CITE]で提案されている確率的モデルを用いた統計的仮名漢字変換について述べる．
日本語の仮名漢字変換システムは，計算機のキーボードからの入力記号列[MATH]を仮名漢字混じり文である文字列[MATH]に変換する．
ここでは，出力を文字列[MATH]とする代わりに単語列[MATH]とし，入力記号列[MATH]に対応する候補[MATH]を以下に示す事後確率[MATH]が大きいものから順に列挙する．
最尤の変換結果[MATH]は，[MATH]をベイズの定理により以下のように変形することで求めることができる．
式([REF_equation:KKC])において，後半の[MATH]は言語モデルであり，[REF_subsect:W-ngram]節で述べた単語[MATH]-gramモデルを用いることができる．
前半の[MATH]は確率的仮名漢字モデルと呼ばれ，単語列[MATH]が与えられた際の入力記号列の生成確率を表す．
ここで述べている変換モデルでは出力を文字列[MATH]ではなく単語列[MATH]とみなしているため，単語と入力記号列との対応関係がそれぞれ独立であると仮定することで[MATH]は以下の式で表される．
ここで，部分入力記号列[MATH]は単語[MATH]に対応する入力記号列であり，全体の入力記号列は[MATH]となる．
仮名漢字モデルのパラメータ推定には，単語ごとに入力記号列が付与されたコーパスを用い，式([REF_equation:PMg])における確率[MATH]の値は，以下の式によって計算される．
ここで[MATH]は単語と読みの組の出現頻度であり，[MATH]は単語出現頻度である．
[MATH]-gramモデルの性能はパラメータ学習のためのコーパスに大きく依存する．
しかし，決定的な単語分割を行うコーパスを単語[MATH]-gramモデルのパラメータ推定に用いる場合，分割誤りによって未知語の出現頻度が0となっている可能性がある．
このようなコーパスから構築される単語[MATH]-gramモデルは未知語に対する頑健性に欠けるため，本項では，確率的単語分割コーパス並びにその近似である疑似確率的単語分割コーパス[CITE]の枠組みを用いてこの問題に対処する方法を述べる．
日本語の単語分割は，入力文における各文字間に単語境界があるかどうかを決定する問題とみなせる．
入力となるコーパスを長さ[MATH]の文字列[MATH]としたとき，確率的単語分割コーパスは隣接する2文字[MATH]と[MATH]の間に単語境界確率[MATH]を与えたものとして定義される．
ここでは，確率的単語分割コーパスを作成するために最大エントロピーモデルを用いて単語境界確率の推定を行う[CITE]．
単語境界をある文字列境界が単語境界であるか否かを決めるための素性として，単語境界の周辺[MATH]の範囲の文字[MATH]-gram ([MATH])と文字種の情報を用いる．
ここで，確率的単語分割コーパス内での単語の扱いについて述べる．
決定的に単語分割されたコーパスにおいて，単語0-gram頻度はコーパス中の全単語数，単語1-gram頻度はそれぞれの単語の出現頻度である．
確率的単語分割コーパスにおいては，単語0-gram頻度[MATH]はコーパス中に現れる全ての部分文字列の期待頻度として，以下の式で定義される．
また，確率的単語分割コーパス中のある1箇所に現れる単語[MATH]の期待頻度[MATH]は，文字列[MATH]が単語[MATH]である確率を以下に示す式から計算することで得られる．
これは[MATH]の左側（[MATH]番目の文字列境界）が単語境界，[MATH]の間にある文字列境界が単語境界ではない，[MATH]の右側が単語境界である，というときに文字列[MATH]が単語[MATH]である確率を示している．
確率的単語分割コーパス中における単語[MATH]とその期待頻度の扱いを図[REF_figure_sect2_SSC_freq]に示す．
[MATH]は1箇所の[MATH]に対する期待頻度なので，単語1-gram期待頻度はコーパス中の全ての出現にわたる期待頻度の合計となる．
単語[MATH]-gram期待頻度([MATH])についても，単語境界である確率[MATH]と単語境界ではない確率([MATH])から同様に期待頻度の計算を行う．
単語[MATH]-gram確率は，式([REF_equation:para1])([REF_equation:para2])における[MATH]-gram頻度を[MATH]-gram期待頻度として推定する．
以上に述べた確率的単語分割コーパスから構築される単語[MATH]-gramモデルは，テキスト中に出現する全ての部分文字列が語彙となるため，未知語に対して頑健なモデルとなる．
上述の確率的単語分割コーパスを用いて[MATH]-gram確率の推定を行う場合，単語の出現頻度を計算するために多くの計算時間が必要となる．
本節では，この問題に対処するために提案されている疑似確率的単語分割コーパス[CITE]の枠組みについて述べる．
これにより，決定的に単語分割されたコーパスを用いて確率的単語分割コーパスに近い[MATH]-gram確率を推定することができ，かつ未知語に対する頑健性を保持することができる．
疑似確率的単語分割コーパスは，確率的単語分割コーパスに対して以下の処理を最初の文字から最後の文字まで([MATH])行うことで得られる．
文字[MATH]を出力する．
乱数[MATH]を発生させ[MATH]と比較する．
[MATH]の場合には単語境界記号（空白）を出力し，そうでない場合には何も出力しない．
これにより，確率的単語分割コーパスの特徴をある程度反映し，かつ決定的に単語分割されたコーパスを得ることができる．
この処理を1回行って得られるコーパスにおいて，文字列としての出現頻度が低い単語[MATH]-gramの頻度は，確率的単語分割コーパスから期待頻度を計算した場合と大きく異なる可能性がある．
近似による誤差を減らすためには，上記の手続きを[MATH]回行って得られる単語分割コーパス全てを単語[MATH]-gram頻度の計数の対象とすればよい．
このコーパスを疑似確率的単語分割コーパスと呼び，[MATH]をその倍率と呼ぶ．
本節では，仮名漢字変換の対象となる分野のテキストと音声を用いて未知語の読み・文脈情報を自動獲得し，統計的仮名漢字変換で用いられる言語モデルならびに仮名漢字モデルの性能を改善させる手法について述べる．
本項では提案手法の概略について述べる．
図[REF_figure_sect4_overview]に提案手法全体の概要を示す．
本研究では，人手によって読みと単語境界が付与されている一般分野のコーパス[MATH]があらかじめ用意されているものとする．
また，以下では一般分野のコーパスから読みを取り除いたコーパスを一般分野の単語分割コーパスと記述し，その中に存在する単語を既知語，それ以外の単語を未知語と定義する．
提案手法では，以下に示す4段階の処理により，未知語の読み・文脈情報を未知語を含む単語と読みの組の列として音声認識結果から獲得し，統計的仮名漢字変換のモデルを更新する．
情報の付与されていない対象分野のテキストから疑似確率的単語分割コーパスを作成し，未知語の候補となる単語（以下，未知語候補と記述する）の抽出を行う（3.2項を参照）．
疑似確率的単語分割コーパスを用いて音声認識のための言語モデルを構築する．
また，未知語候補の読みを複数推定し，音声認識のための発音辞書を作成する（3.3項参照）．
準備した言語モデル，発音辞書，音響モデルを用いて対象分野の音声を認識し，音声認識結果から単語と読みの組の列を獲得する（3.4項を参照）．
獲得した単語と読みの組の列を統計的仮名漢字変換の学習コーパスに追加して言語モデルと仮名漢字モデルを更新する（3.5項を参照）．
以下では，これらの処理について詳細を述べる．
まず，獲得対象となる未知語候補を単語境界の付与されていない対象分野のテキストから抽出する．
本項では，[REF_subsection:SSC]項で述べた疑似確率的単語分割コーパスを用いた未知語候補の抽出について述べる．
疑似確率的単語分割コーパスは決定的に単語分割されたコーパスの集合であるが，全く同様の文であっても単語境界に揺れが存在するため，未知語の分割誤りを抑制可能である．
しかしながら，テキスト中に出現する全ての部分文字列が単語になり得るという疑似確率的単語分割コーパスの性質上，低頻度の文字列は単語として適切ではないものが多い．
このため，出現頻度閾値を設定して適切な未知語候補を抽出する．
以下では，未知語候補「守屋」を抽出する場合を例にとり，その手続きを示す．
一般分野の単語分割コーパスから単語境界確率を推定するためのモデル（[REF_subsubsection:P-SSC]項を参照）を構築し，対象分野のテキストに単語境界確率を付与する．
= 1.5pt
単語境界確率と乱数の比較を行い，倍率[MATH]の疑似確率的単語分割コーパスを作成する．
作成した疑似確率的単語分割コーパス内に出現する単語のうち，頻度[MATH]以上の未知語（一般分野のコーパスに出現しない単語）を未知語候補として抽出する．
次項では，未知語候補の音声認識を行うための言語モデルと発音辞書について述べる．
音声認識システムを用いて未知語候補を正しい読みとともに認識するためには，未知語候補が語彙に含まれる言語モデルと発音辞書が必要である．
本項では，未知語候補を考慮した言語モデルならびに発音辞書の作成方法について述べる．
まず，音声認識のための言語モデルを構築する．
大語彙連続音声認識システムを用いる場合には，対象分野のコーパスと一般分野のコーパスを用いて対象分野に適合した言語モデルの構築を行うことが一般的である[CITE] [CITE]．
本研究では，[REF_subsect:ext_unk]項で作成した疑似確率的単語分割コーパスを一般分野の単語分割コーパスに追加し，言語モデルを構築する．
次に，未知語候補の読みを複数推定し，既知語から作成された発音辞書に追加する．
読みの推定は，[REF_subsect:W-ngram]項の[MATH]-gramモデルにおける単語[MATH]を文字とその読みの組に置き換えた[MATH]-gramモデルによって行う．
以下では，未知語候補「守屋」を例にとって説明する．
単語を1文字ごとに分割し，それぞれの文字について単漢字辞書から得られる読みを列挙する．
各文字の読みを組み合わせ，可能性のある単語の読みを列挙する．
\cfbox{マモヤ,マモオク,シュヤ,シュオク,モリヤ,モリオク}
文字と読みの組を単位とする[MATH]-gramモデルにより，単語表記からの読みの生成確率を計算する．
読みが付与されている一般分野のコーパスから発音辞書を作成し，(3)で推定した未知語候補と読みの組の中から，確率の上位[MATH]個を追加する．
この際，[MATH]個の未知語候補と読みの組の生成確率を反映させるため，単語の読みごとの確率を発音辞書に記述する．
上記の例における「守屋」の正しい読みは「モリヤ」であるが，(3)で述べた[MATH]-gramモデルによって与えられる確率[MATH]は最大とならないため，確率の比較による正しい読みの選択は難しい．
次項では，本項で作成した言語モデルと発音辞書を用いた音声認識によって未知語候補の正しい読みを選択する方法について述べる．
前項の処理で発音辞書中に列挙される未知語候補の読みの中に正しい読みが含まれている場合には，音声認識によって未知語候補を含む単語と読みの組の列が得られる．
しかし，前項の処理で推定した読みの多くは誤った読みであるため，音声認識の際に似た発音の単語を取り違え，誤った読みの未知語候補を出力する可能性がある．
この問題に対処するため，ここでは言語モデルならびに音響モデルの尤度を反映した事後確率から計算される信頼度[CITE]を用いて，認識結果における単語の文脈上の妥当性を判定する．
ある単語の信頼度[MATH]は0から1の間の値で与えられ，大きい値であるほど信頼性が高いとみなされる．
以下では，音声認識を用いて未知語の読み・文脈情報を単語とその読みの列として獲得する手順を示す．
対象分野のテキストと同様の話題を扱った音声と，その音声に適合した音声認識用の音響モデルを用意する．
(1)の音響モデルと，[REF_subsect:ucest]項の処理によって得られた言語モデルならびに発音辞書を用いて(1)の音声に対し音声認識を行い，単語，読み，単語信頼度の3つ組の列を出力する．
音声認識結果のうち，単語信頼度が[MATH]以上の単語を抽出し，連続する単語とその読みの組の列を作成する．
なお，単語信頼度が[MATH]より小さい単語は抽出せず，それまでに抽出された単語とその読みの列を独立した文とみなす．
仮名漢字変換のモデル性能を改善するには，対象分野の学習コーパスを大量に用意することが重要である．
人手によって十分な量のコーパスを作成することはコストの面で実用的ではないため，まずテキストの読み推定を行うことによって対象分野のテキストに単語境界と読みを自動的に付与する．
ここでは，[REF_subsect:W-ngram]項の式(1)(2)において単語[MATH]を単語と読みの組に置き換え，読み推定のための[MATH]-gramモデルを一般分野のコーパス[MATH]から構築する．
この結果得られるコーパスを[MATH]とする．
一般的には，情報の付与されていない対象分野のテキストのみを大量に入手可能である，という状況が多いため，上述の読み推定システムや形態素解析器の利用によって大規模なコーパス[MATH]を作成し，[MATH]と[MATH]からモデルを構築することによって変換精度を向上させることが可能である．
しかしながら[MATH]は一般分野のコーパス[MATH]から構築されるモデルを用いたシステムによって単語境界や読みを付与されるため，[MATH]の内部に出現しない未知語の情報をモデルに反映させることは難しい．
この問題を解決するため，提案手法では[REF_subsect:ASR]項の処理によって獲得される，未知語を含む単語と読みの列をコーパス[MATH]とみなし，[MATH]によって未知語の読み・文脈情報をモデルに反映させ，未知語の変換精度の向上を図る．
