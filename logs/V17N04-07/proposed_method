単語$n$-gramモデルとその応用
\label{sect2_LM}

確率的言語モデルとは，任意の記号列\footnote{ここで述べる「記号」は処理単位としての記号であり，文字や単語，品詞など様々な
ものを考えることができる．}に対して，その記号列が
ある自然言語から生成された確率を計算する枠組みを与えるためのモデルである
\cite{確率的言語モデル}
．
本節では，最も一般的な確率的言語モデルの1つである
単語$n$-gramモデルとその応用について述べる．


\subsection{単語$n$-gramモデル}
\label{subsect:W-ngram}

本項では，確率的言語モデルとして広く用いられる
単語$n$-gramモデルならびにモデルパラメータの推定について述べる．

単語$n$-gramモデルは，文を単語列$\Bdma{w} = \Conc{w}{h}$とみなし，
単語の生起を($n-1$)重マルコフ過程で近似したモデルである．すなわち，
単語$n$-gramモデルにおいて，
ある時点での単語$w_{i}$の生起は直前の$(n-1)$単語に依存する．
ここで，単語列$\Bdma{w}$の生成確率$M_{w,n}(\Bdma{w})$は以下の式で与えられる．
\[
  M_{w,n}(\Bdma{w}) = 
  \displaystyle{\prod_{i=1}^{h+1}P(w_{i}| \Bdma{w}_{i-n+1}^{i-1})}
\]
この式で，$w_{i}\;(i \leq 0)$と$w_{h+1}$はそれぞれ文頭と文末を表す特別な記号である．

言語モデル構築の際には，学習コーパス内で観測されたデータの生じる確率を最大にするよ
うに最尤推定法でモデルパラメータを決定することが一般的である．
最尤推定で単語$n$-gramモデルのパラメータ推定を行う場合は，あらかじめ単語分割されている
コーパス内に出現する単語$n$-gramの頻度を計数し，以下の式によって単語$n$-gramの確率を求
める．
\begin{align}
 \label{equation:para1}
& \qquad \qquad &  P(w_i) &= \cfrac{f(w_i)}{f( \cdot )} & (\text{if} \quad  n = 1) &\qquad \qquad & \\
 \label{equation:para2}
&&  P(w_{i}| \Bdma{w}_{i-n+1}^{i-1}) & = \cfrac{f(\Bdma{w}_{i-n+1}^i)}{f(\Bdma{w}_{i-n+1}^{i-1})} & (\text{if} \quad n > 1) && 
\end{align}
式(\ref{equation:para1})において，
$f(w_i)$はコーパス内の単語$w_i$の出現頻度（1-gram頻度）を表し，
$f( \cdot )$はコーパス内における全ての単語の出現頻度（0-gram頻度）を表す．
式(\ref{equation:para2})において，$f(\Bdma{w}_{i-n+1}^i)$はコーパス内における
連続する$n$単語の組の出現頻度（$n$-gram頻度）を表す．

ここで，未知語を含む単語列の生成確率を単語$n$-gramモデルで計算する場合を考える．
未知語を含む単語列の生成確率が0となることを防ぐ
ため，未知語を表す特別な記号$\UW$を用意して，モデル構築の際に他の語彙エントリと同様
に0より大きい確率を与えておく．
未知語を予測するには，まず単語$n$-gramモデルにより$\UW$ 
を予測し，さらにその表記（文字列）$\Bdma{x}$を以下の文字$n$-gramモデルにより予
測する．
\[
  M_{x,n}(\Bdma{x}) = \prod_{i=1}^{h'+1}P(x_{i}| \Bdma{x}_{i-n+1}^{i-1})
\]
ここで$x_{i}\;(i \leq 0)$と$x_{h'+1}$は，それぞれ語頭と語末
を表す特別な記号である．

本項で述べた$n$-gramモデルの応用として，
文献\cite{統計的言語モデルとN-best探索を用いた日本語形態素解析法}
では日本語や中国語のように分かち書きされない言語に対する
形態素解析器を提案している．
また，
文献\cite{N-gramモデルを用いた音声合成のための読み及びアクセントの同時推定}では，
文献\cite{統計的言語モデルとN-best探索を用いた日本語形態素解析法}で提案された手法
の拡張として，
式(\ref{equation:para1})(\ref{equation:para2})における$w_i$を単語，読み，アクセント，品
詞の4つ組に置き換えた$n$-gramモデルによってテキストの読みとアクセントの推定を行うシス
テムを提案している．



\subsection{統計的仮名漢字変換}
\label{subsect:KKC}

本項では，\cite{確率的モデルによる仮名漢字変換}で提案されている
確率的モデルを用いた統計的仮名漢字変換について述べる．

日本語の仮名漢字変換システムは，計算機のキーボードからの入力記号列\footnote{入力記号列の記号とは，キーボードから入力可能なラテン文字，記号，仮名文字を表
す．}
$\Bdma{z}$
を仮名漢字混じり文である文字列$\Bdma{x}$に変換する．
ここでは，出力を文字列$\Bdma{x}$とする代わりに単語列$\Bdma{w}$とし，
入力記号列$\Bdma{z}$に対応する
候補$\Bdma{w}$を以下に示す事後確率$P(\Bdma{w}|\Bdma{z})$が大
きいものから順に列挙する．
\begin{equation}
 P(\Bdma{w}|\Bdma{z}) = \cfrac{P(\Bdma{z}|\Bdma{w})P(\Bdma{w})}{P(\Bdma{z})} 
\end{equation}
最尤の変換結果$\hat{\Bdma{w}}$は，
$P(\Bdma{w}|\Bdma{z})$をベイズの定理により以下のように変形することで
求めることができる．
\begin{equation}
 \label{equation:KKC}
 \hat{\Bdma{w}} = \argmax_{\Bdma{w}} P(\Bdma{z}|\Bdma{w})P(\Bdma{w})
\end{equation}
式(\ref{equation:KKC})において，後半の$P(\Bdma{w})$は言語モデルであり，
\ref{subsect:W-ngram}節で述べた
単語$n$-gramモデルを用いることができる．
前半の$P(\Bdma{z}|\Bdma{w})$は
確率的仮名
漢字モデルと呼ばれ，単語列$\Bdma{w}$が与えられた際の入力記号列の生成確率を表す．
ここで述べている変換モデルでは出力を文字列$\Bdma{x}$ではなく単語列
$\Bdma{w}$とみなしているため，単語と入力記号列との対応関係がそれぞれ独立
であると仮定することで$P(\Bdma{z}|\Bdma{w})$は以下の式で表される．
\begin{equation}
  \label{equation:PMg}
  P(\Bdma{z}|\Bdma{w}) = \prod_{i=1}^{h} P(z_{i}|w_{i})
\end{equation}
ここで，部分入力記号列$z_{i}$は単語$w_{i}$に対応する入力記号列であり，
全体の入力記号列は$\Bdma{z} = \Conc{z}{h}$となる．

仮名漢字モデルのパラメータ推定には，単語ごとに入力記号列が付与されたコーパス
を用い，
式(\ref{equation:PMg})における確率$P(z_{i}|w_{i})$の値は，
以下の式によって計算される．
\begin{equation}
 \label{equ_param_PM}
  P(z_{i}|w_{i}) = \frac{f(z_{i},\;w_{i})}{f(w_{i})}
\end{equation}
ここで$f(z_{i},\;w_{i})$は単語と読みの組の出現頻度であり，
$f(w_{i})$は単語出現頻度である．



\subsection{確率的単語分割コーパス}
\label{subsection:SSC}

$n$-gramモデルの性能は
パラメータ学習のためのコーパスに大きく依存する．
しかし，決定的な単語分割を行うコーパスを単語$n$-gramモデルのパラメータ推定に用いる場合，
分割誤りによって未知語の出現頻度が0となっている可能性がある．
このようなコーパスから構築される単語$n$-gramモデルは
未知語に対する頑健性に欠けるため，
本項では，確率的単語分割コーパス
並びにその近似である疑似確率的単語分割コーパス
\cite{擬似確率的単語分割コーパスによる言語モデルの改良}
の枠組みを用いてこの問題に対処する方法を述べる．


\subsubsection{確率的単語分割コーパスを用いた$n$-gram確率の推定}
\label{subsubsect:WBP}

日本語の単語分割は，入力文における各文字間に単語境界があるかどうかを決定する問題と
みなせる．
入力となるコーパスを
長さ$n_r$の文字列$\Bdma{x}_1^{n_r} = \Conc{x}{n_r}$
としたとき，
確率的単語分割コーパスは
隣接する2文字$x_{i}$と$x_{i+1}$の間に単語境界確率$P_{i}$を与えたものとして
定義される．
ここでは，確率的単語分割コーパスを作成するために最大エントロピーモデルを用い
て単語境界確率の推定を行う
\cite{擬似確率的単語分割コーパスによる言語モデルの改良}．
単語境界をある文字列境界が単語境界であるか否かを決めるための素性として，
単語境界の周辺$x_{i-2}^{i+2}$の範囲の文字
$n$-gram ($n = 1, 2, 3$) と文字種の情報を用いる．

ここで，確率的単語分割コーパス内での単語の扱いについて述べる．
決定的に単語分割されたコーパスにおいて，単語0-gram頻度はコーパス中の全単語数，単語
1-gram頻度はそれぞれの単語の出現頻度である．確率的単語分割コーパスにおいては，単語
0-gram頻度$f(\cdot)$はコーパス中に現れる全ての部分文字列の期待頻度として，以下の式
で定義される．
\[
 f(\cdot) = 1 + \sum_{i=1}^{n_{r}-1} P_{i}
\]
また，確率的単語分割コーパス中のある1箇所に現れる単語$w$の期待頻度$f(w)$
は，文字列$x_{i+1} x_{i+2}\cdots x_{i+k}$が単語$w$である確率を以下に示す式から
計算することで得られる．
\[
     f(w) = 
     P_{i}\left[\prod_{j=1}^{k-1} (1-P_{i+j})\right]
     P_{i+k}
\]
これは$x_{i+1}$の左側（$i$番目の文字列境界）が単語境界，$x_{i+1} x_{i+2}\cdots x_{i+k}$
の間にある文字列境界が単語境界ではない，$x_{i+k}$の右側が単語境界である，というときに
文字列$x_{i+1} x_{i+2}\cdots x_{i+k}$が単語$w$である確率を示している．
確率的単語分割コーパス中における単語$w$とその期待頻度の扱いを
図\ref{figure_sect2_SSC_freq}に示す．
$f(w)$は1箇所の$w$に対する期待頻度なので，単語1-gram
期待頻度はコーパス中の全ての出現にわたる期待頻度の合計
となる．単語$n$-gram期待頻度 ($n\geq 2$) についても，
単語境界である確率$P_i$と単語境界ではない確率($1-P_i$)から同様に期待頻度の計算を行う．
単語$n$-gram確率は，式(\ref{equation:para1})(\ref{equation:para2})における
$n$-gram頻度を$n$-gram期待頻度として推定する．

以上に述べた確率的単語分割コーパスから構築される単語$n$-gramモデルは，テキスト中に出現
する全ての部分文字列が語彙となるため，未知語に対して頑健なモデルとなる．

\begin{figure}[t]
  \begin{center}
\includegraphics{17-4ia8f1.eps}
  \end{center}
  \caption{確率的単語分割コーパスにおける期待頻度}
  \label{figure_sect2_SSC_freq}
\end{figure}


\subsubsection{疑似確率的単語分割コーパス}
\label{subsubsection:P-SSC}

上述の確率的単語分割コーパスを用いて$n$-gram確率の推定を行う場合，単語の出現頻度を計算するために多くの計算時間が必要となる．

本節では，この問題に対処するために提案されている
疑似確率的単語分割コーパス
\cite{擬似確率的単語分割コーパスによる言語モデルの改良}
の枠組みについて述べる．
これにより，決定的に単語分割されたコーパスを用いて確率的単語分割コーパスに近い$n$-gram
確率を推定することができ，かつ未知語に対する頑健性を保持することができる．

疑似確率的単語分割コーパスは，確率的単語分割コーパスに対して以下の処理を最初の
文字から最後の文字まで($1 \leq i \leq n_{r}$)行うことで得られる．
\begin{enumerate}

\item 文字$x_{i}$を出力する．

\item 乱数$r_{i} (0 \leq r_{i} < 1)$を発生させ$P_{i}$と比較する．$r_{i} < P_{i}$の場合
      には単語境界記号（空白）を出力し，そうでない場合には何も出力しない．

\end{enumerate}
これにより，確率的単語分割コーパスの特徴をある程度反映し，かつ決定的に単語分割されたコ
ーパスを得ることができる．
この処理を1回行って得られるコーパスにおいて，文字列としての出現頻度が低い単語$n$-gram
の頻度は，確率的単語分割コーパスから期待頻度を計算した場合と大きく異なる可能性がある．
近似による誤差を減らすためには，上記の手続きを$M$回行って得られる単語分割コーパス全て
を単語$n$-gram頻度の計数の対象とすればよい．
このコーパスを
疑似確率的単語分割コーパスと呼び，$M$をその倍率と呼ぶ．



未知語とその読み・文脈情報の自動獲得
\label{sect3_Ext}

本節では，仮名漢字変換の対象となる分野のテキストと音声を用いて
未知語の読み・文脈情報を自動獲得し，
統計的仮名漢字変換で用いられる言語モデルならびに仮名漢字モデル
の性能を改善させる
手法について述べる．



\subsection{提案手法の概略}
\label{subsect:ext_overview}

\begin{figure}[b]
  \begin{center}
\includegraphics{17-4ia8f2.eps}
  \end{center}
  \caption{提案手法の概要図}
  \label{figure_sect4_overview}
\end{figure}

本項では提案手法の概略について述べる．
図\ref{figure_sect4_overview}に提案手法全体の概要を示す．
本研究では，人手によって読みと単語境界が付与されている一般分野のコーパス
$C_b$があらかじめ用意されているものとする．
また，以下では一般分野のコーパスから読みを取り除いたコーパスを
一般分野の単語分割コーパスと記述し，
その中に存在する単語を既知語，それ以外の単語を未知語と定義する．

提案手法では，以下に示す4段階の処理により，
未知語の読み・文脈情報を未知語を含む単語と読みの組の列として
音声認識結果から獲得\footnote{
音声認識には大語彙音声認識システムJulius
\cite{Julius.--.An.Open.Source.Real-Time.Large.Vocabulary.Recognition.Engine}
を用いる．}
し，統計的仮名漢字変換のモデルを更新する．
\begin{enumerate}
 \item 情報の付与されていない対象分野のテキストから疑似確率的単語分割コーパスを作成し，
       未知語の候補となる単語（以下，未知語候補と記述する）の抽出を行う（3.2項を参照）．
 \item 疑似確率的単語分割コーパスを用いて音声認識のための言語モデルを構築する．
       また，未知語候補の読みを複数推定し，音声認識のための発音辞書を作成する
       （3.3項参照）．
 \item 準備した言語モデル，発音辞書，音響モデルを用いて
       対象分野の音声を認識し，音声認識結果から単語と読みの組の列を獲得する
       （3.4項を参照）．
 \item 獲得した単語と読みの組の列を統計的仮名漢字変換の学習コーパスに追加して言語
       モデルと仮名漢字モデルを更新する（3.5項を参照）．
\end{enumerate}
以下では，これらの処理について詳細を述べる．


\subsection{疑似確率的単語分割コーパスを用いた未知語候補の抽出}
\label{subsect:ext_unk}

まず，獲得対象となる未知語候補を単語境界の付与されていない
対象分野のテキストから抽出する．
本項では，
\ref{subsection:SSC}項で述べた
疑似確率的単語分割コーパスを用いた未知語候補の抽出について述べる．

疑似確率的単語分割コーパスは決定的に単語分割されたコーパスの集合であるが，
全く同様の文であっても単語境界に揺れが存在するため，未知語の分割誤り
を抑制可能である．しかしながら，テキスト中に出現する全ての部分文字列が単語になり得ると
いう疑似確率的単語分割コーパスの性質上，低頻度の文字列は
単語として適切ではないものが多い．このため，出現頻度閾値を設定して適切な
未知語候補を抽出する．

以下では，
未知語候補「守屋」を抽出する場合を例にとり，その手続きを示す．
\begin{enumerate}
 \item 一般分野の単語分割コーパスから単語境界確率を推定するためのモデル
       （\ref{subsubsection:P-SSC}項を参照）を構築し，
       対象分野のテキストに単語境界確率を付与する．
{\tabcolsep = 1.5pt
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|ccccccccccccccccccccc|}\hline
	 $\cdots$&&昨&&日&&、&&\underline{守}&&\underline{屋}&&前&&次&&官&&が
	 &&$\cdots$ \\
	 $\cdots$&0.8&&0.1&&0.9&&0.9&&0.4&&0.7&&0.8&&0.3&&0.8&&0.8&$\cdots$ \\\hline
	\end{tabular}
       \end{center}
       \vspace{10pt}
}
 \item 単語境界確率と乱数の比較を行い，倍率$M$の疑似確率的単語分割コーパスを作成する．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|c|c|}\hline
	 （試行1） &  $\cdots$ 昨日　、　\underline{守屋}　前　次官　が$\cdots$\\
	 （試行2） & $\cdots$ 昨日　、　\underline{守屋}　前次官　が$\cdots$\\
	 （試行3） & $\cdots$ 昨日　、　守　屋前　次官　が$\cdots$\\
	 $\vdots$ & $\vdots$\\
	 （試行$M$） & $\cdots$ 昨日　、　\underline{守屋}　前　次　官が$\cdots$
	 \\\hline
	\end{tabular}
       \end{center}
       \vspace{10pt}
 \item 作成した疑似確率的単語分割コーパス内に出現する単語のうち，
       頻度$F_{th}$以上の未知語（一般分野のコーパスに出現しない単語）
       を未知語候補として抽出する．
\end{enumerate}
次項では，未知語候補の音声認識を行うための言語モデルと発音辞書について述べる．


\subsection{未知語候補を含む言語モデルと発音辞書の作成}
\label{subsect:ucest}

音声認識システムを用いて未知語候補を正しい読みとともに認識するためには，
未知語候補が語彙に含まれる言語モデルと発音辞書が必要である．
本項では，未知語候補を考慮した言語モデルならびに発音辞書の作成方法について述べる．

まず，音声認識のための言語モデルを構築する．
大語彙連続音声認識システムを用いる場合には，対象分野のコーパスと一般分野のコーパスを用
いて対象分野に適合した言語モデルの構築を行うことが一般的である
\cite{Task.adaptation.in.stochastic.language.models.for.continuous.speech.recognition}
\cite{N-gram出現回数の混合によるタスク適応の性能解析}
．
本研究では，\ref{subsect:ext_unk}項で作成した疑似確率的単語分割コーパスを
一般分野の単語分割コーパスに追加し，言語モデルを構築する．

次に，未知語候補の読みを複数推定し，
既知語から作成された発音辞書に追加する．
読みの推定は，\ref{subsect:W-ngram}項の
$n$-gramモデルにおける単語
$w$を文字とその読みの組に置き換えた$n$-gramモデルによって行う．
以下では，未知語候補「守屋」を例にとって説明する．
\begin{enumerate}
 \item 単語を1文字ごとに分割し，それぞれの文字について単漢字辞書から得られる読みを列挙
       する．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|ll|}\hline
	 守：&マモ，シュ，モリ\\
	 屋：&ヤ，オク\\\hline
	\end{tabular}
        \end{center}
        \vspace{10pt}
 \item 各文字の読みを組み合わせ，可能性のある単語の読みを列挙する．
       \vspace{10pt}
       \cfbox{マモヤ, マモオク, シュヤ, シュオク, モリヤ, モリオク}
       \vspace{10pt}
 \item 文字と読みの組を単位とする$n$-gramモデルにより，単語表記からの読みの生成確率
       を計算する．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|rcr|}\hline
       $P(マモヤ|守屋)$ &$=$& $0.53$\\
       $P(モリヤ|守屋)$ &$=$& $0.22$\\
       $P(シュオク|守屋)$ &$=$& $0.05$\\
	 \multicolumn{3}{|c|}{$\vdots$}\\\hline
	\end{tabular}
        \end{center}
        \vspace{10pt}
 \item 読みが付与されている一般分野のコーパスから発音辞書を作成し，
       (3)で推定した未知語候補と読みの組の中から，確率の上位$L$個を追加する．
       この際，$L$個の未知語候補と読みの組の生成確率を反映させるため，
       単語の読みごとの確率を発音辞書に記述する\footnote{
	上位$L$個の確率の合計が1となるように正規化を行う．}．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|rrr|rrr|}\hline
	 \multicolumn{3}{|c|}{既知語}&\multicolumn{3}{c|}{未知語候補}\\\hline
	 \multicolumn{3}{|c|}{$\vdots$}&\multicolumn{3}{c|}{$\vdots$}\\
       国会 & 1.00 & コッカイ&\underline{守屋} & 0.53 & マモヤ\\
       前 & 0.50 & ゼン & \underline{守屋} & 0.22 & モリヤ\\
       前 & 0.50 & マエ & \underline{守屋} & 0.05 & シュオク\\
	 \multicolumn{3}{|c|}{$\vdots$}&\multicolumn{3}{c|}{$\vdots$}\\\hline
	\end{tabular}
        \end{center}
       \vspace{10pt}
\end{enumerate}

上記の例における「守屋」の正しい読みは「モリヤ」であるが，(3)で述べた
$n$-gramモデルによって与えられる確率$P(モリヤ|守屋)$は最大とならないため，
確率の比較による正しい読みの選択は難しい．
次項では，本項で作成した言語モデルと発音辞書を用いた音声認識によって
未知語候補の正しい読みを選択する方法について述べる．


\subsection{未知語の読み・文脈情報の獲得}
\label{subsect:ASR}

前項の処理で発音辞書中に
列挙される未知語候補の読みの中に正しい読みが含まれている場合には，
音声認識によって未知語候補を含む単語と読みの組の列が得られる．
しかし，前項の処理で推定した読みの多くは誤った読みであるため，
音声認識の際に似た発音の単語を取り違え，
誤った読みの未知語候補を出力する可能性がある．
この問題に対処するため，ここでは
言語モデルならびに音響モデルの尤度を反映した事後確率から計算される信頼度
\cite{Confidence.measures.for.large.vocabulary.continuous.speech.recognition}
\footnote{ある単語を含む全ての
単語列候補
（音声認識結果）の相対的な尤度の比率を，その単語の
信頼度として表す．
なお，
本研究で用いる音声認識システムJuliusに
実装されている単語信頼度は，信頼度計算の対象となる単語を含む最尤パスの確率で全
体の確率の和を近似することによって計算される
\cite{2パス探索アルゴリズムにおける高速な単語事後確率に基づく信頼度算出法}
．}を用いて，認識結果における単語の文脈上の妥当性を判定する．
ある単語の信頼度$CM$は0から1の間の値で与えられ，
大きい値であるほど信頼性が高いとみなされる．

以下では，音声認識を用いて未知語の読み・文脈情報を
単語とその読みの列として獲得する手順を示す．
\begin{enumerate}
 \item  対象分野のテキストと同様の話題を扱った音声と，その音声に適合
       した音声認識用の音響モデルを用意する．
 \item (1)の音響モデルと，\ref{subsect:ucest}項の処理によって得られた
       言語モデルならびに発音辞書を用いて(1)の音声に対し音声認識を行い，単語，読み
       ，単語信頼度の3つ組の列を出力する．
       \vspace{-5pt}
       \begin{center}
	\begin{tabular}{|c|}\hline
	 $\cdots$　\underline{\mbox{守屋/モリヤ/0.7}}　前/ゼン/0.8　事務/ジム/0.8　次官/ジカ
	 ン/0.9　
	 $\cdots$\\
	 $\cdots$ 全体/ゼンタイ/0.4 の/ノ/0.7
	 守屋/シュヤ/0.05
	 が/ガ/0.8 狭/セマ/0.9 い/イ/0.9 
	 $\cdots$
	 \\\hline
	\end{tabular}
       \end{center}
       \vspace{10pt}
 \item 音声認識結果のうち，単語信頼度が$CM_{th}$以上の単語を抽出し，
       連続する単語とその読みの組の列を作成する．
       なお，単語信頼度が$CM_{th}$より小さい単語は抽出せず，それまでに抽出された単語と
       その読みの列を独立した文
       とみなす．
       \vspace{10pt}
       \begin{center}
	\begin{tabular}{|c|}\hline
	 $\cdots$　\underline{\mbox{守屋/モリヤ}}　前/ゼン　事務/ジム　次官/ジカン
	 $\cdots$\\
	 $\cdots$　全体/ゼンタイ　の/ノ\\
	 が/ガ　狭/セマ　い/イ　$\cdots$
	 \\\hline
	\end{tabular}
       \end{center}
       \vspace{10pt}
\end{enumerate}



\subsection{統計的仮名漢字変換のためのモデル構築}
\label{subsect:mkmodel}

仮名漢字変換のモデル性能を改善するには，対象分野の学習コーパスを大量に用意すること
が重要である．
人手によって十分な量のコーパスを作成することはコストの面で実用的ではないため，
まずテキストの読み推定を行うこと
によって対象分野のテキストに単語境界と読みを自動的に付与する．
ここでは，\ref{subsect:W-ngram}項の式(1)(2)において
単語$w$を単語と読みの組に置き換え，読み推定のための$n$-gramモデル
を一般分野のコーパス$C_b$から構築する．
この結果得られるコーパスを$C_n$とする．
一般的には，情報の付与されていない対象分野のテキストのみを大量に入手可能である，
という状況が多いため，上述の読み推定システム
や形態素解析器の利用によって大規模なコーパス$C_n$を作成し，$C_b$と$C_n$からモデルを構
築することによって変換精度を向上させることが可能である．
しかしながら$C_n$は一般分野のコーパス$C_b$から構築されるモデルを用いた
システムによって単語境界や読みを付与
されるため，
$C_b$の内部に出現しない未知語の情報をモデルに反映させることは難しい．
この問題を解決するため，提案手法では\ref{subsect:ASR}項の処理によって獲得される，
未知語を含む単語と読みの列をコーパス$C_r$とみなし，
$C_r$によって未知語の読み・文脈情報をモデルに反映させ，未知語の変換精度の向上を図る．



