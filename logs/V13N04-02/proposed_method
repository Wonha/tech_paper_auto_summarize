確率的言語モデル
\label{section:LM}

自然言語処理における確率的言語モデルの役割は，与えられた文字列がある言語の文である尤
度を数値化することである．確率的言語モデルに基づく言語処理は，候補から解を選択する際
にこの尤度を参照する．自動単語分割は解析系の一例であり，文字列が与えられると尤度が最
大になる単語の列を計算する．認識系の代表例の音声認識では，音響信号列を入力として，尤
度が最大となる文字列を算出する際に，音響モデルと併せて確率的言語モデルを参照する．



\subsection{確率的言語モデル}

最も一般的な言語モデルは，単語$w$を単位とする$n$-gramモデル$M_{w,n}$である．このモデ
ルは，文を単語列$\Bdma{w}_{1}^{h} = \Conc{w}{h}$ ($h$は単語数)とみなし，これらを文頭
から順に予測する\footnote{以下では，太字は列を表すとし，必要に応じて最初の要素の添字
を右下に，最後の要素の添字を右上に書くこととする．}．
\begin{displaymath}
  M_{w,n}(\Bdma{w}_{1}^{h}) = \prod_{i=1}^{h+1}P(w_{i}| \Bdma{w}_{i-n+1}^{i-1})
\end{displaymath}
この式の中の$w_{i}\;(i \leq 0)$は，文頭に対応する特別な記号であり，$w_{h+1}$は，文末
に対応する特別な記号である．完全な語彙を定義することは不可能であるから，未知語を表わ
す特別な記号$\UW$を用意する．未知語の予測の際は，まず，単語$n$-gramモデルにより$\UW$ 
を予測し，さらにその表記を文字$x$の列$\Bdma{x}_{1}^{h'}$ ($h'$は文字数)とみなし，文字
を単位とする$n$-gramモデル$M_{x,n}$によって以下のように予測する．
\begin{displaymath}
  M_{x,n}(\Bdma{x}_{1}^{h'}) = \prod_{i=1}^{h'+1}P(x_{i}| \Bdma{x}_{i-n+1}^{i-1})
\end{displaymath}
この式の中の$x_{i}\;(i \leq 0)$は，語頭に対応する特別な記号であり，$x_{h'+1}$は，語末
に対応する特別な記号である．したがって，$w_{i}$が未知語の場合には以下のように予測され
る\footnote{正確には，履歴$\Bdma{w}_{i-n+1}^{i-1}$に含まれるすべての未知語は$\UW$に置
き換えられ同一視される．}．
\begin{displaymath}
  P(w_{i}|\Bdma{w}_{i-n+1}^{i-1})
  = M_{x,n}(w_{i})P(\UW|\Bdma{w}_{i-n+1}^{i-1})
\end{displaymath}



\subsection{応用}
\label{subsection:word-segmenter}

確率的言語モデルの応用は，自然言語認識と自然言語解析に大別できる．

認識系の代表例は，音声認識である．確率的言語モデルを用いる音声認識では，以下の式のよ
うに，音響特徴量の列$\Bdma{s}$を入力とし，語彙${\cal W}_{k}$のクリーネ閉包(空列を含む
任意長の文字列の集合)のうち，確率最大となる要素(単語列) $\Bdma{w}$を出力する．
\begin{displaymath}
  \hat{\Bdma{w}}
  = \argmax_{\Bdma{w} \in {\cal W}_{k}^{*}} P(\Bdma{s}|\Bdma{w})P(\Bdma{w})
\end{displaymath}
この式における$P(\Bdma{w})$が確率的言語モデルである．確率的言語モデルの予測力と認識
系の精度との関係は，解析的に導出できるような確固とした関係ではない．音声認識に対して
実験的に得られた関係として，西村ら
\cite{単語を認識単位とした日本語ディクテーションシステム}は相関係数0.6を報告している．

解析系の代表例は，単語分割(と品詞付与)である．確率的言語モデルによる単語分割
\cite{A.Stochastic.Japanese.Morphological.Analyzer.Using.a.Forward-DP.Backward-A*.N-Best.Search.Algorithm} 
は，以下の式が示すように，ある言語の文字列$\Bdma{x}$を入力とし，生成確率が最大となる
単語列$\Bdma{w}$を出力する．
\begin{displaymath}
  \hat{\Bdma{w}} = \argmax_{\Bdma{w}=\Bdma{x}}P(\Bdma{w})
\end{displaymath}
ここで$\Bdma{w}=\Bdma{x}$は，単語列$\Bdma{w}$を文字列とみなした場合，入力$\Bdma{x}$
と等しいことを表す．


単語リストと生コーパスによる分野適応
\label{section:adaptation}

この節では，適応対象の分野の単語リストと，それらが出現する生コーパスが利用可能である
場合に，それらから確率的言語モデルを推定する方法を述べる．



\subsection{確率的単語分割コーパスからの単語$n$-gram確率の推定}

単語分割済みコーパスは，各文字間に単語境界が存在するか否かの情報が人手により付与され
ている．生コーパスはこの情報を持たないが，各文字間に単語境界が存在する確率を付与し，
それによって生コーパスを確率的に単語に分割されたコーパス(確率的単語分割コーパス)とみ
なすことにより，無限の語彙に対する単語$n$-gram頻度や単語$n$-gram確率を計算する方法が
提案されている\cite{Word.N-gram.Probability.Estimation.From.A.Japanese.Raw.Corpus}．
以下では，この方法を説明する．

生コーパス$C_{r}$(以下，長さ$n_{r}$の文字列$\Bdma{x}_1^{n_{r}}$として参照)を所与とし
て，連続する2 文字$x_{i},x_{i+1}$の間に単語境界が存在する確率$P_{i}$を付与したものを
考える．最初の文字の前と最後の文字の後には単語境界が存在するとみなせるので，$i = 0,\;
i = n_{r}$の時は便宜的に$P_{i} = 1$とする．

\begin{list}{}{}

\item[\textbf{単語0-gram頻度}] 確率的単語分割コーパスにおける単語0-gram頻度$f_{r}
  (\cdot)$は，そのコーパス中の期待単語数であり，以下のように定義される．
  \begin{displaymath}
    f_{r}(\cdot) = 1 + \sum_{i=1}^{n_{r}-1} P_{i}
  \end{displaymath}

\item[\textbf{単語1-gram頻度}] 確率的に単語分割された生コーパスに出現する文字列
  $\Bdma{x}_{i+1}^{k}$が$l = k-i$ 文字からなる単語$w$である必要十分条件は以下の4つで
  ある．
  \begin{enumerate}
  \item 文字列$\Bdma{x}_{i+1}^{k}$が単語$w$に等しい．
  \item 文字$x_{i+1}$の直前に単語境界がある．
  \item 単語境界が文字列中にない．
  \item 文字$x_{k}$の直後に単語境界がある．
  \end{enumerate}
  したがって，単語$w$の生コーパス中の単語1-gram頻度$f_{r}$は，単語$w$の表記の全ての
  出現$O_{1} = \{(i,k)\,|\,\Bdma{x}_{i+1}^{k} = w\}$に対する期待頻度の和として以下の
  ように定義される．
  \begin{displaymath}
    f_{r}(w)
    = \sum_{(i,k) \in O_{1}}P_{i}\left[\prod_{j=i+1}^{k-1}(1-P_{j})\right]P_{k}
  \end{displaymath}

\item[\textbf{単語$n$-gram頻度}] 単語1-gram頻度と同様に，$L$文字からなる単語列$\Bdma{w}
  _{1}^{n} = \Bdma{x'}_{1} ^{L}$ の生コーパス$\Bdma{x}_{1}^{n_{r}}$における頻度，すな
  わち単語$n$-gram頻度について考える．このような単語列に相当する文字列が生コーパスの
  $(i+1)$文字目から始まり$k = i+L$文字目で終る文字列と等しく($\Bdma{x}_{i+1}^{k} =
  \Bdma{x'}_{1}^{L} $)，単語列に含まれる各単語$w_{m}$に相当する文字列が生コーパスの
  $b_{m}$文字目から始まり$e_{m}$文字目で終る文字列と等しい($\Bdma{x}_{b_{m}}^{e_{m}}
  = w_{m},\; 1 \leq \forall m \leq n$; $e_{m}+1=b_{m+1},\; 1 \leq \forall m \leq n-1
  $; $b_{1} = i+1$; $e_{n} = k$)状況を考える(\figref{figure:SSC}参照)．単語1-gram頻度
  の計算の場合と同様に，単語列と生コーパスの部分文字列は，文字列として対応しているこ
  とに加えて，各文字間における単語境界の有無も対応している場合にのみ単語列が出現して
  いると考えられる．したがって，確率的に単語分割されたコーパスに出現する文字列
  $\Bdma{x}_{i+1}^{k}$が単語列$\Bdma{w}_{1}^{n}=\Bdma{x'}_{1}^{L}$ である必要十分条件
  は以下の4つである．

\begin{figure}[t]
  \begin{center}
        \includegraphics[scale=0.6]{SSC.eps}
  \end{center}
  \caption{確率的単語分割コーパスにおける単語$n$-gram頻度}
  \label{figure:SSC}
\end{figure}

  \begin{enumerate}
  \item 文字列$\Bdma{x}_{i+1}^{k}$が単語列$\Bdma{w}_{1}^{n}$に等しい．
  \item 文字$x_{i+1}$の直前に単語境界がある．
  \item 単語境界が各単語に対応する文字列中にない．
  \item 単語境界が各単語に対応する文字列の後にある．
  \end{enumerate}
  したがって，生コーパスにおける単語$n$-gram頻度を以下のように定義することができる．
  \begin{displaymath}
    f_{r}(\Bdma{w}_{1}^{n})
    = \!\!\!
    \sum_{(i,e_{1}^{n}) \in O_{n}} \!\!\!\! P_{i} \left[
    \prod_{m=1}^{n} \! \left\{
    \prod_{j=b_{m}}^{e_{m}-1} \!\! (1-P_{j}) \right\}
    P_{e_{m}} \right]
  \end{displaymath}
  ここで
  \begin{eqnarray*}
    e_{1}^{n}
    & = & (e_{1},e_{2},\cdots,e_{n}) \\
    O_{n}     
    & = & \{(i,e_{1}^{n}) | \Bdma{x}_{b_{m}}^{e_{m}} = w_{m}, 1 \leq m \leq n \}
  \end{eqnarray*}
  とした．

\item[\textbf{単語1-gram確率}] 決定的に単語に分割されたコーパスからの単語1-gram確率の最
  尤推定の場合と同様に，確率的単語分割コーパスにおける単語1-gram確率を以下のように定
  義する．
  \begin{equation}
    \label{equation:1-gram}
    P_{r}(w) = \frac{f_{r}(w)}{f_{r}(\cdot)}
  \end{equation}

\item[\textbf{単語$n$-gram確率}] 決定的に単語に分割されたコーパスからの単語$n$-gram確率
  の最尤推定の場合と同様に，確率的単語分割コーパスにおける単語$n$-gram確率を以下のよ
  うに定義する．
  \begin{equation}
    \label{equation:n-gram}
    P_r(w_{n}|\Bdma{w}_{1}^{n-1})
    = \frac{f_r(\Bdma{w}_{1}^{n})}{f_r(\Bdma{w}_{1}^{n-1})}
  \end{equation}

\end{list}



\subsection{有限の語彙に対する確率的単語分割コーパスからの単語$n$-gram確率の推定}

確率的言語モデルを用いる音声認識においては，認識される語彙には発音が付与されている必
要がある．また，確率的言語モデルを用いる仮名漢字変換においてもキー入力列が付与されて
いる表記(単語)のみが変換結果として出現し得る．このように，現実的な応用では有限の語彙
に対する確率的言語モデルを構築する必要がある．分野適応において単語リストが与えられて
いる場合には，一般コーパスから得られる語彙と対象分野の単語リストを語彙として，対象分
野の生コーパスから確率的言語モデルを構築する．この際に，未知語モデルを含めて確率的言
語モデルの条件を満たすためには，未知語記号を含む単語$n$-gram確率を正しく定義する必要
がある．

単語分割済みコーパスにおいては，まず語彙${\cal W}_{k}$に属さない単語をコーパスの全て
の出現場所において未知語記号$\UW$に置き換え，その上で未知語記号を語彙に含まれる単語
と同様に扱って頻度計算を行なう．決定的に単語に分割されていない確率的単語分割コーパス
に対しては，この方法を採ることができない．また，語彙以外の任意の文字列に対する単語
$n$-gram頻度を計数しその和を計算する方法も考えられる．語彙以外の任意の文字列は，実際
には無限集合ではなく，コーパスの部分文字列のみを対象とすれば十分であるが，これは非常
に大きな数となるので，この計算方法も現実的ではない．しかしながら，単語$n$-gram 頻度
の以下の性質を用いることにより，確率的単語分割コーパスに対しても未知語記号を含む単語
$n$-gram頻度を容易に計算することができる．
\begin{eqnarray}
  \label{equation:UT=sum}
  f_{r}(\Bdma{w}_{u}\UW\Bdma{w}_{v})
    & = & \!\!\!\!\!\!\!
    \sum_{w \in {\cal X}^{+}-{\cal W}_{k}}f_{r}(\Bdma{w}_{u}w\Bdma{w}_{v}) \\
  \label{equation:decomp}
  \sum_{w \in {\cal X}^{+}} \!\!\! f_{r}(\Bdma{w}_{u}w\Bdma{w}_{v})
    & = & \!\!\!\!\!\!\!
    \sum_{w \in {\cal X}^{+}-{\cal W}_{k}}
    \!\!\!\! f_{r}(\Bdma{w}_{u}w\Bdma{w}_{v})
    + \!\! \sum_{w \in {\cal W}_{k}} \!\!\! f_{r}(\Bdma{w}_{u}w\Bdma{w}_{v})
\end{eqnarray}
ここで$\Bdma{w}_{u},\Bdma{w}_{v} \in ({\cal W}_k\cup\{\UW\})^{*}$は語彙と未知語記号か
らなる長さ0以上の任意の列であり，${\cal X}^{+}$は文字集合${\cal X}$の正閉包(1文字以上
の任意長の文字列の集合)を表す．\equref{equation:UT=sum}の意味は，ある1箇所に未知語記
号を含む単語$n$-gram頻度がその箇所を既知語以外のすべての文字列に置き換えた単語
$n$-gram頻度の合計に等しいということである．また\equref{equation:decomp}は，単語
$n$-gram頻度においてある箇所の単語を任意とした場合の合計が，その箇所が任意の既知語
(${\cal W}_{k}$)である場合の頻度の合計と任意の未知語(${\cal X}^{+}-{\cal W}_{k}$)であ
る場合の頻度の合計の和に等しいことを意味する．
\equref{equation:UT=sum}(\ref{equation:decomp})から，ある1箇所に未知語を含む単語
$n$-gramの頻度に対して以下の式が成り立つことがわかる\footnotemark．\footnotetext{正確
には複数の未知語記号を含む$n$-gramに対するの記述も必要であるが，式が繁雑になるため，
ここでは1つの未知語記号のみを含む場合のみ記述した．}
\begin{equation}
  \label{equation:UT}
  f_{r}(\Bdma{w}_{u}\UW\Bdma{w}_{v}) = 
    \!\!\! \sum_{w \in {\cal X}^{+}} \!\!\! f_{r}(\Bdma{w}_{u}w\Bdma{w}_{v})
    - \!\!\! \sum_{w \in {\cal W}_{k}} \!\!\! f_{r}(\Bdma{w}_{u}w\Bdma{w}_{v})
\end{equation}

\begin{list}{}{}
\item[\textbf{未知語記号の単語1-gram頻度}] 確率的単語分割コーパスにおける未知語記号の単
  語1-gram頻度$f_{r}(\UW)$は，コーパスに対して計数した単語1-gram頻度と単語0-gram頻度
  に対して成り立つ関係
  \begin{displaymath}
    f_{r}(\cdot) = \sum_{w \in {\cal X}^{+}} f_{r}(w)
  \end{displaymath}
  と式(\ref{equation:UT})において$\Bdma{w}_{u} = \Bdma{w}_{v} = \varepsilon$
  ($\varepsilon$は空列を表す)とすることで得られる等式
  \begin{displaymath}
    f_{r}(\UW) = \sum_{w \in {\cal X}^{+}} \!\!\! f_{r}(w)
    - \!\! \sum_{w \in {\cal W}_{k}} \!\!\! f_{r}(w)
  \end{displaymath}
  から以下のように，単語0-gram頻度と語彙に対する単語1-gram頻度の和から計算される．
  \begin{displaymath}
    f_{r}(\UW) = f_{r}(\cdot) - \sum_{w \in {\cal W}_{k}}f_{r}(w)
  \end{displaymath}

\item[\textbf{未知語記号を含む単語2-gram頻度}] 任意の単語$w_{1} \in {\cal W}_{k}$と未知
  語記号からなる列の確率的単語分割コーパスにおける頻度$f_{r}(w_{1}\UW)$はコーパスに対
  して計数した単語2-gram頻度と単語1-gram頻度に対して成り立つ関係
  \begin{displaymath}
    f_{r}(w_{1}) = \sum_{w \in {\cal X}^{+}} f_{r}(w_{1}w),
    \;\;\;\forall w_{1} \in {\cal W}_k\cup\{\UW\}
  \end{displaymath}
  と式(\ref{equation:UT})において$\Bdma{w}_{u} = w_{1},\;\Bdma{w}_{v} = \varepsilon$ 
  とすることで得られる等式
  \begin{displaymath}
    f_{r}(w_{1}\UW) = \sum_{w \in {\cal X}^{+}} \!\!\! f_{r}(w_{1}w)
    - \!\! \sum_{w \in {\cal W}_{k}} \!\!\! f_{r}(w_{1}w)
  \end{displaymath}
  から以下のように単語1-gram頻度と既知語に対する単語2-gram頻度の和から計算される．
  \begin{displaymath}
    f_{r}(w_{1}\UW) = f_{r}(w_{1}) - \sum_{w \in
      {\cal W}_{k}}f_{r}(w_{1}w)
  \end{displaymath}
  同様に未知語記号と任意の単語$w_{2} \in {\cal W}_{k}$からなる列の確率的単語分割コー
  パスにおける頻度$f_{r}(\UW w_{2})$は，以下のように計算される．
  \begin{displaymath}
    f_{r}(\UW w_{2}) = f_{r}(w_{2}) - \sum_{w \in
      {\cal W}_{k}}f_{r}(ww_{2})
  \end{displaymath}
  さらに未知語記号の単語2-gram頻度$f_{r}(\UW\,\UW)$は
  \begin{displaymath}
    f_{r}(\cdot)
    = \sum_{w_{1} \in {\cal X}^{+}} \sum_{w_{2} \in {\cal X}^{+}} f_{r}(w_{1}w_{2})
  \end{displaymath}
  を用いることで以下のように計算される．
  \begin{eqnarray*}
    f_{r}(\UW\,\UW)
    & = & f_{r}(\cdot) - \!\! \sum_{w_{1} \in {\cal W}_{k}} f_{r}(w_{1}\UW)
                       - \!\! \sum_{w_{2} \in {\cal W}_{k}} f_{r}(\UW w_{2}) \\
    &   &
        - \!\!\!\!\!\!\!\! \sum_{(w_{1}w_{2}) \in {\cal W}_{k}\times{\cal W}_{k}}
          \!\!\!\!\!\!\!\! f_{r}(w_{1}w_{2})
  \end{eqnarray*}

\item[\textbf{未知語記号を含む単語$n$-gram頻度 ($n \geq 3$)}] 未知語記号を含む一般の
  $n$-gram頻度も2-gram頻度の場合と同様に計算することが可能である．

\item[\textbf{未知語記号を含む単語$n$-gram確率 ($n \geq 1$)}] 未知語記号を含まない場合の
  \equref{equation:1-gram}(\ref{equation:n-gram})と同様に，確率的単語$n$-gram頻度を確
  率的単語$(n-1)$-gram頻度で割ることで未知語記号を含む単語$n$-gram確率が得られる．

\end{list}{}{}

以上から，語彙を有限とし未知語記号を仮定する場合でも，確率的単語分割コーパスに対する
単語$n$-gram確率を推定できることが示された．

生コーパスの利用方法
\label{section:raw-corpus}

適応対象の分野のコーパスは，その分野の言語的な特徴を的確に捉えるために重要である．こ
の利用方法としては，以下の3つが代表的である．
\begin{itemize}

\item 未知語の取り出し
  \begin{quote}
    生コーパスに対して文字$n$-gramの統計などを取り，ある程度の頻度があり，かつ前後の
    文字の分布にばらつきがある文字列などを単語候補として抽出する
    \cite{nグラム統計によるコーパスからの未知語抽出}
    \cite{統計的手法による単語の切り出しについて}
    この結果得られた単語候補は，人手でチェックされる．さらに確率的言語モデルの応用に
    応じて読みの付与などを行なう．
  \end{quote}

\item 自動分割による単語分割結果
  \begin{quote}
    自動単語分割システム
    \cite{A.Stochastic.Japanese.Morphological.Analyzer.Using.a.Forward-DP.Backward-A*.N-Best.Search.Algorithm} 
    により単語境界を推定し，これを単語分割済みコーパスとして利用する．単語分割システ
    ムは，人手により正しく単語に分割された一般的なコーパスから構築されるので，適応対
    象の分野の文に対する解析精度は必ずしも高くない．特に，適応分野に特有の単語や表現
    の周辺で分割を誤る傾向がある．しかしながら，適応対象の分野の単語分割済みコーパス
    は，多少の誤りが含まれていても，確率的言語モデルの構築に有用であることが知られて
    いる．
  \end{quote}

\item 人手による単語分割結果
  \begin{quote}
    理想的には，適応対象の分野のコーパスの全ての文が正しく(単語分割の指針に沿って)単
    語に分割されていることが望ましい．このときに確率的言語モデルの能力は最高になる．
  \end{quote}

\end{itemize}

確率的言語モデルの能力は，単語分割の修正量を増やせば増やすほど高くなる．現実には，単
語分割の修正作業はコストや時間がかかるので，コーパスの一部分を修正の対象とし，残りの
部分に関しては自動分割の結果をそのまま用いるということが行なわれる．しかし，この方法
が有限の作業量を割り当てる最良の方法であるか疑問が残る．

\begin{figure}[t]
  \begin{center}
    \begin{tabular}{cccc}
      \hline
         & 女性エコノミスト、キャ & サリン & ・カミリさんなどは「今 \\
         & ローム・デーヴィッド・ & サリン & ジャーは20世紀アメリカ \\
      ○ & に次ぐおぞましい地下鉄 & サリン & 事件、長い不況に追い打 \\
      ○ & 理が始まった中川被告は & サリン & 生成を認めながら「目的 \\
         & っているのを知りながら & サリン & 流出を阻止する義務を怠 \\
      \hline
    \end{tabular}
  \end{center}
  \caption{単語リストのKWICによる単語境界情報付与の例}
  \label{figure:KWIC}
\end{figure}

単語分割の修正作業は，コーパスに単語境界の情報を付与することである．単語境界の情報の
最小単位は各文字の間に単語境界があるか否かである．しかし，一般的に行なわれる修正作業
は文単位であり，文頭から順に各文字の間の単語境界情報が正しいかを確認し，必要に応じて
修正する．これに対して，我々は修正作業の単位をより細かくとること，具体的には，単語リ
ストなどで与えられる適応分野に特有の単語の周辺に集中することを提案する．具体的には，
\figref{figure:KWIC}に示されるように，単語リストに含まれる語(例では「サリン」)の対象
分野のコーパスでの出現位置をKWIC (Key Word In Context)形式で提示し，注目している文字
列が各文脈において単語として用いられているかのチェックをする．単語として用いられてい
る箇所にマーク(図中では「○」)を付け，それ以外の箇所では何もしないという作業を行なう．
各単語についてマークする箇所の数を制限するということも有効であろう．そうすれば，判断の
難しい箇所で時間を浪費することを避けることもできる．

