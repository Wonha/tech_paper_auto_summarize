自然言語処理における確率的言語モデルの役割は，与えられた文字列がある言語の文である尤度を数値化することである．
確率的言語モデルに基づく言語処理は，候補から解を選択する際にこの尤度を参照する．
自動単語分割は解析系の一例であり，文字列が与えられると尤度が最大になる単語の列を計算する．
認識系の代表例の音声認識では，音響信号列を入力として，尤度が最大となる文字列を算出する際に，音響モデルと併せて確率的言語モデルを参照する．
最も一般的な言語モデルは，単語[MATH]を単位とする[MATH]-gramモデル[MATH]である．
このモデルは，文を単語列[MATH] ([MATH]は単語数)とみなし，これらを文頭から順に予測する．
この式の中の[MATH]は，文頭に対応する特別な記号であり，[MATH]は，文末に対応する特別な記号である．
完全な語彙を定義することは不可能であるから，未知語を表わす特別な記号[MATH]を用意する．
未知語の予測の際は，まず，単語[MATH]-gramモデルにより[MATH]を予測し，さらにその表記を文字[MATH]の列[MATH] ([MATH]は文字数)とみなし，文字を単位とする[MATH]-gramモデル[MATH]によって以下のように予測する．
この式の中の[MATH]は，語頭に対応する特別な記号であり，[MATH]は，語末に対応する特別な記号である．
したがって，[MATH]が未知語の場合には以下のように予測される．
確率的言語モデルの応用は，自然言語認識と自然言語解析に大別できる．
認識系の代表例は，音声認識である．
確率的言語モデルを用いる音声認識では，以下の式のように，音響特徴量の列[MATH]を入力とし，語彙[MATH]のクリーネ閉包(空列を含む任意長の文字列の集合)のうち，確率最大となる要素(単語列) [MATH]を出力する．
この式における[MATH]が確率的言語モデルである．
確率的言語モデルの予測力と認識系の精度との関係は，解析的に導出できるような確固とした関係ではない．
音声認識に対して実験的に得られた関係として，西村ら[CITE]は相関係数0.6を報告している．
解析系の代表例は，単語分割(と品詞付与)である．
確率的言語モデルによる単語分割[CITE]は，以下の式が示すように，ある言語の文字列[MATH]を入力とし，生成確率が最大となる単語列[MATH]を出力する．
ここで[MATH]は，単語列[MATH]を文字列とみなした場合，入力[MATH]と等しいことを表す．
この節では，適応対象の分野の単語リストと，それらが出現する生コーパスが利用可能である場合に，それらから確率的言語モデルを推定する方法を述べる．
単語分割済みコーパスは，各文字間に単語境界が存在するか否かの情報が人手により付与されている．
生コーパスはこの情報を持たないが，各文字間に単語境界が存在する確率を付与し，それによって生コーパスを確率的に単語に分割されたコーパス(確率的単語分割コーパス)とみなすことにより，無限の語彙に対する単語[MATH]-gram頻度や単語[MATH]-gram確率を計算する方法が提案されている[CITE]．
以下では，この方法を説明する．
生コーパス[MATH](以下，長さ[MATH]の文字列[MATH]として参照)を所与として，連続する2文字[MATH]の間に単語境界が存在する確率[MATH]を付与したものを考える．
最初の文字の前と最後の文字の後には単語境界が存在するとみなせるので，[MATH]の時は便宜的に[MATH]とする．
確率的単語分割コーパスにおける単語0-gram頻度[MATH]は，そのコーパス中の期待単語数であり，以下のように定義される．
確率的に単語分割された生コーパスに出現する文字列[MATH]が[MATH]文字からなる単語[MATH]である必要十分条件は以下の4つである．
文字列[MATH]が単語[MATH]に等しい．
文字[MATH]の直前に単語境界がある．
単語境界が文字列中にない．
文字[MATH]の直後に単語境界がある．
したがって，単語[MATH]の生コーパス中の単語1-gram頻度[MATH]は，単語[MATH]の表記の全ての出現[MATH]に対する期待頻度の和として以下のように定義される．
単語1-gram頻度と同様に，[MATH]文字からなる単語列[MATH]の生コーパス[MATH]における頻度，すなわち単語[MATH]-gram頻度について考える．
このような単語列に相当する文字列が生コーパスの[MATH]文字目から始まり[MATH]文字目で終る文字列と等しく([MATH])，単語列に含まれる各単語[MATH]に相当する文字列が生コーパスの[MATH]文字目から始まり[MATH]文字目で終る文字列と等しい([MATH]; [MATH]; [MATH]; [MATH])状況を考える(\figref{figure:SSC}参照)．
単語1-gram頻度の計算の場合と同様に，単語列と生コーパスの部分文字列は，文字列として対応していることに加えて，各文字間における単語境界の有無も対応している場合にのみ単語列が出現していると考えられる．
したがって，確率的に単語分割されたコーパスに出現する文字列[MATH]が単語列[MATH]である必要十分条件は以下の4つである．
文字列[MATH]が単語列[MATH]に等しい．
文字[MATH]の直前に単語境界がある．
単語境界が各単語に対応する文字列中にない．
単語境界が各単語に対応する文字列の後にある．
したがって，生コーパスにおける単語[MATH]-gram頻度を以下のように定義することができる．
ここで
とした．
決定的に単語に分割されたコーパスからの単語1-gram確率の最尤推定の場合と同様に，確率的単語分割コーパスにおける単語1-gram確率を以下のように定義する．
決定的に単語に分割されたコーパスからの単語[MATH]-gram確率の最尤推定の場合と同様に，確率的単語分割コーパスにおける単語[MATH]-gram確率を以下のように定義する．
確率的言語モデルを用いる音声認識においては，認識される語彙には発音が付与されている必要がある．
また，確率的言語モデルを用いる仮名漢字変換においてもキー入力列が付与されている表記(単語)のみが変換結果として出現し得る．
このように，現実的な応用では有限の語彙に対する確率的言語モデルを構築する必要がある．
分野適応において単語リストが与えられている場合には，一般コーパスから得られる語彙と対象分野の単語リストを語彙として，対象分野の生コーパスから確率的言語モデルを構築する．
この際に，未知語モデルを含めて確率的言語モデルの条件を満たすためには，未知語記号を含む単語[MATH]-gram確率を正しく定義する必要がある．
単語分割済みコーパスにおいては，まず語彙[MATH]に属さない単語をコーパスの全ての出現場所において未知語記号[MATH]に置き換え，その上で未知語記号を語彙に含まれる単語と同様に扱って頻度計算を行なう．
決定的に単語に分割されていない確率的単語分割コーパスに対しては，この方法を採ることができない．
また，語彙以外の任意の文字列に対する単語[MATH]-gram頻度を計数しその和を計算する方法も考えられる．
語彙以外の任意の文字列は，実際には無限集合ではなく，コーパスの部分文字列のみを対象とすれば十分であるが，これは非常に大きな数となるので，この計算方法も現実的ではない．
しかしながら，単語[MATH]-gram頻度の以下の性質を用いることにより，確率的単語分割コーパスに対しても未知語記号を含む単語[MATH]-gram頻度を容易に計算することができる．
ここで[MATH]は語彙と未知語記号からなる長さ0以上の任意の列であり，[MATH]は文字集合[MATH]の正閉包(1文字以上の任意長の文字列の集合)を表す．
\equref{equation:UT=sum}の意味は，ある1箇所に未知語記号を含む単語[MATH]-gram頻度がその箇所を既知語以外のすべての文字列に置き換えた単語[MATH]-gram頻度の合計に等しいということである．
また\equref{equation:decomp}は，単語[MATH]-gram頻度においてある箇所の単語を任意とした場合の合計が，その箇所が任意の既知語([MATH])である場合の頻度の合計と任意の未知語([MATH])である場合の頻度の合計の和に等しいことを意味する．
\equref{equation:UT=sum}([REF_equation:decomp])から，ある1箇所に未知語を含む単語[MATH]-gramの頻度に対して以下の式が成り立つことがわかる．
確率的単語分割コーパスにおける未知語記号の単語1-gram頻度[MATH]は，コーパスに対して計数した単語1-gram頻度と単語0-gram頻度に対して成り立つ関係
と式([REF_equation:UT])において[MATH] ([MATH]は空列を表す)とすることで得られる等式
から以下のように，単語0-gram頻度と語彙に対する単語1-gram頻度の和から計算される．
任意の単語[MATH]と未知語記号からなる列の確率的単語分割コーパスにおける頻度[MATH]はコーパスに対して計数した単語2-gram頻度と単語1-gram頻度に対して成り立つ関係
と式([REF_equation:UT])において[MATH]とすることで得られる等式
から以下のように単語1-gram頻度と既知語に対する単語2-gram頻度の和から計算される．
同様に未知語記号と任意の単語[MATH]からなる列の確率的単語分割コーパスにおける頻度[MATH]は，以下のように計算される．
さらに未知語記号の単語2-gram頻度[MATH]は
を用いることで以下のように計算される．
未知語記号を含む一般の[MATH]-gram頻度も2-gram頻度の場合と同様に計算することが可能である．
未知語記号を含まない場合の\equref{equation:1-gram}([REF_equation:n-gram])と同様に，確率的単語[MATH]-gram頻度を確率的単語[MATH]-gram頻度で割ることで未知語記号を含む単語[MATH]-gram確率が得られる．
{}{}
以上から，語彙を有限とし未知語記号を仮定する場合でも，確率的単語分割コーパスに対する単語[MATH]-gram確率を推定できることが示された．
適応対象の分野のコーパスは，その分野の言語的な特徴を的確に捉えるために重要である．
この利用方法としては，以下の3つが代表的である．
未知語の取り出し
生コーパスに対して文字[MATH]-gramの統計などを取り，ある程度の頻度があり，かつ前後の文字の分布にばらつきがある文字列などを単語候補として抽出する[CITE] [CITE]この結果得られた単語候補は，人手でチェックされる．
さらに確率的言語モデルの応用に応じて読みの付与などを行なう．
自動分割による単語分割結果
自動単語分割システム[CITE]により単語境界を推定し，これを単語分割済みコーパスとして利用する．
単語分割システムは，人手により正しく単語に分割された一般的なコーパスから構築されるので，適応対象の分野の文に対する解析精度は必ずしも高くない．
特に，適応分野に特有の単語や表現の周辺で分割を誤る傾向がある．
しかしながら，適応対象の分野の単語分割済みコーパスは，多少の誤りが含まれていても，確率的言語モデルの構築に有用であることが知られている．
人手による単語分割結果
理想的には，適応対象の分野のコーパスの全ての文が正しく(単語分割の指針に沿って)単語に分割されていることが望ましい．
このときに確率的言語モデルの能力は最高になる．
確率的言語モデルの能力は，単語分割の修正量を増やせば増やすほど高くなる．
現実には，単語分割の修正作業はコストや時間がかかるので，コーパスの一部分を修正の対象とし，残りの部分に関しては自動分割の結果をそのまま用いるということが行なわれる．
しかし，この方法が有限の作業量を割り当てる最良の方法であるか疑問が残る．
単語分割の修正作業は，コーパスに単語境界の情報を付与することである．
単語境界の情報の最小単位は各文字の間に単語境界があるか否かである．
しかし，一般的に行なわれる修正作業は文単位であり，文頭から順に各文字の間の単語境界情報が正しいかを確認し，必要に応じて修正する．
これに対して，我々は修正作業の単位をより細かくとること，具体的には，単語リストなどで与えられる適応分野に特有の単語の周辺に集中することを提案する．
具体的には，\figref{figure:KWIC}に示されるように，単語リストに含まれる語(例では「サリン」)の対象分野のコーパスでの出現位置をKWIC (Key Word In Context)形式で提示し，注目している文字列が各文脈において単語として用いられているかのチェックをする．
単語として用いられている箇所にマーク(図中では「○」)を付け，それ以外の箇所では何もしないという作業を行なう．
各単語についてマークする箇所の数を制限するということも有効であろう．
そうすれば，判断の難しい箇所で時間を浪費することを避けることもできる．
