比喩性の尺度
本論文では，与えられた表現が比喩であるかを判断する基準として，「クローズ
アップされる特徴がいかに明確か」という点と，「与えられた表現がどの程度新
鮮か」という点が重要であることに着目する．
比喩表現の理解とは，概念が持つ，ある特徴を強調することによって，新たな理
解を促すものであるから，強調される特徴が明確でなければならない．
``顕現性落差"は，クローズアップされる特徴を抽出し，それらの特徴がいかに明
確であるかをはかる尺度である．
また，比喩表現として対比される概念が新鮮であることは，その表現に強い印象
を与え，理解を促すことになる．
``意外性"は，対比される概念の組み合わせの新鮮さをはかる尺度である．
このような，二つの尺度を設定することで，その表現の比喩らしさ，すなわち，
比喩性を検出できると考えられる．
以下，``顕現性落差''および``意外性''について，比喩性との関係を説明し，
両尺度が，比喩性検出にどのように利用できるかについて述べる．

Ortny\cite{ortony79}は，比較される概念間の共有特徴が少ない場合でも，それ
らの類似性が認識されて比喩性が理解される点や，類似性の非対称性に着目し，
相互作用モデルを示した．
例えば，``卵のような車''という比喩の場合，たとえる言葉(source概念)``卵''と
たとえられる言葉(target概念)``車''の共有特徴
$(卵{\cap}車)=\{丸い，白い，小さい，…\}$は，
``車''においては顕著な特徴ではない
が，``卵''においては，これらの共有特徴は非常に顕著な特徴(顕現特徴)
である
．
したがって，``車''に対して，``卵''のイメージを重ね合わせることによって，
``車''における$\{丸い，白い，小さい，…\}$などの特徴を同時に強調し，
その結果，``顕現性落差''が生じて，比喩性が検出される．
``顕現性落差''からは，類似性の非対称性が生じるので，
同じ概念を比較した場合でも，``車のような卵''という表現
\footnote{このような表現を反転比喩という．}
からは，比喩性は検出されにくい．

また，source概念が顕著な特徴を持っていたとしても，対比される概念間に共有
特徴が認められない場合は，``顕現性落差''が生じないため，比喩性は認識されない．
例えば，``谷底のような車''という表現では，
``谷底''と``車''の間には共有特徴が見つけられないので，
``顕現性落差''は生じず，比喩性も生じない．
``自動車のような車''では，組み合わせ概念が類似概念であるため，両者の顕現
特徴もほとんど共通である．この場合も，``顕現性落差''は生じにくく，比喩性も
あらわれにくいと考えられる．

さらに，比喩とは，意表を突いた言葉(ここでは単語)の組合せによって，伝えた
い内容をより鮮明にしたり，強調する働きを持つ．
例えば，``スポーツカーのような車''という比喩の場合，``スポーツカー''と
``車''の共有特徴($スポーツカー{\cap}車$)=\{速い，格好いい，燃費が悪い…\}は，
``車''においては顕著な特徴ではない
が，``スポーツカー''において非常に顕著である．
したがって，``車''に対して，``スポーツカー''のイメージを重ね合わせるこ
とによって，``車''における\{速い，恰好いい，…\}などの特徴を同時に強調
するが，比喩性は認識されにくい．
この理由は，両概念が上位下位関係を持つために，重複する特徴が多く，かつ，
ありふれた組み合わせであるために，表現の新鮮さに欠けるからと考えられる．
本論文では，このような単語間の組合せの新鮮さの度合を``意外性''として扱う．

一般に，同一話題中に頻出する単語対は，たとえ2章の``顕現性落差''の条件を
満たしていても，``意外性''が低い．
その結果，比喩としての「新しさ」や「意外さ」が認識されず，比喩性を高
める要因とはならない．
反対に，めったに同一話題中に現れない単語対は``意外性''が高く，「新鮮」で
「意外」であると認識され，比喩性を高める要因となる．

\begin{table}[tb]
  \begin{center}
     \caption{顕現性落差と意外性に基づく概念対の分類}
     \label{tbl:relation}
\begin{tabular}{|c|c|c|c|c|}
\hline
   &           &\multicolumn{3}{|c|}{顕現性落差}\\
\hline
   &	&  大   & 小     	&  負\\
\hline
意 & 高 & 比喩 & 比喩／例示	& 無意味\\
\cline{2-5}
外 & ： & ：   & ： 		& ： 	\\
\cline{2-5}
性 & 小 & 例示  & 比喩／例示	& 無意味？ 	\\
\hline
\end{tabular}
  \end{center}
\end{table}

上記の見地から，``顕現性落差''が大きく，かつ``意外性''も大きい概念対
ほど，特徴が明確であり，表現も新鮮に受け取られ，比喩性も大きくなると考
えられる．
この考え方と，概念対(比喩・例示・無意味)の区別を対応付け
ると，表\ref{tbl:relation}のような関係が仮定できる．
概念対において，``顕現性落差''によって，無意味な概念対(無関係対)と意味
のある概念対(比喩関係対・例示関係対)が区別でき，``意外性''によって，例
示関係対と非例示の概念対(比喩関係対・無関係対)が区別できる．
よって，両者を統合的に利用することで，比喩関係にある概念対が区別できる
\footnote{
ただし，共有特徴がない場合については，本論文では議論の対象外とする．
}．

顕現性落差の定量化
\subsection{確率的プロトタイプを用いた顕現性落差の計算}
2章で述べたような共有特徴の顕現性落差を扱うために，
属性値集合を用いた，確率的な概念記述
を用いる．
確率的な
概念記述モデルでは，概念は属性値とその生起確率の集合として記
述される\cite{Iwayama1990,Masui1999}．
概念$\ast(W)$が属性値$w_i$をもち，その生起確率が$p_i$であるとき，
$\ast(W)$ は以下のように，確率的な属性値集合として記述できる
(式(\ref{exp:collection}))．
\begin{eqnarray}
\label{exp:collection}
	{\ast}(W) = \{p_1\#w_1,p_2\#w_2,...,p_i\#w_i,...,p_m\#w_m\}
\end{eqnarray}
このとき，概念の顕現性は，これらの属性値集合の冗長度(ばらつき具合)から予
測可能である．
(\ref{exp:collection})で示した概念${\ast}(W)$が，$m$種類の属性値から成る
属性値集合として記述される場合，その冗長度$r(W)$は，
式(\ref{exp:info})を用いて定量化できる\cite{Iwayama1991j}．
\begin{eqnarray}
\label{exp:info}
r(W) = &
	\left\{\begin{array}{cc}
	 1 - \frac{\sum_{i=1}^{m}{p_i}\log{\frac{1}{p_i}}}{\log{m}} & (m{\neq}1)\\
	{1} & (m=1)
		\end{array}\right \}
\end{eqnarray}

\begin{figure}[tb]
    \begin{center}
      \epsfile{file=proto2.eps,scale=0.5}
\caption{確率的な概念記述における特徴集合の顕現性落差}
\label{fig:rep-kage}
\end{center}
\end{figure}
ところで，比喩表現の顕現特徴は，比較される概念間の共有特徴から選ばれるが，
同時に，source概念の顕現特徴になっているはずである．
よって，source概念の属性値集合から主要な属性値を取り出し，それらと，
target概念の属性値との間で共有できるものを取り出すことで，顕現性落差を考
えるためにクローズアップされる共有属性値集合が取り出される
\footnote{
比喩表現を構成する概念の間で共有される属性値は，必ずしも一つではない
\cite{Kusumi1996ja}ので，計算対象となる共有属性値は集合でもよい．
もちろん，概念が共有する属性値の数が少なければ少ないほど，顕現特徴が特定
しやすいため，結果として比喩として理解されやすいといえる．}．
取り出された共有属性値集合について，source概念とtarget概念の各々における
生起確率を用いて冗長度を計算することで，顕現性落差が予測できる．
したがって，
source概念${\ast}(W_s)$ が，降順で整列した$m$個の属性値から成る属性値集合
で記述される場合を考えると，まず，生起確率上位から，閾値$\alpha$までの範
囲内に存在する$n$個の属性値(${\sum_{i=1}^{n}{p_i} > {\alpha}}$)を，その概
念の主要な属性値集合とみなして取り出し(式(\ref{exp:set}))，
\begin{eqnarray}
\label{exp:set}
	&{\ast}(W_s) = \{p_1\#w_1,p_2\#w_2,...,p_n\#w_n,...,p_m\#w_m\} \nonumber\\
	if &(p_1>p_2>...>p_n)\ \cap \sum_{i=1}^{n}{p_i} > \alpha\\
	 then &  T(W_s) = \{p_1\#w_1,p_2\#w_2,...,p_n\#w_n\} \nonumber
\end{eqnarray}
次に，取り出した属性値集合$T(W_s)$と，target概念${\ast}(W_t)$との間で
共有される属性値を探し，それらを，各々の概念における相対頻度の値とともに
取り出す．
(\ref{exp:set})に関して，source概念$\ast(W_s)$の主要な属性値集合$T(W_s)$と，
target概念${\ast}(W_t)$の間で共有される属性値集合は，
$W_{s,(T(W_s)\cap{\ast}(W_t))}$，$W_{t,(T(W_s)\cap{\ast}(W_t))}$であり，
それらの主要な共有属性値集合(属性値数$x$個，$y$個)は，
式(\ref{exp:set1}),(\ref{exp:set2})のように表せる．
さらに，それぞれの共有属性値集合の冗長度，
$r(T(W_{s,(T(W_s)\cap{\ast}(W_t))}))$，$r(T(W_{t,(T(W_s)\cap{\ast}(W_t))}))$は，
式(\ref{exp:var1})，(\ref{exp:var2})のように
計算できる．
\begin{eqnarray}
\label{exp:set1}
&T(W_{s,(T(W_s)\cap{\ast}(W_t))})
 = \{p_{s,1}\#w_1,p_{s,2}\#w_2,...,p_{s,x}\#w_x\} \\
\label{exp:set2}
&T(W_{t,(T(W_s)\cap{\ast}(W_t))})
 = \{p_{t,1}\#w_1,p_{t,2}\#w_2,...,p_{t,y}\#w_y\} 
\end{eqnarray}

ここで，上記の手順で求められた冗長度は，単に属性値のばらつき具合を示しているにすぎない．
そのため，source概念における共有属性値集合の冗長度が，target概念のそれよ
り小さい(ばらついている)場合でも，共有属性値の生起確率が概念記述全体に対し
て占める割合が大きいと，顕現特徴となる場合があり，
冗長度の差のみでは，顕現性落差を正確に反映できない．
そこで，顕現性落差を反映させるために，対象となる属性値がどの程度主要である
かによって冗長度に重み付けをする．
例えば，図\ref{fig:rep-kage}では，属性値$\{幼い\}$の生起確率は，概念
$\ast(子供)$において$0.222$であり，概念$\ast(顔)$において$0.003$である．
この場合，属性値$\{幼い\}$は，概念$\ast(子供)$において最も主要な属性値
であるが，概念$\ast(顔)$においてはそれほど主要ではないといえる．

このように，ある属性値集合における属性値が，集合全体に対して，どの程度主要
であるかということは，その属性値が集合内において保持する生起確率から把握できる．
主要な属性値を用いた冗長度と，主要でない属性値を用いた冗長度を比較した場合，
前者が主要であることが，顕現性の強調に影響すると考えられる．
よって，各々の冗長度に対して，対象となった共有属性値集合の生起
確率の総和を乗じて重み付けをし，比較した結果を顕現性落差として判断する
(式(\ref{exp:dif}))．
比較した結果が正の場合，顕現性落差は比喩性を上げるように働き，負の場合は
比喩性を下げるように働く，または生じないとみなす．
\begin{eqnarray}
\label{exp:var1}
r(T(W_{s,(T(W_s)\cap{\ast}(W_t))})) = &
   \left\{ \begin{array}{cc}
	 1 - \frac{\sum_{i=1}^{x}{{p_{s,i}}\log{\frac{1}{p_{s,i}}}}}{\log{x}}&(x{\neq}1)\\
	{1} & (x=1)
      \end{array}\right\}\\
\label{exp:var2}
r(T(W_{t,(T(W_s)\cap{\ast}(W_t))})) = &
   \left\{ \begin{array}{cc}
	 1 - \frac{\sum_{i=1}^{y}{{p_{t,i}}\log{\frac{1}{p_{t,i}}}}}{\log{y}}&(y{\neq}1)\\
	{1} & (y=1)
      \end{array}\right\}
\end{eqnarray}
\begin{eqnarray}
\label{exp:dif}
Gap(W_s,W_t) =  & 
	r(T(W_{s,(T(W_s)\cap{\ast}(W_t))}))×\sum_{i=1}^{x}{p_{s,i}} \nonumber \\
&	- r(T(W_{t,(T(W_s)\cap{\ast}(W_t))}))×\sum_{i=1}^{y}{p_{t,i}}
\end{eqnarray}

\subsubsection*{計算例}
以下に，顕現性落差の計算手法についての具体例を示す．
``子供のような顔''という比喩表現に関して，二つの構成概念，${\ast}(子供)$と
${\ast}(顔)$が図\ref{fig:rep-kage}のように，記述されている．
仮に，主要な属性値集合を決める閾値$\alpha$を0.5とした場合，
source概念${\ast}(子供)$における上位の属性値集合$T(子供)$は，
$\{幼い，小さい，たくましい，可愛い，健康だ，弱い，いたいけだ\}$
(合計0.505)となる．
さらに，$T(子供)$と${\ast}(顔)$の共有属性値として，
$\{幼い，たくましい\}$が得られるので，クローズアップされる属性値集合は
以下のように表せ，
\begin{eqnarray}
\label{exp:int}
T(子供_{T((子供)\cap\ast(顔))}) = &\{幼い\#0.222，たくましい\#0.030\}\\%$，
\label{exp:int2}
T(顔_{T((子供)\cap\ast(顔))}) = &\{幼い\#0.003，たくましい\#0.005\}
\end{eqnarray}
それぞれの冗長度は次のように計算できる．
\begin{eqnarray}
\label{exp:int1-2}
r(T(子供_{T((子供)\cap\ast(顔))})) = &
1 - \frac{
	{0.222}\log{\frac{1}{0.222}}+{0.030}\log{\frac{1}{0.030}}
	}{\log{2}}
& = 0.471\\
\label{exp:int2-2}
r(T(顔_{T(子供)\cap\ast(顔)})) = &
1 - \frac{
	{0.003}\log{\frac{1}{0.003}}+{0.005}\log{\frac{1}{0.005}}
	}{\log{2}}
& = 0.082
\end{eqnarray}
共有属性値の生起確率の総和によって重み付けをして，両者を比較すると，
\begin{eqnarray}
Gap(子供,顔) = & 
0.471{\ast}0.253 - 0.082{\ast}0.008 = & 0.118
\end{eqnarray}
$0.118$という``顕現性落差''が得られ，この概念対は，
$\{幼い，たくましい\}$という
属性値に関して，比喩性を高めるようにはたらくと判断する．

\subsection{顕現性落差計算のための知識ベース構築}
本節では，顕現性落差の定量化に用いる知識ベースの構築について述べる．
前節で説明した顕現性落差を定量化するためには，
対象となる単語を表現できる属性値集合に関する知識ベースが必要である．
知識ベース構築において，従来のように被験者を用いた心理学的実験に基
づいた場合，妥当性の高い知識は期待できるが，同手法によって，数万，
数十万と，知識を大規模化することは，極めて困難である．
コンピュータを用いた汎用的な比喩性判定を考えた場合，大規模な知識ベー
スを効率良く構築することも非常に重要である．
よって，大規模コーパスを利用した統計的なアプローチも
有効な手段の一つである．

そこで，我々は，従来の心理学的実験手法を用いず，統計的手法を用いて，
テキストコーパスから大規模な知識を自動的に抽出し，知識ベースを構築する．
具体的には，対象とするテキストコーパスを形態素解析
\footnote{茶筅 $version 2.02$\cite{Chasen1999j}を用いた．}し，得られた
結果から，``修飾語−名詞''の共起関係とその共起頻度を抽出する．
抽出された共起情報は，確率的プロトタイプモデルに基づいて，
知識ベース化する．
\begin{itemize}
\item[] 例文(1) {\em 第一日目には，赤い花が一本売れた\cite{Dazai1947j}．}
\item[] 例文(2) {\em 二人は白い花のイバラの影から出て，水蓮の咲いている小さい沼の方へ歩いて行きます\cite{Dazai1988jb}．}
\end{itemize}
例えば，例文(1)を形態素解析した結果から，共起関係``花−赤い''が抽出される．
この関係を共起頻度とともに，``$花 = \{赤い\#1\}$ ''のように記録する．
同様に，例文(2)を処理すると，共起関係``花−白い''と``沼−小さい''が抽出
できる．
その結果，知識は，``$花 = \{赤い\#0.50, 白い\#0.50\}，
沼 = \{小さい\#1.0\}$''に更新される．

上記の方法に従って，1年分の新聞記事コーパス\cite{Mainichi1995j}から知識
ベースを構築した．
知識ベース構築に際して，知識を抽出する共起範囲は1文とした
\footnote{
抽出される知識は名詞とその名詞にかかる修飾語である．
したがって，例え頻度が少なくとも，それは偶然出現したものではなく，
名詞の属性を表現するために用いられているため，属性値となる語句の範囲
が正しく抽出できれば，頻度に関わりなく，属性値として適切なはずである．
また，比喩的関係を考える場合には，通常は思い付きにくい属性値がクロー
ズアップされる可能性があることから，低頻度の属性値を用いることにも
意味があると判断し，頻度に対する閾値を設けなかった．
}．
知識として抽出された共起ペアは79,712組，属性値集合は27,958組であった．
属性値集合あたりの属性値数は1〜339，平均は2.5であった．

表\ref{tbl:nov-ex0}に知識ベースの例を示す．
``山：高い''，
``海：青い''，
``学生：若い''，
など，概念の顕現特徴を示す属性値が概ね上位に位置した．
下位の順位においても，
``山：険しい''，
``海：暗い，神秘的だ''，
``学生：無気力だ，忙しい''，
のように，概念の特徴として連想可能な属性値を見ることができる．

構築した知識ベースから，ランダムに100組を抜きだし，人手による大まかな評
価を行った.
評価は，(A)見出しの属性値として連想し易い，(B)見出しの属性値とし
て連想することが可能，(C)見出しの属性値として連想不可能，の三段階
に分類することで行った．
その結果，(A)が85組，(B)が15組，(C)が5組となった．
したがって，抽出した属性値のうち，85\%程度は顕現特徴として理解でき，
95\%程度は連想可能なものであると考えられる．

\begin{table}[tb]
\begin{center}
\caption{知識ベースの例}
\label{tbl:nov-ex0}
\begin{tabular}{|c|l|}
\hline
概念	&\multicolumn{1}{|c|}{属性値集合}\\
\hline
山&
高い\#0.111,
青い\#0.063,
静かだ\#0.048,
小高い\#0.048,
深い\#0.048,
低い\#0.048,
\\&
なだらかだ\#0.032,
丸い\#0.032,
巨大だ\#0.032,
厳しい\#0.032,
いい\#0.016,
\\&
いろいろだ\#0.016,
さびしい\#0.016,
のどかだ\#0.016,
ふしぎだ\#0.016,
\\&
険しい\#0.016,
悪い\#0.016,
遠い\#0.016,
楽しい\#0.016,
美しい\#0.016,
...\\
\hline
海&
青い\#0.228,
きれいだ\#0.087,
美しい\#0.087,
豊かだ\#0.065,
真っ青だ\#0.054,
\\&
新鮮だ\#0.054,
静かだ\#0.043,
穏やかだ\#0.043
浅い\#0.033,
暗い\#0.022,
\\&
黒い\#0.022,
真っ暗だ\#0.022,
豊富だ\#0.022,
遠い\#0.011,
輝かしい\#0.011,
\\&
広い\#0.011,
新しい\#0.011,
神秘的だ\#0.011,
素晴らしい\#0.011,
壮大だ\#0.011,
...\\
\hline
学生&
若い\#0.176,
優秀だ\#0.118,
高い\#0.059,
困難だ\#0.059,
未熟だ\#0.039,
いい\#0.039,
\\&
近い\#0.039,
まじめだ\#0.020,
よい\#0.020,
フレキシブルだ\#0.020,
暗い\#0.020,
\\&
活発だ\#0.020,
賢い\#0.020,
厳しい\#0.020,
素直だ\#0.020,
貧乏だ\#0.020,
\\&
不自由だ\#0.020,
無気力だ\#0.020,
練習熱心だ\#0.020,
忙しい\#0.020
...\\
\hline
子供&
幼い\#0.222,
小さい\#0.162,
弱い\#0.030,
たくましい\#0.030,
可愛い\#0.030,
\\&
健康だ\#0.030,
いたいけだ\#0.022,
愛らしい\#0.022,
可愛らしい\#0.022,
高い\#0.022,
\\&
長い\#0.022,
必要だ\#0.022,
未熟だ\#0.022,
あやふやだ\#0.010,
いい\#0.010,
\\&
かわいい\#0.010,
ほほえましい\#0.010,
やんちゃだ\#0.010,
悪い\#0.010,
...\\
\hline
米国&
強い\#0.091,
多い\#0.091,
厳しい\#0.055,
高い\#0.055,
広大だ\#0.036,
重要だ\#0.036,
\\&
積極的だ\#0.036,
薄い\#0.036,
必要だ\#0.036,
ワイルド\#0.036,
いい\#0.018,
\\&
さまざまだ\#0.018,
一般的だ\#0.018,
及び腰だ\#0.018,
好きだ\#0.018,
好調だ\#0.018,
\\&
広い\#0.018,
国際的だ\#0.018,
慎重だ\#0.018,
新しい\#0.018,
敏感だ\#0.018,
...\\
\hline
夢&
怖い\#0.061,
悪い\#0.061,
ささやかだ\#0.051,
遠い\#0.051,
壮大だ\#0.041,
\\&
ルナティックだ\#0.031,
不思議だ\#0.031,
変だ\#0.031,
でかい\#0.031,
\\&
はかない\#0.031,
ふさわしい\#0.020,
いい\#0.020,
こわい\#0.020,
むなしい\#0.020,
\\&
明るい\#0.020,
甘い\#0.020,
恐ろしい\#0.020,
嫌だ\#0.020,
長い\#0.020,
...\\
\hline
書類&
必要だ\#0.464,
いろいろだ\#0.107,
膨大だ\#0.107,
簡単だ\#0.071,
高い\#0.071,
\\&
さまざまだ\#0.036,
形式的だ\#0.036,
短い\#0.036,
不必要だ\#0.036,
分厚い\#0.036,
...\\
\hline
クリスマス&
寂しい\#0.222,
よい\#0.111,
楽しい\#0.111,
巨大だ\#0.111,
孤独だ\#0.111,
\\&
豪華だ\#0.111,
神聖だ\#0.111,
暖かい\#0.111
\\
\hline
イチゴ&
甘い\#0.333,
新鮮だ\#0.333,
真っ赤だ\#0.333,
\\
\hline
\end{tabular}
\end{center}
\end{table}

上の評価結果は，個々の属性値が妥当かどうかを測るものであり，
評価後の数値がそのまま属性値集合の妥当性を示すものではないが，
大規模な知識を，概ね直観に合うレベルで抽出することができたといえる．

一方で，以下に述べるような，方式限界の存在も明らかになった．
属性値の中には，ほとんどの概念と頻繁に共起するために，
概念の特徴を示す属性値とはならないケースが見られた．
``よい''や``いい''がその一例である．
これらの語は，概念によっては，その特徴を反映していない場合も多く，
属性値集合のノイズとなってしまうことがわかった．
他にも，``夢''の属性値の例があげられる．
文献\cite{GJDict1996}に記載されている``夢''の語義文を参考にして属性値を
考えると，``はかない''，``非現実的だ''，``実現困難だ''，``理想的だ
''が得られる．
上記手法によって構築された知識ベース(表\ref{tbl:nov-ex0})の``夢''の属性
値集合をみると，``はかない''以外の属性値が抽出されていないことがわかる．
これは，コーパス中において，``非現実的な''，``実現困難な''，``理想的な
''などは，非制限的な属性であり，``夢''の属性値としては一般的過ぎるため
に，修飾語として出現しにくいためと考えられる．

上で述べたような問題点は，
以降で扱う，比喩性判定の過程に影響を与えることが予想
されるが，本論文では，これを本構築手法の限界と考えている\footnote{
この問題については，6章でも議論する．}
．

以上の見地から，本手法によって構築される知識ベースは，
強調されやすい属性値の集合によって表現される，確率的な概念記述であると
いえ，概念の顕現特徴やそれらの集合を近似することは可能である．
したがって，比喩性の判定という目的においては，十分利用可能である．
\begin{table}[tb]
\begin{center}
\caption{``$AのようなB$''におけるAB間の顕現性落差の例}
\label{tbl:gap-ex}
\begin{tabular}{|c|c|c|c|}
\hline
分類	&A	&B 	&顕現性落差\\
\hline
比喩	&神様	&人	&0.993 \\
比喩	&粉	&骨	&0.811 \\
比喩	&影	&人物	&0.387 \\
比喩	&オウム&宗教	&0.231 \\
比喩	&ジャズ&リズム	&0.196 \\
例示 	&サリン	&毒物	&0.033 \\
例示 	&中国	&国	&0.008 \\
例示 	&フランス&大国	&0.000 \\
無意味 	&人間	&キャラクター	&-0.009 \\
無意味 	&夢	&馬	&-0.024 \\
無意味 	&キャンペーン	&形	&-0.042 \\
例示 	&米国	&リストラ&-0.136 \\
\hline
\end{tabular}
\end{center}
\end{table}

構築した知識ベースを用いて，単語間の顕現性落差を計算した例を表
\ref{tbl:gap-ex}に示す．
$A,B$の項の単語対は，前述のコーパスから，``$AのようなB$''というパター
ンで現れる表現の構成単語である．
表\ref{tbl:gap-ex}から，顕現性落差が大きい単語対は，比喩または例示の組
み合わせが多く，顕現性落差が小さい単語対は，例示や無意味単語対が多いこ
とがわかる．

意外性の定量化
\subsection{単語間の意味距離を用いた意外性の定量化}
2章で述べたように，比喩表現の``意外性''とは，構成概念の組み合わせの
新鮮さである．
単語同士の組み合わせがいかに新鮮かを決める要因として，それらの単語
が，日常的に用いられる文章中でどれほど頻繁に共起するか，という点があげら
れる．
よって，テキストデータ中の単語の共起情報に基づく意味距離を利用すれば，
単語組み合わせの``意外性''を定量化できるはずである．
大規模なテキストデータから，単語の共起頻度を用いて単語間の意味距離を定
量化する手法は，これまでにも数多く提案されている
\cite{Salton1989,Church1990,Smaja1993}．
本論文では，計算対象となる頻度が小さい場合でも，比較的信頼できる結果が
得られる dice係数を利用する．
 dice係数は，本来，単語間の意味距離を示す値であるため，単語間の結び付きが
強い程値が小さな値をとる．
これは，``意外性''の定義とは反対の概念であるため，dice係数の逆数を
``意外性''の値として用いることにする．
したがって，あるテキスト中において出現する二つの単語$W_s$，$W_t$が，それ
ぞれ，$p_s$，$p_t$の頻度で出現しており，そのうち両者が共起する頻度が
$p_s・p_t$である場合，``意外性''$Nov(W_s,W_t)$は，式
(\ref{exp:novelty})で表される．
\begin{eqnarray}
\label{exp:novelty}
Nov(W_s,W_t) = & \frac{p_s＋p_t}{2(p_s・p_t)}
\end{eqnarray}

この計算方法では，dice係数の値が0となる場合，すなわち，対象とする
単語間の共起頻度が0の場合は値が得られない．
共起頻度が得られないということは，単語の抽出元であるコーパス中か
らは，意外性を判断するための基本情報が得られなかったと判断できる．
したがって，単語間の共起頻度が0であった場合は，判定不能として扱う
．
この場合，顕現性落差の計算が可能であったとしても，比喩性の判定は
不能となる．

\begin{table}[tb]
\begin{center}
\caption{``$AのようなB$''における$AB$間の意外性の例}
\label{tbl:nov-ex}
\begin{tabular}{|c|c|c|c|}
\hline
分類	&A	&B 	&意外性\\
\hline
無意味	&人間 &キャラクター	&2917\\%A
比喩	&神様 &人	&1429\\%M
無意味	&キャンペーン &形&837\\%A
比喩	&矢 &パス	&595\\%M
比喩	&少年 &笑顔	&308\\%M
比喩	&夢 &存在	&226\\%M
例示	&フランス &主張	&131\\%S
例示	&中国 &大国	&94\\%S
比喩	&ジャズ &リズム	&39\\%M
例示	&サリン &物質	&14\\%S
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection*{計算例}
以下，意外性の計算手法について，具体例を示す．
``山のような書類''という比喩表現に関して考える．
ある新聞記事\cite{Mainichi1995j}では，二つの単語``山''および``書類''の頻度は
それぞれ，2695，1033であり，両者の共起頻度が4である．
このとき``意外性''は，
\begin{eqnarray}
\label{calc:novelty1}
Nov(山,書類) = & \frac{2695＋1033}{2{\ast}4} = 466
\end{eqnarray}
となる．
同様に，``文書のような書類''という表現の場合，
二つの単語``文書''および``書類''の頻度はそれぞれ，1898，1633，両者の共起頻度
が20なので，``意外性''は，
\begin{eqnarray}
\label{calc:novelty2}
Nov(文書,書類) = & \frac{1898＋1633}{2{\ast}20} = 88.25
\end{eqnarray}
となり，``山，書類''と比較して意外性が小さいことがわかる．

\subsection{意外性計算のための知識ベース構築}
顕現性落差同様，テキストコーパスから，統計的手法を用いて大規模な知識を取
り出し，意外性の定量化に用いる知識ベースを構築する．
具体的には，対象とするテキストを形態素解析\cite{Chasen1999j}し，得られた
その処理結果から，全ての名詞とその出現頻度，および，1文をスコープとした場合
の名詞共起とその共起頻度を抽出して構築する．

例文(2)には5つの名詞``二人''，``花''，``イバラ''，``影''，``水蓮''，``沼''
が存在する．
共起範囲を一文として各名詞のペア組合せを考えると，14組の名詞ペアが考えら
れる．
このとき，各名詞の出現頻度と共起頻度，それらに基づいて名詞間の意味距離
が計算できる．
以上の結果を知識ベースとして，$\{二人,花：29,32,4\}$のように記録する．


表\ref{tbl:nov-ex}に，単語間の意外性の例を示す．
これらは，1年分の新聞記事コーパス\cite{Mainichi1995j}を利用して
構築した知識ベースを用いて，計算した結果である．
知識ベース構築のための共起範囲は1文とし，共起頻度5以上のものを対象とした．
$A,B$の項の単語対は，前述のコーパスから，``$AのようなB$''というパターン
で現れる表現の構成単語である．
表\ref{tbl:nov-ex}から，意外性が高い単語対は比喩または無関係の組み合わせ
が多く，意外性が下位のものは例示の組み合わせが多いことがわかる．

