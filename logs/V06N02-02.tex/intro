\section{はじめに}
音声認識・文字認識の精度向上のため，より高い性能を持つ言語モデルを求め
ることは重要である．近年は，モデル構築やメンテナンスの容易さの点から，
コーパスに基づく統計的言語モデルの研究が盛んである．大語彙ないしタスク
非依存のシステムのための統計的言語モデルとして今日もっとも有望視されて
いるものに，$n$-gramが挙げられる．$n$-gramは大量のテキストコーパスから
の単純な数え上げによって得られる統計量であり，強力かつ頑健性に優れてい
る．

英語などのヨーロッパ系言語においては，$n$-gram の単位として単語を用い
ることが多い．大語彙のシステムでは単語はカテゴリ数が非常に大きくなるた
め，単語の代わりに品詞を用いる\cite{nagata94}，または単語クラスタリング
によって得られる単語クラスを用いることが多い．これらの言語においては単
語は分かち書きされるため機械的に取り出すことができ，数え上げも容易に行
える．

これに対し，日本語や中国語には分かち書きの習慣がない．朝鮮語は文節ごと
に分かち書きをするが，その分かち方は一定しないうえ，$n$-gramの単位とし
ては大き過ぎて汎化性に難がある．よって，これらの言語を$n$-gramによって
モデル化する際には，テキストコーパスに何らか\break
の前処理が必要である．
これには次の可能性が考えられる．
\begin{itemize}
\item 人手によって分割されたタグ付きコーパスを使う
\item 自動形態素解析システムによって単語に分割する
\item 経験的な統計基準によって文字列に分割する
\end{itemize}
このうちタグ付きコーパスを使う方法には，コーパス自体の入手が質的・量的
な困難を伴うという欠点がある．形態素解析に基づく方法は有効であるが，モ
デルを学習するためにはまず形態素解析システムを用意せねばならないうえ，
特定タスクに対して高い性能を得るためには予め辞書をチューニングする必要
があると考えられ，メンテナンスのコストがかかる．また，形態素解析システ
ムの文法規則によっては機能語が短めに分割される傾向があり，$n$-gramの性
能を必ずしも最大にするものではない．

これらの手法に対して，伊藤ら\cite{aito96}は統計的な基準によって文
\mbox{字列の集合を選}定し，その文字列に分割されたテキストを使って$n$-gramを学
\mbox{習する方法を提案している．文字}列を選定する基準としては，単純な頻度，お
よび語彙の自動獲得のために提案されている正規\break
化頻度\cite{nakawatase95}の高い
ものから選ぶ方式が\mbox{有効であったとされる．この方法は，形態素解}析を必要と
しない点で優れている．しかし，抽出すべき文字列の最適な個数を見出す方法
については述べられていない．また，用いられている基準と言語モデルの能力
との理論的関係は浅く，最良の分割方法である保証はない．さらに，この手法
ではテキストが明示的に分割される．このため，接辞を伴った語や複合語など
の長い文字列が抽出された場合，その文字列を構成するもっと短い語は出現し
なかったのと同様な扱いを受けることになる．有限のテキストから汎化性の高
い言語モデルを構築したい場合に，このような明示的な分割が最良の結果を与
えるとは限らない．

本論文では，高い曖昧性削減能力を持つ新しい言語モデルを提案する．このモ
デルは，superwordと呼ぶ文字列の集合の上の$n$-gramモデルとして定義され
る．superword は訓練テキスト中の文字列の再現性のみに基づいて定義される
概念であり，与えられた訓練テキストに対して一意に定まる．具体的な確率分
布は，訓練テキストからForward-Backwardアルゴリズムによって求める．訓練
テキストを明示的に分割せぬまま学習を行うため，長い文字列中の部分文字列
を「再利用」することが可能となり，少量の訓練テキストでも効率の良いモデ
ル化が期待できる．本論文ではまた，いくつかのモデルの融合による汎化性の
向上についても検討する．

実時間性が要求される大語彙連続音声認識システムにおいては，緩い言語モデ
ルを用いて\mbox{可能性をしぼり込んだ後，詳細な言語モデルによって最終出力を導}
く２パス処理が一般的である．本論文で提案するような字面の適格性を与える
言語モデルは，ディクテーションシステムの第２パス，すなわち後処理用の言
語モデルとして有用であるものと考えられる．また，文字$n$-gramを用いた認
識手法\cite{yamada94}を本手法に応用することも可能である．
