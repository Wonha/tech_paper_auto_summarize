評価実験
提案した言語モデルの能力を，文字を単位としたパープレキシティ
によって評価する．パープレキシティは，式(\ref{forward})において評価用
テキストを$\futo{C}$として次式で求められる．
\begin{equation}
\mbox{{\it PP}}\simeq\hat{P}(\futo{C})^{-1/k}
\end{equation}
ただし，$k$は評価用テキストの全字数である．長さ1のsuperwordに対しては，
確率が設定した\break
底値を下回る場合には底上げした．対象タスクは朝日新聞「社
説」とした．実験に用いたテキストの量を表\ref{tab4-1}に示す．表中，held-out 
とは式(\ref{limited}), 式(\ref{composite}), 式(\ref{charint})の重み係
数を求めるために用いたテキストである．各々のテキストは，共通部分を持た
ない．
\begin{table}
\vspace{-3mm}
\begin{center}
\caption{訓練テキスト・評価テキストの量}
\label{tab4-1}
\begin{tabular}{|c|rr|} \hline
& 字 & (文) \\ \hline
訓練 & 969497 & (21767) \\
held-out & 85654 & (1953) \\
評価 & 80098 & (1779) \\ \hline
\end{tabular}
\end{center}
\end{table}

長さ制限の効果を見るため，superword unigramモデル
について最大長を変化させてパープレキシティを求めた．その結果を
図\ref{unigram}に示す．
\begin{figure}
\begin{center}
\epsfile{file=unigram1.eps,width=.7\hsize}
\caption{superword unigramモデルにおける長さ制限の効果}
\label{unigram}
\end{center}
\end{figure}
この結果から，長いsuperwordを許してもパープ\break
レキシティは上がらないこと
がわかる．これは，superwordの再現性の条件が適当であったことを示す．
以下の実験では，\(L=20\)の場合を一般superword unigramモデルとして
扱う．

表\ref{tab2}に，提案したモデルおよび従来のモデルのパープレキシティを示
す．上から4項目までがsuperwordに基づくモデルである．bigramとあるのは式
(\ref{limited})の長さ制限付きモデルである．ここでは最大長を3とした．一
般unigram+bigramとあるのは式(\ref{composite})の複合モデル，一般
unigram+bigram+文字とあるのはさらに文字trigramで補間した式
(\ref{charint})のモデルである．その\break
場合の式(\ref{charmodel})の分布関数
としては，指数分布を仮定した．表\ref{tab2}の残りの4項目は比較のために
示\break
してある．単語trigramは，訓練テキストをあらかじめ形態素解析システム
JUMAN\cite{juman94}により分割して求めたものであり，削除補間法によ
りスムージングしたものである．文字+単語trigramは，さらに文字のtrigram
でスムージングしたもので，\break
式\hspace{-0.1mm}(\ref{charmodel})\hspace{-0.1mm}と同様の式を用いている．
文字列\hspace{-0.1mm}trigram\hspace{-0.1mm}は，
訓練テキストに伊藤らの実験\hspace{-0.1mm}\cite{aito96}で最も有効で
あった左最長一致による高頻度文字列への分割法を適用し，さらに文字の\break
trigramでスムージングしたものである．抽出文字列数は約4000から12000まで
変化させ，パープレキシティが極小となった約6000個を用いた時の値を示して
ある．スムージングのためのheld-outデータにはsuperwordモデルと同じもの
を用いている．
\begin{table}[t]
\begin{center}
\caption{各モデルの性能評価}
\label{tab2}
\begin{tabular}{|c|c|} \hline
モデル & パープレキシティ \\ \hline
unigram & 32.4 \\
bigram & 29.8 \\
一般unigram+bigram & 28.5 \\
一般unigram+bigram+文字 & 25.7 \\ \hline
文字trigram & 28.9 \\
単語trigram & 28.6 \\
文字+単語trigram & 26.6 \\
文字列trigram & 28.6 \\ \hline
\end{tabular}
\end{center}
\end{table}

この結果から，次のことがわかる．まず，superword unigramモデルの性能が
良くない．図\ref{unigram}\break
の結果をも考慮すると，これはsuperwordの長さの
問題ではなく，unigramでは語と語の連接関\break
係が本質的に表現できないものと
考えられる．これはATISデータベースの上でのmultigramの評価\cite{deligne95}
といくぶん矛盾する結果であるが，伊藤ら\cite{aito96}も
同様の結果を導いている．

長さ制限付きsuperword bigramモデルの導入によって，性能の向上が見られた．
しかし，ま\break
だその性能は文字trigramモデルに及ばない．

長さ制限付きsuperword bigramモデルと一般superword unigramモデルを
融合させることで，若干の性能向上が見られた．これは，長いsuperwordは単
独ではあまり性能に貢献しないが，語と語の連接関係だけでは表現しきれない
部分を補う効果を持っているものと考えることができる．語と語の関係に関す
る知識と語彙知識とを独立に表現する枠組は，形態素解析の原理と
類似している．

さらに，文字trigramモデルでスムージングすることにより，大き
く性能が向上した．その結果，形態素解析を用いたモデルを超える性能
が得られた．superwordに基づいたモデル単独では訓練テキストに対して
過学習する傾向があり，未知テキストに対して脆弱な側面があるが，未知テキ
ストに対して頑健な文字trigramモデルとの融合によりそれが克服できること
を意味する．
