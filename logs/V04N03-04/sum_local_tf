================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[1553] 本実験で得られた結果は，言語学で確立された言語系統樹と非常に似ており，提案した手法の有効性を示すことができた．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[1612] 確率的言語モデルは，従来，自然言語処理や音声処理などの工学分野で用いられ，その有効性を実証してきたが，比較言語学，方言研究，言語類型論，社会言語学など，言語学の諸分野においても有用な手法を提供するものと思われる．

================================================================
[section type  : proposed_method]
[section title : 従来の研究]
================================================================
[1498] 以上のように，従来の研究では，あらかじめ人間が言語を分類する上で有用であると思われる音韻や語形等の言語的特徴を抽出したり，あるいは比較のための基礎語彙を選定するなどの作業が必要であった．

================================================================
[section type  : proposed_method]
[section title : 確率モデルに基づく言語のクラスタリング]
================================================================
[1415] この方法では，まず各言語の言語データから確率的言語モデルを自動的に学習し，次に確率モデル間に距離を導入することにより，言語間の距離を定義する．
-----------------------------------------------------
  [subsection title : $N$-gram モデル]
-----------------------------------------------------
  [1442] [MATH]-gramモデルを用いた場合，文字列[MATH]の生成確率は，次のようにして計算することができる．
-----------------------------------------------------
  [subsection title : $N$-gram モデルのスムージング]
-----------------------------------------------------
  [1390] 式([REF_Eq:NgramLinearInterpolation])の補間では，学習データ中に三つ組[MATH]が出現しない場合には，bigramとunigramから[MATH]の値を推定している．
-----------------------------------------------------
  [subsection title : 言語モデル間の距離]
-----------------------------------------------------
  [1564] 式([REF_Eq:DistanceMeasure0])では，言語[MATH]と[MATH]の間の距離を，言語[MATH]のモデル[MATH]からデータ[MATH]が生成される確率と，言語[MATH]のモデル[MATH]から同一のデータ[MATH]が生成される確率の差に基づいて決めている．

================================================================
[section type  : experiment_result]
[section title : 評価実験]
================================================================
[0] 
-----------------------------------------------------
  [subsection title : 言語データ]
-----------------------------------------------------
  [1628] 次に，多言語の言語データ間に均質性を持たせるために，単語表記中にアルファベット大文字が使われている場合は小文字に変換し，言語によってはウムラウトやアクセント記号等を表す特殊符号が入っていたが，英語式アルファベット26文字以外の特殊文字は，すべて対応するアルファベットに変換した．
-----------------------------------------------------
  [subsection title : 実験結果および考察]
-----------------------------------------------------
  [1641] 以上のように，実験結果は，言語の細分類に関しても，かなりの部分で言語学での分類と一致しており，提案したクラスタリング手法が有効なものであることを示している．
-----------------------------------------------------
  [subsection title : 言語識別の実験]
-----------------------------------------------------
  [1761] 実験に用いた言語データや対象言語の数などが異なることから，本論文の結果と直接比較することはできないが，Cavnarらの結果も本論文の結果も，[MATH]-gramが言語の自動識別に非常に有効であることを示している．

================================================================
[section type  : conclusion]
[section title : おわりに]
================================================================
[1481] 言語類型論の分野では，言語の語順等により諸言語間の比較を行うことなどが行われているが，語順等に対する言語モデルを設定することができれば，言語類型性という観点から見たクラスタリングを行うことができるであろう．

