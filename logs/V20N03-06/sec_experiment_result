評価実験では，東日本大震災時のツイートデータを用いて，誤情報の抽出を行い，その精度と再現率を測った．
抽出された誤情報を，その代表キーワードの式[REF_eq1]で並べ替え，上位100件を評価対象とした．
考察では，ツイートデータから抽出できなかった事例や，誤って抽出された事例を分類し，今後の対策について述べる．
誤情報の抽出元となるコーパスには，東日本大震災ビックデータワークショップでTwitter Japanから提供された2011年3月11日09:00から2011年3月18日09:00までの日本語のツイートデータ179,286,297ツイートを利用した．
このデータのうち，リツイート（自分の知り合いへのツイートの転送）は単順に同じ文が重複しているだけであるため，取り除いた．
東日本大震災の際に発信された誤情報を網羅的にまとめたデータは存在しない．
評価実験の正解データは，誤情報を人手でまとめた以下の4つのウェブサイトに掲載されている事例を利用した．
絵文録ことのは「震災後のデマ80件を分類整理して見えてきたパニック時の社会心理」
荻上式BLOG「東北地方太平洋沖地震，ネット上でのデマまとめ」
原宿・表参道.
jp地震のデマ・チェーンメール
NAVERまとめ注意！地震に関するデマ・チェーンメールまとめ
以上の4サイトに掲載されているすべての事例のうち，Twitterデータの投稿期間内（2011 3/11 09:00から2011 3/18 09:00まで）に発信されたと判断できる事例は60件存在した．
この60件の誤情報を正解データとした．
作成した正解データの一部を以下に列挙する．
関西以西でも大規模節電の必要性
ワンピースの尾田栄一郎さん15億円寄付
天皇陛下が京都に避難された
ホウ酸を食べると放射能を防げる
双葉病院で病院関係者が患者を置き去りにして逃げた
いわき市田人で食料も水も来ていなく餓死寸前
宮城県花山村が孤立
韓国が震災記念Tシャツを作成
民主党がカップ麺を買い占め
抽出された誤情報の正否は，同等の内容が60件の正解データに含まれるかどうかを一件ずつ人手で判断した．
また，正解データに含まれていないが，誤情報であると判断できるものもある．
そこで抽出された情報が正解データに含まれなかった場合は，関連情報を検索することで，その正否を検証した．
本研究の目的は，出来るだけ多くの誤情報を抽出し，人に提示することにある．
しかし人が一度に見ることのできる情報には限界があり，出来るだけ多くの誤情報を人に提示するには，提示する誤情報の中にある，冗長な誤情報を取り除きたい．
この目的のため，抽出した誤情報のうち，同じ内容と判断できるものが複数ある場合は，正解は一つとし，他の重複するものは不正解とした．
また，日本語として不自然なものも不正解とした．
提案手法はスコアの高い順にN件まで出力可能であるため，Nをいくつか変化させたときの精度@N，再現率@N，F値@Nによって評価した．
精度には，正解データに含まれるかどうかで判断したもの（精度@N（60件））と，人手により検証を行ったもの（精度@N（人手検証））を用意した．
また，人手による検証に加え，重複を許した場合（精度@N（重複））も評価に加えた．
この評価を行うことで，目的の一つである「誤情報抽出」がどの程度達成されているかを知ることができる．
それぞれは以下の式で表される．
精度@N（60件）& = \frac{N事例のうち，60件の誤情報に含まれる事例数（重複を除く）}{N}
[1zw]精度@N（人手検証）& = \frac{N事例のうち，人手で誤情報と検証された事例数（重複を除く）}{N}
[1zw]精度@N（重複）& = \frac{N事例のうち，人手で誤情報と検証された事例数（重複を許す）}{N}
[1zw]再現率@N & = \frac{N事例のうち，60件の誤情報に含まれる事例数（重複を除く）}{正解の誤情報の数（60件）}
[1zw] F値@N & = \frac{2*精度@N（60件）*再現率@N}{精度@N（60件）+再現率@N}
評価結果を表[REF_kekka]に示す．
Nが100のとき，提案手法が抽出した情報のうち，60件の正解データにも含まれる情報は31件であった．
さらに，正解データには含まれないが，誤情報と判断できる事例が23件存在したことから，提案手法は54%の精度で誤情報を抽出できた．
次に，上位N件に限定しない場合の再現率について述べる．
「上限(N=189)」は500個のキーワードをクラスタリングし得られた189個のクラスタから，代表フレーズをすべて出力した時の再現率であり，「上限（クラスタなし）」は，提案手法ステップ1で収集された被訂正フレーズ集合約2万件をすべて出力した時の再現率である．
「上限(N=189)」は，キーワードを189個に絞った時の，ランキング改善による性能向上限界を表すに対し，後者はキーワードの選択，ランキング，クラスタリング改善による性能向上限界，つまり訂正パターンに基づく抽出手法の限界を表す．
被訂正フレーズ集合の段階でカバーされている50件は，キーワードの選択やクラスタリングなど，後段の処理を改善することで抽出できる可能性があるが，残る10件は，訂正パターンに基づく抽出手法の改善が必要となる，難解な事例である．
本節では，評価結果の誤りを分析する．
抽出された誤情報の上位100件のうち，31件は正解データに含まれていたが，残りの69件は正解データに含まれていなかった．
そこで，不正解データに対する誤判定の原因を調べたところ，8種類の原因に分類できた．
表[REF_FP]に理由と件数を示す．
(a)から(d)は，明らかに誤抽出と判断できる事例である．
(e)と(f)は，正解データの構築に用いた4つの誤情報まとめサイトに掲載されてはいなかったが，ウェブ上で調べることで，明らかに誤情報であると認められる事例である．
(g)と(h)は，人手でも誤情報であるかを判断できない事例である．
以下でそれぞれの詳細と，改善案を述べる．
キーワード抽出による誤り
代表キーワードが誤抽出につながったと考えられる事例である．
以下に例を示す．
括弧の中は，選定に利用した代表キーワードである．
\enumsentence{陰謀論とか、「悪意の行動があった」}とかいうデマを信じる人って…（悪意）「善意」や「悪意」といった単語は，元々「デマ」などの訂正表現の周辺文脈に出現しやすい単語であるため，条件付き確率([REF_eq1])が高く，キーワードとして選ばれた．
しかし，特定の誤情報に関連するキーワードではないため，上記の例のように，具体性に欠ける被訂正フレーズが誤情報として抽出された．
このようなキーワードは，誤情報の拡散時に限らず，通常時から訂正表現と共起すると考えられる．
そこで対策として，被訂正フレーズに含まれる確率（式[REF_eq1]）を使用するのではなく，通常時の共起度合いを組み込むことで，改善が望めると考えらる．
クラスタリングによる誤り
抽出された誤情報上位100件のうち，同じ内容と判断できる誤情報が重複している事例である．
例を以下に示す．
括弧の中は，選定に利用した代表キーワードである．
\enumsentence{市原市のコスモ石油千葉製油所LPGタンクの爆発により，千葉県，近隣圏に在住の方に有害な雨などと一緒に飛散する（コスモ石油千葉製油所）
千葉県の石油コンビナート爆発で，空気中に人体に悪影響な物質が空気中に舞い雨が降ると酸性雨になる（石油コンビナート爆発）}これはステップ3でクラスタリングを行ったとき，同じクラスタに分類できなかったため，重複として表れた．
誤情報検出の目的は達成できているものの，冗長な誤情報を抜き出しているため厳しめに評価して不正解とした．
キーワードのクラスタリングには，被訂正フレーズの中で共起する単語を素性としているが，素性に表層の情報を加えることで，誤りを減らすことができると考えられる．
内容が不正確な情報
抽出された誤情報の内容が，誤情報を説明するのに内容が不足していると思われる事例である．
以下に例を示す．
\enumsentence{餓死者や凍死者が出た．
}正解データの中には「いわき市で餓死者や凍死者が出た」というものが存在するが，それと比べると具体性に欠けているため，不正解とした．
より的確な候補を抽出するには，候補が多いほど作成したパターンの精度や再現率を考慮した選定が必要である．
正しい情報
誤情報として抽出されたが，事実を確認したところ，誤情報ではなかった事例である．
以下に例を示す．
\enumsentence{東京タワーの先端が曲がった}この例に関連するツイートを観察したところ，根拠とされる写真を提示されても信じてもらえないほど，突拍子のない情報として扱われていた．
そのため，訂正ツイートが多く投稿されたようである．
提案手法は訂正の数が多い情報ほど，ランキングが上位になる仕組みになっているため，この事例は誤って抽出された．
本研究の目的は「誤情報の抽出」であることを考えると，(a)から(c)の誤りに比べ，深刻な誤りである．
しかし，始めは誤情報として疑っていたユーザーの中には，誤情報出なかったことを知り，以下のようなツイートをしている人も存在した．
\enumsentence{東京タワーが曲がったってデマじゃなかったんだ
東京タワー曲がったとかデマだと思ったら本当だった}このように，訂正を訂正しているツイートも存在し，二重否定を判別することが出来れば，この問題の改善につながると考えられる．
まとめサイトに掲載されていない誤情報（過去）
これは誤情報まとめサイトに掲載されていないが，人手で検証したところ，誤情報と判別された事例である．
その中でも今回利用したツイートコーパスの期間より前の事象に関する誤情報である．
以下に例を示す．
\enumsentence{関東大震災の時「朝鮮人が井戸に毒を入れた」}というのはデマだったはず
\ulinej{阪神淡路大震災は三時間後に最大の揺れが来た}というのは誤った情報のようです。
\ulinej{明治43年（1910年）にハレー彗星が大接近した時、地球上の空気が5分間ほどなくなる}というデマが一部で広まり，…上記の例は訂正ツイートであり，下線部は被訂正フレーズとして抽出された部分である．
一度過去に誤情報として認識されたことは間違いないが，人々に悪影響を与える可能性があり，誤情報として抽出し，拡散・訂正の動向を監視する必要がある．
まとめサイトに掲載されていない誤情報（現在）
これは誤情報まとめサイトに掲載されていないが，人手で検証を行ったところ，誤情報と判別された事例である．
その中でも今回利用したツイートコーパスの期間中に発生した誤情報である．
以下に例を示す．
\enumsentence{ VIPで韓国の救助犬1匹が逃亡
巷説にある遺体には感染症のリスクがある}
未来予測
(h)の真偽不明の事例のうち，未来に起こりうる事象について述べたものを抽出した事例である．
以下に例を示す．
\enumsentence{福島で核爆発が起こる
富士山が噴火する}未来に起こりうる事象である以上，現時点での真偽は不明である．
抽出されたものの多くは，上記の例のように人々の不安を煽る情報であり，パニックを防ぎたいと思い訂正ツイートを発信した人が多かったため，抽出されたと考えられる．
真偽不明
複数のウェブサイトを検索して検証を行ったが，誤情報かどうかを判別できなかった事例である．
以下に例を示す．
\enumsentence{サントリーが自販機無料開放
築地で魚が余っている}
次に，正解データにある誤情報60件のうち，抽出されなかった誤情報29件についても同様に原因を調査したところ，3つに分類できることが判明した．
3つの原因の件数と割合を表[REF_FN]に示す．
訂正パターンで候補を抽出できなかったもの
今回作成した訂正パターンでは，抽出できなかった誤情報である．
「仙台市三条中学校が中国人・韓国人が7割の留学生の心ない行動で避難所機能停止」という誤情報に対して，以下のようなツイートが数多く存在した．
\enumsentence{コレ本当? RT @XXXXX今，祖母と叔母に確認．
何と仙台市の三条中学校の避難所，閉鎖！避難所用救援物資を根こそぎ，近隣の外国人留学生（中国韓国で七割強）が運び出してしまい，避難所の機能停止だそうです．
}上の例では，明示的に誤情報だと否定している人は少ないが，元のツイートコメントする形で，その情報を疑っている人は多かった．
このことから，改善案とし訂正パターンのみではなく，懐疑を表す表現も利用できるのではないかと思われる．
訂正パターンで抽出できたが，クラスタリングによる誤り
訂正パターンにより候補の抽出はできたが，クラスタリングにより，誤って他の誤情報に含まれた事例である．
しかし，全体に比べ，事例数が少ないため，それほど問題ではないと思われる．
訂正パターンで抽出できたが，ランキング外
訂正パターンにより候補を抽出できたが，条件付き確率が低かったため，キーワードとして抽出できなかった事例である．
例えば，「東京電力を装った男が表れた」という誤情報では，「東京電力」というキーワードは誤情報以外の話題でも頻出したため，条件付き確率が低くなった．
対策としては，キーワード単独をスコアリングするのではなく，被訂正フレーズそのものをスコアリングするような手法が必要である．
