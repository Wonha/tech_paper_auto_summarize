分類誤りの原因を追求すると，訓練データに現れない表現あるいは頻度の低い表現の
部分で分類が誤っている\footnote{先の実験により示した本手法が検出できなかった未知語の
出現数（336）から考えて，全体の誤りの数（851）が多いようにも感じられる．
しかしこれは，本実験では頻度7以下の証拠を間引いているために，
本手法における未知語の実質的な総数は，先の実験で示した数よりも多いことによる．}．
これは未知語の問題そのものであり，
未知語への対処が単語分割の中心の課題と言える．
この解決策は3つ考えられる．1つ目は規則の一般化を精度良く行うことである．
例えば文字クラス\cite{oda99}などの導入などが考えられる．
2つ目は別リソースの利用である．例えば辞書の利用である．
単語分割に本手法の分類手法と辞書による最長一致法を利用することも考えられる．
3つ目は訓練データの拡充である．
事例ベースの手法\cite{yamashita98,ito99}は訓練データつまり事例を大規模化することで精度が上がる．
ただし大規模な正解付きの訓練データが用意できない現状では，
正解のない訓練データをどう使うかが鍵となる\cite{shinno00}．
１つ目のアプローチ以外は，未知語の検出に対して理論的な保証がない．
しかしだからといって，単語分割を文字ベースの手法によって解くことに意味がないわけではない．
辞書に基づいた分割では数値表現や字種区切りが有効になるような未知語しか
解析できず，解析できる未知語が限定されている．このような未知語の多くは，
実験に示したように，本手法でもその多くを検出できる．
さらに文字ベースの手法では，その他のタイプの未知語も検出できる
場合が多々あるが，辞書に基づいた分割では確実に検出できない．この違いは大きい．
score of this paragraph is 3
