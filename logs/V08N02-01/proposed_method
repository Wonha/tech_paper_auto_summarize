[MATH]文字からなる入力文を[MATH]（各[MATH]は文字を表す）とすると，日本語単語分割は文字[MATH]と[MATH]の間（[MATH]と名付ける）に単語境界がある([MATH])かない([MATH])かを与えることによって行える。
つまり[MATH]（[MATH]）に[MATH]か[MATH]を与える分類問題としてとらえられる.
例えば，「太郎は海でアイスクリームを食べた。
」という文に対しては，図[REF_zu1]のように各文字間にクラス[MATH]あるいは[MATH]を付与し，[MATH]の部分を単語境界に置き換えることにより単語分割が行える．
分類問題を解く手法は様々なものがある．
どの手法が優れているかは問題に依存するために一概には言えない．
本論文では決定リストを利用して上記の分類問題を解く．
決定リストは帰納学習手法の一種であり，正解付きの訓練データから，分類規則を学習する．
決定リストの場合，分類規則は証拠とクラスの組の順序付きの表となる．
ここで証拠とは属性とその属性の値の組である．
実際の分類はリストの上位のものから順に，その証拠があるかどうかを調べ，その証拠があれば，それに対応するクラスを出力する．
決定リストの作成は概ね以下の手順による．
属性を設定する．
例えば[MATH]個の属性を[MATH]とする。
訓練データから証拠とクラスの組の頻度を調べる．
訓練データ中のあるデータの属性[MATH]の値が[MATH]であるとし，そのデータのクラスが[MATH]だとする．
その場合，[MATH]という証拠とクラス[MATH]の組[MATH]の頻度に1を足す．
これを訓練データ中の全データに対する全属性について行う．
証拠の判別力と分類クラスを導く．
[MATH]の頻度が[MATH]であった場合，[MATH]の最大値を与える[MATH]が証拠[MATH]に対する分類クラスとなる．
またそのときの判別力[MATH]は以下で定義される．
判別力の順に並べる．
全ての証拠と分類クラスの組を判別力の大きい順に並べる．
これによって作成できた表が決定リストである．
各文字間[MATH]がどのクラスに属するかを判断する材料が属性である．
本論文では[MATH]の属性として，表[REF_attribute]の7種類を用意した．
[h]
\leavevmode \ecaption{Setting attributes}
{|c|cc|} \hline属性&値&
\hline[MATH] &文字列& [MATH]
\hline[MATH] &文字列& [MATH]
\hline
[MATH] &文字列& [MATH]
\hline[MATH] &文字列& [MATH]
\hline[MATH] &文字列& [MATH]
\hline[MATH] &字種の接続関係1 & [MATH]
\hline[MATH] &字種の接続関係2 & [MATH]
\hline
6，7番目の属性として，字種の情報を利用している形になっている．
ここでは字種を大分類と細分類の二つの観点から分類した．
字種の大分類は6番目の属性，字種の細分類は7番目の属性で利用した．
字種の大分類は表[REF_dai-bunrui]に示した9種類である．
字種の細分類は大分類の平仮名の部分をその文字自身にしたものである．
また注意として，本論文の決定リストでは[MATH]の証拠を導入していない．
決定リストでは通常[MATH]という証拠を設けて，それ以下の判別力の証拠は表には入れない．
[MATH]は文脈上の証拠が決定リストに存在しない場合の処理ととらえられるが，ここでは大分類の字種の情報が必ずヒットするので，[MATH]の証拠を含める必要がない．
6番目の属性からの証拠の最下位のものが，決定リストの最下位の証拠となる．
決定リストの利用例を示す．
例えば「太郎は海でアイスクリームを食べた。
」という入力文の5番目の文字``で''と6番目の文字``ア''の間，つまり[MATH]にクラス[MATH]あるいは[MATH]を与えてみる．
[MATH]の持つ証拠は以下の7種である．
[MATH]，[MATH]，[MATH]，
[MATH]，[MATH]，[MATH]，[MATH]
後述する実験で得られた決定リストを用いると，各証拠の分類クラスと判別力は以下の通りである．
表の中で``--''の記号のものは，決定リスト中にその証拠がないことをあらわす．
また本来ならば，決定リスト中の順位を求めなければならないが，ここでは相対的な順位関係だけが必要であり，順位の値自体は必要でない．
判別力の最も大きなものが最上位の順位になるはずである．
この場合，証拠[MATH]が最も大きな判別力を持つので，この証拠の分類クラス+1が判定結果となる．
つまり[MATH]には単語境界を置くと判定する．
精度の低い分類規則を組み合わせて精度の高い分類規則を得る方式をブースティングという．
アダブーストはブースティング方式の一つであり，現在まで多くの理論的検証と実験的実証から有効性が示されている．
アダブーストのアルゴリズムを図[REF_algo]に示す．
分類クラス(図[REF_algo]の[MATH])をここでは[MATH]の2値とする．
また訓練データを[MATH]で表す．
ここで各[MATH]はデータを表し，[MATH]はデータ[MATH]のクラスである．
具体的に[MATH]は[MATH]あるいは[MATH]の値である．
この訓練データに対して，分類問題に対する学習アルゴリズム，例えば，決定木や決定リストなどを適用して，分類規則[MATH]を学習する．
得られた分類規則[MATH]を訓練データに適用すると，[MATH]によって各[MATH]の判定クラスが得られる．
今，[MATH]の実際のクラス[MATH]は与えられているので，分類規則[MATH]が各[MATH]に対して正しい判定を行ったかどうかを調べられる．
これによって不正解のデータを集め，それら不正解のデータに対してある重みを付加して，訓練データ[MATH]を再構成する．
そしてこの再構成された訓練データに対して，再び学習アルゴリズムを適用して，分類規則[MATH]を学習する．
これを[MATH]回繰り返す．
この繰り返しによって，[MATH]組の分類規則[MATH]が得られる．
実際の判定は入力データに対して各分類規則が出力するクラスの重み付き多数決により行われる．
例えば，[MATH]とし，入力データ[MATH]に対して，分類器[MATH]による判定クラスが[MATH]，[MATH]による判定クラスが[MATH]，[MATH]による判定クラスが[MATH]であり，各重みが1，2.0，2.2であった場合，重み付き多数決の結果は[MATH]である．
最終的な判定クラスは総和の符合により求まる．
この例の場合，符合は正であるので，[MATH]が判定クラスになる．
アダブーストのポイントは不正解のデータに課す重みの与え方である．
概略，得られた分類規則の誤り確率（図[REF_algo]における[MATH])が小さいほど重みが大きくなるように設定している．
本論文では．
分類問題に対する学習アルゴリズムを決定リストに設定する．
不正解データに与える重みをどのように反映させるかが問題である．
ここでは，重みを頻度として与えることにした．
例えば，「太郎が東京へ行く。
」という文に以下のように単語境界``/''が置かれたものが訓練データである．
太郎/が/東京/へ/行く/。
今，4番目の文字``東''と5番目の文字``京''の間，つまり[MATH]に対する証拠は以下の通りである．
[MATH]，[MATH]，[MATH]，
[MATH]，[MATH]，[MATH]，[MATH]
``東''と``京''の間には，単語境界がないので，クラスは[MATH]である．
そして，決定リスト作成のstep 2で示したように，以下の証拠の頻度に1が足される．
[MATH]，[MATH]，[MATH]，
[MATH]，[MATH]，[MATH]，[MATH]
この頻度に加算される1という数値に重みを反映させる．
例えば，決定リスト[MATH]により上記例文の4番目の文字``東''と5番目の文字``京''の間の判定クラスが[MATH]と判定された場合，この判定は不正解である．
そこで次の決定リスト[MATH]を作成するときに，上記の７つの各証拠の頻度に1ではなく，重み自身を加える．
つまり決定リストを作成する際には各訓練データには重みがついているとして，その重みが決定リスト作成のstep 2で各証拠と正解の組に付加する数値とする．
図[REF_algo]のアルゴリズムでは正規化するために重みの総和が１になっているが，ここでは重みの最小値が１となるようにして計算を簡単にした．
このため最初の決定リストを作成する際の各訓練データの重みは１であり，２回目では正解のデータの重みは１で変化せず，不正解の部分の重みが大きくなる．
