近年の著しい計算機速度の向上，及び，音声処理技術/自然言語処理技術の向上により，音声ディクテーションシステムやパソコンで動作する連続音声認識のフリーソフトウェアの公開など，音声認識技術が実用的なアプリケーションとして社会に受け入れられる可能性がでてきた[CITE]．
我が国では，大量のテキストデータベースや音声データベースの未整備のため欧米と比べてディクテーションシステムの研究は遅れていたが，最近になって新聞テキストデータやその読み上げ文のデータが整備され[CITE],ようやく研究基盤が整った状況である．
このような背景を踏まえ，本研究では大規模コーパスが利用可能な新聞の読み上げ音声の精度の良い言語モデルの構築を実験的に検討した．
音声認識のためのN-gram言語モデルでは，N=3[MATH]4で十分であると考えられる[CITE]．
しかし，N=3ではパラメータの数が多くなり，音声認識時の負荷が大きい．
そこで，大語彙連続音声認識では，第1パス目はN=2のbigramモデルで複数候補の認識結果を出力し，N=3のtrigramで後処理を行なう方法が一般的である．
本研究では，第2パスのtrigramの改善ばかりでなく，第1パス目のbigram言語モデルの改善を目指し，以下の3つの点に注目した．
まずタスクについて注目する．
言語モデルをN-gramベースで構築する場合(ルールベースで記述するのとは異なり)，大量の学習データが必要となる．
最近では各種データベースが幅広く構築され，言語モデルの作成に新聞記事などの大規模なデータベースを利用した研究が行なわれている[CITE]．
しかしN-gramはタスクに依存するのでタスクに関する大量のデータベースを用いて構築される必要がある．
例えば，観光案内対話タスクを想定し，既存の大量の言語データに特定タスクの言語データを少量混合することによって，N-gram言語モデルの性能の改善が行なわれている[CITE]．
また，複数のトピックに関する言語モデルの線形補間で適応化する方法が試みられている[CITE]．
本研究ではタスクへの適応化のために，同一ジャンルの過去の記事を用いる方法とその有効性を示す．
次に言語モデルの経時変化について注目する．
例えば新聞記事などでは話題が経時的に変化し，新しい固有名詞が短期的に集中的に出現する場合が多い．
以前の研究では、直前の数百単語による言語モデルの適応化(キャッシュ法)が試みられ[CITE]，小さいタスクではその有効性が示されてはいるが，本論文では直前の数万〜数十万語に拡大する．
つまり，直前の数日間〜数週間の記事内容で言語モデルを適応化する方法を検討し，その有効性を示す．
最後に認識単位に注目する．
音声認識において，認識単位が短い場合認識誤りを生じやすく，付属語においてその影響は大きいと考えられ，小林らは，付属語列を新たな認識単位とした場合の効果の検証をしている[CITE]．
また高木らは，高頻度の付属語連鎖，関連率の高い複合名詞などを新しい認識単位とし，これらを語彙に加えることによる言語モデルの性能に与える影響を検討している[CITE]．
なお，連続する単語クラスを連結して一つの単語クラスとする方法や句を一つの単位とする方法は以前から試みられているが，いずれも適用されたデータベースの規模が小さい[CITE]．
同じような効果を狙った方法として，N-gramのNを可変にする方法も試みられている[CITE]．
なお，定型表現の抽出に関する研究は，テキスト処理分野では多くが試みられている(例えば，新納,井佐原1995;北,小倉,森本,矢野1995)．
新聞テキストには，使用頻度の高い(特殊)表現や，固定的な言い回しなどの表現(以下，定型表現と呼ぶ)が非常に多いと思われる．
定型表現は，音声認識用の言語モデルや音声認識結果の誤り訂正のための後処理に適用できる．
そこでまず，定型表現を抽出した．
次に，これらの(複数形態素から成る)定型表現を1形態素として捉えた上で，N-gram言語モデルを構築する方法を検討する．
評価実験の結果，長さ2および3以下である定型表現を1形態素化してbigram, trigram言語モデルを作成することで，bigramに関しては，エントロピーが小さくなり，言語モデルとして有効であることを示す．
なお，これらの手法に関しては様々な方法が提案されているが，大規模のテキストデータを用いて，タスクの適応化と定型表現の導入の有効性を統一的に評価した研究は報告されていない．
近年の著しい計算機速度の向上，及び，音声処理技術/自然言語処理技術の向上により，音声ディクテーションシステムやパソコンで動作する連続音声認識のフリーソフトウェアの公開など，音声認識技術が実用的なアプリケーションとして社会に受け入れられる可能性がでてきた[CITE]．
我が国では，大量のテキストデータベースや音声データベースの未整備のため欧米と比べてディクテーションシステムの研究は遅れていたが，最近になって新聞テキストデータやその読み上げ文のデータが整備され[CITE],ようやく研究基盤が整った状況である．
このような背景を踏まえ，本研究では大規模コーパスが利用可能な新聞の読み上げ音声の精度の良い言語モデルの構築を実験的に検討した．
音声認識のためのN-gram言語モデルでは，N=3[MATH]4で十分であると考えられる[CITE]．
しかし，N=3ではパラメータの数が多くなり，音声認識時の負荷が大きい．
そこで，大語彙連続音声認識では，第1パス目はN=2のbigramモデルで複数候補の認識結果を出力し，N=3のtrigramで後処理を行なう方法が一般的である．
本研究では，第2パスのtrigramの改善ばかりでなく，第1パス目のbigram言語モデルの改善を目指し，以下の3つの点に注目した．
まずタスクについて注目する．
言語モデルをN-gramベースで構築する場合(ルールベースで記述するのとは異なり)，大量の学習データが必要となる．
最近では各種データベースが幅広く構築され，言語モデルの作成に新聞記事などの大規模なデータベースを利用した研究が行なわれている[CITE]．
しかしN-gramはタスクに依存するのでタスクに関する大量のデータベースを用いて構築される必要がある．
例えば，観光案内対話タスクを想定し，既存の大量の言語データに特定タスクの言語データを少量混合することによって，N-gram言語モデルの性能の改善が行なわれている[CITE]．
また，複数のトピックに関する言語モデルの線形補間で適応化する方法が試みられている[CITE]．
本研究ではタスクへの適応化のために，同一ジャンルの過去の記事を用いる方法とその有効性を示す．
次に言語モデルの経時変化について注目する．
例えば新聞記事などでは話題が経時的に変化し，新しい固有名詞が短期的に集中的に出現する場合が多い．
以前の研究では、直前の数百単語による言語モデルの適応化(キャッシュ法)が試みられ[CITE]，小さいタスクではその有効性が示されてはいるが，本論文では直前の数万〜数十万語に拡大する．
つまり，直前の数日間〜数週間の記事内容で言語モデルを適応化する方法を検討し，その有効性を示す．
最後に認識単位に注目する．
音声認識において，認識単位が短い場合認識誤りを生じやすく，付属語においてその影響は大きいと考えられ，小林らは，付属語列を新たな認識単位とした場合の効果の検証をしている[CITE]．
また高木らは，高頻度の付属語連鎖，関連率の高い複合名詞などを新しい認識単位とし，これらを語彙に加えることによる言語モデルの性能に与える影響を検討している[CITE]．
なお，連続する単語クラスを連結して一つの単語クラスとする方法や句を一つの単位とする方法は以前から試みられているが，いずれも適用されたデータベースの規模が小さい[CITE]．
同じような効果を狙った方法として，N-gramのNを可変にする方法も試みられている[CITE]．
なお，定型表現の抽出に関する研究は，テキスト処理分野では多くが試みられている(例えば，新納,井佐原1995;北,小倉,森本,矢野1995)．
新聞テキストには，使用頻度の高い(特殊)表現や，固定的な言い回しなどの表現(以下，定型表現と呼ぶ)が非常に多いと思われる．
定型表現は，音声認識用の言語モデルや音声認識結果の誤り訂正のための後処理に適用できる．
そこでまず，定型表現を抽出した．
次に，これらの(複数形態素から成る)定型表現を1形態素として捉えた上で，N-gram言語モデルを構築する方法を検討する．
評価実験の結果，長さ2および3以下である定型表現を1形態素化してbigram, trigram言語モデルを作成することで，bigramに関しては，エントロピーが小さくなり，言語モデルとして有効であることを示す．
なお，これらの手法に関しては様々な方法が提案されているが，大規模のテキストデータを用いて，タスクの適応化と定型表現の導入の有効性を統一的に評価した研究は報告されていない．
