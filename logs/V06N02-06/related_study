まずタスクについて注目する．言語モデルをN-gramベースで構築する場合(ルールベースで記述するのとは異なり)，大量の学習データが必要となる．最近では各種データベースが幅広く構築され，言語モデルの作成に新聞記事などの大規模なデータベースを利用した研究が行なわれている[CITE]．しかしN-gramはタスクに依存するのでタスクに関する大量のデータベースを用いて構築される必要がある．例えば，観光案内対話タスクを想定し，既存の大量の言語データに特定タスクの言語データを少量混合することによって，N-gram言語モデルの性能の改善が行なわれている[CITE]．また，複数のトピックに関する言語モデルの線形補間で適応化する方法が試みられている[CITE]．本研究ではタスクへの適応化のために，同一ジャンルの過去の記事を用いる方法とその有効性を示す．

次に言語モデルの経時変化について注目する．例えば新聞記事などでは話題が経時的に変化し，新しい固有名詞が短期的に集中的に出現する場合が多い．以前の研究では、直前の数百単語による言語モデルの適応化(キャッシュ法)が試みられ[CITE]，小さいタスクではその有効性が示されてはいるが，本論文では直前の数万〜数十万語に拡大する．つまり，直前の数日間〜数週間の記事内容で言語モデルを適応化する方法を検討し，その有効性を示す．

最後に認識単位に注目する．音声認識において，認識単位が短い場合認識誤りを生じやすく，付属語においてその影響は大きいと考えられ，小林らは，付属語列を新たな認識単位とした場合の効果の検証をしている[CITE]．また高木らは，高頻度の付属語連鎖，関連率の高い複合名詞などを新しい認識単位とし，これらを語彙に加えることによる言語モデルの性能に与える影響を検討している[CITE]．なお，連続する単語クラスを連結して一つの単語クラスとする方法や句を一つの単位とする方法は以前から試みられているが，いずれも適用されたデータベースの規模が小さい[CITE]．同じような効果を狙った方法として，N-gramのNを可変にする方法も試みられている[CITE]．なお，定型表現の抽出に関する研究は，テキスト処理分野では多くが試みられている(例えば，新納,井佐原1995;北,小倉,森本,矢野1995)．

paragraph score: 1.00798387289598
