これまでの単一テキストを対象とする要約研究の多くは，人間の被験者の作成した要約文と，システムの作成した要約文を比較し，システムの要約文の再現率，適合率を評価尺度とした評価を行なっていた．
しかし，人間においても要約というタスクは必ずしも容易ではなく，人間の被験者による要約が必ずしも高い割合で一致するとは言えない．
また，この評価法の前提とする「ただ一つ正しい要約が存在する」という仮定が不自然であるという批判が以前からあり，要約システムの評価方法は再検討される段階にあると言える．
これに対して，Miikeら[CITE]は，要約を利用して人間がタスクを行なう場合の，タスクの達成率が間接的に要約の評価となるという考え方に基づき，評価を行なっている．
具体的には，情報検索における検索テキストの適切性の判断をする際に要約を用いることで，要約を評価し，タスクに要する時間と，検索の再現率，適合率で評価を行なっている．
DARPA Tipsterプロジェクト(Phase III)の評価[CITE]においても，同様に，上の仮定の不自然さから，タスクに基づく評価方法が採用されている．
Tipsterプロジェクトでは，テキストの分類，情報検索における検索テキストの適切性の判断それぞれに要約を利用し，被験者のタスクに要する時間(要約しないテキスト全体を用いた場合とも比較する)，タスクの精度により要約を評価する．
一方，間瀬ら[CITE]は，原文を読んだ後および，その要約だけを読んだ後，原文の内容を問うテストを被験者に行ない，テストの得点比で要約の評価を行なっている．
テストの問題作成の困難さが問題点として残るが，原文を伴わない状況での利用を想定した要約の内容の十分性の評価としては興味深い手法である．
このように，要約を用いて人間の被験者が何らかのタスクを実行する際の精度等を問題にするのではなく，要約を利用して何らかのタスクを実行する応用プログラムの精度を示すことで，間接的に要約の評価を行なうという試みも見られる．
隅田ら[CITE]は，抽出した要約文のみを索引およびスコアづけの対象としたテキスト検索システムの評価を行ない，テキスト全体を索引等に用いた場合に比べ，精度の向上が実現できることを示すことで，抽出した要約文がテキストの大意の把握に成功していることを間接的に実証している．
良い要約が得られれば，重要な概念や単語のみが索引語として利用されるので，検索の精度が改善されるはずであるという仮定にこの評価は基づいている．
このような，要約文の内容に関する評価とは別に，要約文の「文章としての読み易さ」を評価する評価方法も考えられる[CITE]．
2.1節(2)で紹介した，Brandowら，Wassonは，人間の受容可能性判断に基づいて要約を評価している．
受容可能性は，人間が，原文と照らし合わせて，内容と読み易さに関して，受容可能/不可能の判定を要約に対して行ない求められる指標である．
要約は，本来このように，内容に関する評価と，読み易さに関する評価の，両方の次元で評価されるべきであると言え，今後もより良い要約の評価方法の模索は続けられるものと考えられる．
1節で述べたように，要約は一般に，その利用目的に応じて，次の2つのタイプに分けられることが多い[CITE]．
原文の適切性を判断するなど，原文を参照する前の段階で用いる
原文の代わりとして用いる
Miikeら，Tipsterプロジェクトの評価は，要約をindicativeなものとして評価していると言うことができる．
一方，間瀬ら，隅田らの評価は，informativeなものとしての要約の評価を行なっていることになる．
ここで，Tipsterプロジェクトにおける評価方法について，もう少し詳しく触れておく．
Tipsterプロジェクトの評価法は，上にも述べたように，タスクに基づくものであるが，そのタスクは，以下の3つからなる．
`query-based'要約は，4節で述べた，ユーザの要求に特化した要約，`generic'な要約は，特化しない要約を意味する．
最初の2つのタスクでは，10%の要約率での要約と，開発者が「最も良い」と考える要約(長さは問わない)を基に評価を行なう．
3つ目のタスクでは，質問に対する解答の正当率で要約を評価する．
質問はテキストごとに変わるものではなく，queryで示されたtopicごとに5つ用意される．
あるtopicに関する質問の正解は，質問作成者自身が，(質問に対する正解を与えていると判断した)原文のpassageを選ぶことで決定される．
評価は，このpassageを要約がどの程度含むかで人間が判断する．
評価の指標であるAnswer Recallは，correct, partially correct, missingの3段階で判断される．
要約の評価方法としては，上述した，間瀬らの手法と同様なものと考えられる．
一方，5節で述べた，複数テキストを対象とする要約研究や，6節で述べた，文中の重要箇所抽出による要約研究の評価は,研究が始まったばかりでもあり，十分な議論がなされてきていないと言って良い.
5節で述べたように，複数テキストを対象とする場合，冗長な重複箇所を検出し，削除することが必要となるため，「冗長箇所をどの程度正しく削除できているか」[CITE]，「テキスト間の類似箇所と相違箇所をどの程度正しく抽出できているか」[CITE]という観点での評価が行なわれている．
また，難波ら[CITE]は，「要約に必要な記述内容(参照箇所)をどの程度正しく抽出できているか」を評価している．
しかし，複数テキストから作成された要約文全体に関する評価はこれまでなされておらず，どのような点を評価すべきかということも明らかではない．
今後,作成された要約全体の評価について検討していく必要があると考えられる.
これまでの単一テキストを対象とする要約研究の多くは，人間の被験者の作成した要約文と，システムの作成した要約文を比較し，システムの要約文の再現率，適合率を評価尺度とした評価を行なっていた．
しかし，人間においても要約というタスクは必ずしも容易ではなく，人間の被験者による要約が必ずしも高い割合で一致するとは言えない．
また，この評価法の前提とする「ただ一つ正しい要約が存在する」という仮定が不自然であるという批判が以前からあり，要約システムの評価方法は再検討される段階にあると言える．
これに対して，Miikeら[CITE]は，要約を利用して人間がタスクを行なう場合の，タスクの達成率が間接的に要約の評価となるという考え方に基づき，評価を行なっている．
具体的には，情報検索における検索テキストの適切性の判断をする際に要約を用いることで，要約を評価し，タスクに要する時間と，検索の再現率，適合率で評価を行なっている．
DARPA Tipsterプロジェクト(Phase III)の評価[CITE]においても，同様に，上の仮定の不自然さから，タスクに基づく評価方法が採用されている．
Tipsterプロジェクトでは，テキストの分類，情報検索における検索テキストの適切性の判断それぞれに要約を利用し，被験者のタスクに要する時間(要約しないテキスト全体を用いた場合とも比較する)，タスクの精度により要約を評価する．
一方，間瀬ら[CITE]は，原文を読んだ後および，その要約だけを読んだ後，原文の内容を問うテストを被験者に行ない，テストの得点比で要約の評価を行なっている．
テストの問題作成の困難さが問題点として残るが，原文を伴わない状況での利用を想定した要約の内容の十分性の評価としては興味深い手法である．
このように，要約を用いて人間の被験者が何らかのタスクを実行する際の精度等を問題にするのではなく，要約を利用して何らかのタスクを実行する応用プログラムの精度を示すことで，間接的に要約の評価を行なうという試みも見られる．
隅田ら[CITE]は，抽出した要約文のみを索引およびスコアづけの対象としたテキスト検索システムの評価を行ない，テキスト全体を索引等に用いた場合に比べ，精度の向上が実現できることを示すことで，抽出した要約文がテキストの大意の把握に成功していることを間接的に実証している．
良い要約が得られれば，重要な概念や単語のみが索引語として利用されるので，検索の精度が改善されるはずであるという仮定にこの評価は基づいている．
このような，要約文の内容に関する評価とは別に，要約文の「文章としての読み易さ」を評価する評価方法も考えられる[CITE]．
2.1節(2)で紹介した，Brandowら，Wassonは，人間の受容可能性判断に基づいて要約を評価している．
受容可能性は，人間が，原文と照らし合わせて，内容と読み易さに関して，受容可能/不可能の判定を要約に対して行ない求められる指標である．
要約は，本来このように，内容に関する評価と，読み易さに関する評価の，両方の次元で評価されるべきであると言え，今後もより良い要約の評価方法の模索は続けられるものと考えられる．
1節で述べたように，要約は一般に，その利用目的に応じて，次の2つのタイプに分けられることが多い[CITE]．
原文の適切性を判断するなど，原文を参照する前の段階で用いる
原文の代わりとして用いる
Miikeら，Tipsterプロジェクトの評価は，要約をindicativeなものとして評価していると言うことができる．
一方，間瀬ら，隅田らの評価は，informativeなものとしての要約の評価を行なっていることになる．
ここで，Tipsterプロジェクトにおける評価方法について，もう少し詳しく触れておく．
Tipsterプロジェクトの評価法は，上にも述べたように，タスクに基づくものであるが，そのタスクは，以下の3つからなる．
`query-based'要約は，4節で述べた，ユーザの要求に特化した要約，`generic'な要約は，特化しない要約を意味する．
最初の2つのタスクでは，10%の要約率での要約と，開発者が「最も良い」と考える要約(長さは問わない)を基に評価を行なう．
3つ目のタスクでは，質問に対する解答の正当率で要約を評価する．
質問はテキストごとに変わるものではなく，queryで示されたtopicごとに5つ用意される．
あるtopicに関する質問の正解は，質問作成者自身が，(質問に対する正解を与えていると判断した)原文のpassageを選ぶことで決定される．
評価は，このpassageを要約がどの程度含むかで人間が判断する．
評価の指標であるAnswer Recallは，correct, partially correct, missingの3段階で判断される．
要約の評価方法としては，上述した，間瀬らの手法と同様なものと考えられる．
一方，5節で述べた，複数テキストを対象とする要約研究や，6節で述べた，文中の重要箇所抽出による要約研究の評価は,研究が始まったばかりでもあり，十分な議論がなされてきていないと言って良い.
5節で述べたように，複数テキストを対象とする場合，冗長な重複箇所を検出し，削除することが必要となるため，「冗長箇所をどの程度正しく削除できているか」[CITE]，「テキスト間の類似箇所と相違箇所をどの程度正しく抽出できているか」[CITE]という観点での評価が行なわれている．
また，難波ら[CITE]は，「要約に必要な記述内容(参照箇所)をどの程度正しく抽出できているか」を評価している．
しかし，複数テキストから作成された要約文全体に関する評価はこれまでなされておらず，どのような点を評価すべきかということも明らかではない．
今後,作成された要約全体の評価について検討していく必要があると考えられる.
