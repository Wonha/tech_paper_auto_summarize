期待損失最小化からみた共変量シフト


対象単語\( w \)の語義の集合を\( C \)，また
\( w \)の用例\( \boldsymbol{x} \)内の\( w \)の語義を\( c \)と識別したときの
損失関数を\( l(\boldsymbol{x},c,d) \)で表す．\( d \)は\( w \)の語義を識別する分類器である．
\( P_T(\boldsymbol{x},c) \) をターゲット領域上の分布とすれば，
領域適応の問題における期待損失\( L_0 \)は以下で表せる．
\[
L_0 = \sum_{\boldsymbol{x},c} l(\boldsymbol{x},c,d) P_T(\boldsymbol{x},c)
\]
また\( P_S(\boldsymbol{x},c) \) をソース領域上の分布とすると以下が成立する．
\[
L_0 = \sum_{\boldsymbol{x},c} l(\boldsymbol{x},c) \frac{P_T(\boldsymbol{x},c)}{P_S(\boldsymbol{x},c)} P_S(\boldsymbol{x},c)
\]
ここで共変量シフトの仮定から
\[
\frac{P_T(\boldsymbol{x},c)}{P_S(\boldsymbol{x},c)} = \frac{P_T(\boldsymbol{x})P_T(c|\boldsymbol{x})}{P_S(\boldsymbol{x})P_S(c|\boldsymbol{x})} = \frac{P_T(\boldsymbol{x})}{P_S(\boldsymbol{x})}
\]
となり，\( r(\boldsymbol{x}) = P_T(\boldsymbol{x})/P_S(\boldsymbol{x}) \)とおくと以下が成立する．
\[
L_0 = \sum_{\boldsymbol{x},c} r(\boldsymbol{x}) l(\boldsymbol{x},c,d) P_S(\boldsymbol{x},c)
\]

訓練データを\( D = \{ (\boldsymbol{x_i},c_i) \}_{i = 1}^N \)とし，
\( P_S(\boldsymbol{x},c) \)を経験分布で近似すれば，
\[
 L_0 \approx  \frac{1}{N} \sum_{i=1}^N r(\boldsymbol{x_i}) l(\boldsymbol{x_i},c_i,d) 
\]
となるので，期待損失最小化の観点から考えると，共変量シフトの問題は以下の式\( L_1 \)を
最小にする\( d \)を求めればよいことがわかる．
\begin{equation}
L_1 = \sum_{i=1}^N r(\boldsymbol{x_i}) l(\boldsymbol{x_i},c_i,d) 
\label{eq:1}
\end{equation}


重み付き対数尤度の最大化


分類器\( d \)として以下の事後確率最大化推定に基づく識別を考える．
\[
d(\boldsymbol{x}) = \arg \max_{c} P_T(c|\boldsymbol{x})
\]
また損失関数として対数損失\( - \log P_T(c|\boldsymbol{x}) \)を用いれば，
\mbox{式(\ref{eq:1})}は以下となる．
\[
L_1 = - \sum_{i=1}^N r(\boldsymbol{x_i}) \log P_T(c|\boldsymbol{x_i}) 
\]
つまり，分類問題の解決に\( P_T(c|\boldsymbol{x},\boldsymbol{\lambda}) \)のモデルを導入するアプローチを取る場合，
共変量シフト下での学習では，確率密度比を重みとした以下に示す
重み付き対数尤度\( L(\boldsymbol{\lambda}) \)を最大化する
パラメータ\(\boldsymbol{\lambda}\)を求める形となる．
\begin{equation}
    L(\boldsymbol{\lambda}) = \sum_{i=1}^N r(\boldsymbol{x_i}) \log P(c_i|\boldsymbol{x_i},\boldsymbol{\lambda})        
     \label{eq:2}
\end{equation}

ここではモデルとして以下の式で示される最大エントロピー法を用いる．
\begin{equation}
P_T(c|\boldsymbol{x},\boldsymbol{\lambda}) = \frac{1}{Z(\boldsymbol{x},\boldsymbol{\lambda})} \exp \left(
\sum_{j=1}^M \lambda_j f_j(\boldsymbol{x},c)
\right)
     \label{eq:3}
\end{equation}
\( \boldsymbol{x} = (x_1,x_2,\cdots,x_M) \)が入力で\( c \)がクラスである．
関数\( f_j(\boldsymbol{x},c) \)は素性関数であり，実質\( \boldsymbol{x} \)の真のクラスが
\( c \)のときに\( x_j \)を返し，そうでないとき 0 を返す関数に設定される．
\( Z(\boldsymbol{x},\boldsymbol{\lambda}) \)は正規化項であり，以下で表せる．
\begin{equation}
  Z(\boldsymbol{x},\boldsymbol{\lambda}) = \sum_{c \in C} \exp \left(
\sum_{j=1}^M \lambda_j f_j(\boldsymbol{x},c) 
\right)
     \label{eq:4}
\end{equation}
\noindent
そして\( \boldsymbol{\lambda} = (\lambda_1,\lambda_2,\cdots,\lambda_M) \)が
素性に対応する重みパラメータとなる．

共変量シフト下ではない通常のケースでは，重みパラメータは最尤法から求める．
つまり，訓練データ\( D = \{(\boldsymbol{x_i},c_i)\}_{i=1}^N \)とすると，
以下の式\(  F(\boldsymbol{\lambda}) \)を最大にする\( \boldsymbol{\lambda} \)を求める．
\[
 F(\boldsymbol{\lambda}) = \sum_{i=1}^N \log P(c_i|\boldsymbol{x_i})
\]
これを各\( \lambda_j \)で偏微分し極値問題に直すと以下が成立する．
\[
\frac{\partial F(\boldsymbol{\lambda})}{\partial \lambda_j} =
\sum_{i=1}^N f_j (\boldsymbol{x_i},c_i) - 
\sum_{i=1}^N \sum_{c \in C} P_T(c|\boldsymbol{x_i},\boldsymbol{\lambda}) f_j(\boldsymbol{x_i},c) = 0
\]
これを勾配法などで解くことにより\( \boldsymbol{\lambda} \)が求まる．

共変量シフト下の学習では\mbox{式(\ref{eq:2})}の\( L(\boldsymbol{\lambda}) \)を最大にする\( \boldsymbol{\lambda} \)を求める．上記と全く同じ手順で，
\[
\frac{\partial L(\boldsymbol{\lambda})}{\partial \lambda_j} =
\sum_{i=1}^N r(\boldsymbol{x_i}) f_j (\boldsymbol{x_i},c_i) - 
\sum_{i=1}^N \sum_{c \in C} P(c|\boldsymbol{x_i},\boldsymbol{\lambda}) r(\boldsymbol{x_i}) f_j(\boldsymbol{x_i},c) = 0
\]
が得られる．これを勾配法などで解くことにより\( \boldsymbol{\lambda} \)が求まる．

今，事例\( \boldsymbol{x_i} \)の頻度を\( h_i \)とすると，尤度は以下となる．
\[
\prod_{i=1}^N P(c_i|\boldsymbol{x_i})^{h_i}
\]
対数を取れば以下が得られる．
\[
\sum_{i=1}^N h_i \log P(c_i|\boldsymbol{x_i})
\]

この式は重み付き対数尤度の\mbox{式(\ref{eq:2})}と同じ形なので，
実際に\( \boldsymbol{\lambda} \)を求めるためには，事例\( \boldsymbol{x_i} \)の頻度\( h_i \)を\( r(\boldsymbol{x_i}) \)と考えて，
最大エントロピー法のツールなどを用いればよい
\footnote{ただし利用できるツールは頻度を実数値として与えられるものでなくてはならない．
事例の重みを頻度の拡張として実装したツールであるともいえる．
本稿で用いた機械学習ツール Classias \cite{Classias}はこの条件を満たすため利用可能である．}．


確率密度比の算出


共変量シフト下の学習では確率密度比の算出が鍵である．
直接的には\( P_S(\boldsymbol{x}) \)と\( P_T(\boldsymbol{x}) \)を推定し，その比を取ればよいが，
\( P_S(\boldsymbol{x}) \)や\( P_T(\boldsymbol{x}) \)を正確に推定することは困難であり，
その比をとれば更に誤差が大きくなると予想できる．
そのため確率密度比を直接モデル化して求める手法が活発に研究されている\cite{sugiyama-2010}．

ただし本稿では簡易な手法を利用して確率密度比を算出することにした．
本稿の目的はこのような簡易な手法による確率密度比の算出法であっても，
WSD の領域適応の有力な解法になることを示すことである．

対象単語\( w \)の用例\( \boldsymbol{x} \)の素性リストを\( \{ f_1,f_2,\cdots, f_n \} \) とする．
求めるのは領域\( R \in \{S, T\} \)上の\( \boldsymbol{x} \)の分布\( P_R (\boldsymbol{x}) \)である．
ここでは Naive Bayes で使われるモデルを用いて算出する．
Naive Bayes のモデルでは以下を仮定する．
\[
P_R (\boldsymbol{x}) = \prod_{i=1}^{n} P_R (f_i) 
\]

領域\( R \)のコーパス内の\( w \)の全ての用例について素性リストを作成しておく．
ここで用例の数を\( N(R) \)とおく．
また\( N(R) \)個の用例の中で，素性\( f \)が現れた用例数を\( n(R,f) \)とおく．
MAP 推定でスムージングを行い，\( P_R (f) \)を以下で定義する\cite{takamura}．
\[
P_R (f) = \frac{n(R,f) + 1}{N(R) + 2}
\]

以上より，ソース領域\( S \)の用例\(\boldsymbol{x}\)に対して，
確率密度比\( r(\boldsymbol{x}) = \frac{P_T (\boldsymbol{x})}{P_S (\boldsymbol{x})} \)が計算できる．
ターゲット領域\( T \)の用例\(\boldsymbol{x}\)に対しては\( r(\boldsymbol{x}) = 1 \)とする．
また\( r_x < 0.01 \)となる用例\(\boldsymbol{x}\)は訓練データから削除した
\footnote{この削除は処理の効率化のために行っている．
また本稿の実験では削除しない場合よりもわずかによい結果となっていた．}．



提案手法


「関連手法」の節で素性空間拡張法を紹介した．素性空間拡張法は
データの表現を領域適応で効果が出るように拡張する手法である．そして拡張されたデータに対しては
任意の学習手法が利用できる．つまり拡張されたデータに対して，
共変量シフト下の学習も可能である．本稿では，素性空間拡張法に
より拡張されたデータに対して，4章で説明した共変量シフト下の学習を行うことを
提案手法する．

具体的に示す．素性空間拡張法により，ソース領域の訓練データ\( \boldsymbol{x_s} \)は
\( \boldsymbol{u_s} = (\boldsymbol{x_s},\boldsymbol{x_s},\boldsymbol{0}) \)という3倍の長さのベクトルに拡張され，
ターゲット領域の訓練データ\( \boldsymbol{x_t} \)は
\( \boldsymbol{u_t} = (\boldsymbol{0},\boldsymbol{x_t},\boldsymbol{x_t}) \)という3倍の長さのベクトルに拡張される．
ここで\( \boldsymbol{u_s} \)に対しては確率密度比\( r(\boldsymbol{x_s}) = P_T(\boldsymbol{x_s})/P_S(\boldsymbol{x_s}) \)の
重みをつけ，\( \boldsymbol{u_t} \)に対しては重み 1 をつける．
また\( P_T(c|\boldsymbol{u}) \)のモデルに最大エントロピー法を用い，
重み付き対数尤度を最大化するパラメータを求めることで，\( P(c|\boldsymbol{u}) \)を推定する．

上記の重み付き対数尤度の式（目的関数）を示しておく．
今，ソース領域の訓練データを\( D_s = \{ (\boldsymbol{x_s^{(i)}},c_s^{(i)}) \}_{i = 1}^n \)，
ターゲット領域の訓練データを\( D_t = \{ (\boldsymbol{x_t^{(i)}},c_t^{(i)}) \}_{i = 1}^m \)とおく．
また\( \boldsymbol{x_s^{(i)}} \)と\( \boldsymbol{x_t^{(i)}} \)を素性空間拡張法により
拡張したデータをそれぞれ\( \boldsymbol{u_s^{(i)}} \)と\( \boldsymbol{u_t^{(i)}} \)とおく．
ここで\( \boldsymbol{x_s^{(i)}} \)と\( \boldsymbol{x_t^{(i)}} \)は\( M \)次元，
\( \boldsymbol{u_s^{(i)}} \)と\( \boldsymbol{u_t^{(i)}} \)は\( 3M \)次元のベクトルであることに注意する．
提案手法の重み付き対数尤度の式は以下となる．
\begin{gather*}
L(\boldsymbol{\lambda}) = \sum_{i=1}^n r(\boldsymbol{x_s^{(i)}}) \log P(c_s^{(i)}|\boldsymbol{u_s^{(i)}},\boldsymbol{\lambda}) 
+ \sum_{i=1}^m \log P(c_t^{(i)}|\boldsymbol{u_t^{(i)}},\boldsymbol{\lambda})  \\
P(c|\boldsymbol{u},\boldsymbol{\lambda}) = \frac{1}{Z(\boldsymbol{u},\boldsymbol{\lambda})} \exp \left(
\sum_{j=1}^{3M} \lambda_j f_j(\boldsymbol{u},c)
\right) \\
Z(\boldsymbol{u},\boldsymbol{\lambda}) = \sum_{c \in C} \exp \left(
\sum_{j=1}^{3M} \lambda_j f_j(\boldsymbol{u},c) 
\right)
\end{gather*}





