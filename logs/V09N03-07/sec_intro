近年，テキスト自動要約の研究が活発化するとともに，要約の評価方法が研究分野内の重要な検討課題の一つとして認識されてきている．
これまで提案されてきた要約の評価方法は，内的な(intrinsic)評価と外的な(extrinsic)評価の2種類に分けることができる[CITE]．
内的な評価とは，システムの出力した要約そのものを，主に内容と読みやすさの2つの側面から評価する方法である．
一方，外的な評価とは，要約を利用して人間がタスクを行う場合の，タスクの達成率が間接的に要約の評価となるという考え方に基づいて評価を行う方法である．
本研究では，近年活発にその評価方法が議論され，改良が試みられている内的な評価，特に内容に関する評価方法に焦点を当てる．
これまでの要約の内容に関する評価は，人手で作成した抜粋と要約システムの出力との一致の度合を，F-measure等の尺度を用いて測るのが典型的な方法であった．
しかし，Jingら[CITE]は，要約のF-measureによる評価と外的な評価を分析し，F-measureには「テキスト中に類似の内容を含む文が複数存在する場合，どちらの文が正解として選択されるかにより，システムの評価は大きく変化する」という問題があることを指摘している．
この問題点を解決する方法がこれまでにいくつか提案されている．
Radevら[CITE]は，文のutilityという概念を用いた評価方法を示している．
文のutilityとは，そのテキストの話題に対する各文の適合度(重要度)を10段階で表したものであり，正解の文のutilityにどのくらい近いutilityの文を選択できるかで評価を行なう．
しかし，このような適合性の評価は被験者への作業負荷が大きいという問題がある．
Donawayら[CITE]は，人間の作成した正解要約の単語頻度ベクトルとシステムの要約の単語頻度ベクトルの間のコサイン距離で評価するcontent-basedな評価を提案している．
content-basedな評価では，指定された要約率の正解要約を一つだけ用意すれば評価可能であるため，utilityに基づく評価に比べ，被験者への負荷が少ない．
しかし，この評価方法で2つの要約を比較する場合，どの程度意味があるのかについては，これまで十分な議論がなされていない．
そこで，本研究では，まず，utilityに基づく評価の問題点を改良する新しい評価方法を提案する．
一般に低い要約率の抜粋に含まれる文は高い要約率の抜粋中の文よりも重要であると考えられる．
このような考えに基づけば，あるテキストに関して複数の要約率のデータが存在する場合，テキスト中の各文に重要度を割り振ることが可能であるため，utilityに基づく評価を疑似的に実現することができる．
これまでの要約研究において，1テキストにつき複数の要約率で正解要約が作成されたデータは数多く存在する(例えば，[CITE])ことから，提案する評価方法に用いるデータの作成にかかる負荷は決して非現実的なものではなく，utilityを直接被験者が付与するより負荷は小さいと考えられる．
本研究では，評価型ワークショップNTCIR 2の要約サブタスクTSC(Text Summarization Challenge)[CITE]で作成された10%，30%，50%の3種類の要約率の正解データを用いて，提案方法により評価を行う．
この評価結果をF-measureによる結果と比較し，提案方法がF-measureによる評価を改善できることを示す．
次に，本研究では，content-basedな評価を取り上げる．
同様にTSCのデータを用いて，人間の主観評価の結果と比較し，これまで十分議論されていないその有用性に関する議論を行う．
本論文の構成は以下のとおりである．
次節では，まず，これまで提案されてきた内的な評価方法，特にF-measureの問題点の解消方法について述べる．
3節では，本研究で提案する評価方法について説明する．
4節では，F-measureと提案する評価方法を比較し，結果を報告する．
また，content-basedな評価に関する調査についても述べる．
最後に結論と今後の課題について述べる．
近年，テキスト自動要約の研究が活発化するとともに，要約の評価方法が研究分野内の重要な検討課題の一つとして認識されてきている．
これまで提案されてきた要約の評価方法は，内的な(intrinsic)評価と外的な(extrinsic)評価の2種類に分けることができる[CITE]．
内的な評価とは，システムの出力した要約そのものを，主に内容と読みやすさの2つの側面から評価する方法である．
一方，外的な評価とは，要約を利用して人間がタスクを行う場合の，タスクの達成率が間接的に要約の評価となるという考え方に基づいて評価を行う方法である．
本研究では，近年活発にその評価方法が議論され，改良が試みられている内的な評価，特に内容に関する評価方法に焦点を当てる．
これまでの要約の内容に関する評価は，人手で作成した抜粋と要約システムの出力との一致の度合を，F-measure等の尺度を用いて測るのが典型的な方法であった．
しかし，Jingら[CITE]は，要約のF-measureによる評価と外的な評価を分析し，F-measureには「テキスト中に類似の内容を含む文が複数存在する場合，どちらの文が正解として選択されるかにより，システムの評価は大きく変化する」という問題があることを指摘している．
この問題点を解決する方法がこれまでにいくつか提案されている．
Radevら[CITE]は，文のutilityという概念を用いた評価方法を示している．
文のutilityとは，そのテキストの話題に対する各文の適合度(重要度)を10段階で表したものであり，正解の文のutilityにどのくらい近いutilityの文を選択できるかで評価を行なう．
しかし，このような適合性の評価は被験者への作業負荷が大きいという問題がある．
Donawayら[CITE]は，人間の作成した正解要約の単語頻度ベクトルとシステムの要約の単語頻度ベクトルの間のコサイン距離で評価するcontent-basedな評価を提案している．
content-basedな評価では，指定された要約率の正解要約を一つだけ用意すれば評価可能であるため，utilityに基づく評価に比べ，被験者への負荷が少ない．
しかし，この評価方法で2つの要約を比較する場合，どの程度意味があるのかについては，これまで十分な議論がなされていない．
そこで，本研究では，まず，utilityに基づく評価の問題点を改良する新しい評価方法を提案する．
一般に低い要約率の抜粋に含まれる文は高い要約率の抜粋中の文よりも重要であると考えられる．
このような考えに基づけば，あるテキストに関して複数の要約率のデータが存在する場合，テキスト中の各文に重要度を割り振ることが可能であるため，utilityに基づく評価を疑似的に実現することができる．
これまでの要約研究において，1テキストにつき複数の要約率で正解要約が作成されたデータは数多く存在する(例えば，[CITE])ことから，提案する評価方法に用いるデータの作成にかかる負荷は決して非現実的なものではなく，utilityを直接被験者が付与するより負荷は小さいと考えられる．
本研究では，評価型ワークショップNTCIR 2の要約サブタスクTSC(Text Summarization Challenge)[CITE]で作成された10%，30%，50%の3種類の要約率の正解データを用いて，提案方法により評価を行う．
この評価結果をF-measureによる結果と比較し，提案方法がF-measureによる評価を改善できることを示す．
次に，本研究では，content-basedな評価を取り上げる．
同様にTSCのデータを用いて，人間の主観評価の結果と比較し，これまで十分議論されていないその有用性に関する議論を行う．
本論文の構成は以下のとおりである．
次節では，まず，これまで提案されてきた内的な評価方法，特にF-measureの問題点の解消方法について述べる．
3節では，本研究で提案する評価方法について説明する．
4節では，F-measureと提案する評価方法を比較し，結果を報告する．
また，content-basedな評価に関する調査についても述べる．
最後に結論と今後の課題について述べる．
