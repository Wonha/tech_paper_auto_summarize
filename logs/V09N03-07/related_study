要約の内容に関する評価について，Jingら[CITE]は，典型的な評価方法の1つであるF-measureをとりあげ，その問題点をいくつか指摘している．
Jingらは，システムの要約と人間の被験者の作成した抜粋との比較による評価と，要約を利用して人間がタスクを行なう場合のタスクの達成率による評価の2つの評価方法を分析し，評価結果に影響を与える要因を同定することを試みているが，その結果少なくとも次の2つの点において，これまでの人間の抜粋を用いた評価方法は問題であるとの知見を得ている．
問題点1(要約率の変化に伴う評価値の変化):
人間の抜粋との比較による評価では，要約率を変化させると，システムの評価がかなり変化する．
このため，特定の要約率でシステム間の性能の比較をする意味がどの程度あるのかは疑問が残る．
問題点2(テキスト中の複数類似個所の選択問題):
テキスト中に類似の内容を含む文が複数存在する場合，どちらの文が正解として選択されるかにより，システムの評価は大きく変化する．
これまで，問題点1(要約率の変化に伴う評価値の変化)を解消するいくつかの方法が提案されている．
Mittalら[CITE]は，要約率の違いによるシステムの評価の違いに関して，さまざまな要約率における精度を求めた上で，情報検索の評価で用いられている11点平均精度(11 point average precision)のように，複数の要約率での精度の平均として結果を示すべきであるとしている．
また，コーパスとするテキスト集合の違いが精度に影響を与えることから，コーパスの要約のしやすさを計る指標として，ランダムに文を選択して要約を作成した場合の精度をベースラインとして示すべきであると主張している．
そして，システムの性能を評価する場合，
(ここで，p，b，p'はそれぞれシステム，ベースライン，補正後のシステムの精度)のように，ベースラインを用いて補正した精度を用いるべきであるとしている．
一般に，F-measureで要約の精度を評価する場合，ベースライン値＝要約率と考えることができるため，要約率が大きくなるにつれ，F-measure値は大きくなる傾向にある．
従って，ベースラインを用いて評価値を補正する上記の評価方法は，Jingらの指摘する問題点1の解消には有用であると考えられる．
一方，被験者間の一致の度合をJとすると，Jは要約システムの精度の上限と考えられ，また，ランダムに選択した時の精度Rは下限と言える．
そのため，Radevら[CITE]も，Mittalらと同様に，システムの性能を計る値を示す際，普通に計算された値Sを単に用いるのではなく，これらの値で正規化した値
を示すべきであるとしている．
問題点2(テキスト中の複数類似個所の選択問題)を解消する方法もいくつか提案されている．
Jingら[CITE]は，人間が選択した重要文を用いて評価を行なう際，正解と一致した場合正解数1，一致しない場合0として再現率，精度を計算するのではなく，正解数を被験者間の一致の度合として計算する方法を提案している．
たとえば，5人の被験者中3人，2人がそれぞれ一致して選択した文が存在する場合，これまでの評価方法では，前者をシステムが選択した場合正解数1(過半数以上の被験者が選択しているので)，後者では0となるが，提案する方法では，システムの正解数は，前者では3/5，後者では2/5となる．
Radevら[CITE]は，文のutilityという概念を用いた評価方法を示している．
文のutilityは，文がそのテキストの話題に対してどの程度適合した内容であるかを示す尺度であり，[0-10]の値をとる．
人間が選択した重要文を用いたこれまでの評価方法は，正解と一致した場合正解数1，一致しない場合0として再現率，精度を計算していたが，utilityに基づく評価値は，システムが選択した文に対して人間が割り当てたutilityの総和を，正解の文のutilityの総和で割った値として計算する．
これまでの評価方法では，システムが選択した不正解の文は，全く評価が得られなかったのに対し，utilityに基づく評価の場合，Jingらの方法と同様に，たとえ不正解でもその文がある程度の重要度を持つ場合，その重要度に対する部分的な評価が得られる点が異なる．
ただ一つ正解が存在し，それとまさに一致することを要求されていたこれまでの評価に比べ，正解の文のutilityにどのくらい近いutilityの文を選択できるかで評価を行なう．
Donawayら[CITE]は，2種類の評価方法を提案している．
一つは，人間にも，システムにも，テキスト中の文にすべて順位をつけさせるようにして，その文の序列を比較して評価を行なう方法である．
これは，これまでの方法がテキスト中の文を重要/非重要の2つに分類して評価に利用していたのに対し，テキスト中の文数に分類して利用することに相当する．
Donawayらが提案するもう一つの評価尺度は，人間の作成した正解要約の単語頻度ベクトルとシステムの要約の単語頻度ベクトルの間のコサイン距離で評価する方法(以後，content-basedな評価)である．
Donawayらは，この2種類の評価尺度にこれまでの評価方法である再現率に基づいた評価を加え，これらを実験により比較，検討している．
正解の抜粋に含まれる個所が要約作成者毎に異なっていても，内容の類似した個所を抜き出しているのであれば，どの要約作成者の抜粋を用いても似たような評価値が得られる必要がある．
Donawayらは，4人の要約作成者の作った抜粋を用いて，上で述べたいくつかの尺度で要約を評価し，尺度毎に評価値の相関を調べている．
その結果，content-basedな評価が人間の要約との比較による評価方法としては，Jingらの指摘する問題点2に対する解決策ともなっており，もっとも優れていると結論づけている．
