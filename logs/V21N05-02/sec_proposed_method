対象単語[MATH]の語義の集合を[MATH]，また[MATH]の用例[MATH]内の[MATH]の語義を[MATH]と識別したときの損失関数を[MATH]で表す．
[MATH]は[MATH]の語義を識別する分類器である．
[MATH]をターゲット領域上の分布とすれば，本タスクにおける期待損失[MATH]は以下で表せる．
また[MATH]をソース領域上の分布とすると以下が成立する．
ここで共変量シフトの仮定から
となり，[MATH]とおくと以下が成立する．
訓練データを[MATH]とし，[MATH]を経験分布で近似すれば，
となるので，期待損失最小化の観点から考えると，共変量シフトの問題は以下の式[MATH]を最小にする[MATH]を求めればよいことがわかる．
分類器[MATH]として以下の事後確率最大化推定に基づく識別を考える．
また損失関数として対数損失[MATH]を用いれば，式([REF_eq:1])は以下となる．
つまり，分類問題の解決に[MATH]のモデルを導入するアプローチを取る場合，共変量シフト下での学習では，確率密度比を重みとした以下に示す重み付き対数尤度[MATH]を最大化するパラメータ[MATH]を求める形となる．
ここではモデルとして以下の式で示される最大エントロピー法を用いる．
[MATH]が入力，[MATH]がクラスである．
関数[MATH]は素性関数であり，実質[MATH]の真のクラスが[MATH]のときに[MATH]を返し，そうでないとき0を返す関数に設定される．
[MATH]は正規化項であり，以下で表せる．
そして[MATH]が素性に対応する重みパラメータとなる．
確率密度比[MATH]の算出法は大きく2つに分類できる．
1つは[MATH]と[MATH]を各々推定し，その比を取る手法であり，もう1つは[MATH]を直接モデル化する手法である．
ここでは前者の方法として論文[CITE]において提案された手法を利用する．
簡単化のために本論文ではこの手法をNB法と名付ける．
また後者の方法としては論文[CITE]において提案された拘束無し最小二乗重要度適合法(unconstrained Least-Squares Importance Fitting, uLSIF)を利用する．
対象単語[MATH]の用例[MATH]の素性リストを[MATH]とする．
求めるのは領域[MATH]上の[MATH]の分布[MATH]である．
ここでNaive Bayesで使われるモデルを用いる．
Naive Bayesのモデルでは以下を仮定する．
領域[MATH]のコーパス内の[MATH]の全ての用例について素性リストを作成しておく．
ここで用例の数を[MATH]とおく．
また[MATH]個の用例の中で，素性[MATH]が現れた用例数を[MATH]とおく．
MAP推定でスムージングを行い，[MATH]を以下で定義する[CITE]．
以上より，ソース領域[MATH]の用例[MATH]に対して，確率密度比[MATH]が計算できる．
ソース領域内のデータを[MATH]，ターゲット領域内のデータを[MATH]とするuLSIFでは確率密度比[MATH]を以下の式でモデル化する．
w(x) & = \sum_{l = 1}^b \alpha_l \psi_l (x)
& = \alpha\cdot\psi(x)
ただしここで，[MATH]，[MATH]である．
また[MATH]は正の実数であり，[MATH]は基底関数と呼ばれるソース領域のデータ[MATH]から正の実数値への関数である．
uLSIFでは，概略，自然数[MATH]と基底関数[MATH]を定めた後に，パラメータ[MATH]を推定する手順をとる．
説明の都合上，[MATH]と[MATH]が定まった後の[MATH]の推定を先に説明する．
[MATH]のモデルを[MATH]とおくと，パラメータ[MATH]を推定するには，[MATH]と[MATH]の平均2乗誤差[MATH]を最小にするような[MATH]を求めれば良い．
[MATH]に注意すると，[MATH]は以下のように変形できる．
J_0(\alpha) & = \frac{1}{2} \int(\hat{w}(x) - w(x))^2 P_S(x) dx
& = \frac{1}{2} \int\hat{w}(x)^2 P_S(x) dx - \int\hat{w}(x) w(x) P_S(x) dx + \frac{1}{2} \int w(x)^2 P_S(x) dx
& = \frac{1}{2} \int\hat{w}(x)^2 P_S(x) dx - \int\hat{w}(x) P_T(x) dx + \frac{1}{2} \int w(x)^2 P_S(x) dx
3項目の式は定数なので，[MATH]を最小にするには，以下の[MATH]を最小にすればよい．
[MATH]を経験分布で近似した[MATH]は以下となる．
ここで[MATH]は[MATH]の行列であり，その[MATH]行[MATH]列の要素[MATH]は以下である．
また[MATH]は[MATH]次元のベクトルであり，その[MATH]次元目の要素[MATH]は以下である．
[MATH]の最小値を求める際に正則化を行う．
このとき付加する正則化項をL2ノルムに設定し，[MATH]の条件を外して，以下の最小化問題を解く．
ここでパラメータ[MATH]が導入されることに注意する．
[MATH]は基底関数を設定する際に決められる．
この最小化問題は制約のない凸2次計画問題であるために，唯一の大域解が得られる．
その解は以下である．
最後に[MATH]の条件に合わせるように，以下の調整を行う．
パラメータ[MATH]と基底関数の設定であるが，まず，[MATH]については以下で設定する．
次にターゲット領域のデータから重複を許さずに[MATH]個の点をランダムに取り出す．
それらの点を[MATH]とおく．
そして基底関数[MATH]を以下のガウシアンカーネルで定義する．
以上より，確率密度比を求めるために残されているパラメータは正則化項の係数[MATH]とガウシアンカーネルの幅[MATH]の2つである．
これらのパラメータはグリッドサーチの交差検定で求める．
まずソース領域のデータとターゲット領域のデータをそれぞれ交わりのない[MATH]個の部分集合に分割する．
それらの部分集合の中で[MATH]番目の部分集合を除き，残りを結合した集合を作る．
それらを新たなソース領域のデータとターゲット領域のデータと見なす．
そして[MATH]と[MATH]をある値に設定し，式([REF_eq:alp-kai1])と式([REF_eq:alp-kai2])より[MATH]を求め，式([REF_jhatalpha])より[MATH]の値を求める．
[MATH]を1から[MATH]まで変化させることで，[MATH]個の[MATH]の値が求まり，それらを平均した値を[MATH]と[MATH]に対する[MATH]の値とする．
次に[MATH]と[MATH]を変化させ，上記手順で得られる[MATH]の値が最小となる[MATH]と[MATH]を求め，これを[MATH]と[MATH]の推定値とする．
WSDのタスクではNB法あるいはuLSIFで算出される確率密度比は小さい値を取る傾向があり，実際の学習で用いる際には，少し上方に修正した値を取る方が最終の識別結果が改善されることが多い．
これは以下の2点から生じていると考えられる．
[MATH]に[MATH]が入っているかは確率的であるが，[MATH]には必ず[MATH]が入っている．
[MATH]を推定するために[MATH]を用いるため，訓練データである[MATH]に過学習した結果[MATH]は[MATH]に比べて高く見積もられてしまう．
このため，求まった確率密度比を上方に修正する手法が存在する．
論文[CITE]では確率密度比[MATH]を[MATH]乗([MATH])することを提案している．
また論文[CITE]では以下で示される相対確率密度比[MATH]を確率密度比として利用することを提案している．
ここで[MATH]である．
確率密度比[MATH]が1以下である場合，[MATH]を[MATH]乗すると上方に修正できることは，それらの比の対数を取れば，[MATH]であることから明らかである．
また相対確率密度比[MATH]は以下の変形から[MATH]を上方に修正していると見なせる．
w'(x) & = \frac{P_T(x)}P_S(x) + (1-\alpha) P_T(x)
& = \frac{1}+(1-\alpha) w(x) w(x)
& > \frac{1}+(1-\alpha) w(x)
& = w(x)
確率密度比が1以上である場合，これらの手法は確率密度比を下方に修正するので，正確には確率密度比を1に近づける手法である．
しかし，ほとんどの訓練データの確率密度比は1以下であるために，ここではこれらの手法を上方修正する手法と呼び，提案手法と対比させる．
本論文では確率密度比を上方に修正するために，ソース領域のデータとターゲット領域のデータを合わせたデータを新たにソース領域のデータとみなし，NB法を用いて[MATH]を補正することを提案する．
これは[MATH]のスパース性を緩和させることを狙ったものである．
確率密度比が真の値よりも低く見積もられる原因の1つは，[MATH]が真の値よりも高く見積もられるからだと考える．
さらにその原因が[MATH]のスパース性なので，スパース性を緩和するために[MATH]にデータを追加するというアイデアである．
ただし追加するデータは[MATH]と類似の領域のデータであることが望ましい．
WSDの領域適応の場合，[MATH]と[MATH]は完全に異なることはなく，比較的似ているために，追加するデータとして[MATH]のデータが利用できると考えた．
提案手法の新たなソース領域を[MATH]で表せば，[MATH]が成立していると考えるのは自然であり，この不等式が成立していれば，提案手法により確率密度比は上方に修正される．
ただし，ここで提案手法は必ずしもNB法の確率密度比を上方に修正できるとは限らないことに注意する．
また提案手法はNB法の確率密度比が1以下かどうかには無関係であることにも注意する．
NB法の確率密度比が1以上であっても，上方に修正する可能性がある．
また[MATH]は以下の式を利用して求められる．
P_{S+T} (f) & = \frac{n(S+T,f) + 1}{N(S+T) + 2}
& = \frac{n(S,f) +n(T,f) + 1}{N(S) + N(T) + 2}
対象単語[MATH]の語義の集合を[MATH]，また[MATH]の用例[MATH]内の[MATH]の語義を[MATH]と識別したときの損失関数を[MATH]で表す．
[MATH]は[MATH]の語義を識別する分類器である．
[MATH]をターゲット領域上の分布とすれば，本タスクにおける期待損失[MATH]は以下で表せる．
また[MATH]をソース領域上の分布とすると以下が成立する．
ここで共変量シフトの仮定から
となり，[MATH]とおくと以下が成立する．
訓練データを[MATH]とし，[MATH]を経験分布で近似すれば，
となるので，期待損失最小化の観点から考えると，共変量シフトの問題は以下の式[MATH]を最小にする[MATH]を求めればよいことがわかる．
分類器[MATH]として以下の事後確率最大化推定に基づく識別を考える．
また損失関数として対数損失[MATH]を用いれば，式([REF_eq:1])は以下となる．
つまり，分類問題の解決に[MATH]のモデルを導入するアプローチを取る場合，共変量シフト下での学習では，確率密度比を重みとした以下に示す重み付き対数尤度[MATH]を最大化するパラメータ[MATH]を求める形となる．
ここではモデルとして以下の式で示される最大エントロピー法を用いる．
[MATH]が入力，[MATH]がクラスである．
関数[MATH]は素性関数であり，実質[MATH]の真のクラスが[MATH]のときに[MATH]を返し，そうでないとき0を返す関数に設定される．
[MATH]は正規化項であり，以下で表せる．
そして[MATH]が素性に対応する重みパラメータとなる．
確率密度比[MATH]の算出法は大きく2つに分類できる．
1つは[MATH]と[MATH]を各々推定し，その比を取る手法であり，もう1つは[MATH]を直接モデル化する手法である．
ここでは前者の方法として論文[CITE]において提案された手法を利用する．
簡単化のために本論文ではこの手法をNB法と名付ける．
また後者の方法としては論文[CITE]において提案された拘束無し最小二乗重要度適合法(unconstrained Least-Squares Importance Fitting, uLSIF)を利用する．
対象単語[MATH]の用例[MATH]の素性リストを[MATH]とする．
求めるのは領域[MATH]上の[MATH]の分布[MATH]である．
ここでNaive Bayesで使われるモデルを用いる．
Naive Bayesのモデルでは以下を仮定する．
領域[MATH]のコーパス内の[MATH]の全ての用例について素性リストを作成しておく．
ここで用例の数を[MATH]とおく．
また[MATH]個の用例の中で，素性[MATH]が現れた用例数を[MATH]とおく．
MAP推定でスムージングを行い，[MATH]を以下で定義する[CITE]．
以上より，ソース領域[MATH]の用例[MATH]に対して，確率密度比[MATH]が計算できる．
ソース領域内のデータを[MATH]，ターゲット領域内のデータを[MATH]とするuLSIFでは確率密度比[MATH]を以下の式でモデル化する．
w(x) & = \sum_{l = 1}^b \alpha_l \psi_l (x)
& = \alpha\cdot\psi(x)
ただしここで，[MATH]，[MATH]である．
また[MATH]は正の実数であり，[MATH]は基底関数と呼ばれるソース領域のデータ[MATH]から正の実数値への関数である．
uLSIFでは，概略，自然数[MATH]と基底関数[MATH]を定めた後に，パラメータ[MATH]を推定する手順をとる．
説明の都合上，[MATH]と[MATH]が定まった後の[MATH]の推定を先に説明する．
[MATH]のモデルを[MATH]とおくと，パラメータ[MATH]を推定するには，[MATH]と[MATH]の平均2乗誤差[MATH]を最小にするような[MATH]を求めれば良い．
[MATH]に注意すると，[MATH]は以下のように変形できる．
J_0(\alpha) & = \frac{1}{2} \int(\hat{w}(x) - w(x))^2 P_S(x) dx
& = \frac{1}{2} \int\hat{w}(x)^2 P_S(x) dx - \int\hat{w}(x) w(x) P_S(x) dx + \frac{1}{2} \int w(x)^2 P_S(x) dx
& = \frac{1}{2} \int\hat{w}(x)^2 P_S(x) dx - \int\hat{w}(x) P_T(x) dx + \frac{1}{2} \int w(x)^2 P_S(x) dx
3項目の式は定数なので，[MATH]を最小にするには，以下の[MATH]を最小にすればよい．
[MATH]を経験分布で近似した[MATH]は以下となる．
ここで[MATH]は[MATH]の行列であり，その[MATH]行[MATH]列の要素[MATH]は以下である．
また[MATH]は[MATH]次元のベクトルであり，その[MATH]次元目の要素[MATH]は以下である．
[MATH]の最小値を求める際に正則化を行う．
このとき付加する正則化項をL2ノルムに設定し，[MATH]の条件を外して，以下の最小化問題を解く．
ここでパラメータ[MATH]が導入されることに注意する．
[MATH]は基底関数を設定する際に決められる．
この最小化問題は制約のない凸2次計画問題であるために，唯一の大域解が得られる．
その解は以下である．
最後に[MATH]の条件に合わせるように，以下の調整を行う．
パラメータ[MATH]と基底関数の設定であるが，まず，[MATH]については以下で設定する．
次にターゲット領域のデータから重複を許さずに[MATH]個の点をランダムに取り出す．
それらの点を[MATH]とおく．
そして基底関数[MATH]を以下のガウシアンカーネルで定義する．
以上より，確率密度比を求めるために残されているパラメータは正則化項の係数[MATH]とガウシアンカーネルの幅[MATH]の2つである．
これらのパラメータはグリッドサーチの交差検定で求める．
まずソース領域のデータとターゲット領域のデータをそれぞれ交わりのない[MATH]個の部分集合に分割する．
それらの部分集合の中で[MATH]番目の部分集合を除き，残りを結合した集合を作る．
それらを新たなソース領域のデータとターゲット領域のデータと見なす．
そして[MATH]と[MATH]をある値に設定し，式([REF_eq:alp-kai1])と式([REF_eq:alp-kai2])より[MATH]を求め，式([REF_jhatalpha])より[MATH]の値を求める．
[MATH]を1から[MATH]まで変化させることで，[MATH]個の[MATH]の値が求まり，それらを平均した値を[MATH]と[MATH]に対する[MATH]の値とする．
次に[MATH]と[MATH]を変化させ，上記手順で得られる[MATH]の値が最小となる[MATH]と[MATH]を求め，これを[MATH]と[MATH]の推定値とする．
WSDのタスクではNB法あるいはuLSIFで算出される確率密度比は小さい値を取る傾向があり，実際の学習で用いる際には，少し上方に修正した値を取る方が最終の識別結果が改善されることが多い．
これは以下の2点から生じていると考えられる．
[MATH]に[MATH]が入っているかは確率的であるが，[MATH]には必ず[MATH]が入っている．
[MATH]を推定するために[MATH]を用いるため，訓練データである[MATH]に過学習した結果[MATH]は[MATH]に比べて高く見積もられてしまう．
このため，求まった確率密度比を上方に修正する手法が存在する．
論文[CITE]では確率密度比[MATH]を[MATH]乗([MATH])することを提案している．
また論文[CITE]では以下で示される相対確率密度比[MATH]を確率密度比として利用することを提案している．
ここで[MATH]である．
確率密度比[MATH]が1以下である場合，[MATH]を[MATH]乗すると上方に修正できることは，それらの比の対数を取れば，[MATH]であることから明らかである．
また相対確率密度比[MATH]は以下の変形から[MATH]を上方に修正していると見なせる．
w'(x) & = \frac{P_T(x)}P_S(x) + (1-\alpha) P_T(x)
& = \frac{1}+(1-\alpha) w(x) w(x)
& > \frac{1}+(1-\alpha) w(x)
& = w(x)
確率密度比が1以上である場合，これらの手法は確率密度比を下方に修正するので，正確には確率密度比を1に近づける手法である．
しかし，ほとんどの訓練データの確率密度比は1以下であるために，ここではこれらの手法を上方修正する手法と呼び，提案手法と対比させる．
本論文では確率密度比を上方に修正するために，ソース領域のデータとターゲット領域のデータを合わせたデータを新たにソース領域のデータとみなし，NB法を用いて[MATH]を補正することを提案する．
これは[MATH]のスパース性を緩和させることを狙ったものである．
確率密度比が真の値よりも低く見積もられる原因の1つは，[MATH]が真の値よりも高く見積もられるからだと考える．
さらにその原因が[MATH]のスパース性なので，スパース性を緩和するために[MATH]にデータを追加するというアイデアである．
ただし追加するデータは[MATH]と類似の領域のデータであることが望ましい．
WSDの領域適応の場合，[MATH]と[MATH]は完全に異なることはなく，比較的似ているために，追加するデータとして[MATH]のデータが利用できると考えた．
提案手法の新たなソース領域を[MATH]で表せば，[MATH]が成立していると考えるのは自然であり，この不等式が成立していれば，提案手法により確率密度比は上方に修正される．
ただし，ここで提案手法は必ずしもNB法の確率密度比を上方に修正できるとは限らないことに注意する．
また提案手法はNB法の確率密度比が1以下かどうかには無関係であることにも注意する．
NB法の確率密度比が1以上であっても，上方に修正する可能性がある．
また[MATH]は以下の式を利用して求められる．
P_{S+T} (f) & = \frac{n(S+T,f) + 1}{N(S+T) + 2}
& = \frac{n(S,f) +n(T,f) + 1}{N(S) + N(T) + 2}
