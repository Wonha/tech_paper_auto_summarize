階層型複数ラベル文書分類では，与えられた文書に対して，それをもっともよく表すラベルの集合[MATH]を返す．
ここで，[MATH]はあらかじめ定義されたラベルの集合である．
[MATH]は図[REF_fig:tree]のように木構造で組織化されているとする．
また，付与対象のラベルは葉のみであり，内部ノードはラベルとならないとする．
図[REF_fig:tree]の場合，[MATH]，[MATH]，[MATH]および[MATH]がラベル候補となる．
いくつかの記法を整理しておく．
[MATH]は，[MATH]の子孫である葉の集合を返す．
例えば，[MATH]．
ただし，[MATH]自身が葉の場合は，[MATH]．
[MATH]は親[MATH]から子[MATH]への辺を表す．
[MATH]は[MATH]と[MATH]を結ぶ辺の集合を返す．
例えば，[MATH]．
また，[MATH]とする．
これは[MATH]を被覆する最小の部分木に対応する．
例えば，[MATH]．
文書[MATH]は[MATH]により特徴量ベクトルに変換される．
特徴量として，例えば，文書分類タスクで一般な単語かばん(bag-of-words)手法を用いることができる．
本タスクは教師あり設定であり，訓練データ[MATH]が与えられる．
[MATH]を用いてモデルを訓練し，これとは別のテストデータによって性能を評価する．

ラベル間依存を利用するための準備として，入力文書[MATH]に対して出力ラベル集合[MATH]を同時に推定する大域モデルを提案する．
具体的には，階層的複数ラベル文書分類を構造推定問題とみなし，[MATH]が作る部分木に対してスコアを定義する．
[MATH]は重みベクトルであり，訓練データを用いて学習すべきパラメータである．
[MATH]は，辺に対応する局所的な重みベクトルを連結することにより構成される．
例えば，図[REF_fig:tree]の場合は
となる．
特徴関数[MATH]は，文書[MATH]と[MATH]を入力とし，[MATH]と同次元のベクトルを返す．
具体的には，各[MATH]に対応する部分ベクトルに[MATH]を，残りの要素に[MATH]を入れた特徴量ベクトルを返す．
したがって，[MATH]は以下のように書き換えられる．
この定式化により，[MATH]が与えられた時，部分木のスコアを最大化する[MATH]を探す問題となる．
[t]   \DeclarePairedDelimiter\norm   \algsetup{indent=1.2em}
[1] \REQUIRE文書[MATH],木のノード[MATH] \ENSUREラベル集合[MATH],スコア[MATH] \STATE[MATH] \FORALL{[MATH]の各子[MATH] } \IF{[MATH]が葉} \STATE[MATH] \ELSE\STATE[MATH] \STATE[MATH] \ENDIF\ENDFOR\STATE[MATH] \IF{[MATH]が空} \STATE[MATH]ただし，[MATH]は[MATH]のなかで[MATH]が最大のもの\ENDIF\STATE[MATH] \STATE[MATH] \RETURN[MATH]
大域モデルの，現在のパラメータ[MATH]のもとでの厳密解は，動的計画法により効率的に求められる．
Algorithm [REF_alg:bu-search]に動的計画法の擬似コードを示す．
[MATH]は，[MATH]を根とする部分木の集合から，スコアが最大のものを再帰的に探索する．
したがって，我々が呼び出すのは[MATH]である．
子[MATH]は，(1) [MATH]を根とするスコア最大の部分木を作るラベル集合，および(2)そのスコアとひも付けされている．
ただし，葉のスコアは0である．
[MATH]から見た[MATH]のスコアは，[MATH]の部分木のスコアと辺[MATH]のスコアの和である（3--8行目）．
[MATH]の部分木のスコアを最大にするには，正のスコアを持つ[MATH]をすべて採用すればよい（10行目）．
いずれの子も正のスコアを持たない場合は，最大のスコアを持つ子を1個採用する（11--13行目）．
採用された子の集合により，[MATH]のラベル集合とスコアが決定される（14--15行目）．
このアルゴリズムの拡張としては，上位[MATH]個の候補集合を出すというものが考えられる．
木に対する動的計画法としては，構文解析[CITE]よりもはるかに簡単なため，上位[MATH]個への拡張[CITE]もさほど難しくない．
以上の準備により，ラベル間依存を利用する条件が整った．
ラベル間依存の捕捉は，大域モデルに対する特徴量の追加により実現される．
具体的には，あるノードがいくつの子を採用しやすいかを制御する枝分かれ特徴量を導入する．
枝分かれ特徴量は[MATH]により表される．
ここで[MATH]は根あるいは内部ノードであり，[MATH]は[MATH]が採用する子の数である．
ただし，あらゆる[MATH]の値に対して特徴量を設けると疎になるため，ある[MATH]について，[MATH]個([MATH]もしくは[MATH])の特徴量に限定する．
さらに，ノードごとの特徴量だけでなく，すべての根あるいは内部のノードが共有する[MATH]個の特徴量も設ける．
つまり，追加される特徴量は[MATH]個であり，各ノードに対して2個の特徴量が発火する．
ここで，[MATH]はラベル階層における根および内部ノードの個数とする．
[t]   \DeclarePairedDelimiter\norm   \algsetup{indent=1.2em}
[1]  \STATE[MATH]を[MATH]により降順にソートした配列\STATE[MATH],　[MATH],　[MATH] \FOR{[MATH]r[MATH]} \STATE[MATH] \STATE[MATH],　[MATH] \STATE[MATH] \ENDFOR\STATE[MATH] [MATH]のなかでスコア[MATH]が最大の要素
この枝分かれ特徴量は，動的計画法による厳密解探索が維持できるように設計されている．
この特徴量を組み込むには，Algorithm [REF_alg:bu-search]の10--15行目をAlgorithm [REF_alg:bu-branch]で置き換えればよい．
枝分かれ特徴量のスコア[MATH]は[MATH]のみに依存する．
そこで，まずは採用する子の数[MATH]によって候補をグループ分けし，各グループのなかでスコアが最大の候補を選ぶ（12--16行目）．
最後に，異なるグループ同士を比較し，スコアが最大となる候補を採用する（17行目）．
グループ内でスコアが最大の候補を選ぶには，子をスコア順に並べ，上位[MATH]個を採用すれば良い．
候補のスコアは，[MATH]から見た各子のスコアと枝分かれ特徴量のスコアの和となる（15行目）．
枝分かれ特徴量の導入により，ラベルの採否の判断が，ラベル同士の相対的な比較によって行われるようになる．
[REF_sec:introduction]節で触れた，「林業政策」と「環境問題」というラベルが付与された文書を再び例に挙げる．
この文書に対して「林業一般」というラベルはそれほど不適切には見えないが，枝分かれ特徴量を持たないモデルは，「林業一般」を付与しない理由を，[MATH]に対応する重みですべて説明しなければならない．
[REF_sec:discussion]節で示すように，枝分かれ特徴量の重みは，一般に，負の値を持ち，ペナルティとして働く．
また，子の数が増えるにつれてペナルティが増えるように学習される．
したがって，子を2個採用するとよりペナルティがかかるので，「林業一般」に対応する重みを無理に引き下げることなく，相対的により適切な「林業政策」のみを採用することが可能となる．
大域モデルの訓練手法をここでは大域訓練と呼ぶ．
本稿では，パーセプトロン系のオンライン学習アルゴリズムを採用する．
具体的には，構造推定問題に対するPassive-Aggressiveアルゴリズム[CITE]を用いる．
Passive-Aggressiveを採用した理由としては，実装の簡便さ，バッチ学習と異なり，大量の訓練データに容易に対応可能なオンライン学習であること，次節で述べるように並列分散化が容易に実現できることが挙げられる．
ただし，これは提案手法がパーセプトロン系アルゴリズムでしか実現できないことを意味せず，構造化SVM [CITE]を含む他の構造学習アルゴリズムの導入も検討に値する．
[t]   \DeclarePairedDelimiter\norm   \algsetup{indent=1.2em}
[1] \REQUIRE訓練データ[MATH] \ENSURE重みベクトル[MATH] \STATE[MATH] \FOR{[MATH]} \STATE[MATH]をシャッフル\FORALL{[MATH]} \STATE[MATH] \STATE[MATH] \IF{[MATH]} \STATE[MATH] \STATE[MATH] \STATE[MATH] \ENDIF\ENDFOR\ENDFOR
大域モデルの場合の擬似コードをAlgorithm [REF_alg:pa-global]に示す．
ここで，[MATH]は訓練の反復数を表し，パラメータ[MATH]は[MATH]とする．
現在のパラメータにおける厳密解は上述の動的計画法により求まる（5行目）．
予測を誤った場合，正解ラベル集合を出力する方向に重みを更新する（10行目）．
ここで，コスト[MATH]はモデル予測の誤り度合いを表し，重みの更新幅を変化させる．
[MATH]は，正解ラベル集合とシステムの出力の一致の度合いに基づいている．
大域訓練には学習が非常に遅いという欠点がある．
ラベル集合の分類はラベル1個の2値分類とは比較にならないほど遅い．
しかも，大域訓練はモデルを一枚岩とするため，モデルを局所分類器に分割して並列化することができない．
そこで，繰り返しパラメータ混ぜ合わせ法[CITE]を用いて並列分散化を行う．
基本的な考えは，モデルを分割する代わりに，訓練データを分割することで並列化を行うというものである．
別々の訓練データ断片から学習されたモデル群を繰り返し混ぜ合わせることで収束性を保証している．
Algorithm [REF_alg:ipm]に繰り返しパラメータ混ぜ合わせ法の擬似コードを示す．
ここで[MATH]は繰り返しパラメータ混ぜ合わせ法の反復数，[MATH]は訓練データの分割数を表す．
繰り返しパラメータ混ぜ合わせ法では，断片ごとに並列に訓練を行う．
各反復の最後に，並列に訓練された複数のモデルを平均化する．
次の反復では，この平均化されたモデルを初期値として用いる．
繰り返しパラメータ混ぜ合わせ法はパーセプトロン向けに提案されたものである．
しかし，[CITE]が言及している通り，Passive-Aggressiveアルゴリズムに対しても収束性を証明することができる．
[t]   \DeclarePairedDelimiter\norm   \algsetup{indent=1.2em}
[1] \REQUIRE訓練データ[MATH] \ENSURE重みベクトル[MATH] \STATE[MATH]を[MATH]に分割\STATE[MATH] \FOR{[MATH]} \FOR{[MATH]} \STATE[MATH]非同期的にAlgorithm [REF_alg:pa-global]を呼び出す．
ただしいくつかの修正を加える．
[MATH]を[MATH]で置き換える．
[MATH]を[MATH]ではなく[MATH]で初期化する．
反復数を[MATH]とする．
\ENDFOR\STATE非同期処理の終了を待つ\STATE[MATH] \ENDFOR
