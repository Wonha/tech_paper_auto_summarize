自己組織化マップ(self-organizing map; SOM)[CITE]は，教師なし競合学習により，高次元データを低次元データに写像する2階層型のニューラルネットワークである．
自己組織化マップでは，高次元空間での近傍関係をできる限り保ちつつ，低次元空間へデータを配置するという位相的整列性と呼ばれる特徴を持っている．
自己組織化マップの典型的な適用例は，多次元データの可視化であり，この場合には高次元データを2次元平面上に配置するということを行なう[CITE]．
図[REF_Fig:SOM]は，[MATH]次元の入力データを2次元平面上に配置する自己組織化ネットワークの例を示している．
ネットワークの入力層は，2次元平面上に格子状に配置されたすべてのユニットと結合されており，各ユニットには，入力層に入力されるデータと同じ次元数の参照ベクトル(reference vector)が対応している．
学習の過程では，入力層に入力されたベクトルと最も近い参照ベクトルを持つユニットを探し，このユニットとその近傍にあるユニットの参照ベクトルを入力ベクトルに近づけるという操作を繰り返す．
このようにして，同じような位相的特徴を持ったユニットが近傍領域に集まり，結果的に入力データの位相的特徴を反映した自己組織化マップが作られることになる．
自己組織化マップの学習アルゴリズムをまとめると，以下のようになる．
参照ベクトル[MATH]をランダムな値で初期化する．
入力ベクトル[MATH]に最も近い参照ベクトル[MATH]を持つユニット[MATH]を見つける．
ユニット[MATH]および[MATH]の近傍領域の参照ベクトル[MATH]を次式により更新する．
ここで，[MATH]はユニット[MATH]から離れるにつれ，小さな値になるように設定する．
また，[MATH]は学習が進むにつれ，単調に減少するようにする．
ステップ2より繰り返す．
上で述べたように，自己組織化マップでは，高次元空間での近傍関係をできる限り保ちつつ，入力データを低次元空間へ配置することができるという特徴を持っている．
この特徴を用いると，高次元空間での最近傍検索を低次元空間での最近傍検索問題に置き換えることができると考えられる．
しかし，自己組織化マップの学習には誤差がともなううえ，低次元のマップ上では，高次元空間での距離が保存されていないため，低次元マップだけを用いて最近傍検索を行なうことは不可能である．
我々は，自己組織化マップにより得られた低次元空間での近傍関係から，最近傍検索の探索範囲を限定し，限定されたデータに関してだけ，元の高次元空間上で距離を計算するという方法を考えた．
また，探索範囲の限定を効率的に行なうことができるように，1次元の自己組織化マップを用いることにした．
以下に，1次元自己組織化マップを用いた最近傍検索手法をまとめる(図[REF_Fig:SOMIndexing]参照)．
多次元インデキシングの作成
自己組織化マップの学習アルゴリズムにより，多次元データを1次元上に配置する．
ユニット数を[MATH]とすると，データは[MATH]個のクラスタに分割されることになる．
各クラスタに属するデータを，2次記憶上の連続した領域に格納する．
また，この際，1次元マップ上の各ユニットに2次記憶領域へのポインタを持たせる．
なお，2次記憶領域には，元の多次元データを格納する．
最近傍検索
与えられた検索質問ベクトルに最も近い参照ベクトルを持つユニット[MATH]を見つける．
ユニット[MATH]の近傍ユニットに配置されたデータに対してのみ，検索質問との距離計算を行なう．
距離計算の際には，2次記憶上に格納されている多次元データを用いる．
上記で計算された結果を，距離の小さい順にソートし，これを最近傍検索の結果として出力する．
検索質問ベクトルと距離計算の行なわれるデータは，2次記憶上の連続した領域に格納しているため，2次記憶へのアクセスはきわめて効率的に行なうことが可能である．
3節で実験結果を述べるが，1次元マップ上の各ユニットに割り当てられるデータ数が大きく偏ることはなく，概ね平均化している．
したがって，2次記憶へのアクセス回数は数回程度である．
なお上記では，1次元の自己組織化マップを用いたが，2次元の自己組織化マップを用いることも可能である．
ただし，2次元自己組織化マップを用いる場合には，近傍ユニットに属するデータを必ずしも2次記憶上の連続した領域に格納できるとは限らないため，最近傍検索の手続きが多少複雑になる．
上記で提案した多次元インデキシング手法の本質は，多次元データのクラスタリングに1次元自己組織化マップを用いている点であり，クラスタ(あるいは近傍クラスタ)内の検索は基本的に線形探索によって行われている．
自己組織化マップ以外にも，他のクラスタリング手法を用いて同様のインデキシングを行うことも考えられる．
たとえば，主成分分析を用いて第1主成分により1次元上にデータをマッピングすることもできるが，主成分分析はデータの分布が正規分布に近い場合には有効であるが，そうでない場合には自己組織化マップを用いたほうが近似の精度が高いという利点がある．
また，ベクトル量子化は，ユニット中のセントロイド(コードブック)により入力データを近似するという点で自己組織化マップに類似しているが，ベクトル量子化では高次元空間での近傍関係を保つという位相的整列性を特別に考慮していない．
提案した最近傍検索手法では，検索の際に近傍ユニットを探索することから，位相的整列性を備えた自己組織化マップを用いた手法のほうが良いと考えられる．
自己組織化マップ(self-organizing map; SOM)[CITE]は，教師なし競合学習により，高次元データを低次元データに写像する2階層型のニューラルネットワークである．
自己組織化マップでは，高次元空間での近傍関係をできる限り保ちつつ，低次元空間へデータを配置するという位相的整列性と呼ばれる特徴を持っている．
自己組織化マップの典型的な適用例は，多次元データの可視化であり，この場合には高次元データを2次元平面上に配置するということを行なう[CITE]．
図[REF_Fig:SOM]は，[MATH]次元の入力データを2次元平面上に配置する自己組織化ネットワークの例を示している．
ネットワークの入力層は，2次元平面上に格子状に配置されたすべてのユニットと結合されており，各ユニットには，入力層に入力されるデータと同じ次元数の参照ベクトル(reference vector)が対応している．
学習の過程では，入力層に入力されたベクトルと最も近い参照ベクトルを持つユニットを探し，このユニットとその近傍にあるユニットの参照ベクトルを入力ベクトルに近づけるという操作を繰り返す．
このようにして，同じような位相的特徴を持ったユニットが近傍領域に集まり，結果的に入力データの位相的特徴を反映した自己組織化マップが作られることになる．
自己組織化マップの学習アルゴリズムをまとめると，以下のようになる．
参照ベクトル[MATH]をランダムな値で初期化する．
入力ベクトル[MATH]に最も近い参照ベクトル[MATH]を持つユニット[MATH]を見つける．
ユニット[MATH]および[MATH]の近傍領域の参照ベクトル[MATH]を次式により更新する．
ここで，[MATH]はユニット[MATH]から離れるにつれ，小さな値になるように設定する．
また，[MATH]は学習が進むにつれ，単調に減少するようにする．
ステップ2より繰り返す．
上で述べたように，自己組織化マップでは，高次元空間での近傍関係をできる限り保ちつつ，入力データを低次元空間へ配置することができるという特徴を持っている．
この特徴を用いると，高次元空間での最近傍検索を低次元空間での最近傍検索問題に置き換えることができると考えられる．
しかし，自己組織化マップの学習には誤差がともなううえ，低次元のマップ上では，高次元空間での距離が保存されていないため，低次元マップだけを用いて最近傍検索を行なうことは不可能である．
我々は，自己組織化マップにより得られた低次元空間での近傍関係から，最近傍検索の探索範囲を限定し，限定されたデータに関してだけ，元の高次元空間上で距離を計算するという方法を考えた．
また，探索範囲の限定を効率的に行なうことができるように，1次元の自己組織化マップを用いることにした．
以下に，1次元自己組織化マップを用いた最近傍検索手法をまとめる(図[REF_Fig:SOMIndexing]参照)．
多次元インデキシングの作成
自己組織化マップの学習アルゴリズムにより，多次元データを1次元上に配置する．
ユニット数を[MATH]とすると，データは[MATH]個のクラスタに分割されることになる．
各クラスタに属するデータを，2次記憶上の連続した領域に格納する．
また，この際，1次元マップ上の各ユニットに2次記憶領域へのポインタを持たせる．
なお，2次記憶領域には，元の多次元データを格納する．
最近傍検索
与えられた検索質問ベクトルに最も近い参照ベクトルを持つユニット[MATH]を見つける．
ユニット[MATH]の近傍ユニットに配置されたデータに対してのみ，検索質問との距離計算を行なう．
距離計算の際には，2次記憶上に格納されている多次元データを用いる．
上記で計算された結果を，距離の小さい順にソートし，これを最近傍検索の結果として出力する．
検索質問ベクトルと距離計算の行なわれるデータは，2次記憶上の連続した領域に格納しているため，2次記憶へのアクセスはきわめて効率的に行なうことが可能である．
3節で実験結果を述べるが，1次元マップ上の各ユニットに割り当てられるデータ数が大きく偏ることはなく，概ね平均化している．
したがって，2次記憶へのアクセス回数は数回程度である．
なお上記では，1次元の自己組織化マップを用いたが，2次元の自己組織化マップを用いることも可能である．
ただし，2次元自己組織化マップを用いる場合には，近傍ユニットに属するデータを必ずしも2次記憶上の連続した領域に格納できるとは限らないため，最近傍検索の手続きが多少複雑になる．
上記で提案した多次元インデキシング手法の本質は，多次元データのクラスタリングに1次元自己組織化マップを用いている点であり，クラスタ(あるいは近傍クラスタ)内の検索は基本的に線形探索によって行われている．
自己組織化マップ以外にも，他のクラスタリング手法を用いて同様のインデキシングを行うことも考えられる．
たとえば，主成分分析を用いて第1主成分により1次元上にデータをマッピングすることもできるが，主成分分析はデータの分布が正規分布に近い場合には有効であるが，そうでない場合には自己組織化マップを用いたほうが近似の精度が高いという利点がある．
また，ベクトル量子化は，ユニット中のセントロイド(コードブック)により入力データを近似するという点で自己組織化マップに類似しているが，ベクトル量子化では高次元空間での近傍関係を保つという位相的整列性を特別に考慮していない．
提案した最近傍検索手法では，検索の際に近傍ユニットを探索することから，位相的整列性を備えた自己組織化マップを用いた手法のほうが良いと考えられる．
