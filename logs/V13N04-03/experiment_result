自動獲得された属性語の良さを評価するには，何らかの定まった評価手順が必要になる．
本研究では，我々が質問回答可能性と呼ぶ基準に基づいた評価手順を提案する．
質問回答可能性とは，ある語に関して，その値を問う質問文を生成でき，答えが存在する，ということである．
我々は，質問回答可能性が成り立つならばその語は属性語であるという仮定をした．
これは，我々の属性の利用方法（QAや要約）を考えると，直感的に妥当な仮定だと思われる．
例えば，「車」について考えると，「この車の製造会社はどこか？」といった質問文が可能であり，誰かが「A社」と答えることができる（質問回答可能性が成り立つ）ので，「製造会社」は属性語と考えても良いということになる．
ある人が「車」について知りたいとき「製造会社」が何であるかは重要な情報であるので，これは妥当であろう．
提案する評価手順では，評価者は最大で4つのYES-NO質問に答えることで判定を行う．
これら4つの質問とは，評価者に提示される順に，下位語テスト（節[REF_sec:hyp-test]），QAテスト（節[REF_sec:qa-test]），接尾拡張QAテスト（節[REF_sec:qa-suffix-test]），一般性テスト（節[REF_sec:gen-test]），である．
QAテスト・接尾拡張QAテストの2つが，質問回答可能性を直接用いたテストである．
下位語テスト・一般性テストは，QAテストを補強するためのものである．
評価手順の全体の流れは，図[REF_fig:eval-proc]に示すようになる．
以下の節では，各テストの詳細を説明する．
実際の順番とは異なるが，まず，本研究の提案の中心的な判定手順である質問回答可能性テスト（QAテスト）から説明する．
我々は，前述した質問回答可能性基準に従って，図[REF_fig:qa-test]に示すQAテストを設計した．
このQAテストについて，いくつか注意点がある．
第一に，属性語に対する値は実際にはクラスのインスタンスに対して定義されるため，このQAテストでは，[MATH]を「この」で限定することで（ある）インスタンスを指すようにして質問が不自然になるのを防いでいる．
また，質問文中で[MATH]の後にスペースを入れることで評価者が適切な係り受け（「この[MATH]」が[MATH]に係る）を想像できるようにした．
第二に，[MATH]に対する適切な質問の種類をあらかじめ（自動で）決めることは難しいので，テストでは図[REF_fig:qa-test]に挙げたように考えられる全ての種類の質問を生成し，そのいずれかが許容できるかを判定するようにした．
第三に，質問は文法的に正しいだけでなく「常識的に自然」でなければならない．
本研究の実験では，この「自然」さを「その質問が通常の会話の第一発話としてあり得るか」で判断するようにした．
我々の考えでは，属性語はインスタンスを述べる際に重要なものでなければならない．
そこで，この「自然」さを満たす属性がそのような重要な属性語であるという仮定を置いた．
例えば，おそらく全ての「会社」は「机」を所有しているが，我々の考えでは「机」は「会社」の属性語ではない．
「この会社の机は何ですか？」といった質問は文法的に正しいけれども，「会社」についての通常の会話の第一発話としては不自然であるから，この「自然」さのチェックによって「机」が属性語になるのを防ぐことができる．
重要な点は，Woodsの言語テスト[CITE]（「the [MATH] of [MATH] is [MATH]」が可能か）だけでは「the desk of (= used in) Com-X is Desk-Y」のように言えてしまうので，「机」を棄却することができないことである．
また，質問は質問者が重要であると考えることについてするのであるから，質問文を判定に用いることでより重要さを重視することができる．
最後に，質問への答えは必ずしも言語で表現できなくてもよい．
例えば，「地図」「姿」「設計図」などに対する値は言語では表現できず，他の手段で表現されるが，これらも重要な属性語であることは明らかである．
Webページには言語以外の表現（画像，音声など）を含めることができるため，それらを値とする属性への言及も増えると考えられる．
そのため，Webから属性語を獲得する場合，そのような属性語も獲得される可能性が大きい．
従って，質問への答えが言語に限らないことをあらかじめ評価者に明確にしておいた．
獲得された属性語のいくつかは，正しいと思われるにもかかわらず前節のQAテストで棄却されてしまうことがある．
大きな理由の一つは，獲得された属性語が，実際に意味している属性の標準的な属性語とは異なる文字列として獲得される場合があることである．
これは，日本語が省略的であること，我々の獲得手法が実際にコーパスに現れた表層形を処理して獲得すること，また，1単語の属性語しか獲得しないことなどに起因している．
例えば，下の例文中で「生徒」は属性「生徒数」の意味で用いられている．
（a）この学校の生徒は500人です．
このような文を手がかりにすると，提案手法では「生徒」が「学校」の属性語として獲得される可能性が高い．
一部が省略されている属性語でも，実際に（a）のような文で使用されることから応用の面で有用であると考えられるので，正しいと判定されるのが我々の立場では好ましい．
ところが，省略があると前節のQAテストで棄却されてしまうことがある．
例えば，上の「生徒」を判定する場合，これが「生徒数」を意味しているときには, QAテストの質問の中では「この学校の生徒はいくつ？」が一番適当であるが，これは，人数に「いくつ」は使えないため文法的ではない．
そのため，「生徒」は棄却されることになる．
日本語では，省略された部分のほとんどは属性語の後に適切な接尾辞を付加するか，「の＋名詞化形容詞・形容動詞」を付加することで復元することができる．
例えば，上の例では接尾辞「数」を付ければ良い．
そこで，最初のQAテストで棄却された場合には，適切な付加を行って属性語を拡張した上で，それを評価すべき語としてQAテストを再度行うようにした．
上の例では，「生徒数」をQAテストで再び評価する．
実際のテストで許される拡張は，図[REF_fig:suffix]に示した通りである．
可能な接尾辞については，図[REF_fig:suffix]に示した適用範囲が広いと思われるものに限定し，これでカバーできないものは，名詞化した形容詞・形容動詞を適切に付加してもらうことで対処した．
このテストを，接尾拡張QAテストと呼ぶことにする．
我々の当初の目的は与えられたクラスに対する属性語を獲得する，つまり，クラスの全てのインスタンスに共通の属性語を獲得することであった．
しかし，評価者によっては，全てのインスタンスに共通の属性語ではないが興味深い属性語を正しいものとして判定することが予備実験において分かった．
例えば，クラス「映画」に対する「字幕」や「車」に対する「後席」などである．
全ての映画に字幕がある訳ではないので，厳密には「字幕」は映画の属性語ではない（例えば，日本では字幕はほとんど外国映画に対して付与される）し，全ての車に後席がある訳ではないので厳密には「後席」は「車」の属性語ではない．
しかし，厳密に属性語ではなくてもそれを持つようなインスタンスの割合・重要性が高い場合には，正しいと評価される傾向があることが推測される．
このような属性語はその割合・重要性から実用的に有用であると考えられる．
このような全てのインスタンスに共通する属性語とそうでない属性語の性質を調べることができるように，QAテストで受理された属性語に関しては，それがそのクラスの「ほとんど全て」のインスタンスに共通するかを最後に判定するようにした．
このテストを一般性テストと呼ぶ．
一般性テストで受理された属性語を「厳密な属性語」と呼び，QAテストで受理されて一般性テストで棄却された属性語を「非一般属性語」と呼ぶ．
また，「厳密な属性語」と「非一般属性語」を合わせて「緩い属性語」と呼ぶことにする．
実験では，厳密な属性語としての獲得精度，緩い属性語としての獲得精度を調査比較する．
最後に，下位語テストについて説明する．
我々の提案手法では，誤って獲得された属性語の中にクラス[MATH]の下位語やインスタンスと考えられる語が多く含まれることが分かった．
もし，評価対象の[MATH]が[MATH]の下位語やインスタンスなら，それは[MATH]の属性語にはなり得ないが，「[MATH]の[MATH]」という表現が自然になってしまうためQAテストで混乱を引き起こしやすい．
例えば，「アニメ」のインスタンスとして「ドラゴンX」があるとすると，「アニメのドラゴンX」という表現は自然であり，「ドラゴンX」は明らかに「アニメ」の属性語でないのにQAテストで誤って受理されてしまう可能性がある．
そこで，QAテストの前に図[REF_fig:hypo]で示される下位語テストにより[MATH]が[MATH]の下位語やインスタンスであるかを判定し，下位語やインスタンスでない場合にだけQAテスト以降に進むようにした．
逆に，[MATH]が[MATH]の上位語である場合には，上位語であっても必ずしも[MATH]が属性語でないとは言えないのでそのようなテストは行わない．
この節では，提案獲得手法を前節で述べた評価手順で評価した実験について述べる．
まず，評価のために32個のクラスを用意した．
Webに現れるようなクラスで評価を行うため，この32個のクラスは，新里らの上位下位関係獲得手法[CITE]によってWebから獲得された共通の上位語をもつ単語クラス1,589個の中から選んだものであり，このときの上位語をクラス語として用いる．
単語クラスに含まれる下位語は，クラス語の意味が曖昧な場合に意味を特定するための情報として評価者が参照できるようにした．
また，我々の目的は上位下位関係獲得の評価ではないので，上の獲得手法でうまく獲得されている単語クラスを選んだ．
さらに，この32個のクラスからランダムに22個のクラスを選び（表[REF_table:classes]），評価の対象とした．
提案獲得手法で用いられる局所文書集合[MATH]の収集にはWeb検索エンジンであるgoo (http://www.goo.ne.jp)を用いた．
[MATH]の大きさはクラス平均で857文書(URL)であった．
また，この[MATH]から得られた属性語候補はクラス平均で約2万語であった．
サブスコア[MATH]の計算に必要な大域文書集合[MATH]としてはWebからランダムに収集した[MATH]文書を用いた．
各クラスについて提案手法および後で述べる比較手法による上位50個の属性を出力し，評価対象とした．
効率的に評価するため，上記の全ての手法による属性語を一つの集合にまとめ（重複があれば取り除く），評価の公平性を保つためランダムに並べ替えた．
このように重複を除くと，評価すべき属性語は全てのクラスで合わせると[MATH]個だった．
これらの属性語を，提案評価手順を実装したGUIツールを用いて4人の評価者がそれぞれ4日間かけて評価した．
この評価結果から，それぞれの手法の上位50個の出力に対する評価が生成できる．
この実験に関して，評価者間の一致度を示すkappa値[CITE]は，厳密な属性語の評価としては[MATH]，緩い属性語の評価としては[MATH]となり，両者とも「中程度」の一致を示した．
図[REF_fig:wid]に提案獲得手法による緩い属性語としての精度，図[REF_fig:gen]に厳密な属性語としての精度を示す．
それぞれの図において左のグラフは各評価者（Evaluator1-4）による適合率，右は評価者に関する平均・3人一致（3-consensus）・4人一致（4-consensus）の適合率である．
グラフの[MATH]軸は上位何個まで集計するかを，[MATH]軸はそのときの適合率を表している．
大まかに言って，グラフの[MATH]軸は再現率に対応する．
上位[MATH]個における評価者[MATH]による適合率[MATH]は，[MATH]を評価に用いたクラスの集合とすると,
で計算される．
評価者に関する平均の適合率は，[MATH]で計算される（[MATH]は評価者の集合）．
また，[MATH]人一致の適合率は,
で計算される．
グラフをみると，適合率自体は評価者に大きく依存するが，提案手法の順位付けと適合率の間の正の相関は共通して存在することが分かる．
これは提案手法の妥当性をある程度示していると言える．
また，緩い属性語としての評価と厳密な属性語としての評価を比べると，厳密な属性語の方が獲得が難しいことが分かる．
加えて，厳密な属性語としての評価（つまり，一般性テスト）は評価者によって大きく傾向が違うことが分かる．
緩い属性語の評価の場合にはプロットはほとんど交差していない．
これは評価者間で許容度の差はあっても評価の傾向は変わらないことを示している．
一方，厳密な属性語の場合には，プロットが交差しており,評価者によって評価の傾向が異なることを示している．
緩い属性語の場合に一番許容的であった評価者3（図中，Evaluator3）が厳密な属性語の場合にはそうでもない点などは興味深い．
これらのことから，一般性テストは他のQAテストなどに比べて一致を得るのが難しいテストになっていることが推測され，先に示したkappa値の違いもそれを示唆している．
提案手法による獲得精度はおおむね期待の持てるものである．
どの評価尺度を採用するのが妥当かは難しい問題であるが,例えば[CITE]で用いられた多数決基準（本実験の場合，3人一致）を用いるとすれば，提案手法は上位20個の属性語を出力した場合，緩い属性語は0.852，厳しい属性語は0.727の適合率で獲得できることになる．
表[REF_table:example]に，実際に獲得された属性語の上位20個をいくつかのクラスに対して示す．
これらをみると，実際に興味深い属性語が獲得できていることが分かる．
次に，提案手法のスコア（式[REF_eq:score]）における各サブスコアの効果を調べるため，式[REF_eq:score]から各サブスコアを除いたスコアを用いたときに精度がどの程度変化するかをみた．
まず，評価者平均精度の変化でみると，全てのサブスコアに関して効果がある（各サブスコアを除く事で精度が低下する）ことが観察された．
さらに，各評価者ごとにみると，評価者によらず似た傾向の精度の変化があることが分かった．
これは，前の実験で示したように評価自体は評価者によってかなり異なる事を考えると興味深い．
この点をより詳しく分析するため，各サブスコアを除いた場合に関して，評価者ごとに精度の変化を計算し，変化の評価者に関する平均・標準偏差などを求めた．
図[REF_fig:feature.diff]はそのようにして求めた平均・標準偏差をプロットしたものである．
このグラフから，全般的に，ほぼ順位にかかわらず，全ての評価者で効果があることが分かる．
左のグラフで示された緩い属性語の場合を詳しくみると，[MATH]と[MATH]の効果が特に大きい．
また，[MATH]は[MATH]と同様に効果はあるが絶対値は小さいことが分かる．
これは，前述したように利用できる文書の量の差によるものと考えられる．
[MATH]は，低ランクまででみると正の効果を示しているが，高ランク域（1位--5位）ではわずかではあるが負の効果になっている．
右のグラフの厳密な属性語の場合にも，全般としては正の効果があることが分かる．
しかし，効果の程度は全般に小さくなり分散も大きくなっている．
特に，[MATH]の効果は緩い属性語の場合に比べて大幅に小さくなっている．
一方で，[MATH]の効果は逆に大きくなっている．
提案手法では，順位付けで用いられる局所文書集合を収集する際の検索エンジンに対するキーワードとしてクラス語を使用した．
しかし，もし上位下位関係知識が利用できる場合には，クラスに属する下位語をキーワードとして局所集合を収集する事も可能であり，その場合に属性語の獲得精度がどのように影響されるかは興味深い問題である．
そこで，この実験では提案手法のようにクラス語を用いる場合と下位語を用いる場合を比較した．
実験で用いたクラスには，上位下位関係獲得手法[CITE]により下位語が対応付けられているので，これを検索キーワードとして用いた．
ここでは，収集された文書の質を比較するため，収集された文書から提案手法で収集された文書数とほぼ同数の文書をランダムで取り出して用いた．
このようにして得られた局所文書集合を用いてスコア中の[MATH]を計算する際には，[MATH]の代わりに下位語[MATH]を用いた場合に得られる頻度を全ての下位語について和をとって[MATH]の値とした（つまり，[MATH]）．
他のサブスコアの値については提案手法と同じ値を用いた．
図[REF_fig:hyponym.diff]に，前節の実験と同じ方法で緩い属性語と厳密な属性語について提案手法と比較手法の精度の差（差の平均[MATH]標準偏差）を示す．
負の差は比較手法の精度が提案手法に比べて悪いことを表す．
従って，この結果から，少なくともこの設定では局所文書集合の収集にはクラス語のほうが適していることが分かる．
