\section{関連研究}\label{sec:kanren}

ここでは，
本稿の基礎として，
クラス所属確率を推定する代表的な方法である Platt の方法および，Zadrozny らにより提案されたビニングによる方法と Isotonic 回帰による方法について述べる．
これらはいずれも 2 値分類を想定しているが，
Isotonic 回帰による方法においては，2 値分類を多値分類に対応させる方法についても述べる．
最後に，
Platt の 方法と Isotonic 回帰による方法について，多種類の分類器とデータセットによる実験を行って比較した Caruana らによる研究~\cite{Caruana04,Mizil05} について述べる．

\subsection{Platt の方法}

Platt~\cite{Platt99} は，分類器を SVM に限定し，
分類スコアを事例に対してクラスが予測された際の分離平面からの距離 $f$ として，
シグモイド関数 $P( f ) = 1/\{1+\exp(Af+B)\}$ により [0,1] 区間に変換される値 $P (f)$ をクラス所属確率の推定値として用いることを提案した．
ただし，パラメータ $A$ および $B$ は，あらかじめ最尤法により推定しておく必要がある．
シグモイド関数による方法の利点は，
分類スコアから直接，クラス所属確率の推定値を求めることができるため，
パラメータ $A$ および $B$ が推定されていれば，手続きが容易であることである．
Platt は，シグモイド関数の過学習を避けるために，out-of-sample モデルを用いて，
Reuters~\cite{Joachims98} を含む 5 種類のデータセットを用いて実験を行い，
この方法の有効性を示した．
データセットが Adult の場合における結果を図\ref{Platt}~\cite{Platt99} に示す．
図~\ref{Platt} において，$X$ 軸は分類スコア，$Y$ 軸はクラス所属確率を表し，$+$ 印は分類スコアを 0.1 の区間に分けた場合に対応するクラス所属確率の実測値，実線は推定値を表す．


\begin{figure}[b]
  \begin{center}
\includegraphics{15-2ia1f1.eps}
\caption{Platt の方法による推定値と実測値の例}
\label{Platt}
  \end{center}
\end{figure}


しかし，Bennett~\cite{Bennett00} は，Platt の方法は分類器がナイーブベイズの場合にうまくいかないことを Reuters 21,578 データセット\footnote{
	\texttt{http://www.daviddlewis.com/resources/testcollections/reuters21578/}
}により示した\footnote{
	Bennett~\cite{Bennett00} の実験では，特に，出現頻度が少ないクラス（例えば $Corn$ など）
	において信頼度曲線（3.2.1 節を参照のこと）による評価が悪かった．
}. 
また，Zadrozny ら~\cite{Zadrozny02} も，この方法がデータセットによっては適合しない場合があることを示し\footnote{
	Zadrozny ら~\cite{Zadrozny02} の実験では，Adult データセットと 
	TIC (The Insurance Company Benchmark) データセットにナイーブベイズ分類器を適用した場合は，
	スコアの変換がうまくいかなかった．
}，以下に述べる方法を提案した．

\subsection{ビニングによる方法}

Zadrozny らは，分類器としてナイーブベイズを想定し，ビニングによる方法（ヒストグラム法）を提案した~\cite{Zadrozny01a,Zadrozny01b}. 
ビニングによる方法は，未知の事例のクラス所属確率を直接推定せずに，
あらかじめ作成しておいた「ビン」を参照し，そのビンにある正解率を用いて間接的に推定を行う方法である．
ビニングによる方法における処理手順は次の通りである．

まず，訓練事例を分類スコアの値順に並べ，
各区間に属する事例数が等しくなるように区切ってビンを決める．
このとき，各ビンに属する事例の分類スコアから，そのビンに所属する事例における分類スコアの最大値と最小値を調査しておく．
ここまでの処理を図~\ref{bining} に示す．
図~\ref{bining} はナイーブベイズ分類器の例で，数値（斜体）は分類スコアを表す．


\begin{figure}[b]
  \begin{center}
\includegraphics{15-2ia1f2.eps}
\caption{ビンの作成例（訓練事例数が 12 でビンの数が 3 個の場合）}
\label{bining}
  \end{center}
\end{figure}


次に，各ビンごとに正解の事例を数えてそのビンに属す全事例数で割り，正解率を計算する（表~\ref{bining1} を参照のこと）．
最後に，未知の事例の分類スコアから該当するビンを見つけ，そのビンの正解率を未知の事例のクラス所属確率値とする．

実験は KDD'98 データセット\footnote{
	\texttt{http://kdd.ics.uci.edu/}
}を用いて行われ，
平均二乗誤差や平均対数損失による評価の結果，有効性が示された（ビンの数が 10 個の場合）．
ビニングによる方法は処理が単純であるという利点があるが，
最適なビンの個数をどのようにして決めればよいか（各ビンに含まれる事例数をいくつにするか）という問題がある．

なお，Zadrozny らは，この後に，誤分類に対するコストを考慮した方法として，ビニングによる方法を改良した「Probing」という方法を提案したが，
実験の結果，有効性を示さない場合も多かった\footnote{
	決定木，バギングされた決定木，SVM, ナイーブベイズ，ロジスティック回帰において，
	UCI machine learning repository や UCI KDD archive, 2004 KDD における
	計 15 種類のデータセットを用いて実験し，二乗誤差，クロスエントロピー，
	AUC (Area under the ROC curve) によりを評価を行った．
}~\cite{Zadrozny05}. 

\subsection{Isotonic 回帰による方法}

Zadrozny らは，ビニングによる方法の問題点を解決する方法として，
次には，分類スコアと正解率が単調非減少な関係にあるという観察に基づく Isotonic 回帰による方法\footnote{
	Chanらは，語の曖昧性解消タスクにおける EM アルゴリズムで，Isotonic 回帰による方法を
	用いてクラス所属確率の推定を行った~\cite{Chan06}.
}を提案した~\cite{Zadrozny02}. 
ここで，Isotonic 回帰問題とは，
実数の有限集合 $Y=\{y_{1}, y_{2}, \cdots, y_{n}\}$ が与えられたとき，
制約条件 $x_{1}\le \cdots \le x_{n}$ の下で目的関数 $\sum_{i = 1}^{n} w_i(x_i - y_i)^{2}$ を最小化する 2 次計画問題である~\cite{Kearsley96}. 
ただし，$w_i$ は正値重みを表す．

Isotonic 回帰問題の解法としては，PAV (pool-adjacent violators または pair-adjacent violators) アルゴリズム（以下では，PAV と略す）が最も代表的であり~\cite{Kearsley96,Ahuja01,Mizil05,Fawcett06}, Zadrozny らが提案した Isotonic 回帰による方法も PAV が適用されている．
ここで，PAV とは，単調非減少ではないブロックがある場合に，そのブロック内に存在する値のすべてをブロック内の値の平均値で置き換える処理を繰り返すことにより，全体の単調非減少性を保つ方法である．
例えば，
前述の目的関数において重みがすべて 1 のとき，
\{1, 3, 2, 4, 5, 7, 6, 8\} において，まず \{3, 2\} 
のブロックが単調非減少ではないために，ブロック内のすべての値を平均値 2.5 で置き換えて \{1, 2.5, 2.5, 4, 5, 7, 6, 8\} に修正する．
次に，\{7, 6\} のブロックが単調非減少ではないために，同様に平均値 6.5 で置き換えて \{1, 2.5, 2.5, 4, 5, 6.5, 6.5, 8\} に修正する方法である~\cite{Kearsley96}. 

PAV を用いた Isotonic 回帰による方法も，ビニングによる方法と同様に，最初に訓練事例を分類スコア順にソートする必要があるが，
事例をまとめて扱わずに，各事例に対して正解率（正例の場合は 1, 負例の場合は 0 となる）を付ける点が異なる（図~\ref{Isotonic} における開始時点の表を参照のこと）．
正解率が分類スコアと単調非減少な関係になるまで正解率の修正を繰り返し，最終的に定まった値を正解率とする（図~\ref{Isotonic} における終了時点の表を参照のこと）．
図~\ref{Isotonic} では 1 回修正された値が再度修正されることはなかったが，値の並び方によっては再修正される可能性が高く，一般的には何度も修正が繰り返される場合が多い~\cite{Kearsley96,Ahuja01,Mizil05,Fawcett06}. 


\begin{figure}[t]
  \begin{center}
\includegraphics{15-2ia1f3.eps}
\caption{Isotonic 回帰による方法における正解率の修正例（SVM を利用し事例数が 10 の場合）}
\label{Isotonic}
  \end{center}
\end{figure}

実験は，
ナイーブベイズ分類器と SVM において KDD'98 データセットなどを用い，
ビニングによる方法やシグモイド関数による方法と比較された（ビニングの数は 5 個から 50 個まで変えて行われた）．
平均二乗誤差による評価の結果，PAV による方法はビニングによる方法を常に上回ったが，
シグモイド関数による方法との差は少しであった．

Zadrozny らは，次に，多値分類においては，分類器は各々の予測クラスに対して分類スコアを 1 つずつ出力すると仮定し，多値分類における PAV の効果を調査した．すなわち，2 値分類において PAV により推定したクラス所属確率値を統合した場合と，PAV を用いずに推定した値を統合した場合との比較を行った~\cite{Zadrozny02}. 
Zadrozny らは，この実験の前に，あらかじめ，ナイーブベイズ分類器とブーステッドナイーブベイズにおいて 20 Newsgroups データセット\footnote{
	\texttt{http://people.csail.mit.edu/jrennie/20Newsgroups/}
}などを用いた実験を行って，2 値分類への分解法である all-pairs と one-against-all の間で精度の差がないことを確認し，実験ではすべて one-against-all を用いた．
2 値分類における推定値を統合する方法としては，
one-against-all に対応した正規化の方法の他に，
どちらの分解方法にも対応可能な最小 2 乗法による方法や対数損失を最小化するカップリングの方法が用いられたが，正規化の方法が最もよい結果を示した．


PAV の有効性については，まず，ナイーブベイズ分類器とブーステッドナイーブベイズによりデータセット Pendigit を用いた実験の結果，
分類器や統合する方法に関係なく，平均二乗誤差による評価では改善がみられたが，
エラー率による評価ではほとんど改善されなかった．
次に，ナイーブベイズ分類器によりデータセット 20 Newsgroups を用いた実験結果も，
多値分類への統合方法に関係なく，平均二乗誤差による評価では改善がみられたが，
エラー率による評価ではほとんど改善されなかった．
ここで，2 値分類における推定値の 3 種類の統合方法を比較すると，
ナイーブベイズ分類器による値を PAV により修正した値を正規化する方法がよかったが（平均二乗誤差により評価した場合），他の分類器や評価法においては差がなかった．

なお，Zadrozny らは，この後さらに提案した Probing とよばれるクラス所属確率の推定方法を多値分類へ拡張する場合には，ここで述べた統合方法を用いずに，
one-against-all により分解した各組において 2 値分類として推定した値をそのまま用いるという非常に単純な方法を示した~\cite{Zadrozny05}. 
ただし，この方法に対する評価実験は行っていない．

\subsection{方法の比較}

Caruana ら~\cite{Caruana04,Mizil05} は，
アンサンブル学習を含めた 10 種類の分類器（SVM, ニューラルネット，決定木，k 近傍法，
bagged trees, random forests, boosted trees, boosted stumps,  ナイーブベイズ分類器，
ロジスティック回帰）を，
8 種類のデータセット（UCI Repository から 4 種類，医療分野から 2 種類選んだデータセット，IndianPine92 データセット~\cite{Gualtieri99}, Stanford Linear Accelerator）に適用し，
Platt の方法と Isotonic 回帰による方法 (PAV) の比較を行った．
その結果，
Platt の方法はデータが少ないとき（約 1,000 サンプル未満）に効果的であり，
Isotonic 回帰による方法は過学習しない程度に十分なデータがあるときによかった．

Jones ら~\cite{Jones06} は，検索を成功させるために，ユーザが入力したクエリから新しくクエリを生成して置き換えるというタスクにおいて，置き換えられたクエリの正確さの程度を予測するために確信度スコアが必要であると考え，Isotonic 回帰による方法 (PAV) とシグモイド関数による方法についての簡単な比較実験を行った． 
その結果，
Isotonic 回帰による方法は過学習の問題があり，
平均二乗誤差および対数損失のいずれにおいてもシグモイド関数による方法の方が上回ったため，
彼らのタスクではシグモイド関数による方法が採用された．

