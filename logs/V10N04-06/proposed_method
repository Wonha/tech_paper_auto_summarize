サポートベクタマシン(Support Vector Machine,以下SVM) [CITE]は，[MATH]個の素性を持つ事例を[MATH]次元ベクトルによって表し，[MATH]において2つのクラスに線形分離する二値分類器である．
与えられた事例[MATH]がクラス[MATH]のどちらに属するかを式([REF_eq:classify])によって判別する．
ここで[MATH]は2つのクラスを分離する超平面であり，[MATH]と[MATH]は学習によって決定する．
訓練事例[MATH]に対する教師信号[MATH]を以下のように与える．
訓練事例が線形分離可能である場合には，式([REF_eq:classify])を満たすような[MATH], [MATH]は複数存在することから以下のような制約を与える．
SVMでは，訓練事例と分割境界の間の距離(マージン)を最大化する戦略に基づきパラメータ[MATH], [MATH]を決める．
詳しい導出は文献[CITE]に譲るが，最終的にマージンの最大化の問題は式([REF_eq:constraint])の条件の下に[MATH]を最小化する問題に帰着する．
これを2次計画法によって解くことで最適な分離平面[MATH]が得られる．
事例[MATH]に対して[MATH]の符号はクラスを表し，絶対値はクラス分けの確信度を表す．
また，以下のようにシグモイド関数によって[MATH]がクラス[MATH]に分類される確率を近似することができる[CITE]．
線形分離が困難な事例に対しても，前処理として非線形な写像[MATH]を用いてそれらをより高次元に写像することによって線形分離できる場合がある．
写像先の空間[MATH]において線形分離を行えば元の空間[MATH]において非線形分離を行っているのと同じことになる．
詳しい導出は省略するが，SVMでは学習，識別アルゴリズムにおいて事例間の内積しか使用していない点を生かし，各事例間の内積[MATH]を式([REF_eq:kernel])に置き換えることによって高次元への写像を実現する．
[MATH]はカーネル関数と呼ばれる．
実際には[MATH]自体の計算をする必要がないので，計算量の面でも非常に効率的である．
よく使われるカーネル関数の例としては多項式型カーネル関数([REF_eq:poly_kernel])などが知られている．
[MATH]次の多項式型カーネル関数による非線形分離は，元の空間[MATH]においては[MATH]個の素性の依存関係を考慮していることに相当する．
本論文で提案する手法は，対訳文となっている日本語文と英語文から，その中に含まれる句の対訳関係を抽出する．
その手法は以下の2つの手順から構成される．
訓練コーパスにおいて対訳関係となっている表現(対訳対)とそうでない表現を人手によって分類，前者を正事例，後者を負事例とし，これらからSVMによって対訳モデルを学習する([REF_sec:learn]節)．
対訳文となっている日英両言語の文を構文解析し，得られた句構造から対訳対と成り得る候補(対訳対候補)の集合を作成する．
それらを対訳モデルに入力することによって対訳関係であるかどうかを判別する([REF_sec:extraction]節)．
本手法の概略図を図[REF_fig:struct]に示す．
SVMを対訳関係の抽出に用いるためには，対訳対候補から素性ベクトルを作成する必要がある．
本論文で提案する手法では表[REF_tab:features]のような素性を用いて素性ベクトルを構成した．
既存の辞書を使用する素性を2種類用いる．
素性(1a)は，対訳対候補に含まれる語について辞書引きを行い，対訳となっている単語の組(対訳単語対)が対訳対候補に含まれていればそれを素性とする．
対訳関係となっている表現には対訳単語対が多く含まれることに基づく素性である．
辞書に含まれる対訳単語対を素性ベクトルの次元に割り当て，対訳対候補内に対訳単語対が現れた場合は対応する次元の値を1とし，そうでなければ0とする．
素性(1b)は，対訳対候補の近傍に出現した語について辞書引きを行い，辞書に含まれる対訳単語対を素性とする．
「対訳関係にある表現は近傍に出現している語の出現文脈も(言語の違いこそあれ)似ている」という考え[CITE]に基づく素性である．
本論文における実験では，同一文に現れる語を近傍とした．
辞書に含まれる対訳単語対を素性ベクトルの次元に割り当て，対訳対候補の近傍に対訳単語対が現れた場合は対応する次元の値を1とし，そうでなければ0とする．
対訳辞書という既存の知識を素性という形で有効に利用することによって精度の向上を期待することができる．
素性(2a)(2b)は，対訳関係となっている表現は両言語の句の構成語数に相関関係があるという考えに基づく．
日英それぞれの句に含まれる語数を素性とした．
素性(3a)(3b)は，対訳関係となっている表現は内容語に関してはその構成比率について両言語間に相関関係があるという考えに基づく．
日英それぞれについて句の語数に対する内容語の出現数の割合を素性とする．
なお，日本語の内容語は名詞，動詞，形容詞，形容動詞，副詞とし，英語の内容語は名詞，動詞，形容詞，副詞とした．
素性(4a)(4b)は，日英両言語を構成する内容語に素性ベクトルの次元を割り当て，語が出現すれば対応する次元の値を1とし，そうでなければ0とする素性である．
素性(5a)(5b)は，対訳対候補の近傍に現れた内容語に素性ベクトルの次元を割り当て，語が出現すれば対応する次元の値を1とし，そうでなければ0とする素性である．
対訳文中に既存の対訳辞書によって辞書引きできない語が多数含まれる場合がある．
素性(4a)(4b)(5a)(5b)はそのような場合に既存の対訳辞書を用いた素性(1a)(1b)を補完する目的で導入した．
カーネル関数によって素性(4a)と(4b)の依存関係，素性(5a)と(5b)の依存関係をモデルに組み込むことによって，既存の対訳辞書に現れない対訳単語対における素性(1a)(1b)と同じ役割を期待することができる．
訓練コーパス中の各対訳文において対訳関係となっている表現とそうでない表現を人手によって作成する．
対応する日英の両文を構文解析し，得られた両言語の句の組合わせについて，対訳表現となっているものを正事例とし，そうでないものを負事例とする．
本論文における実験では，組合わせの対象とする句は名詞句と動詞句とした．
また，巨大すぎる句構造は対訳表現としての実用的な価値が少ないと思われることから，句の部分構文木の高さが5以下のものを組合わせの対象とした．
本論文における実験では，日本経済新聞社英文ビジネスレター文例大事典[CITE]を対訳コーパスとして用いた．
英文ビジネスレター文例大事典の各対訳文は対訳対となる部分があらかじめマークアップされており，対訳表現として抽出すべき句の制約(部分構文木の高さが5以下の名詞句，動詞句)を満たす対訳対を正事例とした．
負事例は，Apple Pie Parser とKNP によって構文解析した結果から対訳表現として抽出する句の制約を満たすもののうち，対訳表現になっていないものを選んだ．
具体的には，各対訳文に1対ずつある対訳対[MATH] ([MATH]は日本語句，[MATH]は英語句)に対して，日英各文を構文解析することによって得られた句[MATH]や[MATH]を用いた[MATH]や[MATH]を負事例とした．
このようにして得られた全ての事例から[REF_sec:feature]節で述べた方法によって素性ベクトルを作成し，教師信号として正事例には[MATH]，負事例には[MATH]を与える．
これを訓練データとして[REF_sec:svm]節で述べたSVMによって対訳モデルの学習を行い，式([REF_eq:hyperplain])における最適な分離平面[MATH]を得る．
まず，抽出の対象となる対訳対の候補を作成する．
対訳文になっている日英両言語の文を構文解析し，得られた日英両言語の句の組合わせを対訳対候補の集合とする．
訓練データと条件を同じにするために，対象とする句は部分構文木の高さが5以下の名詞句と動詞句とした．
生成した対訳対候補から[REF_sec:feature]節で述べた方法によって素性ベクトルを作成する．
それらと[REF_sec:learn]節で述べた方法によって得た最適な分離平面[MATH]を用いて，その対訳対候補の「対訳対らしさ」を測る．
対訳対候補[MATH]に対応する素性ベクトルを[MATH]とした時，最適な分離平面[MATH]を用いて[MATH]の「対訳対らしさ」を以下の式によって表す．
任意の[MATH]に対して[MATH]であり，[MATH]が大きいほど[MATH]が「対訳対らしい」ことを表す．
一つの句が複数の句と対応することはないことから，以下のようなアルゴリズムによって対訳対の抽出を行う．
閾値[MATH]
対訳文中の対訳対候補の集合[MATH]
抽出された対訳対の集合[MATH]
[MATH]
[MATH]となる[MATH]がなければ終了．
[MATH]となる[MATH]を対訳対として抽出し[MATH]に追加する．
[MATH]や[MATH]を含む対訳対候補を[MATH]から削除する．
2.へ戻る．
[MATH]の値によって得られる対訳対の品質を調節することができる．
[MATH]の値が[MATH]に近い時には抽出数が少なくなる代わりに確信度が高い対訳対だけを抽出し，逆に[MATH]の値が[MATH]に近い時には確信度が多少低いものも抽出することによって抽出数を優先する．
上記の処理は1文単位で行う．
そのため[REF_sec:learn]節によって対訳モデルを一旦学習してしまえば抽出対象となるコーパスは小規模なものでもよく，たとえ1文からでもそこに含まれる対訳対を抽出することができる．
