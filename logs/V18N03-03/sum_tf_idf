================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:0, score:0.24354] 本稿では，訓練データの自動拡張による語義曖昧性解消の精度向上方法について述べる．
[i:3, score:0.27431] 更に，辞書の例文，配布データ以外のセンスバンク，ラベルなしコーパスなど，さまざまなコーパスを利用して，訓練データの自動拡張を試みた結果を紹介する．
[i:5, score:0.22427] 更に，対象語の難易度に基づき，追加する訓練データの上限を制御したところ，最高80.0%の精度を得ることができた．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[i:22, score:0.27393] Bootstrapping法では，まずラベル（語義）の付与された訓練データで学習し，ラベルなしデータのラベルを推定し，ある基準において最も信頼できるものをラベルありデータに追加する[CITE]．
[i:24, score:0.25862] しかしこれらの方法の場合，ラベルなしデータから，いくら訓練データを追加したところで，もともとの訓練データに出現しないような語義を推測することはできない，という問題がある．
[i:27, score:0.33319] 彼らは，WordNetの同義語(synset)のうち，単義語（例えば，\eng{``[MATH]''}に対して\eng{``recollect''}など）や，定義文(gloss)の中のユニークな表現（例えば，\eng{``[MATH]''}に対して，glossの一部である\eng{``bring onto the market''}など）を検索語としてWeb検索を行い，獲得したスニペット中の対象語に語義を付与し，訓練データに追加している．

================================================================
[section type  : proposed_method]
[section title : データ]
================================================================
-----------------------------------------------------
  [subsection title : SemEval-2010: Japanese WSD タスク配布データ]
-----------------------------------------------------
  [i:lead, score:0.10156] [REF_sec:intro]章で述べたように，SemEval-2010: Japanese WSDタスクは，対象コーパスの分野が多岐にわたるという特徴がある．
.....
  [i:40, score:0.19460] また，本データには，岩波国語辞典[CITE]の語義を元に，語義IDが付与されている．
  [i:43, score:0.22174] 訓練データでは，2種類の新語義(\X)が出現している．
  [i:49, score:0.24194] 例えば，図[REF_fig:trnORG]，6行めの形態素「取っ」の場合，語義ID '37713-0-0-1-1'が付与されている．
-----------------------------------------------------
  [subsection title : 岩波国語辞典の例文]
-----------------------------------------------------
  [i:lead, score:0.11125] 本稿では，まず，岩波国語辞典の例文を抽出する．
.....
  [i:57, score:0.33368] それにより，例えば，図[REF_fig:iwanami]に示した見出し語「とる」の場合，37713-0-0-1-1の例文として([REF_s:toru:ex1])，37713-0-0-3-1の例文として([REF_s:toru:ex3])，37713-0-0-6-3の例文として([REF_s:toru:ex6])などが獲得できる．
  [i:63, score:0.33152] また，例文([REF_s:toru:ex1])--([REF_s:toru:ex6])において, ``—''によって見出し語を補完した部分（\ul{下線部}）には，例文を抽出した語義のIDを付与する．
  [i:66, score:0.26311] つまり，例文([REF_s:toru:ex1])は37713-0-0-1，([REF_s:toru:ex3]), ([REF_s:toru:ex3-3])は37713-0-0-3，([REF_s:toru:ex6])は37713-0-0-6の例文として利用する．
-----------------------------------------------------
  [subsection title : センスバンク：檜]
-----------------------------------------------------
  [i:lead, score:0.02919] 本稿では，更に数種類の言語資源を利用した．
.....
  [i:77, score:0.33831] そのため，リンクが存在する語義なら，檜で付与されている\lxd{}の語義を岩波国語辞典の語義に置き換えて，訓練データとして利用することができる．
  [i:78, score:0.24369] 例えば，岩波国語辞典の「とる」37713-0-0-6-3の語義文は「数える．
  [i:83, score:0.44211] このリンクを用いることで，例えば，\lxd{}の19036420-49の例文([REF_s:toru:lxd])を，岩波国語辞典の37713-0-0-6(-3)の訓練データに追加できる．
-----------------------------------------------------
  [subsection title : 現代日本語書き言葉均衡コーパス]
-----------------------------------------------------
  [i:lead, score:0.13715] [REF_sec:jwsd]章で述べたように，本タスクのデータは，現代日本語書き言葉均衡コーパス(\bccwj)のコアデータから抽出されている．
.....
  [i:91, score:0.32306] このデータから，[REF_sec:iwanami-ex]章で抽出した岩波国語辞典の例文を利用し，訓練データを獲得する．
  [i:94, score:0.32589] 例えば，37713-0-0-3-3の例文「にとって」（図[REF_fig:iwanami]参照）を含む文として，Yahoo!知恵袋(\OC{})から，文([REF_s:toru:oc])を獲得できる．
  [i:95, score:0.27419] 文([REF_s:toru:oc])の\ul{下線部}は，[REF_sec:iwanami-ex]章で抽出した例文([REF_s:toru:ex3-3])と一致した部分である．
-----------------------------------------------------
  [subsection title : 未知語義数，および，獲得データサイズ]
-----------------------------------------------------
  [i:lead, score:0.26454] 表[REF_tb:wsnum-test]に，評価データに出現する語義のうち，訓練データにも出現する語義と，評価データにのみ出現する語義の数を示す．
.....
  [i:104, score:0.33301] また，[REF_sec:iwanami-ex]章から[REF_sec:bccwj]章で紹介した方法で獲得した訓練データのサイズを，表[REF_tb:get:size]に示す．
  [i:105, score:0.37658] 表[REF_tb:get:size]から，評価データにのみ出現する9語義に対しても，例文(\EX)，檜の両方から訓練データが獲得できることがわかる．
  [i:110, score:0.27273] なお，表[REF_tb:get:size]は，獲得傾向を確認するために，評価データに出現する語義かどうかを分けて表示しているが，実験では当然，評価データに出現しない語義の例文であっても区別せずに利用している．

================================================================
[section type  : experiment_result]
[section title : 実験]
================================================================
-----------------------------------------------------
  [subsection title : 学習器]
-----------------------------------------------------
  [i:lead, score:0.08387] 学習には，代表的な識別モデルの一つであり，ラベルありデータを用いて教師あり学習を行う最大エントロピーモデル(Maximum Entropy Method: \MEM, [CITE])を用いた．
.....
  [i:112, score:0.06675] これは，Fujitaら(2010)によると，Support Vector Machine (\SVM, [CITE])より，\MEM{}の精度がはるかに良かったためである．
-----------------------------------------------------
  [subsection title : 素性]
-----------------------------------------------------
  [i:lead, score:0.13985] [基本素性]まず，語義曖昧性解消タスクで一般的に利用される素性を，基本素性として利用する．
.....
  [i:113, score:0.13985] [基本素性]まず，語義曖昧性解消タスクで一般的に利用される素性を，基本素性として利用する．
  [i:120, score:0.09037] [トピック素性] SemEval-2007 English WSDタスクでは，トピック情報を利用したシステムが，最も高い精度を得ている[CITE]．
  [i:123, score:0.14752] 本稿では，訓練データと評価データにgibbslda++を適用し，文書（ファイル）単位でトピック分類を行った．

================================================================
[section type  : proposed_method]
[section title : 結果と議論]
================================================================
-----------------------------------------------------
  [subsection title : 配布データのみを利用]
-----------------------------------------------------
  [i:lead, score:0.17488] Fujitaら(2010)によると，対象語毎に訓練データの分野の組合せを変えて学習するより，分野に関係なくすべての訓練データを学習に用いる方が精度が良い．
.....
  [i:131, score:0.21402] 表[REF_tb:result-given]に，すべての訓練データを学習に用い，素性の組合せを変えた場合の結果を示す．
  [i:135, score:0.35369] そのため，SENSEVAL-2の日本語辞書タスクと同様に，訓練データにおける語義の頻度分布のエントロピー[MATH]（式([REF_s:entropy])）を，単語の難易度の目安として利用し，対象語を，高難易度([MATH], [MATH])，中難易度([MATH], [MATH])，低難易度([MATH], [MATH])の3つにわけた[CITE]．
  [i:139, score:0.18121] 表[REF_tb:result-given]によると，基本素性(\bl)だけを利用した場合でも，SemEval-2010のBest result (76.4%)より高い精度(77.7%)が得られた．
-----------------------------------------------------
  [subsection title : 自動獲得した訓練データも利用]
-----------------------------------------------------
  [i:lead, score:0.25029] 本節では，自動獲得した訓練データ（表[REF_tb:get:size]参照）を利用した場合の結果について紹介する（表[REF_tb:result-add]）．
.....
  [i:144, score:0.25029] 本節では，自動獲得した訓練データ（表[REF_tb:get:size]参照）を利用した場合の結果について紹介する（表[REF_tb:result-add]）．
  [i:146, score:0.24515] 基本素性(\bl)を用いて，配布された訓練データのみで学習した場合の精度を基準とすると，難易度別に傾向が非常に異なることがわかる．
  [i:155, score:0.28199] この原因は，(1)特に\bccwjと新聞データは，岩波の例文を含む文のみを抽出しているため，訓練データのバリエーションに乏しい，(2)例文によって，獲得できる訓練データ数に非常にばらつきがあり，自然な分布にならない，(3)自動獲得しているため誤りが含まれる，などが考えられる．
-----------------------------------------------------
  [subsection title : 学習曲線]
-----------------------------------------------------
  [i:lead, score:0.03584] 前節では，各コーパスから追加可能な文はすべて追加して学習した．
.....
  [i:167, score:0.31597] つまり例えば，最大追加文数を5文と設定する場合，例文([REF_s:toru:ex1])--([REF_s:toru:ex3-3])のそれぞれに対し，条件を満たす文のうち，最初に出てきた5文までを訓練データとして追加する．
  [i:177, score:0.31537] 表[REF_tb:result-lc-BK]の「参考」に，中・高難易語にのみ，300文を上限に訓練データを追加した場合の結果を示す．
  [i:178, score:0.34733] 但し，本稿の手法の利点の一つは，訓練データに出現しなかった語義にも訓練データを獲得できることであるため，訓練データに出現していない語義に対しては，低難易語であっても訓練データを5文を上限として追加している．

================================================================
[section type  : experiment_result]
[section title : 自動獲得した訓練データの評価]
================================================================
[i:180, score:0.31753] 本節では，[REF_sec:bccwj]章の訓練データの自動獲得方法で獲得した訓練データに正しい語義が付与されているかどうかを評価した．
[i:198, score:0.38828] [REF_sec:bccwj]節で述べたように，訓練データの追加条件は，例文を完全に含むこと以外にも，「対象例文の見出し語と，基本形，および，品詞大分類が一致する形態素に，該当する例文の語義IDを付与する．
[i:201, score:0.31956] だが，文([REF_s:toru:error1])の場合，動詞「取る」の目的語部分（「数」と「点数」）は異なっているため，例文側も形態素解析し，前後の形態素も含めて一致する文だけを訓練データに追加すれば，排除できる誤りである．

================================================================
[section type  : conclusion]
[section title : おわりに]
================================================================
[i:208, score:0.34628] 本稿では，辞書の例文，配布データ以外のセンスバンク（檜），ラベルなしコーパス(\bccwj),新聞データなど，さまざまなコーパスを利用して，訓練データの自動拡張を試みた．
[i:209, score:0.34604] 配布データ以外のセンスバンク（檜）を利用する場合，語義が定義された辞書同士のリンクを経由して，訓練データを獲得した．
[i:227, score:0.31036] 同義語も利用すれば，例文のみでは訓練データを新たに獲得できなかった語義についても新しい訓練データを追加できるかもしれない．

