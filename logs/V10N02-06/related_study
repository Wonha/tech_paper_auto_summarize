しかし，従来のベクトル空間法は，多数の単語を基底に用いるため，類似度計算にコストがかかることや，検索要求文に含まれる単語数が少ないとベクトルがスパースになり，検索漏れが多発する恐れのあることなどが問題とされている．

これらの問題を解決するため，さまざまな研究が行われてきた．例えば，簡単な方法としては，[MATH]法[CITE]などによって，文書データベース中での各単語の重要度を判定し，重要と判定された語のみをベクトルの基底に使用する方法が提案されている．また，ベクトル空間法では，ベクトルの基底に使用される単語は，互いに意味的に独立であることが仮定されているのに対して，現実の言語では，この仮定は成り立たない．そこで，基底の一次結合によって，新たに独立性の高い基底を作成すると同時に，基底数を減少させる方法として，KL法[CITE]やLSI法[CITE]，[CITE]，[CITE]が提案されている．

KL法は，単語間の意味的類似性を評価する方法で，クラスタリングの結果得られた各クラスターの代表ベクトルを基底に使用する試みなどが行われている．これに対して，LSI法は，複数の単語の背後に潜在的に存在する意味を発見しようとする方法で，具体的には，データベース内の記事の特性ベクトル全体からなるマトリックスに対して，特異値分解（SVD）の方法[CITE]を応用して，互いに独立性の高い基底を求めるものである．この方法は，検索精度をあまり低下させることなく基底数の削減が可能な方法として着目され，数値データベースへの適用[CITE]も試みられている．しかし，ベクトルの基底軸を変換するための計算コストが大きいことが問題で，規模の大きいデータベースでは，あらかじめ，サンプリングによって得られた一定数の記事のみからベクトルの基底を作成する方法[CITE]などが提案されている．このほか，単語の共起情報のスパース性の問題を避ける方法としては，擬似的なフィードバック法（２段階検索法とも呼ばれる）[CITE]，[CITE]なども試みられている．また，ベクトルの基底とする単語の意味的関係を学習する方法としては，従来から，Mining Term Associationと呼ばれる方法があり，最近，インターネット文書から体系的な知識を抽出するのに応用されている[CITE]．しかし，現実には，単語間の意味的関係を自動的に精度良く決定することは容易でない．

paragraph score: 1.0955637728798
