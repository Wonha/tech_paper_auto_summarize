 しかし，従来のベクトル空間法は，多数の単語を基底に用いるため，類似度計
 算にコストがかかることや，検索要求文に含まれる単語数が少ないとベクトル
 がスパースになり，検索漏れが多発する恐れのあることなどが問題とされてい
 る．

 これらの問題を解決するため，さまざまな研究が行われてきた．例えば，簡単
 な方法としては， $tf \cdot idf$ 法 \cite{Salton}などによって，文書デー
 タベース中での各単語の重要度を判定し，重要と判定された語のみをベクトル
 の基底に使用する方法が提案されている．また，ベクトル空間法では，ベクト
 ルの基底に使用される単語は，互いに意味的に独立であることが仮定されてい
 るのに対して，現実の言語では，この仮定は成り立たない．そこで，基底の一
 次結合によって，新たに独立性の高い基底を作成すると同時に，基底数を減少
 させる方法として，KL法\cite{Borko}やLSI法
 \cite{Golub}，\cite{Faloutsos}，\cite{Deerwester}が提案されている．

 KL法は，単語間の意味的類似性を評価する方法で，クラスタリングの結果得ら
 れた各クラスターの代表ベクトルを基底に使用する試みなどが行われている．
 これに対して，LSI法は，複数の単語の背後に潜在的に存在する意味を発見し
 ようとする方法で，具体的には，データベース内の記事の特性ベクトル全体か
 らなるマトリックスに対して，特異値分解（SVD）の方法\cite{Golub}を応用し
 て，互いに独立性の高い基底を求めるものである．この方法は，検索精度をあ
 まり低下させることなく基底数の削減が可能な方法として着目され，数値デー
 タベースへの適用\cite{Jiang}も試みられている．しかし，ベクトルの基底軸
 を変換するための計算コストが大きいことが問題で，規模の大きいデータベー
 スでは，あらかじめ，サンプリングによって得られた一定数の記事のみからベ
 クトルの基底を作成する方法\cite{Deerwester}などが提案されている．この
 ほか，単語の共起情報のスパース性の問題を避ける方法としては，擬似的なフィー
 ドバック法（２段階検索法とも呼ばれる）
 \cite{Burkley}，\cite{Kwok}なども試みられている．
 また，ベクトルの基底とする単語の意味的関係を学習する方法としては，従来
 から，Mining Term Association と呼ばれる方法があり，最近，インターネッ
 ト文書から体系的な知識を抽出するのに応用されている\cite{Lin}．しかし，
 現実には，単語間の意味的関係を自動的に精度良く決定することは容易でない．
score of this paragraph is 6
