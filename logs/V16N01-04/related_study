関連研究

リスト型質問応答については，米国における大規模検索実験プロジェクトである，TREC (Text REtrieval Conference) における，Qusetion Answering Track（以下，TREC QAと記述）で議論されている．

TREC QA でリスト型質問応答のタスクが始まったのは，2001年からである．
2001年\linebreak
\cite{TRECoverview01}と2002年\cite{TRECoverview02}のリスト型質問応答のタスクでは，正解の個数は質問文中に示されており，
システムは示された個数の解候補を出力し，その精度で評価された．
2003年\cite{TRECoverview03}では，リスト型質問応答はメインタスクに含まれる質問のうちの一種類になった．
2003年からは正解数が陽に示されることはなくなり，システムは
正解の個数を判定しなければならなくなった．
システムの評価はF値で行なわれる．
2004〜2006年\cite{TRECoverview04}\cite{TRECoverview05}\cite{TRECoverview06}のTREC QA 
のメインタスクの質問セットは，シリーズ型質問の集合になっている．
シリーズ型質問には，初めにそのシリーズの話題が示されており，
その次に何問かのfactoid質問，list質問があり，
最後にother質問がある．
factoid質問とlist質問では，どちらも事実を問う質問であり，要求される解の種類は
同じである．factoid質問とlist質問の違いは正解の数で，正解数が一つの質問はfactoid質問，正解数が複数の質問はlist質問という様に分けられている．
factoid質問ではシステムはただ一つの回答を出力し，list質問ではリスト形式で回答を出力する．
other質問は，質問文は与えられておらず，
そのシリーズの話題に関連することを出力することが要求される．ただし，factoid質問とlist質問で問われていないことのみを出力しなければならない．
各質問がfactoid, list, otherのどれであるかは質問文と共に与えられている．

list質問では，システムは与えられた質問の正解を過不足無く出力することが要求される．正解の数は明記されておらず，システム自身が判定する必要がある．
2006年の質問セットでは，全質問の正解の平均数は10個であり，最小のものは2個，最大のものでは50個ある．

factoid質問とlist質問は要求される回答数が違うだけであるので，参加したほとんどのチームは，そのチーム自身のfactoid質問に対するシステムと同じものを用い，出力する回答数のみを変えていた．以下に，具体例を紹介する．
F値のみではリスト型処理の善し悪しが分からないため，factoid質問に対する精度 (Accuracy) も併記する．

Harabagiu et al \cite{Harabagiu:AnswerMiningbyCombiningExtractionTechniqueswithAbductiveReasoning}は，一位の解候補と二位以下の各解候補の間の類似度を求め，
類似度に閾値を設けて回答選択をする手法を提案している．
閾値は一位の解候補と最下位の解候補の類似度を基に求められる．
類似度が閾値以上になる解候補のうち，最下位に順位づけされているものまで
を回答リストに加えている．
このシステムのlist型質問に対する精度は，F値で0.433であった．
また，このシステムの基になったfactoid質問に対するシステムの精度は，0.578であった．

Bos \cite{Bos:TheLaSapienzaQuestionAnsweringsystematTREC2006}は，質問文から正解の個数が推定できる場合には，
上位からその個数を回答とし，
それ以外の場合にはあらかじめ決められた個数の解候補を回答とする手法を用いていた．
このシステムのlist型質問に対する精度は，F値で0.127であった．
また，このシステムの基になったfactoid質問に対するシステムの精度は，0.15であった．

Burger \cite{Burger:MITREsQandaatTREC15}は，期待されるF値を求め，それを最大化するように
回答の個数を決める手法を提案している．
このシステムのlist型質問に対する精度は，F値で0.208であった．
また，このシステムの基になったfactoid質問に対するシステムの精度は，0.087であった．

また，国立情報学研究所主催の質問応答に関する一連の評価型ワークショップであるNTCIR QACにおいても同様にリスト型質問応答について議論されている．

NTCIR4 QAC2のsubtask1では，システムは与えられた質問に対して，順位付けされた5つの回答を出力することが求められる．
システムの精度にはMRR（Mean Reciprocal Rank．正解順位の逆数の各質問平均）が用いられる．
正解が複数存在する質問に対しては，システムはそのうちの一つを出力できれば良いとされている．

NTCIR4 QAC2のsubtask2（リスト型タスク）\cite{Fukumoto:QAC2Subtask12}でもTREC QAのlist質問と同様に，システムは与えられた質問の正解を過不足無く出力することが要求される．全質問の解の平均数は3.2個であり，最小のものは1個，最大のもので15個あり，TREC QAに比べると少なくなっている．各質問に対する正解の数が与えられていないこともTREC QAと同様であるが，
TREC QAでは正解数が1個のfactoid質問と2個以上のlist質問が分けられていたのに対し，NTCIR4 QAC2のリスト型タスクでは分けられていないという違いがある．
システムの精度には，修正F値（MF値）の全質問平均である，MMF値が用いられる．
修正F値の詳しい説明は，\ref{Chapter:exp-eval}節で述べる．

NTCIR4 QAC2に参加したシステムはTREC QAに参加したシステムと同様に，factoid質問に対するシステムを基にしており，各解候補に付けられたスコアの値を基に上位何件を回答するかの線引きを行なっている．
以下に具体例を説明する．MMF値のみでは順位付けの善し悪しが分からないため，
QAC2 subtask1に対する精度 (MRR) も併記する．

秋葉ら\cite{秋葉:質問応答における常識的な解の選択と期待効用に基づく回答群の決定}は期待効用最大化原理に基づく回答群選択手法を提案している．これは，リスト型質問応答の評価指標であるF値に着目し，その期待値を求め，期待値を最大化するように回答数を求める手法である．また，リスト内の解候補の重複を避けるために，複数の解候補が同じ内容を指していると判断される時には，スコアの高いものを残して削除するということをしている．このシステムの，QAC2 subtask2のテストセットに対する精度は，MMFで0.318であった．また，このシステムの
基になったfactoid質問に対するシステムの，QAC2 subtask1のテストセットに対する精度は，MRRで0.495であった．

福本ら\cite{Fukumoto:Rits-QA}は，スコアの差が最も開いているところよりも上位のものを回答とする手法を提案している．さらに，質問文の表層表現から解の個数を判別している（「誰と誰」なら二つ，など）．このシステムのQAC2 subtask2のテストセットに対する精度は，MMFで0.164であった．また，このシステムの
基になったfactoid質問に対するシステムの，QAC2 subtask1のテストセットに対する精度は，MRRで0.311であった．

村田ら\cite{Murata:JapaneseQAsystemUsingDecreasedAddingwithMultipleAnswers}は
最大スコアに対する比率に閾値を設けて回答を選択する手法を採用しており，
QAC2 subtask2のテストセットに対する精度は，MMFで0.321であった．
また，このシステムの
基になったfactoid質問に対するシステムの，QAC2 subtask1のテストセットに対する精度は，MRRで0.566であった．

高木ら\cite{Takaki:NTTDATA-QAatNTCIRQAC2}は
n番目の解候補のスコアとn+1番目の解候補のスコアの比率を求め，
それが閾値以上ならばn番目までの解候補を回答とするという手法を
用いている．このシステムのQAC2 subtask2のテストセットに対する精度は，MMFで0.229であった．また，このシステムの
基になったfactoid質問に対するシステムの，QAC2 subtask1のテストセットに対する精度は，MRRで0.335であった．

上記の各手法と本論文で提案する手法でとでは，
スコアの並びを見て動的に回答の数を変えるという点で類似している．
しかし，スコアが複数の混合分布から生成されると仮定することにより，
スコア分布のパラメタより解候補が適切に見つかっているかどうかを判定できるという付加機能を有する点において，我々の提案手法は新しい．
また，精度についても
他の単純な手法に対して比べて精度が高いという結果となった．



