特定の言語に依存しない一般的な形態素解析システムを考えてみる．
まず，最初に行う処理は，システムに与えられた解析対象文をトークンと呼ばれる文字列単位に分割する処理である．
この処理をトークン認識(tokenization)と呼ぶ．
トークンとは，英語では単語や数字や記号などに該当するが，厳密な定義は無い．
トークンを発展させた概念が本論文で提案する形態素片である．
[REF_MF]節で導入する．
語は一個以上の連続するトークンから構成されるもので用意された辞書のエントリとして存在するものと定義する．
語は形態素と呼ばれることもあるが，言語学的な形態素の定義とは異なるので別名を与えることにした．
次に行う処理は，認識されたトークンの列に対して形態素辞書の検索を行い，語を認識する処理である．
この処理を本論文では単に辞書検索と呼ぶ．
トークン認識と辞書検索は解析対象言語のコンピュータ上での表記の特性によって処理方法が異なる．
言語によって異なる場合もあるし，同じ言語でも清書法によって異なる場合もある．
本論文では「書かれた言語」を対象に形態素解析することを前提としているので，「言語」とは表記，清書法をも含む意味に捉えることにする．
本論文では，言語を表記の特性によって以下の二つに分類する．
わかち書きされる言語
わかち書きされない言語
わかち書きされる言語の例として英語があげられる．
英語では語は空白文字(whitespace)や記号文字(panctuation mark)によって区切られていると考えられることが多く，トークン認識は単純明解な処理とみなされあまり重要視されなかった．
しかし，いくつかの問題がある．
これについては[REF_1t=1l]節で説明する．
わかち書きされない言語の例として日本語や中国語などがあげられる．
単語の境界が視覚的にはっきりしていないという表記上の特性をもっている．
ゆえに，トークン認識は重要かつ困難な処理である．
トークンの処理はわかち書きされる言語とわかち書きされない言語ではまったく異なるとみなされてきた．
英語などのわかち書きされる言語では明白な境界が単語の両側にあるが，日本語や中国語などわかち書きされない言語では明白な単語境界を表すものが必ずしもあるとは言えない．
わかち書きされる言語とされない言語の違いは，言語の特性というよりも表記の特性によるものであり，清書法の方針による分類と言える[CITE]．
本章ではわかち書きされる言語とされない言語のトークン認識方法の違いに着目し，どちらの表記法にも適応できる統一的な枠組を提案する．
この統一的な枠組により，システムの最小限の変更(もしくは変更不要)とデータの入れ換えだけで様々な言語を同じシステムで解析できる．
また，複数の言語が混じった文章を解析することもできる．
わかち書きされない言語では，トークン認識は全ての文字をトークンとして認識すれば良い．
理論的には文中の全ての部分トークン列(この場合，部分文字列)を語の候補として考慮する必要がある．
また，語辞書により同じ字面の候補に複数の品詞候補が与えられることがある．
そのため，区切りの曖昧性と品詞付与の曖昧性の二種類の曖昧性が生じる．
わかち書きされる言語では，トークンは一意に決定され，かつ，1トークン= 1語という単純化を行うことが多く，その場合は品詞付与の曖昧性だけが生じる．
しかし，実際はわかち書きされる言語でも，必ずしも1トークン＝1語とみなすことが困難な場合がある．
これについては[REF_1t=1l]節で説明する．
このような，わかち書きされる言語とされない言語のトークン認識処理の違いは，辞書検索の方法に影響を与える．
わかち書きされる言語が語の両側に必ず明白な区切りを持つならば，辞書検索ではシステムはわかち書きされた文字列が語辞書に存在するかを問い合わせるだけでよい．
もし存在するなら品詞等の情報を辞書から得る．
一方，わかち書きされない言語では文は明白な単語区切りを持っていないため，辞書検索ではシステムは全ての部分トークン列(部分文字列)が語辞書に存在するかそれぞれ問い合わせる必要がある．
一般にわかち書きされない言語の辞書検索は，文中のある位置から始まる全ての語を辞書から一括で取り出す共通接頭辞検索(common prefix search)と呼ばれる手法が用いられる．
一般に共通接頭辞検索を効率的に行うためにTRIEというデータ構造を用いる．
図[REF_fig:trieJ]に日本語TRIE辞書の一部を示す．
TRIE構造は一回の問い合わせで文中のある位置からはじまる全ての語を返すことを保証しているので，効率的な辞書検索ができる．
例えば，「海老名へ行く」という文字列を図[REF_fig:trieJ]のTRIEで検索すれば，枝を一回たどるだけで，「海(名詞)」「海老(名詞)」「海老名(固有名詞)」という語が見つかる．
文中の全ての語を探す単純な方法は，文頭から一文字ずつ文字位置をずらしながら各位置で共通接頭辞検索を行うというものである．
しかし，AhoとCorasickにより提案されたAC法[CITE]を用いれば，入力文を一回スキャンするだけで入力文に含まれる全ての語候補を取り出すことができ，TRIEによる方法と比べ検索速度は格段に向上する．
Maruyama[CITE]はこのAC法を用いて日本語形態素解析の辞書検索の高速化を行っている．
だが，辞書のデータ格納領域が大きくなるという欠点がある．
本論文では前者のTRIEによる方法を用いて以降の解説を行う．
わかち書きされる言語でも，単語は常に明白な単語境界文字列で区切られているわけではない．
明白な単語境界を前提とした単純なトークン認識手法には限界がある．
そこで，本論文では，わかち書きされる言語をわかち書きされない言語と同じ方法で解析する方法を提案する．
英語を例に，問題点とその解決のための方針を述べる．
語の構成要素に記号文字が含まれている場合，区切りの曖昧性が生じてしまう．
``Mr.''や``Inc.''のように，ピリオドがトークンの末尾にある場合，それが文末記号なのか省略記号なのかという曖昧性がある．
これは，わかち書きされる言語の「文の認識」という大きな課題であり，Palmerら[CITE]によって研究されている．
アポストロフィがトークンの途中に含まれている場合も曖昧性が生じる．
所有を表す`` 's''が辞書にあれば解析文中の``John's''を``John'' + `` 's''という二つの語として認識したい．
しかし，``McDonald's''(固有名詞)が辞書にあれば解析文中の``McDonald's''を``McDonald'' + `` 's''ではなく一つの語としても解析したい．
それゆえ，区切りの曖昧性を生じることになる．
ハイフンの場合は，ハイフンでつながれた文字列が辞書にあればそれを候補としたいし(例：``data-base'' )，無ければハイフンを無視したい(例：``55-year-old'' )．
これらの問題は単純なパターンマッチでは対処できない．
そこで，我々はわかち書きされない言語での方法を適用する．
記号文字を含むトークン全てを記号文字で分割し(例：``McDonald's.''→``McDonald'' + `` ' '' + ``s'' + ``.
'')，分割されたトークン列に対して共通接頭辞検索で語辞書を検索する．
記号文字を含めた形で語が辞書に登録されていればそれも候補になる．
語の構成要素に空白文字が含まれている場合について考える．
空白文字などの明白な境界を利用した単純なトークン認識では，複合語固有名詞``South Shields''のような空白文字を含む語が扱えない．
このような場合``South''(固有名詞)と``Shields''(固有名詞)を辞書に登録して解析することが考えられる．
しかし，``South''(形容詞) + ``Shields''(名詞複数形)と認識されてしまう危険も高くなる．
``South Shields''(固有名詞)という空白文字を含んだ語を辞書に登録できれば，この種の誤りは減るであろう．
Penn Treebank[CITE]では，``New York''のような空白文字を含む固有名詞は``New''と``York''に分割され，それぞれに「固有名詞」という品詞タグが与えられている．
そのため，このコーパスから得られる語も空白文字を含まない分割されたものになってしまう．
このように空白文字を含んだまま単独の語として扱うべきものをわざわざ分割してしまうと，形態素解析処理における曖昧性の増加の要因になる．
この問題を解決するために，我々はわかち書きされない言語での方法を適用する．
これは，我々の知る限りWebsterら[CITE]により最初に提示されたアイディアである．
彼らはわかち書きされる語のイディオムや定型表現などを扱うため，空白文字を含む語の辞書登録を可能にし，トークン認識時にそれらをTable-look-up matchingという方法で検索をする．
我々は語辞書を共通接頭辞検索で検索するという方法により，空白文字を含む語を扱う．
これらの問題を同時に解決するためには，「記号文字，及び，空白文字で区切られた文字列単位をベースに語辞書を共通接頭辞検索」すれば良いという結論に達した．
そのためには，この文字列単位を，言語非依存性と処理効率を考慮しきちんと定義する必要がある．
これについては，次節で詳しく解説する．
[REF_1t=1l]節で述べたように，わかち書きされる言語は単語境界が明白であると考えられてきたにもかかわらず，単語内区切り曖昧性，複合語の問題がある．
このような問題を解決する素朴な方法として，わかち書きされる言語をわかち書きされない言語と同じとみなし，「わかち書きされない言語を解析する方法」で解析するという方法が考えられる．
英語を例に考えると，``They'vegonetoschooltogether.''という文の全てのスペース()を削除して，``They'vegonetoschooltogether.''という文を作り，これをわかち書きされない言語を解析する方法で解析すればよい[CITE]．
しかし，このような方法は``They/'ve/gone/to/school/to/get/her/.
''のような余計な曖昧性を含む結果を生んでしまう．
スペースを消した場合の影響を調べるため，簡単な精度測定実験を行った．
Penn Treebank[CITE]の128万形態素から学習されたパラメータ(品詞trigramによる状態遷移表と出現確率が付与された単語辞書)を用いたHMMベースの形態素解析システム\moz([REF_comp:moz]節を参照)で解析精度を計った．
テストデータは学習に用いた全データを使用した．
図[REF_fig:ORGvsNWS]に実験結果を示す．
スペースを削除した場合は区切りの曖昧性が発生するため，recallとpresicionで評価した．
区切りの曖昧性の影響で精度が落ちていることが分かる．
結果を細かく見てみると，``a way'', ``a head'', ``any more'', ``work force''のような複数の語の連続をそれぞれ``away'', ``ahead'', ``anymore'', ``workforce''のように一つの語として認識してしまう傾向にある．
recallよりもprecisionが高いのはこのためである．
また，``a tour'', ``a ton'', ``Alaskan or''を``at our'', ``at on'', ``Alaska nor''のように認識してしまう誤りもある程度見られた．
前者のような区切りの曖昧性はconjunctive ambiguity，後者のような区切りの曖昧性はdisjunctive ambiguityと呼ばれる[CITE][CITE]．
conjunctive ambiguityによる区切り誤りは11267個，disjunctive ambiguityによる区切り誤りは223個あった．
区切り曖昧性は精度以外にも性能に影響を与える．
文の全ての位置から検索ができるので検索回数が増え，それにともない候補となる語も増えるため，解析時間が増大してしまう．
実験ではスペースを削除した方法の解析時間は，そうでない場合の約5倍を要した．
これは重大な問題である．
より精度の高い効率的な解析を行うためには，わかち書きの情報を活かし，余計な曖昧性をできるかぎり排除できる単位を定義すべきである．
また，特定の言語に依存しないように考慮する必要がある．
本論文では，わかち書きされる言語のこのような問題を解決するために，効率的かつ洗練された方法を提案する．
それは，文中での「辞書検索を始めて良い位置・終えて良い位置」を言語ごとに明確に定義し，それを元に共通接頭辞検索で辞書検索するという方法である．
これは，わかち書きされない言語で採用されている方法を一般化したものである．
わかち書きされない言語では「文字」の境界が辞書検索を始めて良い位置・終えて良い位置となる．
このような「辞書検索を始めて良い位置・終えて良い位置」に囲まれた文字列を形態素片[CITE]と呼ぶ．
この形態素片を各言語ごとに定義すれば，本節冒頭の英語の例のような非論理的な曖昧性を含むことなく，わかち書きされる言語とされない言語を統一的に扱える．
形態素片は，言語非依存性や処理の効率を考慮してより厳密に定義されたトークンと言える．
ある言語の形態素片の集合は，その言語の辞書中の全ての語を構成できる文字列の最小集合と定義する．
ただし，デリミタと呼ばれる文字列集合は除く．
デリミタは文中で語の境界を表す空白文字などの文字列で，語の最初と最後には現れないものと定義する．
以降，``''と表記する．
英語ではアルファベットのみが連続する文字列，及び，全ての記号文字は形態素片であると定義できる．
それゆえ，単語中の記号文字で分断される各文字列も形態素片である．
例えば，英語文字列``they're''は``they'', `` ' '', ``re''の3つの形態素片から成る．
当然，複合語を構成する各単語も形態素片である．
辞書に``NewYork''や``New''というエントリがあれば，``New'', ``York''はそれぞれ形態素片であるが，``''はデリミタなので形態素片にはならない．
また，``York''といった文字列は定義により語にならない．
日本語，中国語などのわかち書きされない言語では全ての文字が形態素片になる．
図[REF_fig:MFrslt]に文から形態素片を認識した例を示す．
認識された形態素片は角括弧で囲って表されている．
[bt]
我々の方法では，わかち書きされない言語と同様にわかち書きされる言語の語辞書はTRIEに格納する．
形態素片が枝のラベルになる．
図[REF_fig:trieE]に形態素片ベースの英語TRIE辞書を示す．
図[REF_fig:MFrslt]の英語の例の[New]の位置から図[REF_fig:trieE]のTRIEを検索すれば，一回たどるだけで``New(形容詞)'', ``NewYork''(固有名詞)という二つの語が見つかる．
TRIE辞書構築時と形態素片認識処理時には，連続する二つ以上のデリミタは一つのデリミタと見なして処理を行う．
デリミタの連続には，特に言語的な意味は無いと仮定している．
形態素片は辞書を引き始める位置と引き終える位置を明確にし，わかち書きされる言語でも，効率的な共通接頭辞検索を可能にする概念である．
しかし，ある言語の形態素片の集合を過不足なく定義することは難しい．
そこで，我々は「ユーザが簡単に定義できる必要最小限の情報」のみを用いた疑似的な形態素片の定義法を提案する．
わかち書きされる言語である英語を例に考えてみると「デリミタと記号文字で区切られる文字(アルファベット)の連続」と「記号文字」の二種類が形態素片となり，形態素片認識にはデリミタと記号文字を定義する必要があることが分かる．
わかち書きされない言語である日本語や中国語を例に考えると，各「文字」が形態素片になり，「文字」を定義すれば良いことが分かる．
これらの考察により，次の3種類の情報を用いれば形態素片認識処理ができることが分かる．
文字の定義，及び，全ての文字が形態素片になりうるかどうかの区別
わかち書きされない言語では全ての文字が形態素片になりうる．
つまり，これは，わかち書きされる言語かされない言語かを区別する情報である．
デリミタ辞書
形態素片の境界として働き，それ自体は独立した形態素片にはならない文字列の辞書．
語の開始と終了位置にはデリミタは現れない．
形態素片辞書
形態素片となる特殊な文字・文字列の辞書．
記号文字のように形態素片の境界として働き，それ自体も形態素片として扱われる文字列などを格納する．
全ての文字が形態素片となるわかち書きされない言語では，(2),(3)は不要の場合が多い．
これらの情報の英語(Penn Treebank[CITE]のフォーマットに準拠)での定義例を示す．
1文字≠1形態素片(わかち書きされる言語)
デリミタ辞書:空白文字(\delimi)
形態素片辞書: [.
][,][:][;]['][-][MATH][$][%][MATH][n't]
Penn Treebankでは，``don't''などの縮約形は``do''と``n't''に分割されタグ付与されているので，形態素片辞書に[n't]が必要になる．
日本語での定義例を示す．
1文字＝1形態素片(わかち書きされない言語)
デリミタ辞書:空白文字(\delimi)
形態素片辞書:なし
日本語はわかち書きされない言語であるが，デリミタを定義しておくと，わかち書きした文の解析もできる．
わかち書きした日本語文は区切り曖昧性が減少する．
韓国語の通常の清書法では，句単位でわかち書きする．
これは，日本語の文節に相当する単位である．
わかち書きをする位置は，新国語表記法[CITE]によって定められているが，必ずしも完全に守られているわけではない[CITE]．
我々の視点では，韓国語は日本語のようなわかち書きされない言語に分類できる．
形態素片の定義例は上記の日本語のものをそのまま用いることができる．
しかし，わかち書きの境界の前後の品詞の分布には偏りがある．
平野ら[CITE][CITE]は，境界内部では品詞bigramを用い，境界を越えての連接には品詞trigram (境界も品詞の一つ)を用いることにより，わかち書き境界という情報をうまくとりこんでいる．
わかち書き境界がスペース(\delimi)で表されるとすれば，この場合の韓国語での定義例は次のようになる．
1文字＝1形態素片(わかち書きされない言語)
デリミタ辞書:なし
形態素片辞書:空白文字(\delimi)
もちろん，語辞書にスペース(\delimi)を登録し「わかち書き境界」などといった品詞を持たせておく必要がある．
ドイツ語はわかち書きされる言語であるが，複合名詞は区切りの曖昧性を持っている．
例えば，Staubeckenは，Stau-beckenと区切れば「貯水池」，Staub-eckenでは「ゴミ捨て場」という意味になる[CITE]．
このような区切り曖昧性を扱うためには，わかち書きされない言語として処理を行えば良い．
定義例をあげる．
1文字＝1形態素片(わかち書きされない言語)
デリミタ辞書:空白文字(\delimi)
形態素片辞書:なし
形態素片認識アルゴリズムは，文の先頭から末尾まで1バイトずつずらしながら，形態素片・デリミタを探して行くという単純なものである．
しかし，この方法による形態素片認識結果を用いれば全ての語を認識でき，実用上の問題は無い．
本章では，形態素解析システム内部の様々な処理をそれぞれコンポーネント化した設計・実装について述べる．
コンポーネント化は，形態素解析以外の用途への利用，特殊な機能の追加，言語に特化した処理の追加などに必須である．
本章では，これらの目的を念頭に置いた設計・実装の方針について解説する．
我々の考える言語非依存の形態素解析処理の流れを次に示す．
入力された解析対象文を形態素片列として認識し，辞書検索を簡単にする．
形態素片列に対し語辞書検索を行い品詞候補，及び，語自体のコストを与える．
語を区切り・品詞の曖昧性を保持したままトレリス(trellis)データ構造に格納する．
同時に状態遷移の情報もチェックし格納する．
トレリスから最適解(語の列)を選択する．
結果を出力する．
このような処理の流れに基づき各処理をコンポーネントに分割した設計を行い，形態素解析ツールキットLimaTKを実装した[CITE][CITE][CITE]．
図[REF_fig:TK]に示すようなコンポーネントから成り立っている．
全てのコンポーネントは独立しておりインターフェース等の仕様に従えば自作のコンポーネントと置き換えが可能である．
これらのコンポーネントのうちで言語依存性の高いものは，形態素片認識，辞書検索，未定義語処理である．
形態素片認識と辞書検索は形態素片の導入により，言語依存部分がほぼ解消されたと言える．
形態素片認識の実装については，[REF_tok]章で説明した．
辞書検索の実装については[REF_comp:dic]節，最適解選択の実装については[REF_comp:lattice]節，未定義語処理の実装については[REF_comp:udw]節で述べる．
[REF_comp:moz]節ではLimaTKを用いて実装した形態素解析システム\mozについて述べる．
辞書検索コンポーネントは形態素片列として認識された文から可能性のある語全てを辞書から獲得する．
これらの処理の詳細については，[REF_tok]章で既に述べた．
現在の辞書検索コンポーネントの実装について述べる．
語辞書のデータ構造であるTRIEを単純に実装すると大きなデータ領域が必要になる．
そこで現在は，データ格納領域が小く済む2種類の方法で実装している．
suffix array[CITE]を使うものと，パトリシア木[CITE]を使うものである．
パトリシア木はデータ消費量が若干大きいが高速であり，suffix arrayは若干低速であるがデータ消費量が小さいという特徴がある．
suffix arrayによる実装は高速文字列検索ライブラリ[CITE]を使用している．
語としてどのようなものを辞書に入れておくかということは言語や用途に依存した問題である．
例えば，英語で動詞イディオム``look up'' (``looking up'', ``looked up''なども)を語として辞書登録したいとする．
すると，以下の例の(1)では，登録された語が辞書検索の結果得られるが，(2)では分割されているのでイディオムとして認識されない．
I looked up the answer.
I looked the answer up.
Websterら[CITE]はこのような連続した文字列で表現できないイディオム，定型表現などを形態素解析処理の段階で扱うために，辞書検索とパージングの知識・処理を融合するという枠組を提案している．
コンポーネント化設計により，文字列として連続していない語の認識処理も，他のコンポーネントに影響を与えないように辞書検索コンポーネントなどの内部で実装できる．
しかし，我々はこのような言語の構造に関わる処理は形態素解析より後の高次の処理で扱うべきであると考える．
これは我々の目標が，形態素解析システムの単純化・効率化・言語非依存性を目指すことにあるためである．
我々はHMMによる最適解の選択方法を採用した．
最適解選択に必要なHMMパラメータはある程度の量の品詞タグ付きコーパスがあれば得られるので，特定の言語の解析が容易に始められるという利点がある．
具体的には，品詞タグ付きコーパスから，語と品詞N-gramをカウントし，シンボル出力確率(品詞別単語出現確率)と状態遷移確率(品詞間，または，状態と品詞間の遷移確率)を計算し，動的計画法の一つであるビタビ・アルゴリズムで出現確率最大の解を求める．
実際の実装は，積演算より和演算の方が効率的に処理できるという理由から[CITE]パラメータ(確率値)の逆数の対数に適当な係数をかけた整数値(コスト)を用いている．
コストの和演算で最適な解を選択する方法はコスト最小法とも呼ばれており[CITE][CITE]，JUMAN[CITE]，茶筌[CITE]といった日本語形態素形態素解析システムなどで採用されている．
つまり，和演算による実装は，これらのシステムで長年用いられてきた，人手によって調整されたコスト体系(単語コスト，接続コストなど)も利用できるという柔軟性を持っている．
特定の言語のために形態素解析を行うためにユーザが必要なものは，形態素片認識を行うための情報と，語と接続表(HMMパラメータ)だけである．
HMMパラメータは，十分な大きさの品詞タグ付きコーパスとユーザの望む統計モデル(bigram, trigram, variable memory model[CITE]など)に基づいた学習プログラムがあれば得られる．
形態素情報管理コンポーネントは，前述の方法により最尤解選択を行う．
これは形態素解析における解選択の一般的な実装方法である[CITE][CITE]．
文頭から文末へ向かって，一語ずつトレリス(ラティス)構造に格納してゆき，そのときにその語までの部分解析のコストを求める．
最適解は，文末から文頭へ向かって，最適な部分解析のコストを持つノードを順次辿れば得られる．
格納の際に必要になる，状態遷移(接続)にかかるコストと遷移先状態は，状態遷移表管理コンポーネントから得る．
状態遷移表管理コンポーネントは，現在の状態と次の品詞をキーに状態遷移のコストと遷移先状態を返すという単純な仕事をする．
未定義語処理コンポーネントは辞書に登録されていない語に対して品詞推定を行う．
未定義語の品詞推定は統計的な方法と人手による規則などのヒューリスティックを用いる方法がある．
統計的な方法は，未定義語が全ての品詞を持つと仮定し，トレリスでの曖昧性解消処理の段階で品詞N-gramの統計値により最適な品詞を自動的に選ぶという方法である[CITE]．
これは，言語に依存しない実装が可能である．
しかし，この方法ではデータ格納領域が増大してしまい，処理効率が悪い．
そこで，未定義語が全ての品詞を持つのではなく，あらかじめ「未定義語が推定されうる品詞」の集合を限定する方法が考えられる．
例えば「この言語の未定義語は『名詞』か『固有名詞』である」と定義すれば，曖昧性解消処理で未定義語の品詞はどちらかに選ばれる．
この方法は完全な推定とは言えないがデータ格納領域の増大を押えることができる現実的な方法であり，茶筌[CITE]で採用されている．
実用性と性能のバランスの良さから，LimaTKの未定義語処理コンポーネントの標準の機能として採用した．
ヒューリスティックによる方法は，例えば，英語ならば，「文中で大文字で始まるなら固有名詞」「-tionで終われば名詞」といった規則を用いて品詞を推定する方法である．
これは言語に依存する方法なので，言語ごとに処理系を実装する必要がある．
LimaTKではこのようなルールを埋め込むためには，統計的手法による未定義語処理コンポーネントを修正するか，まったく新しく作り直す必要があるが，作り直す場合でもインターフェース規約を守れば他の処理に影響を与えずに実装できる．
未定義語の長さ，すなわち，未定義語がいくつの形態素片で構成されるかを決定する処理も難しい．
理論的にはある位置から始まる全ての長さの部分形態素片列が未定義語の候補になる可能性がある．
しかし，これでは候補が増大してしまい処理効率に問題がある．
日本語のように字種にバリエーションのある言語は，連続する漢字列・カタカナ列・記号列などを一まとめにするといった字種によるまとめ処理により未定義語の候補を限定できる[CITE]．
このような字種による未定義語候補の決定処理はJUMAN[CITE]や茶筌[CITE]の様な日本語形態素解析システムに採用されている．
単純なまとめ処理ではなく，字種による語の長さの分布の違いに着目して未定義語処理を行うという研究もある[CITE]．
わかち書きされる言語ではこれまでこの問題は起こらなかった．
しかし，本研究では形態素片という概念を導入したため，わかち書きされる言語でも問題になるようになった．
未定義語に複合語は無いと仮定すれば，ある位置から始まり次のデリミタまで間の全ての部分形態素片列を未定義語の候補とすれば良い．
この仮定は正しいものではないが，実用上はさほど問題なく現実的である．
そもそも未定義語処理は言語依存性の高い処理であり，品詞推定精度の高い共通の枠組を構築するのは困難である．
より精度の高い処理を求めるユーザはやはりプログラムの調整を行う必要がある．
ゆえに，我々は各言語共通に利用できる最低限の機能と調整の行いやすい枠組で実装を行った．
言語非依存性とユーザの利便性と処理効率のバランスを考慮した実装と言える．
実装方針をあげておく．
未定義語の品詞推定:あらかじめ「未定義語が推定されうる品詞」の集合を定義し，最適解選択処理にまかせる
未定義語の長さの決定:「形態素片」「字種によるまとまり」「デリミタに挟まれた領域」を未定義語を構成する単位に選択でき，未定義語を構成する「最大単位数」も指定できる
[htb]
日本語
そんな感じがします．
そんな3075 226 3648 [Y:ソンナBF:そんなP:連体詞Pr:100/3864]感じ6960 198 1585 [Y:カンジBF:感じP:名詞-一般Pr:37/144546]が1306 59 744 [Y:ガBF:がP:助詞-格助詞-一般Pr:17509/82739]し0 117 2688 [Y:シBF:するP:動詞-自立/サ変・スル/連用形Pr:10638/10638]ます3048 73 1666 [Y:マスBF:ますP:助動詞/特殊・マス/基本形Pr:813/30431]．
1 11 2072 [PP:記号-句点Y:．
BF:．
P:記号-句点Pr:27418/27452]
英語
What is a word?
What 2208 20 1835 [P:WP Pr:218/3156] is 946 62 1373 [P:VBZ Pr:8789/27619] a 1207 28 1384 [P:DT Pr:25820/111243] word 6629 85 117 [P:NN Pr:59/179722] ? 3771 54 202 [P:.
Pr:556/53362]
中国語
人力基盤構築不能促成．
人力5929 28 28 [P:Na Pr:185/372140]基盤8286 28 28 [P:Na Pr:9/372140]構築6816 70 70 [P:VC Pr:17/106692]不能3989 78 78 [P:D Pr:1003/167440]促成8473 32 32 [P:VH Pr:2/105135]．
2007 31 31 [P:PERIODCATEGORY Pr:6046/79413]
LimaTKを用いて，簡単な多言語対応形態素解析システム\mozを作成した．
日本語，英語，中国語，韓国語など様々な言語を実装した．
図[REF_fig:mar]に解析結果の例を示す．
各行はそれぞれ一つの形態素を表し，各列は左から，「見出し文字列」「各形態素の持つコスト」「品詞コード」「状態コード」「解析には用いないその他の情報(角括弧で囲まれている)」となっている．
使用した言語データを次に示す．
RWCPの品詞タグ付きコーパス[CITE](約92万形態素)から品詞trigramモデルでパラメータ学習を行い，さらに茶筌[CITE]の語辞書エントリを追加した．
解析精度は，同様の方法で作成された解析用データを用いた茶筌のものと同等で，インサイドデータでRecall, Precisionとも97%程度である．
Penn Treebank[CITE]の品詞タグ付きコーパス(約128万形態素)から品詞trigramモデルで学習を行い，電子化テキスト版Oxford Advanced Learner's Dictionary[CITE]の辞書エントリを追加した．
語幹(stem)情報も同じくOxford Advanced Learner's Dictionaryから補完した．
解析精度はインサイドデータで97%，アウトサイドデータで95%程度である．
台湾の中央研究院の品詞タグ付きコーパス[CITE] (約210万形態素)から品詞bigramモデルで学習を行った．
解析精度はRecall, Precisionともインサイドデータで95%，アウトサイドデータで91%程度である．
未定義語品詞推定のチューニング，高次のN-gramの利用，スムージングなどより高度な統計的手法を用いれば，それに応じて精度は向上する．
しかし，本研究の目的は精度の向上ではないので，これ以上は追求しない．
