    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.3}
\usepackage[dvipdfm]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline
\usepackage{array}

\usepackage{amssymb} 

\Volume{21}
\Number{1}
\Month{March}
\Year{2014}

\received{2013}{9}{16}
\revised{2013}{11}{5}
\rerevised{2013}{12}{10}
\accepted{2013}{12}{27}

\setcounter{page}{61}

\jtitle{共変量シフトの問題としての語義曖昧性解消の領域適応}
\jauthor{新納　浩幸\affiref{Author_1} \and 佐々木　稔\affiref{Author_1}}
\jabstract{
本稿では語義曖昧性解消 (Word Sense Disambiguation, WSD) の領域適応が共
変量シフトの問題と見なせることを示し，共変量シフトの解法である確率密
度比を重みにしたパラメータ学習により，WSD の領域適応の解決を図る．共
変量シフトの解法では確率密度比の算出が鍵となるが，ここでは Naive
Bayes で利用されるモデルを利用した簡易な算出法を試みた．そし
て素性空間拡張法により拡張されたデータに対して，共
変量シフトの解法を行う．この手法を本稿の提案手法とする．BCCWJ コーパ
スの3つ領域 OC （Yahoo! 知恵袋），PB（書籍）及び PN（新聞）を選
び，SemEval-2 の日本語 WSD タスクのデータを利用して，多義語 16種類を
対象に，WSD の領域適応の実験を行った．実験の結果，提案手法
は Daum{\'e} の手法と同等以上の正解率を出した．本稿で用いた簡易な確率
密度比の算出法であっても共変量シフトの解法を利用する効果が高いことが
示された．より正確な確率密度比の推定法を利用したり，最大エントロピー
法の代わりに SVMを利用するなどの工夫で更なる改善が可能である．また教
師なし領域適応へも応用可能である．WSD の領域適応に共変量シフトの解法
を利用することは有望であると考えられる．
}
\jkeywords{語義曖昧性解消，領域適応，共変量シフト，Daum{\'e} の手法，BCCWJ コーパス}

\etitle{Domain Adaptations for Word Sense Disambiguation under the Problem of Covariate Shift}
\eauthor{Hiroyuki Shinnou\affiref{Author_1} \and Minoru Sasaki\affiref{Author_1}} 
\eabstract{
In this report, we show that the problem of domain adaptation for
word sense disambiguation (WSD) can be treated as a covariate shift
problem, and we try to solve it by maximizing the log-likelihood by
weighting the probability density ratio, which is the standard
solution of covariate shift.  The key to solving this problem lies
in the estimation of the probability density ratio.  We estimate the
probability density ratio using simple method employing the Naive
Bayes model.  In our proposed method, we apply the covariate shift
method to the training data expanded by the Daum{\'e}'s feature
augmentation method.  In the experiment, we solve six types of
domain adaptations for WSD using three domains, viz., OC (Yahoo!
Chiebukuro), PB (Book), and PN (Newspaper) in the BCCWJ corpus.  The
results show that our proposed method outperforms the Daum{\'e}'s
method.  This report shows that even our simple method of estimating
the probability density ratio is effective for use in the covariate
shift method.  In future, we intend to investigate and find a method
of estimating the probability density ratio more accurately.
Further, we intend to use the SVM instead of the maximum entropy
method.  Moreover, the method of covariate shift is also effective
for unsupervised domain adaptations and is a promising approach for
WSD domain adaptations.
}
\ekeywords{word sense disambiguation, domain adaptation, covariate shift, \\
	Daum{\'e}'s method, BCCWJ corpus}

\headauthor{新納，佐々木}
\headtitle{共変量シフトの問題としての語義曖昧性解消の領域適応}

\affilabel{Author_1}{茨城大学工学部情報工学科}{Department of Computer and Information Sciences, Ibaraki University}



\begin{document}
\maketitle


\section{期待損失最小化からみた共変量シフト}


対象単語\( w \)の語義の集合を\( C \)，また
\( w \)の用例\( \boldsymbol{x} \)内の\( w \)の語義を\( c \)と識別したときの
損失関数を\( l(\boldsymbol{x},c,d) \)で表す．\( d \)は\( w \)の語義を識別する分類器である．
\( P_T(\boldsymbol{x},c) \) をターゲット領域上の分布とすれば，
領域適応の問題における期待損失\( L_0 \)は以下で表せる．
\[
L_0 = \sum_{\boldsymbol{x},c} l(\boldsymbol{x},c,d) P_T(\boldsymbol{x},c)
\]
また\( P_S(\boldsymbol{x},c) \) をソース領域上の分布とすると以下が成立する．
\[
L_0 = \sum_{\boldsymbol{x},c} l(\boldsymbol{x},c) \frac{P_T(\boldsymbol{x},c)}{P_S(\boldsymbol{x},c)} P_S(\boldsymbol{x},c)
\]
ここで共変量シフトの仮定から
\[
\frac{P_T(\boldsymbol{x},c)}{P_S(\boldsymbol{x},c)} = \frac{P_T(\boldsymbol{x})P_T(c|\boldsymbol{x})}{P_S(\boldsymbol{x})P_S(c|\boldsymbol{x})} = \frac{P_T(\boldsymbol{x})}{P_S(\boldsymbol{x})}
\]
となり，\( r(\boldsymbol{x}) = P_T(\boldsymbol{x})/P_S(\boldsymbol{x}) \)とおくと以下が成立する．
\[
L_0 = \sum_{\boldsymbol{x},c} r(\boldsymbol{x}) l(\boldsymbol{x},c,d) P_S(\boldsymbol{x},c)
\]

訓練データを\( D = \{ (\boldsymbol{x_i},c_i) \}_{i = 1}^N \)とし，
\( P_S(\boldsymbol{x},c) \)を経験分布で近似すれば，
\[
 L_0 \approx  \frac{1}{N} \sum_{i=1}^N r(\boldsymbol{x_i}) l(\boldsymbol{x_i},c_i,d) 
\]
となるので，期待損失最小化の観点から考えると，共変量シフトの問題は以下の式\( L_1 \)を
最小にする\( d \)を求めればよいことがわかる．
\begin{equation}
L_1 = \sum_{i=1}^N r(\boldsymbol{x_i}) l(\boldsymbol{x_i},c_i,d) 
\label{eq:1}
\end{equation}


\section{重み付き対数尤度の最大化}


分類器\( d \)として以下の事後確率最大化推定に基づく識別を考える．
\[
d(\boldsymbol{x}) = \arg \max_{c} P_T(c|\boldsymbol{x})
\]
また損失関数として対数損失\( - \log P_T(c|\boldsymbol{x}) \)を用いれば，
\mbox{式(\ref{eq:1})}は以下となる．
\[
L_1 = - \sum_{i=1}^N r(\boldsymbol{x_i}) \log P_T(c|\boldsymbol{x_i}) 
\]
つまり，分類問題の解決に\( P_T(c|\boldsymbol{x},\boldsymbol{\lambda}) \)のモデルを導入するアプローチを取る場合，
共変量シフト下での学習では，確率密度比を重みとした以下に示す
重み付き対数尤度\( L(\boldsymbol{\lambda}) \)を最大化する
パラメータ\(\boldsymbol{\lambda}\)を求める形となる．
\begin{equation}
    L(\boldsymbol{\lambda}) = \sum_{i=1}^N r(\boldsymbol{x_i}) \log P(c_i|\boldsymbol{x_i},\boldsymbol{\lambda})        
     \label{eq:2}
\end{equation}

ここではモデルとして以下の式で示される最大エントロピー法を用いる．
\begin{equation}
P_T(c|\boldsymbol{x},\boldsymbol{\lambda}) = \frac{1}{Z(\boldsymbol{x},\boldsymbol{\lambda})} \exp \left(
\sum_{j=1}^M \lambda_j f_j(\boldsymbol{x},c)
\right)
     \label{eq:3}
\end{equation}
\( \boldsymbol{x} = (x_1,x_2,\cdots,x_M) \)が入力で\( c \)がクラスである．
関数\( f_j(\boldsymbol{x},c) \)は素性関数であり，実質\( \boldsymbol{x} \)の真のクラスが
\( c \)のときに\( x_j \)を返し，そうでないとき 0 を返す関数に設定される．
\( Z(\boldsymbol{x},\boldsymbol{\lambda}) \)は正規化項であり，以下で表せる．
\begin{equation}
  Z(\boldsymbol{x},\boldsymbol{\lambda}) = \sum_{c \in C} \exp \left(
\sum_{j=1}^M \lambda_j f_j(\boldsymbol{x},c) 
\right)
     \label{eq:4}
\end{equation}
\noindent
そして\( \boldsymbol{\lambda} = (\lambda_1,\lambda_2,\cdots,\lambda_M) \)が
素性に対応する重みパラメータとなる．

共変量シフト下ではない通常のケースでは，重みパラメータは最尤法から求める．
つまり，訓練データ\( D = \{(\boldsymbol{x_i},c_i)\}_{i=1}^N \)とすると，
以下の式\(  F(\boldsymbol{\lambda}) \)を最大にする\( \boldsymbol{\lambda} \)を求める．
\[
 F(\boldsymbol{\lambda}) = \sum_{i=1}^N \log P(c_i|\boldsymbol{x_i})
\]
これを各\( \lambda_j \)で偏微分し極値問題に直すと以下が成立する．
\[
\frac{\partial F(\boldsymbol{\lambda})}{\partial \lambda_j} =
\sum_{i=1}^N f_j (\boldsymbol{x_i},c_i) - 
\sum_{i=1}^N \sum_{c \in C} P_T(c|\boldsymbol{x_i},\boldsymbol{\lambda}) f_j(\boldsymbol{x_i},c) = 0
\]
これを勾配法などで解くことにより\( \boldsymbol{\lambda} \)が求まる．

共変量シフト下の学習では\mbox{式(\ref{eq:2})}の\( L(\boldsymbol{\lambda}) \)を最大にする\( \boldsymbol{\lambda} \)を求める．上記と全く同じ手順で，
\[
\frac{\partial L(\boldsymbol{\lambda})}{\partial \lambda_j} =
\sum_{i=1}^N r(\boldsymbol{x_i}) f_j (\boldsymbol{x_i},c_i) - 
\sum_{i=1}^N \sum_{c \in C} P(c|\boldsymbol{x_i},\boldsymbol{\lambda}) r(\boldsymbol{x_i}) f_j(\boldsymbol{x_i},c) = 0
\]
が得られる．これを勾配法などで解くことにより\( \boldsymbol{\lambda} \)が求まる．

今，事例\( \boldsymbol{x_i} \)の頻度を\( h_i \)とすると，尤度は以下となる．
\[
\prod_{i=1}^N P(c_i|\boldsymbol{x_i})^{h_i}
\]
対数を取れば以下が得られる．
\[
\sum_{i=1}^N h_i \log P(c_i|\boldsymbol{x_i})
\]

この式は重み付き対数尤度の\mbox{式(\ref{eq:2})}と同じ形なので，
実際に\( \boldsymbol{\lambda} \)を求めるためには，事例\( \boldsymbol{x_i} \)の頻度\( h_i \)を\( r(\boldsymbol{x_i}) \)と考えて，
最大エントロピー法のツールなどを用いればよい
\footnote{ただし利用できるツールは頻度を実数値として与えられるものでなくてはならない．
事例の重みを頻度の拡張として実装したツールであるともいえる．
本稿で用いた機械学習ツール Classias \cite{Classias}はこの条件を満たすため利用可能である．}．


\section{確率密度比の算出}


共変量シフト下の学習では確率密度比の算出が鍵である．
直接的には\( P_S(\boldsymbol{x}) \)と\( P_T(\boldsymbol{x}) \)を推定し，その比を取ればよいが，
\( P_S(\boldsymbol{x}) \)や\( P_T(\boldsymbol{x}) \)を正確に推定することは困難であり，
その比をとれば更に誤差が大きくなると予想できる．
そのため確率密度比を直接モデル化して求める手法が活発に研究されている\cite{sugiyama-2010}．

ただし本稿では簡易な手法を利用して確率密度比を算出することにした．
本稿の目的はこのような簡易な手法による確率密度比の算出法であっても，
WSD の領域適応の有力な解法になることを示すことである．

対象単語\( w \)の用例\( \boldsymbol{x} \)の素性リストを\( \{ f_1,f_2,\cdots, f_n \} \) とする．
求めるのは領域\( R \in \{S, T\} \)上の\( \boldsymbol{x} \)の分布\( P_R (\boldsymbol{x}) \)である．
ここでは Naive Bayes で使われるモデルを用いて算出する．
Naive Bayes のモデルでは以下を仮定する．
\[
P_R (\boldsymbol{x}) = \prod_{i=1}^{n} P_R (f_i) 
\]

領域\( R \)のコーパス内の\( w \)の全ての用例について素性リストを作成しておく．
ここで用例の数を\( N(R) \)とおく．
また\( N(R) \)個の用例の中で，素性\( f \)が現れた用例数を\( n(R,f) \)とおく．
MAP 推定でスムージングを行い，\( P_R (f) \)を以下で定義する\cite{takamura}．
\[
P_R (f) = \frac{n(R,f) + 1}{N(R) + 2}
\]

以上より，ソース領域\( S \)の用例\(\boldsymbol{x}\)に対して，
確率密度比\( r(\boldsymbol{x}) = \frac{P_T (\boldsymbol{x})}{P_S (\boldsymbol{x})} \)が計算できる．
ターゲット領域\( T \)の用例\(\boldsymbol{x}\)に対しては\( r(\boldsymbol{x}) = 1 \)とする．
また\( r_x < 0.01 \)となる用例\(\boldsymbol{x}\)は訓練データから削除した
\footnote{この削除は処理の効率化のために行っている．
また本稿の実験では削除しない場合よりもわずかによい結果となっていた．}．



\section{提案手法}


「関連手法」の節で素性空間拡張法を紹介した．素性空間拡張法は
データの表現を領域適応で効果が出るように拡張する手法である．そして拡張されたデータに対しては
任意の学習手法が利用できる．つまり拡張されたデータに対して，
共変量シフト下の学習も可能である．本稿では，素性空間拡張法に
より拡張されたデータに対して，4章で説明した共変量シフト下の学習を行うことを
提案手法する．

具体的に示す．素性空間拡張法により，ソース領域の訓練データ\( \boldsymbol{x_s} \)は
\( \boldsymbol{u_s} = (\boldsymbol{x_s},\boldsymbol{x_s},\boldsymbol{0}) \)という3倍の長さのベクトルに拡張され，
ターゲット領域の訓練データ\( \boldsymbol{x_t} \)は
\( \boldsymbol{u_t} = (\boldsymbol{0},\boldsymbol{x_t},\boldsymbol{x_t}) \)という3倍の長さのベクトルに拡張される．
ここで\( \boldsymbol{u_s} \)に対しては確率密度比\( r(\boldsymbol{x_s}) = P_T(\boldsymbol{x_s})/P_S(\boldsymbol{x_s}) \)の
重みをつけ，\( \boldsymbol{u_t} \)に対しては重み 1 をつける．
また\( P_T(c|\boldsymbol{u}) \)のモデルに最大エントロピー法を用い，
重み付き対数尤度を最大化するパラメータを求めることで，\( P(c|\boldsymbol{u}) \)を推定する．

上記の重み付き対数尤度の式（目的関数）を示しておく．
今，ソース領域の訓練データを\( D_s = \{ (\boldsymbol{x_s^{(i)}},c_s^{(i)}) \}_{i = 1}^n \)，
ターゲット領域の訓練データを\( D_t = \{ (\boldsymbol{x_t^{(i)}},c_t^{(i)}) \}_{i = 1}^m \)とおく．
また\( \boldsymbol{x_s^{(i)}} \)と\( \boldsymbol{x_t^{(i)}} \)を素性空間拡張法により
拡張したデータをそれぞれ\( \boldsymbol{u_s^{(i)}} \)と\( \boldsymbol{u_t^{(i)}} \)とおく．
ここで\( \boldsymbol{x_s^{(i)}} \)と\( \boldsymbol{x_t^{(i)}} \)は\( M \)次元，
\( \boldsymbol{u_s^{(i)}} \)と\( \boldsymbol{u_t^{(i)}} \)は\( 3M \)次元のベクトルであることに注意する．
提案手法の重み付き対数尤度の式は以下となる．
\begin{gather*}
L(\boldsymbol{\lambda}) = \sum_{i=1}^n r(\boldsymbol{x_s^{(i)}}) \log P(c_s^{(i)}|\boldsymbol{u_s^{(i)}},\boldsymbol{\lambda}) 
+ \sum_{i=1}^m \log P(c_t^{(i)}|\boldsymbol{u_t^{(i)}},\boldsymbol{\lambda})  \\
P(c|\boldsymbol{u},\boldsymbol{\lambda}) = \frac{1}{Z(\boldsymbol{u},\boldsymbol{\lambda})} \exp \left(
\sum_{j=1}^{3M} \lambda_j f_j(\boldsymbol{u},c)
\right) \\
Z(\boldsymbol{u},\boldsymbol{\lambda}) = \sum_{c \in C} \exp \left(
\sum_{j=1}^{3M} \lambda_j f_j(\boldsymbol{u},c) 
\right)
\end{gather*}





\section{考察}

\subsection{負の転移の有無}

WSD の領域適応では，対象単語毎に領域適応の問題が生じている．
実験では領域の組み合わせで 6通り，対象単語が 16単語あるので，
合計 96 ($= 6 \times 16$) 通りの領域適応の問題を扱ったことになる．
ここでは各領域適応の問題に対して負の転移が生じているかどうかを調べ，
それぞれのケースに分けて，各手法の正解率を調べた．

\begin{table}[b]
\caption{負の転移が生じていない領域適応}
\label{tab:funoteni}
\input{04table03.txt}
\end{table}

まず負の転移が生じているかどうかの判定には，
先の実験でより得られた
\verb|T-Only|，\verb|S-Only| 及び
\verb|S+T| の正解率を利用する．もしも正解率で以下の関係が成立しているなら，
負の転移が生じていないと考えられる．
\begin{center}
\verb|T-Only, S-Only  <  S+T| 
\end{center}
結果を\mbox{表\ref{tab:funoteni}}に示す．チェックがつけられた箇所が
負の転移が生じていない領域適応の問題である．96種類の領域適応の問題の中で
44種類において負の転移が生じていない．


次に負の転移が生じているかいないかのケースに分けて，各手法の平均正解率を調べた．
結果を\mbox{表\ref{tab:funoteni2}}に示す．

\begin{table}[t]
\caption{負の転移と各手法の平均正解率}
\label{tab:funoteni2}
\input{04table04.txt}
\end{table}

\mbox{表\ref{tab:funoteni2}}において領域適応に対処する 3 手法（Daum{\'e}，Cov-Shift，提案手法）を見ると，
提案手法は負の転移の有無に関わらず Cov-Shift よりも
高い正解率であり，提案手法は Cov-Shift の改良になっていることがわかる．
更に負の転移が生じていないケースでは Cov-Shift は Daum{\'e} よりも正解率が高く，
このケースでは素性に重みをつけるよりも事例に重みをつける方が
効果があることがわかる．
ただし負の転移が生じるケースでは，提案手法は Daum{\'e} よりも正解率が若干低い．
つまり提案手法を Daum{\'e} の手法の改良と見た場合，
負の転移が生じるケースでは正解率の低下を抑え，
その代わりに負の転移が生じないケースで正解率を高めることで，
全体的な正解率を改善する手法と見なせる．

また領域適応に対処しない 3 手法 (S-Only, T-Only, S+T)
も含めて比較すると，負の転移が生じるケースでは領域適応に対処する 3 手法（Daum{\'e}，Cov-Shift，提案手法）
の正解率はかなり悪い．
つまり WSD の領域適応では負の転移を検出することで大きな改善が期待できる．
共変量シフト下の学習では，負の転移が生じているケースに対しては，
ソース領域のデータに 0 に近い重みを与えられればよいはずである．
より正確な確率密度比の推定法を利用することで，
このような重み付けが可能だと考える．この点は今後の課題である．


\subsection{確率密度比の調整}

確率密度比を精度良く推定することは困難な問題である．
そのために求まった確率密度比を調整することも行われている．
杉山は
確率密度比\( r \)に\( p \) (\(0 < p < 1\)) 乗した\( r^p \)を重みにすることを
提案している\cite{sugiyama-2006-09-05}．
また Yamada は relative density ratio として確率密度比を
以下の形で求めることを提案してる\cite{yamada2011relative}．
\[
\frac{P_T(\boldsymbol{x})}{\alpha P_T(\boldsymbol{x}) + (1-\alpha)P_S(\boldsymbol{x})}
\]

ここでは\( r^{0.5} \)の重みと\( \alpha = 0.5 \)の relative density ratio を試した．
結果を\mbox{表\ref{tab:mitudohi-hikaku}}に示す．
\mbox{表\ref{tab:mitudohi-hikaku}}における提案手法と Cov-Shift は
\mbox{表\ref{tab:resultall}}における提案手法と Cov-Shift と同じものである．
\( r^{0.5}\)が \mbox{Cov-Shift} の重み\( r \)を 0.5 乗したものであり，
RDR が\( \alpha = 0.5 \)の relative density ratio である．

\mbox{表\ref{tab:mitudohi-hikaku}}をみると，
\( r^{0.5}\)や relative density ratio の調整は一部有効な問題もあったが，
全体として見ると，効果はあまりない．
これも本来の確率密度値\( P_S(\boldsymbol{x}) \)や\( P_T(\boldsymbol{x}) \)の推定が簡易すぎるために
生じていると考える．

\begin{table}[b]
\caption{確率密度比の調整による平均正解率}
\label{tab:mitudohi-hikaku}
\input{04table05.txt}
\end{table}

確率密度比を確率統計的により精緻に求めていくことは重要である．
ただし確率密度比は事例の重み，つまり事例の重要度を意味している．
事例の重要度という自然言語処理的な観点から WSD の領域適応に特化した
重みの設定も可能である．


\subsection{ SVM の利用}

本稿では学習アルゴリズムとして最大エントロピー法を用いた．
共変量シフトの解法として，重み付き対数尤度を最大化する形では，
\( P_T(c|\boldsymbol{x}) \)をモデル化するアプローチに限られる．
しかし共変量シフト下の学習では
確率密度比を重みにして期待損失を最小化すれば良いので，
損失関数ベースの学習手法が利用できる．
例えばヒンジ損失関数に密度比で重みづけすることで共変量シフト下の学習に
SVM を利用できる\cite{sugiyama-book}．
ただし SVM 自体の実装が容易ではないために簡単に試すことはできない．

\begin{table}[b]
\caption{SVM による平均正解率}
\label{tab:result-svm}
\input{04table06.txt}
\end{table}

ここでは共変量シフト下の学習に SVM を用いるのではなく，
素性空間拡張法により拡張されたデータに対して，SVM を利用してみる．
実行にはツールの libsvm\footnote{http://www.csie.ntu.edu.tw/\~{}cjlin/libsvm/}を用いた．
またそこで利用したカーネルは線形カーネルである．
実験結果を\mbox{表\ref{tab:result-svm}}に示す．

提案手法が本稿での提案手法での平均正解率であり，
D3-ME が素性空間拡張法と最大エントロピー法を利用した場合の平均正解率である．
つまり提案手法とD3-MEは，\mbox{表\ref{tab:resultall}}での提案手法と
Daum{\'e} に対応する．
そしてD3-SVM が素性空間拡張法と SVM を利用した場合の平均正解率である．
提案手法は D3-SVM よりもわずかに高い正解率となっているが，
その差は小さく識別能力については，提案手法と D3-SVM は同程度と言える．
また D3-SVM は  D3-ME よりも正解率が高い．
つまり最大エントロピー法ではなく，SVM を利用する方が正解率が高くなると予想できる．
このことから共変量シフト下の学習に SVM を利用すれば，
改善が可能であると考えられる．これは今後の課題である．


\subsection{教師なし手法への適用}

共変量シフト下での学習では訓練データの中にターゲット領域のデータが
含まれる必要はない．ターゲット領域の訓練データを含めなければ，
教師なし領域適応手法となるはずである．
この点を確認した実験を行った．
実験結果を\mbox{表\ref{tab:unsuper}}に示す．
表の S-Only の列はソース領域の訓練データだけで学習した結果である．
これは\mbox{表\ref{tab:resultall}}の S-Only に対応する．
W-S-Only はソース領域の訓練データのみを使った共変量シフト下での学習手法である．
また参考までに提案手法の結果も記している．

\begin{table}[b]
\caption{重み付き教師なし学習による平均正解率}
\label{tab:unsuper}
\input{04table07.txt}
\end{table}

確率密度比を用いる W-S-Only ではソース領域の
データへの重みが小さくなりがちである．
ここでの実験では重みが 0.01 未満の場合はそのデータを省いて学習させている．
そのために W-S-Only では極端にラベル付きデータが減少するケースがあった．
結果として精度が低くなってしまったと考えられる．
また多くの単語で正解率の低下が起こっていた．
この原因としては，重みのあるデータの欠如だと考える．
例えば，語義\( c_1 \)のデータ\( x_1 \)の重みが\( 0.01 \)，
語義\( c_2 \)のデータ\( x_2 \)の重みが\( 0.02 \)である場合，
どちらの重みも「小さく」，その差はほぼ等しいと見なして\( P(c_1) = P(c_2) = 0.5 \)と考えるのが妥当であるが，
「小さい」という点を考えないと\( P(c_1) = 1/3\), \(P(c_2) = 2/3 \)となってしまう．
「小さい」という点を考えるためには比較となるある程度「大きな」データが必要である．
例えば，上記の設定の上で語義\( c_1 \)のデータ\( x_3 \)の重みが 1 などという
データが存在すれば，\( P(c_1) = 101/103\), \(P(c_2) = 2/103 \)となり，
これは妥当である．つまり重みが低いデータが多数を占めるような場合，
信頼性のある推定が行えない．ある程度，重みのあるデータが必要だと思われる．
このため共変量シフト下での学習を教師なしの枠組みに単純に利用することは難しい．
教師なしの枠組みへの利用方法の検討は今後の課題である．


\end{document}
