\section{関連研究}


自然言語処理における領域適応は，帰納学習手法を利用する全てのタスクで生じる問題であるために，
その研究は多岐にわたる．
利用手法をおおまかに分類すると，ターゲット領域のラベル付きデータを利用するかしないかで分類できる．
利用する場合を教師付き領域適応手法，利用しない場合を教師なし領域適応手法と呼ぶ．
本稿における手法は教師付き領域適応手法の範疇に入るので，
ここでは提案手法に関連する教師付き領域適応手法の従来研究を述べる．

教師付き領域適応手法においては，一般に，ターゲット領域の知識は使えるだけ使えばよいはずなので，
ポイントはソース領域の知識の利用方法にある．
ソース領域とターゲット領域間の距離が離れすぎている場合，
ソース領域の知識を使いすぎると分類器の精度が悪化する現象がおこる．
これは負の転移\cite{rosenstein2005transfer}と呼ばれている．
負の転移を避けるには，本質的に，ソース領域とターゲット領域間の距離を測り，
その距離を利用してソース領域の知識の利用を制御する形となる．

Asch は品詞タグ付けをタスクとして領域間の類似性を測り，
その類似度から領域適応を行った際に精度がどの程度悪くなるかを予測できることを示した\cite{vanasch}．
張本は構文解析をタスクとしてターゲット領域を変化させたときの精度低下の要因を調査し，
そこから新たな領域間の類似性の尺度を提案している\cite{harimoto}．
Plank は構文解析をタスクとして領域間の類似性を測ることで，
ターゲット領域を解析するのに最も適したソース領域を選んでいる\cite{plank}．
Ponomareva \cite{ponomareva}や Remus \cite{rem2012}
は感情極性分類をタスクとして領域間の類似度を学習中のパラメータに利用した．
これらの研究はタスク毎に類似性を測るが，WSD がタスクの場合，
領域間の類似性は WSD の対象単語に依存していると考えられる．
古宮は対象単語毎に領域間の距離を含めた性質
\footnote{これら性質を全て含めて，領域間の類似性と呼べる．}によって適用する
学習手法を変化させている\cite{komiya3,komiya2,komiya-nlp2012}．

上記した古宮の一連の研究は広い意味でアンサンブル学習の一種である．
そこでアンサンブルされる各要素となる学習手法をみると
ソース領域のデータとターゲット領域のデータへの各重みが異なるだけである．
つまり領域適応においてはソース領域のデータとターゲット領域のデータへの各重みを調整して，
学習手法を適用するというアプローチが有力である．
Jiang \cite{jiang2007instance} は\( P_S(c|\boldsymbol{x}) \)と\( P_T(c|\boldsymbol{x}) \)との差が極端に大きいデータを
``misleading'' データとして訓練データから取り除いて学習することを試みた．
これは ``misleading'' データの重みを 0 にした学習と見なせるため，
この手法も重み付けの手法と見なせる．
本稿で利用する共変量シフト下での学習もこの範疇の手法といえる．

素性空間拡張法\cite{daume0}も重み付け手法である．
ただしデータではなくデータ中の素性に重みをつける．
そこではソース領域の訓練データのベクトル\( \boldsymbol{x_s} \)を
\( (\boldsymbol{x_s},\boldsymbol{x_s},\boldsymbol{0}) \)と連結した3倍の長さのベクトルに直し，
ターゲット領域の訓練データのベクトル\( \boldsymbol{x_t} \)を
\( (\boldsymbol{0},\boldsymbol{x_t},\boldsymbol{x_t}) \)と連結した3倍の長さのベクトルに直す．
ここで\( \boldsymbol{0} \)は\( \boldsymbol{x_s} \)や\(\boldsymbol{x_t}\)と同じ次元数であり，
しかもすべての次元の値が 0 であるようなベクトルである．

この3倍にしたベクトルを用いて，通常の分類問題として解く．
この手法は非常に簡易でありながら，効果が高い手法として知られている．
この拡張手法はソース領域とターゲット領域に共通している特徴が重なることで，
結果として共通している特徴の重みがつくことで領域適応に効果が出ると考えられる．

また領域適応の問題を共変量シフト下の学習を用いて解決する研究としては，
Jiang の研究\cite{jiang2007instance}と齋木の研究\cite{saiki-2008-03-27}がある．
Jiang は確率密度比を手動で調整し，モデルにはロジステック回帰を用いている．
また齋木は\( P(\boldsymbol{x}) \)を unigram でモデル化することで確率密度比を推定し，
モデルには最大エントロピー法のモデルを用いている．
ただしどちらの研究もタスクは WSD ではない．

また共変量シフト下では\( P_S(c|\boldsymbol{x}) = P_T(c|\boldsymbol{x}) \)を仮定するが，
\( P_S(\boldsymbol{x}|c) = P_T(\boldsymbol{x}|c) \)を仮定するアプローチもある．
この場合，ベイズの定理から
\begin{align*}
\arg \max_{c \in C} P_T (c|\boldsymbol{x}) & = \arg \max_{c \in C} P_T(c) P_T(\boldsymbol{x}|c) \\
                                   & = \arg \max_{c \in C} P_T(c) P_S(\boldsymbol{x}|c) 
\end{align*}
となるので領域適応の問題は\( P_T(c) \)の推定に帰着できる．
実際，Chan らは\( P_S (\boldsymbol{x}|c) \)と\( P_T (\boldsymbol{x}|c)\)の違いの影響は
非常に小さいと考え，\( P_S (\boldsymbol{x}|c) = P_T (\boldsymbol{x}|c)\)を仮定し，
\( P_T (c)\)を EM アルゴリズムで推定することで WSD の領域適応を
行っている\cite{chan2005word,chan2006estimating}．
更に新納らは\( P_S(\boldsymbol{x}|c) = P_T(\boldsymbol{x}|c) \)の仮定があったとしても，
コーパスのスパース性から単純に\( P_T(\boldsymbol{x}|c) \)を\( P_S(\boldsymbol{x}|c) \)で
置き換えることはできないと考え，\( P_T (c)\)の推定の問題と\( P_T(\boldsymbol{x}|c) \)
の推定の問題を個別に対処することを提案している\cite{shinnou-gengo-13}．



