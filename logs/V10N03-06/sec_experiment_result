入力，評価はSenseval-2日本語翻訳タスクのものに従った．
TMは320語のもの(1見出し語に対する用例数は約20)が2001年3月中旬に配布された．
この中から40語(名詞20語，動詞20語)がコンテストの対象単語として選択され，それぞれについて30語(30出現)ずつテストデータが用意された．
対象単語ののべ数は1,200語であった．
対訳単語辞書および対訳コーパスとしてはニフティで利用可能な英辞郎を用いた．
ここから対訳用例を抽出する際，日英見出し語が対応関係にないものを抽出してしまった場合でも，抽出の際に検索語として用いた日英見出し語が正しい対応関係にあると仮定して学習に用いた．
単言語コーパスとしては毎日新聞(1991年から2000年)，日経新聞(1995年から1999年)，産経新聞(1994年から1999年)，LDCデータ(1994年，1995年のデータでWall Street JournalやAP通信，ニューヨークタイムズなど数年分の新聞記事が含まれる)を用いた．
コンテストでは，手法1で類似度最大として選択された用例についてはその用例番号を，手法2で類似度最大として選択された用例集合についてはその英語見出し語を出力して提出した．
以下にその際の条件について述べる．
手法1における類似度の閾値は1.0，分割数の閾値は0とした．
手法2の形態素解析にはJUMAN [CITE]を用いた．
手法2における類似文としては，日本語用例に対しては，文末処理を施して得られる文字列を含む文を，英語用例に対しては，英語見出し語を含む文を抽出した．
機械学習モデルについては，時間の制約があったため，単語によっては学習が終了しない場合があり，クロスバリデーションにより最適なモデルを選択することはできなかった．
コンテストで最終的に選択したモデルの内訳は以下の通りであった．
SVM : 23単語(名詞12，動詞11)
DL : 12単語(名詞8，動詞4)
SB : 5単語(動詞5)
コンテストの結果を表[REF_tab:result]にあげる．
我々のシステムの精度は63.4 %であった．
単語ごとの精度と用例数，学習文数，クラス数との関係は表[REF_tab:result2]の通りである．
正解は各対象単語ごとにひとつあるいは複数与えられ，各正解には，対象単語の翻訳に適切であるかどうかを考慮した複数段階による評価が付与されている．
正解は以下の基準で◎，○，△の各段階に分けられた．
正解がTMの用例の場合
◎:翻訳に利用できる用例の場合．
日本語用例の品詞，時制，単複，微妙なニュアンス等は必ずしも一致しない．
○:評価単語のみに着目すれば妥当な訳語であるが，翻訳用例として使うことは望ましくない用例．
△:評価単語のみに着目すれば妥当な訳語であるが，翻訳用例として直接は使えない用例．
正解が翻訳の場合
◎:翻訳に利用できる場合．
品詞，時制，単複，微妙なニュアンス等は必ずしも一致しない．
○:評価単語のみに着目すれば妥当な訳語であるが，翻訳に使うことは望ましくない場合．
コンテストの結果は一番緩い評価基準で評価したものである．
一番緩い評価基準とは，正解をゆるくとる(上記の基準で，TMの◎，○，△，翻訳の◎，○をすべて正解とする)場合，一番厳しい評価基準とは，正解を厳しくとる(◎のみ)場合を意味する．
一番厳しい評価基準で我々のシステムの出力を評価した場合，全体の精度が50.6 %(607/1,200)，その内訳は，手法1の精度が82.0 %(82/100)，手法2が47.7 %(525/1,100)であった．
表[REF_tab:result]から，一番緩い評価基準で全体の精度を比べると，上位の二システムとは10 %程度以上の差があるが，一番厳しい評価基準では，我々のシステムの精度は50.6 %(607/1,200)で，AnonymY1システムの精度50.2 %(602/1,200)とほぼ同等であった．
また，一番緩い基準でも，名詞全体に対する精度は，我々のシステムの精度は69.3 %(416/600)で，AnonymY1システムの精度66.8 %(401/600)と同等以上の結果が得られている．
最も良かったAnonymXシステムの精度59.0 %(708/1,200)には遠く及ばなかったが，後の[REF_sec:models_and_accuracy]節に示すように，追加実験により我々の手法で62.4 %(749/1,200)の精度が得られ，潜在的には66.0 %(792/1,200)の精度が得られる可能性があることが分かったため，結果のみから判断すると，我々のシステムはAnonymXシステムと同程度以上の性能であるとも考えられる．
手法そのものについては，AnonymX, AnonymY1については具体的な方法が明かされていないため，現時点での比較はできない．
表[REF_tab:result2]から，クラス当たりの学習文数の少ない名詞と，クラス当たりの学習文数の非常に多い動詞の訳語選択精度が悪いといった傾向が見られる．
前者は学習データの不足が原因であると考えられる．
後者については，クラスの数が多く，日本語用例は似ているが異なるクラスに分類されているという場合もあり，また学習データが特定のクラスに偏っているということもなかったため，ベースラインの精度そのものが低い難しい問題であったと考えられる．
実際，すべての入力に対し対象単語ごとに常に学習データで最も学習文数の多いクラスを出力するシステムを作成して実験したところ，これらの単語に対する精度は低いことが分かった．
手法1はTMを最も単純に利用した方法であり，この手法による精度は高いことが望ましい．
実験(コンテスト)では手法1による精度は91.0 %(91/100)であった．
この手法により誤った例(正解と一致しなかった例)を表[REF_tab:error1]にあげる．
誤ったのは，入力文と日本語用例との類似性だけから推定することが困難だったためである．
類似度はすべて1であり，日本語用例そのものは類似していると思われるが，英語用例はそれぞれひとつずつしか与えられておらず，文脈からそれらの用例を翻訳として用いるのは不適切であると判断されたものと思われる．
手法1はTMの日本語用例の文字列情報のみを用いる方法であるため，このような場合，他の用例を適切に選択することはできない．
次に，手法1が適用された100対象単語に対し手法2を適用したときの精度を調べた．
結果は49.3 %(34/69)であった．
ただし，100語のうち31語は，TMの用例に含まれる英語見出し語ではなく対訳辞書を索いて得られた英語見出し語を選択したため評価していない．
仮にこれらがすべて正解だったとしても精度は65 %(65/100)となる．
したがって，手法1では，適用された語に対してはかなり良い精度が得られることが分かる．
手法1で正しく手法2で訳語選択を誤ったものは30語であり，それらの語を含む入力文には慣用的な表現が多く見られた．
そのうち，手法1によって適用された用例には次のようなものがあった．
以下で，＜＞内は見出し語を表わす．
＜胸＞を張る: to＜look proud＞
話に＜花＞が咲く: to engage in＜animated＞conversation
一役＜買う＞: to＜offer＞to help
調子に＜乗る＞: to be＜carried away＞
上記のような慣用的な対訳用例を含む用例集合は，その集合に含まれる用例数が少ないため学習データが不足し，手法2で適切に選択することは難しい．
このように予め学習データが少ないと分かったクラスつまり訳語/訳句候補は，慣用表現である可能性が高いと考え，個別にTMに用例を追加するなどしてTMを充実させるのが効果的である可能性が高い．
この場合，TMに用例を追加するだけでなく，表[REF_tab:error1]にあげたような手法1による誤りもなくす必要があるため，現状のTMを次の手順で変更する必要があると考えている．
各日本語用例の翻訳として可能なものはすべて登録する．
日本語用例が同一の用例が複数ありかつその用例を含む用例集合内の用例数が少ない場合は，日本語用例間に違いが出るまでそれぞれの用例の前後の文脈を伸ばす．
このようにTMの用例を変更することにより，表[REF_tab:error1]の誤りもほぼなくなると考えている．
手法2ではTMの用例だけでなく他の言語資源から抽出した用例も用いる．
もしTMの用例しか用いなければ，1クラスあたりの学習文数は平均1.4となる．
これでは機械学習をするにはデータが少な過ぎ，強力な学習モデルを用いても高い精度は期待できない．
ちなみに，コンテストに参加したシステムのうち，上位の4システム以外は，配布されたTM用例のみを用いていた．
最高のもので50 %程度の精度であり，他の言語資源を用いたことによる効果は10 %以上と考えられる．
我々の手法でもTM用例のみを用いた場合と他の言語資源を用いた場合の結果を比較したところ，他の言語資源を用いた場合の方が6 %から7 %程度良くなることが分かった．
他の言語資源を用いることの有効性については，詳しくは[REF_sec:data_and_accuracy]節で述べる．
補強した学習データでは，1クラスあたりの学習文数の平均は全体で60.9文(名詞44.2文，動詞75.3文)であった．
基本的に学習データが少ない語に対しては，さらに他の言語資源を利用してデータを追加すればよいと考えられる．
しかし，学習文数が平均より多いにもかかわらず精度が平均より悪かったものは，それぞれ名詞が3語(そのうちSVMが2語，DLが1語)，動詞が5語(そのうちSVMが1語，SBが4語)であり，この結果は，単純に学習データを増やしても精度が良くならない場合があることを示している．
1クラスあたりの学習データが多いにもかかわらず精度が良くなかった原因としては，以下のことが考えられる．
SBモデルと素性集合の相性(4例)
SBモデルによる精度はすべて悪かった．
これは実験に用いた素性集合と，すべての素性を独立と仮定して扱うSBモデルの性質が合わなかったためであると考えられる．
追加した学習データの質(4例)
学習データの多くは他の言語資源から抽出したものである．
コンテストでは，他の言語資源から対訳用例を抽出する際に，日英の見出し語が出現しているかどうかだけを手がかりにしていた．
そのため，日英見出し語が対応関係にないものも抽出していた．
例えば，haveやtake，lookなど一般に出現頻度が高く，日本語に訳したときその訳語に曖昧性のある単語が英語見出し語である場合には，見出し語間に対応関係がない対訳用例も多く抽出してしまう．
この単語対応を考慮していなかったことによる影響は，学習の際に顕著に現われる．
学習モデルにおけるクラスは英語見出し語で表わされる．
そのため，日英の見出し語間に対応がとられていないと，ひとつの用例に見出し語となり得る語が複数種類現われるとき，その用例の見出し語が特定できず曖昧になる．
その結果，同じ用例が複数のクラスの正例として用いられることになり，この用例を用いて学習したモデルでは，正しくクラスを分類できなくなる．
今回の実験で，SVMなどで学習が終了しなかったのは，このような例が多くあったことがひとつの原因であると考えている．
以上のような問題を解消し，精度を改善するには，次のような対策を講じる必要がある．
モデル，素性の選択方法を工夫する．
学習データを補強する際，他の言語資源から抽出した対訳用例における単語対応をとり，日英見出し語が対応関係にあるものだけを選択するようにする．
モデルの選択方法については，当初クロスバリデーションによるモデル選択を採用する予定であったが，コンテストの際には時間的な制約のため実現できなかった．
そこで，学習データでクロスバリデーションを行ない，平均精度が最大となるモデルを最適なモデルとして選択するようにし再実験を行なった．
クラスである英語見出し語は，評価，比較が容易になるようにTMの用例のみから選択した．
評価は次の二種類の評価方法で行なった．
見出し語による評価
正解の用例から日英見出し語を取り出し，これを用例番号の代わりに正解として用いて評価する．
コンテストの評価で，正解が翻訳のみからなると仮定した場合の評価方法に相当する．
例えば，図[REF_fig:tm_example2]において「sense id」で表される用例番号の代わりに「headword」で表される日本語見出し語と[MATH]ehead[MATH]/ehead[MATH]で囲まれた英語見出し語のペアを正解として用いる．
用例番号による評価
システムの出力を見出し語とする用例集合からランダムに用例を選び，その用例番号の正否で評価する．
コンテストの評価で，正解がTMの用例のみからなると仮定した場合の評価方法に相当する．
例えば，システムの入力が図[REF_fig:tm_example2]の「headword」で表される日本語見出しであり，出力が[MATH]ehead[MATH]/ehead[MATH]で囲まれた英語見出し語の場合に，この見出し語の代わりに「sense id」で表される用例番号をシステムの出力とする．
同じ見出し語を持つ用例が複数ある場合はランダムに選ぶ．
学習データの数は21,650，クラスの数は平均で11.0(441/40)であった．
学習データを先頭から順番に10個置きに同じ集合に含まれるよう分割し，各単語ごとに10分割のクロスバリデーションをして平均精度が最大となるモデルを選択した結果，選択されたモデルの内訳は次の通りであった．
ME : 21単語(名詞14，動詞7)
SVM : 12単語(名詞4，動詞8)
DL : 7単語(名詞2，動詞5)
結果は表[REF_tab:exp:cross_valid]の通りであった．
手法1での類似度および分割数の閾値としては，学習データに対する精度が最大になったときの値つまり1.0と0，および，テストデータに対して最大の精度が得られたときの値つまり0.8と1の二種類をあげた．
閾値が1.0と0のときの，単語ごとの精度と学習文数，クラス数との関係は表[REF_tab:result3]の通りである．
クラスである英語見出し語は，上述のようにTMの用例のみから選択しているため，表[REF_tab:result2]と表[REF_tab:result3]を単純に比較することはできない．
しかし，今回の追加実験で用いたクラスはコンテストのときに用いたクラスに包含されるため，コンテストの出力のうち追加実験で用いたクラスを出力したもののみを対象として評価した．
ここで対象となった単語は861語であり，コンテストのときの精度は評価方法1で61.8 %(532/861)，評価方法2で58.0 %(500/861)であり，追加実験での精度は評価方法1で68.4 %(589/861)，評価方法2で63.5 %(547/861)であった．
コンテストでモデル選択を行えていたら，5 %程度精度が良かった可能性がある．
二つの評価基準により精度が4 %程度異なるのは，コンテストで正解とされた用例における見出し語と同じものを含む用例が必ずしもすべて正解に含まれているとは限らないためである．
つまり，評価方法1より評価方法2の方が厳しい基準となっているためである．
例えば，「わがままを言わず，全力で頑張りたい」という入力文で対象単語が「言う」のとき，
＜言う＞までもない: to be needless to＜say＞
という用例は正解に含まれていたが，
＜言い＞たいことを言う: to have one's＜say＞
という用例は同じ見出し語「言う」と「say」を持つにも関わらず正解には含まれていなかった．
このような場合，評価方法1では「say」と回答しても正しいと評価されるが，評価方法2では，さらに用例を正しく選択して回答できないと正しいとは評価されない．
このような見出し語と正解用例とのずれが確認されたのは14単語についてであり，残りの26単語については見出し語を含む用例はすべて正解に含まれていた．
ずれがあった単語の内訳は，表[REF_tab:discrepancy]の通りである．
この表で，「ずれがあったもの」とは，テストの対象単語30出現のうち，正解用例の見出し語と同じものを含む用例がひとつでも正解に含まれなかった場合の数のことである．
このずれは，見出し語が同じでも文脈によって意味が違う場合があることを示している．
対象単語の翻訳に使える用例を選択するというタスクでは，訳語選択以上の意味的な曖昧性解消を要求していると言えるだろう．
この節では，各素性集合と精度との関係について述べる．
表[REF_table:exp:feature]に，実験に用いた素性集合の種類とそのときに得られた精度をあげる．
「機械翻訳モデル」の欄にはクロスバリデーションによって選択された機械学習モデルの数を表わす．
手法1での類似度および分割数の閾値はそれぞれ，学習データに対する精度が最大になったときの値つまり1.0，0とした．
括弧内の数字は，素性集合をすべて用いたときに得られた精度からの増減を表わす．
表[REF_table:exp:feature]から素性集合1は精度向上に貢献していることが分かるが，素性集合2と素性集合3は精度を下げる結果となっていたことが分かる．
これは，学習データの文数が平均49.1文(21,650/441)と少なく，過学習に陥ったためと考えられる．
この節では，複数の機械学習モデルから最適なモデルを選択した場合と，単独の機械学習モデルを用いた場合との違いについて述べる．
これまでの実験では，個々の単語に対し，複数の機械学習モデルからクロスバリデーションによりモデルを選択していたが，すべて単一の機械学習モデルを用いた場合との精度の違いが明らかではなかった．
そこで，手法2で各々の機械学習モデルをそれぞれ単独で用いた場合の実験を行なった．
素性としては，前節で最も良い精度が得られた素性集合1を用いた．
手法1における類似度と分割数の閾値は学習データで最適値となった1.0と0に設定した．
一番緩い基準と厳しい基準で評価した結果を表[REF_tab:exp:each_model1]と表[REF_tab:exp:each_model2]にあげる．
この表で，混合とは複数の機械学習モデルから最適なモデルを選択した場合を意味する．
混合(上限値)の行にあげた精度は，個々の単語ごとに，複数の機械学習モデルからテストデータで最も良い精度が得られるモデルを選択した場合の精度であり，複数のモデルを用いる場合の潜在的な上限値を意味している．
また，最頻とは常に学習データで最も学習文数の多いクラスを出力するモデルを意味する．
これらの結果から，これまでの実験で用いてきたクロスバリデーションによるモデル選択の方法は単独の学習モデル(SVM)を用いる方法に比べて劣ること，しかし，潜在的には複数のモデルを組み合わせることにより，より良い精度(5 %程度良い精度)が得られることが分かる．
この節では，他の言語資源から対訳用例を自動抽出して用いた場合の効果について述べる．
学習に，それぞれ，TM用例のみを用いた場合，他の言語資源から自動抽出した対訳用例のみを用いた場合，すべて用いた場合の三種類の比較実験を行なった．
訳語選択モデルとしては，これまでの実験で最も精度の良かった組み合わせのモデル，つまり，手法1(類似度と分割数の閾値はそれぞれ1.0と0に設定)とSVMの組み合わせに素性集合1を用いた場合のものを用いた．
結果を表[REF_table:exp:data]にあげる．
評価は一番緩い基準で行なった．
表[REF_table:exp:data]より，TM用例だけでなく，他の言語資源から自動抽出した対訳用例も用いた場合に，より精度が良くなることが分かる．
他の言語資源から対訳用例を抽出する際には，日英の見出し語が出現しているかどうかだけを手がかりにしていたため，日英見出し語が対応関係にないものも抽出してしまっていたが，現段階ではこの単語対応を考慮していなかったことによる悪影響よりも自動抽出した用例が精度向上へ貢献する度合いの方が顕著に勝っていると言えそうである．
今回用いたTMは新聞記事から抽出した語句を元に人手で作成されたものであり，コンテストの対象である新聞記事と同じ，特化したドメインの知識と考えられる．
一方，自動抽出した用例は一般的な対訳辞書の用例であり，一般的なドメインの知識であると考えれる．
表[REF_table:exp:data]の結果は，一般的なドメインの知識と特化したドメインの知識が相補的に影響した結果であるとも言えるだろう．
入力，評価はSenseval-2日本語翻訳タスクのものに従った．
TMは320語のもの(1見出し語に対する用例数は約20)が2001年3月中旬に配布された．
この中から40語(名詞20語，動詞20語)がコンテストの対象単語として選択され，それぞれについて30語(30出現)ずつテストデータが用意された．
対象単語ののべ数は1,200語であった．
対訳単語辞書および対訳コーパスとしてはニフティで利用可能な英辞郎を用いた．
ここから対訳用例を抽出する際，日英見出し語が対応関係にないものを抽出してしまった場合でも，抽出の際に検索語として用いた日英見出し語が正しい対応関係にあると仮定して学習に用いた．
単言語コーパスとしては毎日新聞(1991年から2000年)，日経新聞(1995年から1999年)，産経新聞(1994年から1999年)，LDCデータ(1994年，1995年のデータでWall Street JournalやAP通信，ニューヨークタイムズなど数年分の新聞記事が含まれる)を用いた．
コンテストでは，手法1で類似度最大として選択された用例についてはその用例番号を，手法2で類似度最大として選択された用例集合についてはその英語見出し語を出力して提出した．
以下にその際の条件について述べる．
手法1における類似度の閾値は1.0，分割数の閾値は0とした．
手法2の形態素解析にはJUMAN [CITE]を用いた．
手法2における類似文としては，日本語用例に対しては，文末処理を施して得られる文字列を含む文を，英語用例に対しては，英語見出し語を含む文を抽出した．
機械学習モデルについては，時間の制約があったため，単語によっては学習が終了しない場合があり，クロスバリデーションにより最適なモデルを選択することはできなかった．
コンテストで最終的に選択したモデルの内訳は以下の通りであった．
SVM : 23単語(名詞12，動詞11)
DL : 12単語(名詞8，動詞4)
SB : 5単語(動詞5)
コンテストの結果を表[REF_tab:result]にあげる．
我々のシステムの精度は63.4 %であった．
単語ごとの精度と用例数，学習文数，クラス数との関係は表[REF_tab:result2]の通りである．
正解は各対象単語ごとにひとつあるいは複数与えられ，各正解には，対象単語の翻訳に適切であるかどうかを考慮した複数段階による評価が付与されている．
正解は以下の基準で◎，○，△の各段階に分けられた．
正解がTMの用例の場合
◎:翻訳に利用できる用例の場合．
日本語用例の品詞，時制，単複，微妙なニュアンス等は必ずしも一致しない．
○:評価単語のみに着目すれば妥当な訳語であるが，翻訳用例として使うことは望ましくない用例．
△:評価単語のみに着目すれば妥当な訳語であるが，翻訳用例として直接は使えない用例．
正解が翻訳の場合
◎:翻訳に利用できる場合．
品詞，時制，単複，微妙なニュアンス等は必ずしも一致しない．
○:評価単語のみに着目すれば妥当な訳語であるが，翻訳に使うことは望ましくない場合．
コンテストの結果は一番緩い評価基準で評価したものである．
一番緩い評価基準とは，正解をゆるくとる(上記の基準で，TMの◎，○，△，翻訳の◎，○をすべて正解とする)場合，一番厳しい評価基準とは，正解を厳しくとる(◎のみ)場合を意味する．
一番厳しい評価基準で我々のシステムの出力を評価した場合，全体の精度が50.6 %(607/1,200)，その内訳は，手法1の精度が82.0 %(82/100)，手法2が47.7 %(525/1,100)であった．
表[REF_tab:result]から，一番緩い評価基準で全体の精度を比べると，上位の二システムとは10 %程度以上の差があるが，一番厳しい評価基準では，我々のシステムの精度は50.6 %(607/1,200)で，AnonymY1システムの精度50.2 %(602/1,200)とほぼ同等であった．
また，一番緩い基準でも，名詞全体に対する精度は，我々のシステムの精度は69.3 %(416/600)で，AnonymY1システムの精度66.8 %(401/600)と同等以上の結果が得られている．
最も良かったAnonymXシステムの精度59.0 %(708/1,200)には遠く及ばなかったが，後の[REF_sec:models_and_accuracy]節に示すように，追加実験により我々の手法で62.4 %(749/1,200)の精度が得られ，潜在的には66.0 %(792/1,200)の精度が得られる可能性があることが分かったため，結果のみから判断すると，我々のシステムはAnonymXシステムと同程度以上の性能であるとも考えられる．
手法そのものについては，AnonymX, AnonymY1については具体的な方法が明かされていないため，現時点での比較はできない．
表[REF_tab:result2]から，クラス当たりの学習文数の少ない名詞と，クラス当たりの学習文数の非常に多い動詞の訳語選択精度が悪いといった傾向が見られる．
前者は学習データの不足が原因であると考えられる．
後者については，クラスの数が多く，日本語用例は似ているが異なるクラスに分類されているという場合もあり，また学習データが特定のクラスに偏っているということもなかったため，ベースラインの精度そのものが低い難しい問題であったと考えられる．
実際，すべての入力に対し対象単語ごとに常に学習データで最も学習文数の多いクラスを出力するシステムを作成して実験したところ，これらの単語に対する精度は低いことが分かった．
手法1はTMを最も単純に利用した方法であり，この手法による精度は高いことが望ましい．
実験(コンテスト)では手法1による精度は91.0 %(91/100)であった．
この手法により誤った例(正解と一致しなかった例)を表[REF_tab:error1]にあげる．
誤ったのは，入力文と日本語用例との類似性だけから推定することが困難だったためである．
類似度はすべて1であり，日本語用例そのものは類似していると思われるが，英語用例はそれぞれひとつずつしか与えられておらず，文脈からそれらの用例を翻訳として用いるのは不適切であると判断されたものと思われる．
手法1はTMの日本語用例の文字列情報のみを用いる方法であるため，このような場合，他の用例を適切に選択することはできない．
次に，手法1が適用された100対象単語に対し手法2を適用したときの精度を調べた．
結果は49.3 %(34/69)であった．
ただし，100語のうち31語は，TMの用例に含まれる英語見出し語ではなく対訳辞書を索いて得られた英語見出し語を選択したため評価していない．
仮にこれらがすべて正解だったとしても精度は65 %(65/100)となる．
したがって，手法1では，適用された語に対してはかなり良い精度が得られることが分かる．
手法1で正しく手法2で訳語選択を誤ったものは30語であり，それらの語を含む入力文には慣用的な表現が多く見られた．
そのうち，手法1によって適用された用例には次のようなものがあった．
以下で，＜＞内は見出し語を表わす．
＜胸＞を張る: to＜look proud＞
話に＜花＞が咲く: to engage in＜animated＞conversation
一役＜買う＞: to＜offer＞to help
調子に＜乗る＞: to be＜carried away＞
上記のような慣用的な対訳用例を含む用例集合は，その集合に含まれる用例数が少ないため学習データが不足し，手法2で適切に選択することは難しい．
このように予め学習データが少ないと分かったクラスつまり訳語/訳句候補は，慣用表現である可能性が高いと考え，個別にTMに用例を追加するなどしてTMを充実させるのが効果的である可能性が高い．
この場合，TMに用例を追加するだけでなく，表[REF_tab:error1]にあげたような手法1による誤りもなくす必要があるため，現状のTMを次の手順で変更する必要があると考えている．
各日本語用例の翻訳として可能なものはすべて登録する．
日本語用例が同一の用例が複数ありかつその用例を含む用例集合内の用例数が少ない場合は，日本語用例間に違いが出るまでそれぞれの用例の前後の文脈を伸ばす．
このようにTMの用例を変更することにより，表[REF_tab:error1]の誤りもほぼなくなると考えている．
手法2ではTMの用例だけでなく他の言語資源から抽出した用例も用いる．
もしTMの用例しか用いなければ，1クラスあたりの学習文数は平均1.4となる．
これでは機械学習をするにはデータが少な過ぎ，強力な学習モデルを用いても高い精度は期待できない．
ちなみに，コンテストに参加したシステムのうち，上位の4システム以外は，配布されたTM用例のみを用いていた．
最高のもので50 %程度の精度であり，他の言語資源を用いたことによる効果は10 %以上と考えられる．
我々の手法でもTM用例のみを用いた場合と他の言語資源を用いた場合の結果を比較したところ，他の言語資源を用いた場合の方が6 %から7 %程度良くなることが分かった．
他の言語資源を用いることの有効性については，詳しくは[REF_sec:data_and_accuracy]節で述べる．
補強した学習データでは，1クラスあたりの学習文数の平均は全体で60.9文(名詞44.2文，動詞75.3文)であった．
基本的に学習データが少ない語に対しては，さらに他の言語資源を利用してデータを追加すればよいと考えられる．
しかし，学習文数が平均より多いにもかかわらず精度が平均より悪かったものは，それぞれ名詞が3語(そのうちSVMが2語，DLが1語)，動詞が5語(そのうちSVMが1語，SBが4語)であり，この結果は，単純に学習データを増やしても精度が良くならない場合があることを示している．
1クラスあたりの学習データが多いにもかかわらず精度が良くなかった原因としては，以下のことが考えられる．
SBモデルと素性集合の相性(4例)
SBモデルによる精度はすべて悪かった．
これは実験に用いた素性集合と，すべての素性を独立と仮定して扱うSBモデルの性質が合わなかったためであると考えられる．
追加した学習データの質(4例)
学習データの多くは他の言語資源から抽出したものである．
コンテストでは，他の言語資源から対訳用例を抽出する際に，日英の見出し語が出現しているかどうかだけを手がかりにしていた．
そのため，日英見出し語が対応関係にないものも抽出していた．
例えば，haveやtake，lookなど一般に出現頻度が高く，日本語に訳したときその訳語に曖昧性のある単語が英語見出し語である場合には，見出し語間に対応関係がない対訳用例も多く抽出してしまう．
この単語対応を考慮していなかったことによる影響は，学習の際に顕著に現われる．
学習モデルにおけるクラスは英語見出し語で表わされる．
そのため，日英の見出し語間に対応がとられていないと，ひとつの用例に見出し語となり得る語が複数種類現われるとき，その用例の見出し語が特定できず曖昧になる．
その結果，同じ用例が複数のクラスの正例として用いられることになり，この用例を用いて学習したモデルでは，正しくクラスを分類できなくなる．
今回の実験で，SVMなどで学習が終了しなかったのは，このような例が多くあったことがひとつの原因であると考えている．
以上のような問題を解消し，精度を改善するには，次のような対策を講じる必要がある．
モデル，素性の選択方法を工夫する．
学習データを補強する際，他の言語資源から抽出した対訳用例における単語対応をとり，日英見出し語が対応関係にあるものだけを選択するようにする．
モデルの選択方法については，当初クロスバリデーションによるモデル選択を採用する予定であったが，コンテストの際には時間的な制約のため実現できなかった．
そこで，学習データでクロスバリデーションを行ない，平均精度が最大となるモデルを最適なモデルとして選択するようにし再実験を行なった．
クラスである英語見出し語は，評価，比較が容易になるようにTMの用例のみから選択した．
評価は次の二種類の評価方法で行なった．
見出し語による評価
正解の用例から日英見出し語を取り出し，これを用例番号の代わりに正解として用いて評価する．
コンテストの評価で，正解が翻訳のみからなると仮定した場合の評価方法に相当する．
例えば，図[REF_fig:tm_example2]において「sense id」で表される用例番号の代わりに「headword」で表される日本語見出し語と[MATH]ehead[MATH]/ehead[MATH]で囲まれた英語見出し語のペアを正解として用いる．
用例番号による評価
システムの出力を見出し語とする用例集合からランダムに用例を選び，その用例番号の正否で評価する．
コンテストの評価で，正解がTMの用例のみからなると仮定した場合の評価方法に相当する．
例えば，システムの入力が図[REF_fig:tm_example2]の「headword」で表される日本語見出しであり，出力が[MATH]ehead[MATH]/ehead[MATH]で囲まれた英語見出し語の場合に，この見出し語の代わりに「sense id」で表される用例番号をシステムの出力とする．
同じ見出し語を持つ用例が複数ある場合はランダムに選ぶ．
学習データの数は21,650，クラスの数は平均で11.0(441/40)であった．
学習データを先頭から順番に10個置きに同じ集合に含まれるよう分割し，各単語ごとに10分割のクロスバリデーションをして平均精度が最大となるモデルを選択した結果，選択されたモデルの内訳は次の通りであった．
ME : 21単語(名詞14，動詞7)
SVM : 12単語(名詞4，動詞8)
DL : 7単語(名詞2，動詞5)
結果は表[REF_tab:exp:cross_valid]の通りであった．
手法1での類似度および分割数の閾値としては，学習データに対する精度が最大になったときの値つまり1.0と0，および，テストデータに対して最大の精度が得られたときの値つまり0.8と1の二種類をあげた．
閾値が1.0と0のときの，単語ごとの精度と学習文数，クラス数との関係は表[REF_tab:result3]の通りである．
クラスである英語見出し語は，上述のようにTMの用例のみから選択しているため，表[REF_tab:result2]と表[REF_tab:result3]を単純に比較することはできない．
しかし，今回の追加実験で用いたクラスはコンテストのときに用いたクラスに包含されるため，コンテストの出力のうち追加実験で用いたクラスを出力したもののみを対象として評価した．
ここで対象となった単語は861語であり，コンテストのときの精度は評価方法1で61.8 %(532/861)，評価方法2で58.0 %(500/861)であり，追加実験での精度は評価方法1で68.4 %(589/861)，評価方法2で63.5 %(547/861)であった．
コンテストでモデル選択を行えていたら，5 %程度精度が良かった可能性がある．
二つの評価基準により精度が4 %程度異なるのは，コンテストで正解とされた用例における見出し語と同じものを含む用例が必ずしもすべて正解に含まれているとは限らないためである．
つまり，評価方法1より評価方法2の方が厳しい基準となっているためである．
例えば，「わがままを言わず，全力で頑張りたい」という入力文で対象単語が「言う」のとき，
＜言う＞までもない: to be needless to＜say＞
という用例は正解に含まれていたが，
＜言い＞たいことを言う: to have one's＜say＞
という用例は同じ見出し語「言う」と「say」を持つにも関わらず正解には含まれていなかった．
このような場合，評価方法1では「say」と回答しても正しいと評価されるが，評価方法2では，さらに用例を正しく選択して回答できないと正しいとは評価されない．
このような見出し語と正解用例とのずれが確認されたのは14単語についてであり，残りの26単語については見出し語を含む用例はすべて正解に含まれていた．
ずれがあった単語の内訳は，表[REF_tab:discrepancy]の通りである．
この表で，「ずれがあったもの」とは，テストの対象単語30出現のうち，正解用例の見出し語と同じものを含む用例がひとつでも正解に含まれなかった場合の数のことである．
このずれは，見出し語が同じでも文脈によって意味が違う場合があることを示している．
対象単語の翻訳に使える用例を選択するというタスクでは，訳語選択以上の意味的な曖昧性解消を要求していると言えるだろう．
この節では，各素性集合と精度との関係について述べる．
表[REF_table:exp:feature]に，実験に用いた素性集合の種類とそのときに得られた精度をあげる．
「機械翻訳モデル」の欄にはクロスバリデーションによって選択された機械学習モデルの数を表わす．
手法1での類似度および分割数の閾値はそれぞれ，学習データに対する精度が最大になったときの値つまり1.0，0とした．
括弧内の数字は，素性集合をすべて用いたときに得られた精度からの増減を表わす．
表[REF_table:exp:feature]から素性集合1は精度向上に貢献していることが分かるが，素性集合2と素性集合3は精度を下げる結果となっていたことが分かる．
これは，学習データの文数が平均49.1文(21,650/441)と少なく，過学習に陥ったためと考えられる．
この節では，複数の機械学習モデルから最適なモデルを選択した場合と，単独の機械学習モデルを用いた場合との違いについて述べる．
これまでの実験では，個々の単語に対し，複数の機械学習モデルからクロスバリデーションによりモデルを選択していたが，すべて単一の機械学習モデルを用いた場合との精度の違いが明らかではなかった．
そこで，手法2で各々の機械学習モデルをそれぞれ単独で用いた場合の実験を行なった．
素性としては，前節で最も良い精度が得られた素性集合1を用いた．
手法1における類似度と分割数の閾値は学習データで最適値となった1.0と0に設定した．
一番緩い基準と厳しい基準で評価した結果を表[REF_tab:exp:each_model1]と表[REF_tab:exp:each_model2]にあげる．
この表で，混合とは複数の機械学習モデルから最適なモデルを選択した場合を意味する．
混合(上限値)の行にあげた精度は，個々の単語ごとに，複数の機械学習モデルからテストデータで最も良い精度が得られるモデルを選択した場合の精度であり，複数のモデルを用いる場合の潜在的な上限値を意味している．
また，最頻とは常に学習データで最も学習文数の多いクラスを出力するモデルを意味する．
これらの結果から，これまでの実験で用いてきたクロスバリデーションによるモデル選択の方法は単独の学習モデル(SVM)を用いる方法に比べて劣ること，しかし，潜在的には複数のモデルを組み合わせることにより，より良い精度(5 %程度良い精度)が得られることが分かる．
この節では，他の言語資源から対訳用例を自動抽出して用いた場合の効果について述べる．
学習に，それぞれ，TM用例のみを用いた場合，他の言語資源から自動抽出した対訳用例のみを用いた場合，すべて用いた場合の三種類の比較実験を行なった．
訳語選択モデルとしては，これまでの実験で最も精度の良かった組み合わせのモデル，つまり，手法1(類似度と分割数の閾値はそれぞれ1.0と0に設定)とSVMの組み合わせに素性集合1を用いた場合のものを用いた．
結果を表[REF_table:exp:data]にあげる．
評価は一番緩い基準で行なった．
表[REF_table:exp:data]より，TM用例だけでなく，他の言語資源から自動抽出した対訳用例も用いた場合に，より精度が良くなることが分かる．
他の言語資源から対訳用例を抽出する際には，日英の見出し語が出現しているかどうかだけを手がかりにしていたため，日英見出し語が対応関係にないものも抽出してしまっていたが，現段階ではこの単語対応を考慮していなかったことによる悪影響よりも自動抽出した用例が精度向上へ貢献する度合いの方が顕著に勝っていると言えそうである．
今回用いたTMは新聞記事から抽出した語句を元に人手で作成されたものであり，コンテストの対象である新聞記事と同じ，特化したドメインの知識と考えられる．
一方，自動抽出した用例は一般的な対訳辞書の用例であり，一般的なドメインの知識であると考えれる．
表[REF_table:exp:data]の結果は，一般的なドメインの知識と特化したドメインの知識が相補的に影響した結果であるとも言えるだろう．
