このタスクでは，単語の多義は翻訳(訳語/訳句)として定義された．
コンテストでは，予め日英のTMが訓練データとして配布された．
具体的には，TMでは，日本語見出し語に対して，それを含む日本語表現とその英語翻訳のペア(以下これを用例と呼ぶ)の集合が与えられた．
図[REF_fig:tm_example]がそのTMのサンプルである．
コンテストのテストでは，対象単語にマークのついたテスト文章が配布された．
参加者には，対象単語に対して，その翻訳に利用できるTMの用例番号，または，翻訳そのものを提出することが求められた．
翻訳の場合は，その語単独の翻訳でも，前後の適当な範囲の翻訳でも，文全体の翻訳でもよいものとされた．
テストの各対象単語には正解が用意された．
正解は必ずしもひとつではなく，複数の場合もある．
評価は，システムの出力のうち正しく推定できたものの割合(精度)により行なわれた．
システムの出力がTMの用例番号の場合は，その出力が正解のいずれかと一致するとき，正しく推定できたものと見なされた．
システムの出力が翻訳の場合は，すべての可能な翻訳を用意することは難しいため，その出力が正しいかどうかは人間の判断に委ねられた．
入力文と対象単語が与えられたとき，対象単語の適切な訳語を選択するタスクを考える．
そして，このタスクで，対象単語に関して入力文との類似度が最大となる用例あるいは用例集合を用いて対象単語の訳語を選択するモデルを考える．
本論文ではこのモデルを訳語選択モデルと呼ぶことにする．
以降では，原言語として日本語を，翻訳の目的言語として英語を仮定して説明する．
入力文と用例との類似度は次の二つの方法により求める．
文字列の類似性に基づく方法(手法1)
類似度は，入力文と日本語用例との間で一致した文字列に基づいて計算する．
機械学習モデルに基づく方法(手法2)
類似度は，対象単語の各訳語候補に対して機械学習モデルにより求められる確率値あるいは確信度と定義する．
入力文と対象単語が与えられたとき，まず手法1で対象単語に関して入力文との類似度が閾値以上となる用例があるかどうかを調べ，ある場合はその類似度が最大となる用例の番号あるいはその用例の英語見出し語を出力し，ない場合は，手法2で対象単語に関して入力文と最も類似した用例集合を選択し，その英語見出し語を出力する．
以降で，各方法について詳細に述べる．
対象単語に関して入力文との類似度が高い日本語用例があれば，TMを信頼しその用例の番号あるいはその英語見出し語を出力する．
入力文と一致する割合を調べる際，日本語用例には文末処理(句末の場合も含む)を施しておく．
文末処理としては，機能語，動詞や形容詞の活用部分，サ変動詞をすべて削除するということを行なった．
例えば，図[REF_fig:tm_example]の用例にこの文末処理を施すとそれぞれ，「母に遠慮」「母への遠慮」「献金を遠慮」となる．
入力文との一致する割合は，動的計画法により入力文と日本語用例を文字単位で比較して差異を求め，一致した文字列の割合として求める．
実験では，この差異をUNIXのdiffコマンドを用いて求めた．
類似度は以下の式により求める．
このとき，用例が複数の部分に分割されて一致する場合があり，類似度が大きくても多くの部分に分割されてしまう場合は類似用例としてふさわしくない場合が多い．
そこで，分割数に閾値を設け，閾値より分割数が多い用例は選択対象外とする．
類似度が最大となる用例が複数ある場合には，最長の日本語用例を持つ用例の番号を返す．
ただし，一致した部分が日本語見出し語の長さより長い場合に限る．
しかしながら，TMに全ての可能な用例を登録することは難しく，常に入力文と表層的にほぼ同じものがあることは期待できないため，類似度が最大となる用例が常に訳語選択に最適な用例であるとは限らない．
そこで，類似度に閾値を設け，閾値以上の類似度を持つ用例がない場合は次節に述べる方法を用いる．
入力文と表層的にほぼ同じ用例がない場合，より多様な情報を用いて類似度を求める必要があると考えられるが，そのために複雑な規則を作成するのは避けたいため，類似度の計算には機械学習モデルを用いることにした．
機械学習モデルによって分類するクラスは対象単語の訳語/訳句候補とした．
訳語選択モデルは，[REF_sec:introduction]節でも述べたように，同じ日英対訳単語ペアを持つ対訳用例をまとめてひとつの用例集合とし，そのペアの日本語単語が同じである用例集合をまとめ，そのまとまりごとに作成する．
したがって，各モデルでは，同じ見出し語を持つ用例は同じクラスとなり，入力文に対しすべて同じ類似度となる．
そして，日英の見出し語つまり各用例集合内で共通する対訳単語ペアのうち，日本語見出し語は共通で，英語見出し語が訳語/訳句候補となるため，各用例集合の英語見出し語がモデルにより分類するクラスとなる．
TMでは見出し語は予め人手で与える．
例えば，図[REF_fig:tm_example]の場合，日本語見出し語は「遠慮」であり，英語見出し語はそれぞれ，「feel constrained」，「constraint」，「refrain」となる．
これらを[MATH]ehead[MATH]/ehead[MATH]のタグで明示すると図[REF_fig:tm_example2]のようになる．
英語見出し語が動詞の場合はすべての語尾変化形を基本形で代表させる．
さらに，TMの各日本語見出し語を対訳辞書で索き，TMになかった訳語/訳句候補が見つかれば，それらも新たなクラスとして追加する．
学習には，TMの用例だけでなく，他の対訳辞書あるいは対訳コーパスから抽出した用例も用いる．
抽出する用例は，TMの各用例集合と同じ日英見出し語を含む対訳用例とし，抽出した用例はTMの各用例集合に追加する．
以降で，用例数および学習文数は，ともに各日本語見出し語に対しその語を含む用例の数を意味するものとし，TMに最初に含まれていた用例の総数を用例数，他の言語資源から抽出して追加した後の用例の総数を学習文数と呼んで区別する．
また，クラス数とは各日本語見出し語に対するクラスの数つまり，訳語/訳句候補の種類の数を意味するものとする．
機械学習モデルとしてはSVM (Support Vector Machine)，ME (Maximum Entropy)，DL (Decision List)，SB (Simple Bayes)を用いる．
日本語見出し語ごとに，各モデルを用いて学習データでクロスバリデーションを行ない，平均精度が最も高いモデルをテストに用いる．
各クラスの確率値あるいは確信度は基本的に，文脈の集合を[MATH]，クラスの集合を[MATH]とするとき，文脈[MATH]でクラス[MATH]となる事象[MATH]の確率分布[MATH]として求められる．
SVMではこのような確率分布は得られないが，便宜的に最適のクラスに対して確信度を1，その他のクラスに対して0とする．
次に，各機械学習モデルの説明，各種パラメータ等の設定について述べる．
このモデルでは，ベイズの定理に基づき，文脈[MATH]のときにクラス[MATH]が生起する確率を推定する．
そして，確率値が最も大きいクラスを最適なクラスとする．
文脈[MATH]のときにクラス[MATH]が生起する確率は次の式で与えられる．
{
}ここで文脈[MATH]は，予め設定しておいた素性[MATH]の集合である．
[MATH]は，文脈[MATH]の生起確率で，今回の場合，クラス[MATH]には依存せず定数のため計算しない．
[MATH]と[MATH]は，ともに学習データから推定される確率で，それぞれ，クラス[MATH]の出現の確率，クラス[MATH]のときに素性[MATH]を持つ確率を意味する．
最尤推定により求めた[MATH]の値は0になることが多く，式([REF_eq:simple_bayes])の値が0になり本来求めるべきクラスが正しく求まらない場合が多い．
このため，本論文では次の式によりスムージングを行なう．
{
}ここで，[MATH]と[MATH]は，それぞれ，素性[MATH]を持ちかつクラスが[MATH]である事例の個数，クラスが[MATH]である事例の個数を意味する．
[MATH]は実験で定める定数であり，実験では0.0001に固定した．
このモデルでは，素性[MATH]とクラス[MATH]の組を規則として，予め定めた優先順序でリストに蓄えておき，リストで優先順位の高いところから，入力と素性が一致する規則を適用してクラスを求める[CITE]．
本論文では優先順序として次の式で表わされるものを用いる．
{
}これは，ある文脈[MATH]でクラス[MATH]を出力する確率[MATH]がもっとも高いクラス[MATH]を解とすることと等価であり，本論文では次の式を用いて最適なクラスを特定する．
{
}ここで，[MATH]は次の式によって与えられる．
{
}また，[MATH]は学習データで素性[MATH]を文脈とするクラス[MATH]の出現の割合である．
このモデルでは，素性[MATH]の集合を[MATH]とするとき，式([REF_eq:constraint])を制約とし，式([REF_eq:entropy])で表わされる目的関数つまりエントロピーを最大にするような確率分布[MATH]を求め，その確率分布にしたがって求まる各クラスの確率のうち，最も大きい確率値を持つクラスを最適なクラスとする[CITE]．
{
} {
}ただし，[MATH]はそれぞれクラスと文脈の集合を意味し，[MATH]は文脈[MATH]に素性[MATH]があってかつクラスが[MATH]の場合1となりそれ以外で0となる二値関数である．
また，[MATH]は，既知データでの[MATH]の出現の割合を意味する．
サポートベクトルマシンとは，空間を超平面で分割することにより2つのクラスからなるデータを分類する二値分類器のことである．
2つのクラスを正例，負例とすると，学習データにおける正例と負例の間隔(マージン)を最大にする超平面を求めそれを用いて分類を行なう．
通常は，学習データにおいてマージンの内部領域に少数の事例が含まれてもよいとする拡張(ソフトマージン)や，超平面の線形の部分を非線型とする拡張(カーネル関数の導入)などがなされたものが用いられる．
これらの拡張によりクラスを判別することは，以下の識別関数の出力値が正か負かによってクラスを判別することと等価である[CITE]．
{
}ここで[MATH]は識別したい事例の文脈(素性の集合)を，[MATH]と[MATH]は学習データの文脈とクラスを意味する．
また，関数[MATH]は，[MATH]のときに1，[MATH]のときに[MATH]となる二値関数であり，各[MATH]は式([REF_eq:svm5])と式([REF_eq:svm6])の制約のもと式([REF_eq:svm4])の[MATH]を最大にするものである．
{
} {
} {
}また，関数[MATH]はカーネル関数と呼ばれ様々なものが提案されているが，本論文では次の式で表わされる多項式カーネルを用いる．
{
}ここで，[MATH]は実験的に設定される定数である．
本論文では[MATH],[MATH]はそれぞれ1と2に固定した．
サポートベクトルマシンは二値分類器であるため，クラスの数が2であるデータしか扱えないが，これにペアワイズ手法を組み合わせることにより，クラスの数が3以上のデータを扱えるようになる[CITE]．
ペアワイズ手法とは，N個のクラスを持つデータの場合，異なる二つのクラスのあらゆるペア(N(N-1)/2個)を作り，各ペアごとにどちらがよいかをサポートベクトルマシンなどの二値分類器で求め，最終的にN(N-1)/2個の二値分類器のクラスの多数決により，最適なクラスを求める方法である．
実験では，サポートベクトルマシン(TinySVM[CITE]を利用)とペアワイズ手法を組み合わせて用いた．
上述のように文脈[MATH]は素性の集合で表わされる．
実験に用いた素性は以下のものである．
形態素情報(素性集合1)
入力文における対象単語の前後三形態素ずつについての文字列，基本形，品詞(大分類，細分類)，活用型，活用形．
最大一致となる用例に関する情報(素性集合2)
入力文の文字列と連続して一致する部分が最大となる用例を調べ，その用例の英語見出し語および一致した長さをそれぞれ素性として用いる．
内容語とその訳語候補の出現頻度(素性集合3)
まず，各英語見出し語(各クラス)ごとに次の六種類の文集合を定義する．
文集合1 :該当する英語見出し語を含む用例において日本語用例を取り出した集合
文集合2 :該当する英語見出し語を含む用例において英語用例を取り出した集合
文集合3 :文集合1の類似文の集合．
類似文は日本語の単言語コーパスから抽出する．
文集合4 :文集合2の類似文の集合．
類似文は英語の単言語コーパスから抽出する．
文集合5 :文集合1と文集合3の和集合
文集合6 :文集合2と文集合4の和集合
ある用例の類似文とは，その用例の見出し語とその単語のまわりの文脈の一部を含む文とする．
入力文における各内容語とその訳語候補について，上記の各文集合における出現頻度を調べ，それぞれ素性として用いる．
内容語は入力文を形態素解析したときに得られる単語のうち，その品詞が名詞，動詞，形容詞，副詞，連体詞であるものとする．
ただし，対象単語は除く．
内容語の訳語候補は内容語を対訳辞書で索いたときに候補としてあがる単語とする．
この素性は文集合，英語見出し語，内容語の出現頻度の和，の組み合せによって表わされ，頻度の和がn回の場合，頻度1からnまでの素性値をもつ素性がすべて観測されたと仮定する．
頻度は1以上のもののみ考慮する．
例えば，入力文に見出し語以外の内容語がひとつあり，その内容語がクラス「buy」の文集合1に3回出現した場合には，「文集合1：buy：1」，「文集合1：buy：2」，「文集合1：buy：3」の素性が観測されたとする．
この素性により，日英の各コーパスにおいて見出し語と共起する単語の頻度を訳語選択の手がかりとして考慮する．
