



\documentstyle[epsbox,jnlpbbl]{jnlp_j_b5}

\setcounter{page}{163}
\setcounter{巻数}{7}
\setcounter{号数}{4}
\setcounter{年}{2000}
\setcounter{月}{10}
\受付{2000}{2}{23}
\再受付{2000}{4}{20}
\採録{2000}{6}{30}

\setcounter{secnumdepth}{2}

\title{コーパスからの語順の学習}
\author{内元 清貴\affiref{CRL} \and 村田 真樹\affiref{CRL} 
  \and 馬 青\affiref{CRL} \and 関根 聡\affiref{NYU} 
  \and 井佐原 均\affiref{CRL}}

\headauthor{内元，村田，馬，関根，井佐原}
\headtitle{コーパスからの語順の学習}

\affilabel{CRL}{郵政省通信総合研究所}
{Communications Research Laboratory, Ministry of Posts and Telecommunications}
\affilabel{NYU}{ニューヨーク大学 コンピュータサイエンス学科}
{Computer Science Department, New York University}

\jabstract{
  本論文では，日本語の語順の傾向をコーパスから学習する手法を提案する．
  ここで語順とは係り相互間の語順，
  つまり同じ文節に係っていく文節の順序関係を意味するものとする．
  我々が提案する手法では，文節内外に含まれるさまざまな情報から
  語順の傾向を自動学習するモデルを用いる．
  このモデルによって，それぞれの情報が語順の決定にどの程度寄与するか，
  また，どのような情報の組み合わせのときにどのような傾向の語順になるか
  を推測することができる．
  個々の情報が語順の決定に寄与する度合は
  最大エントロピー(ME)法によって効率良く学習される．
  学習されたモデルの性能は，そのモデルを用いて語順を決めるテストを行ない，
  元の文における語順とどの程度一致するかを調べることによって
  定量的に評価することができる．
  正しい語順の情報はテキスト上に保存されているため，
  学習コーパスは必ずしもタグ付きである必要はなく，
  生コーパスを既存の解析システムで解析した結果を用いてもよい．
  本論文ではこのことを実験によって示す．
}

\jkeywords{語順，コーパス，学習，最大エントロピーモデル，生成}

\etitle{Word Order Acquisition from Corpora}
\eauthor{Kiyotaka Uchimoto\affiref{CRL} \and Masaki Murata\affiref{CRL} 
  \and Qing Ma\affiref{CRL} \and Satoshi Sekine\affiref{NYU} 
  \and Hitoshi Isahara\affiref{CRL}} 

\eabstract{
  In this paper we propose a method for acquiring word order from corpora. 
  We define word order as the order of modifiers or 
  the order of bunsetsus which depend on the same modifiee. 
  The method uses a model which automatically discovers what the tendency of 
  the word order in Japanese is by using various kinds of information in 
  and around the target bunsetsus. 
  It shows us to what extent each piece of information contributes to 
  deciding the word order and which word order tends to be selected 
  when several kinds of information conflict. 
  The contribution rate of each piece of information in deciding word order 
  is efficiently learned by a model within a maximum entropy (ME) framework. 
  The performance of the trained model can be evaluated 
  by checking how many instances of word order selected by the model 
  agree with those in the original text. 
  A raw corpus instead of a tagged corpus can be used to train the model, 
  if it is first analyzed by a parser. This is possible because text in the 
  corpus is in the correct word order. In this paper, we show that this is
  indeed possible.
}

\ekeywords{word order, corpora, learning, maximum entorpy model, generation}

\def\q{}
\def\p{}

\begin{document}
\maketitle


\section{語順の学習と生成}
\label{sec:learning_and_generation}

\subsection{学習モデル}
\label{sec:model}

この節ではどの語順が妥当であるかを確率として計算するためのモデル
について述べる．
モデルとしては，MEに基づく確率モデルを採用する．
まず，MEの基本について説明し，その後，MEに基づく確率モデルについて述べる．

\subsubsection{ME(最大エントロピー)モデル}
\label{sec:me_model}

一般に確率モデルでは，文脈(観測される情報のこと)とそのときに得られる出力値
との関係は既知のデータから推定される確率分布によって表される．
いろいろな状況に対してできるだけ正確に出力値を予測するためには
文脈を細かく定義する必要があるが，細かくしすぎると既知のデータにおいて
それぞれの文脈に対応する事例の数が少なくなりデータスパースネスの問題が生じる．

MEモデルでは，文脈は素性と呼ばれる個々の要素によって表され，
確率分布は素性を引数とした関数として表される．
そして，各々の素性はトレーニングデータにおける
確率分布のエントロピーが最大になるように重み付けされる．
このエントロピーを最大にするという操作によって，
既知データに観測されなかったような素性あるいは
まれにしか観測されなかった素性については，
それぞれの出力値に対して確率値が等確率になるように
あるいは近付くように重み付けされる．
このように未知のデータに対して考慮した重み付けがなされるため，
MEモデルは比較的データスパースネスに強いとされている．
このモデルは例えば言語現象などのように既知データにすべての現象が現れ得ない
ような現象を扱うのに適したモデルであると言える．

以上のような性質を持つMEモデルでは，
確率分布の式は以下のように求められる．
文脈の集合を$B$，出力値の集合を$A$とするとき，
文脈$b (\in$$B)$で出力値$a (\in$$A)$となる事象$(a,b)$の確率分布$p(a,b)$を
MEにより推定することを考える．
文脈$b$は$k$個の素性$f_j (1\leq j\leq k)$の集合で表す．
そして，文脈$b$において，素性$f_j$が観測され
かつ出力値が$a$となるときに1を返す以下のような関数を定義する．
\begin{eqnarray}
  \label{eq:f}
  g_{j}(a,b) & = & 
  \left\{
    \begin{array}[c]{l}
      1,\ {\rm if}\ exist(b,f_{j})=1 \ \& \ 出力値=a\\
      0,\ それ以外
    \end{array}
  \right.
\end{eqnarray}
これを素性関数と呼ぶ．
ここで，$exist(b,f_j)$は，文脈$b$において素性$f_j$が観測されるか否かによって
1あるいは0の値を返す関数とする．

次に，それぞれの素性が既知のデータ中に現れた割合は
未知のデータも含む全データ中においても変わらないとする制約を加える．
つまり，推定するべき確率分布$p(a,b)$による素性$f_j$の期待値と，
既知データにおける経験確率分布$\tilde{p}(a,b)$による
素性$f_j$の期待値が等しいと仮定する．これは以下の制約式で表せる．
\begin{eqnarray}
  \label{eq:constraint0}
  \sum_{a\in A,b\in B}p(a,b)g_{j}(a,b) 
  & = & \sum_{a\in A,b\in B}\tilde{p}(a,b)g_{j}(a,b)
  \q for\p \forall f_{j}\ (1\leq j \leq k)
\end{eqnarray}
この式で，
$p(a,b)=p(b)p(a|b)\approx\tilde{p}(b)p(a|b)$という近似を行ない以下の式を得る．
\begin{eqnarray}
  \label{eq:constraint}
  \sum_{a\in A,b\in B}\tilde{p}(b)p(a|b)g_{j}(a,b) 
  & = & \sum_{a\in A,b\in B}\tilde{p}(a,b)g_{j}(a,b)
  \q for\p \forall f_{j}\ (1\leq j \leq k) 
\end{eqnarray}
ここで，$\tilde{p}(b)$，$\tilde{p}(a,b)$は，
$freq(b)$，$freq(a,b)$をそれぞれ既知データにおける
事象$b$の出現頻度，出力値$a$と事象$b$の共起頻度として
以下のように推定する．
\begin{eqnarray}
  \tilde{p}(b) & = & 
  \frac{freq(b)}{\displaystyle\sum_{b\in B} freq(b)}\\
  \tilde{p}(a,b) & = & 
  \frac{freq(a,b)}{\displaystyle\sum_{a\in A,b\in B} freq(a,b)}
\end{eqnarray}

次に，式(\ref{eq:constraint})の制約を満たす確率分布$p(a,b)$のうち，
エントロピー
\begin{eqnarray}
  \label{eq:entropy}
  H(p) & = & -\sum_{a\in A,b\in B}\tilde{p}(b)p(a|b)\ log\left(p(a,b)\right)
\end{eqnarray}
を最大にする確率分布を推定するべき確率分布とする．
これは，式(\ref{eq:constraint})の制約を満たす確率分布のうちで
最も一様な分布となる．
このような確率分布は唯一存在し，以下の確率分布$p^{*}$として記述される．
\begin{eqnarray}
  \label{eq:p}
  p^{*}(a|b) & = & \frac{\prod_{j=1}^{k}\alpha_{a,j}^{g_{j}(a,b)}}
  {\sum_{a\in A} \prod_{j=1}^{k}\alpha_{a,j}^{g_{j}(a,b)}}
  \q (0\leq \alpha_{a,j}\leq \infty)
\end{eqnarray}
ただし，
\begin{eqnarray}
  \label{eq:alpha}
  \alpha_{a,j} & = & e^{\lambda_{a,j}}
\end{eqnarray}
であり，$\lambda_{a,j}$は素性関数$g_{j}(a,b)$の重みである．
この重みは文脈$b$のもとで出力値$a$となることを予測するのに
素性$f_{j}$がどれだけ重要な役割を果たすかを表している．
訓練集合が与えられたとき，$\lambda_{a,j}$の推定には
Improved Iterative Scaling(IIS)アルゴリズム
\cite{pietra95}
などが用いられる．
式(\ref{eq:p})の導出については文献
\cite{Jaynes:57,Jaynes:79}
を参照されたい．

\subsubsection{語順モデル}
\label{sec:word_order_model}

本節では語順を学習するためのMEモデルについて述べる．
ここで語順は，ある一つの文節に対しそれに係る文節(係り文節)が複数あるとき，
その係り文節の順序を語順と定義する．
係り文節の数はさまざまであるが，係り文節の数によらず二つずつ取り上げて
その順序を学習するモデルを提案する
\footnote{
  係り文節のうち二つずつではなく，三つあるいはそれ以上ずつ取り上げてその
  順序を学習するモデルを考えることもできる．しかし，データスパースネスの
  問題を考え，本論文では二つずつとりあげて順序を学習するモデルとした．
}．これを語順モデルと呼ぶ．
このモデルは前節のMEモデルにおける式(\ref{eq:p})を用いて以下のように
求められる．
ある文脈$b$において文節$B$に係る文節が二つあるときそれぞれを
文節$B_1$と文節$B_2$とすると，$B_1$の次に$B_2$という順序が適切である
確率$p^{*}(1|b)$は，
出力値$a$を二つの文節の順序が適切であるか否かの1, 0の二値とし，
$k$個の素性$f_j (1\leq j\leq k)$を考えるとき次の式で表される．
\begin{eqnarray}
  \label{eq:p1}
  p^{*}(1|b) & = & \frac{\prod_{j=1}^{k}\alpha_{1,j}^{g_{j}(1,b)}}
  {\prod_{j=1}^{k}\alpha_{1,j}^{g_{j}(1,b)} 
    + \prod_{j=1}^{k}\alpha_{0,j}^{g_{j}(0,b)}}
\end{eqnarray}
この式の$\alpha_{1,j}$，$\alpha_{0,j}$の値を学習するためのデータとしては，
形態素解析，構文解析済みのコーパスを用いる．
一般に係り文節が二つ以上あるときは次のようにする．
ある文脈$b$において文節$B$に係る文節が文節$B_1$，文節$B_2$，
$\ldots$，文節$B_n$ $(n\geq 2)$の$n$個あるとき，
その順序が適切である確率を$P(1|b)$とすると，
この確率は係り文節を二つずつ取り上げたときそれぞれの順序が適切である確率，
つまり，$P(\{W_{i,i+j}=1|1\leq i\leq n-1, 1\leq j\leq n-i\}|b)$で表される．
ここで，$W_{i,i+j}=1$は文節$i$と文節$(i+j)$の順序がこの順で
適切であることを表す．
このとき，$W_{i,i+j}$はそれぞれ独立であると仮定すると，
$P(1|b)$は次の式で表される．
\clearpage
\begin{eqnarray}
  \label{eq:p2}
  P(1|b) 
  & = & P(\{W_{i,i+j}=1|1\leq i\leq n-1, 1\leq j\leq n-i\}|b) \nonumber\\
  & \approx 
  & \prod_{i=1}^{n-1}\prod_{j=1}^{n-i}P(W_{i,i+j}=1|b_{i,i+j}) \nonumber\\
  & = & \prod_{i=1}^{n-1}\prod_{j=1}^{n-i}p^{*}(1|b_{i,i+j})
\end{eqnarray}
ここで，$b_{i,i+j}$は文節$B$とそれに係る文節$B_i$，文節$B_{i+j}$に
着目したときの文脈を表す．

例えば，コーパスに「昨日／太郎は／テニスを／した．」
(／は文節の区切りを表す．)という文があった場合を考える．
動詞「した」に係る文節は「昨日」，「太郎は」，「テニスを」の
三つである．語順モデルでは，このうち二文節ずつ，つまり
「昨日」と「太郎は」，「昨日」と「テニスを」，「太郎は」と「テニスを」
の三つのペアを取り上げ，それぞれこの語順が適切であると仮定して学習する．
素性としては文節の持つ属性などを考える．
例えば，「昨日／太郎は／した．」という関係からは「時相名詞」の方が
「固有名詞」より前に来るという情報，
「太郎は／テニスを／した．」という関係からは「は」格の方が
「を」格より前に来るという情報などを用いる．

\subsection{語順の生成}
\label{sec:generation}

本節では学習した語順モデルを用いて語順を生成するアルゴリズムについて説明する．
語順の生成とは，
ある文節に対し複数の係り文節があるものについて，その係り文節の順序を
決めることを言う．
入力は係り受け関係にある文節および素性の有無を判定するのに必要な情報であり，
出力は係り文節の並びである．
ただし，各文節を構成する語の語彙選択はすでになされており，
文節間の係り受け関係は決まっていると仮定する．
素性の有無を判定するのに必要な情報とは，
形態素情報，文節区切り情報，統語情報，文脈情報などである．
実際に実験で用いた情報については \ref{sec:exp}~章で述べる．

語順の生成は次の手順で行なう．

\underline{手順}

\begin{enumerate}
\item 係り文節について可能性のある並びをすべて考える．
\item それぞれの並びについて，
  その係り文節の順序が適切である確率を語順モデルを用いて求める．
\item 全体の確率が最大となる並びを解とする．
  全体の確率としては式(\ref{eq:p2})を用いる．
\end{enumerate}

例えば，再び「昨日／太郎は／テニスを／した．」という文を考えよう．
動詞「した」に係る文節は「昨日」，「太郎は」，「テニスを」の三つである．
この三つの係り文節の順序を以下の手順で決定する．
\begin{enumerate}
\item 二文節ずつ，つまり「昨日」と「太郎は」，「昨日」と「テニスを」，
  「太郎は」と「テニスを」の三つのペアを取り上げ，
  語順モデルの式(\ref{eq:p1})を用いてそれぞれこの語順が適切である確率
  $P_{昨日,太郎は}$，
  $P_{昨日,テニスを}$，
  $P_{太郎は,テニスを}$を求める．
  例えば，ある文脈においてそれぞれ0.6，0.8，0.7であったと仮定する．
\item 六つの語順の可能性すべてについて全体の確率を
  計算し(表~\ref{table:example})
  \footnote{
    式(\ref{eq:p2})を導出する際，二つの係り文節，文節$i$と文節$(i+j)$の順序
    $W_{i,i+j}$はそれぞれ独立であると仮定したため，
    式(\ref{eq:p2})は近似式となっている．
    したがって，式(\ref{eq:p2})により計算される確率の総和は
    必ずしも1にはならない．
    さらに，ここで例としてあげた確率
    $P_{昨日,太郎は}=0.6$，
    $P_{昨日,テニスを}=0.8$，
    $P_{太郎は,テニスを}=0.7$は適当に与えたものであるため，
    表~\ref{table:example} の六つの語順の可能性すべてについて
    全体の確率を計算し，その総和をとっても1にはならない．
    }，最も確率の高いもの
  「昨日／太郎は／テニスを／した．」が最も適切な語順であるとする．
\end{enumerate}

  \begin{table*}[htbp]
    \begin{center}
      \caption{係り文節の順序が適切である確率の計算例}
      \label{table:example}
      \leavevmode
      \renewcommand{\arraystretch}{}
      \begin{tabular}[c]{|l|p{6.5cm}|}
        \hline
        「昨日／太郎は／テニスを／した．」 
        & $P_{ 昨日,太郎は} 
        \times P_{昨日,テニスを} 
        \times P_{太郎は,テニスを}$
        $= 0.6 \times 0.8 \times 0.7 = 0.336$\\
        「昨日／テニスを／太郎は／した．」
        & $P_{ 昨日,太郎は} 
        \times P_{昨日,テニスを} 
        \times P_{テニスを,太郎は}$
        $= 0.6 \times 0.8 \times 0.3 = 0.144$\\
        「太郎は／昨日／テニスを／した．」
        & $P_{ 太郎は,昨日} 
        \times P_{昨日,テニスを} 
        \times P_{太郎は,テニスを}$
        $= 0.4 \times 0.8 \times 0.7 = 0.224$\\
        「太郎は／テニスを／昨日／した．」
        & $P_{ 太郎は,昨日} 
        \times P_{テニスを,昨日} 
        \times P_{太郎は,テニスを}$
        $= 0.4 \times 0.2 \times 0.7 = 0.056$\\
        「テニスを／昨日／太郎は／した．」
        & $P_{ 昨日,太郎は} 
        \times P_{テニスを,昨日} 
        \times P_{テニスを,太郎は}$
        $= 0.6 \times 0.2 \times 0.3 = 0.036$\\
        「テニスを／太郎は／昨日／した．」
        & $P_{ 太郎は,昨日} 
        \times P_{テニスを,昨日} 
        \times P_{テニスを,太郎は}$
        $= 0.4 \times 0.2 \times 0.3 = 0.024$\\
        \hline
      \end{tabular}
    \end{center}
  \end{table*}

\subsection{性能評価}
\label{sec:evaluation}

本節では語順モデルの性能つまりコーパスにおける語順をどの程度学習できたかを
評価する方法について述べる．
性能の評価は，コーパスから係り受け関係にある文節で
複数の係り文節を持つものを取り出し，
これを入力として\ref{sec:generation} 節で述べた方法で語順を生成し，
どの程度元の文における語順と一致するかを調べることによって行なう．
この一致する割合を一致率と呼ぶことにする．
このように元の文とどの程度一致するかを評価の尺度として用いることによって，
客観的な評価が可能となる．
また，一致率によって評価しておけば，学習したモデルがどの程度学習コーパスに
おける語順に近いものを生成できるかを知った上でそのモデルを使うことができる．

一致率の尺度としては以下の二種類のものを用いる．
\begin{description}
\item[二文節単位] 
  二つずつ係り文節を取りあげたとき，
  順序関係が元の文と一致しているものの割合．
  例えば，「昨日／太郎は／テニスを／した．」が元の文で，
  システムによる生成結果が「昨日／テニスを／太郎は／した．」のとき
  二つずつ係り文節を取り上げると，元の文ではそれぞれ
  「昨日／太郎は」，「昨日／テニスを」，「太郎は／テニスを」の
  順序，システムの結果ではそれぞれ
  「昨日／テニスを」，「昨日／太郎は」，「テニスを／太郎は」の
  順序となる．三つのうち二つの順序が等しいので一致率は$2/3$となる．  
\item[完全一致] 
  係り文節の順序が元の文と一致しているものの割合．
  普通の意味での一致の割合である．
\end{description}

\section{まとめ}

本論文ではコーパスから語順を学習する方法について述べた．
ここで語順は，ある一つの受け文節に対し係り文節が複数あるとき
その係り文節の順序を表すものと定義した．
係り文節の数はさまざまであるが，
係り文節の数によらず二つずつ取り上げてその順序を学習するモデルを提案した．
学習モデルにはME(最大エントロピー)モデルを用いた．
このモデルは，学習コーパスから得られる情報を基に
適切な語順を予測するのに有効な素性を学習することによって得られる．
我々が素性として利用したのは，
文節の持つ属性，統語情報，文脈情報およびそれらの組み合わせである．
これらの素性のうちそれぞれを削除した実験を行なうことによって，
その中でも格や活用部分の情報が語順の傾向を学習する上で特に有効に働くことが
分かった．
また，学習コーパスの量を変えて実験を行なうことによって，
我々の手法が少ない学習データに対しても効率良く語順を学習できるだけでなく，
タグ付コーパスだけでなく生コーパスも学習に利用できることも分かった．
学習したモデルを用いて語順を生成させたとき，
コーパスと一致する割合は，京大コーパスを使用した実験で75.41\%であった．
一致しなかった残りの約25\%をサンプリング調査したところ，
その48\%がモデルを用いて生成した語順でも不自然ではないことが分かった．

今回の実験には新聞記事のような一般的な語順のテキストを用いた．
スタイルが異なれば語順の傾向も異なると考えられるため，
今後，小説などのように新聞記事とはスタイルが異なるテキストを用いて実験し，
我々の提案したモデルがどの程度語順の傾向の違いを学習できるかを調べたい．
また，本論文で扱ったのは日本語の語順であったが，
英語についても同様に語順の傾向を学習できると考えられる．
今後，英語についても同様のモデルを用いて語順を学習し，モデルの評価をしたい．

文生成においては一般に客観的な評価基準がないため評価が難しいが，
本論文で示したようにコーパスに基づく評価方法をとることにより，
少なくとも語順の生成に関しては客観的な評価が可能になったと言えるだろう．

本論文で我々が提案した手法には，以下のような応用が考えられる．
\begin{itemize}
\item 校正支援

  ユーザが作文した文を構文解析して依存構造を得た後，それを入力として
  語順を生成させユーザに提示する．
  語順モデルを用いて生成させた語順の方がユーザの作文における語順より
  自然な語順になっている可能性が高いと考えられる．

\item 機械翻訳における対象言語の語順の生成

  対象言語において，文節間の依存構造が決まり，
  各文節において語彙選択が終了すれば，我々が提案した語順モデルを用いて
  一文全体の語順を決めることができる．
  このとき一文全体の語順としては，一文全体の語順の確率が最大となるものを選ぶ．
  一文全体の語順の確率は，受け文節ごとにその受け文節に係る文節の順序の確率を
  式(\ref{eq:p2})を用いて求め，その積として求める．

\item 構文解析における誤り検出

  構文解析結果に複数の係り文節を持つ文節がある場合，
  その係り文節の順序の確率を式(\ref{eq:p2})を用いて求め，
  その値が著しく低い場合に誤りとして検出する
  \footnote{例えば，A，B，Cの三つの文節からなる文があり，
    [A,[B,C]](AとBがともにCに係る解釈)と[[A,B],C](AがBに，BがCに係る解釈)
    の2つの解析結果が得られたとする．前者の解釈に対しては，
    係り文節の順序の確率を求め，
    その値が著しく低い場合には誤りとして検出することができると考えている．
    後者の解釈では，係り文節の順序の確率を求めることはできないが，
    もう一つの候補である前者の解釈に対して係り文節の順序の確率を求め，
    その値が著しく高い場合には後者の解釈は
    誤りであるとして検出することができると考えている．
    }．

\end{itemize}



\bibliographystyle{jnlpbbl}
\bibliography{jpaper}


\begin{biography}
\biotitle{略歴}
\bioauthor{内元 清貴}{
1994年京都大学工学部卒業．
1996年同大学院修士課程修了．
同年郵政省通信総合研究所入所．研究官．
自然言語処理の研究に従事．
言語処理学会，情報処理学会，ACL，各会員．}
\bioauthor{村田 真樹}{
1993年京都大学工学部卒業．
1995年同大学院修士課程修了．
1997年同大学院博士課程修了，博士（工学）．
同年，京都大学にて日本学術振興会リサーチ・アソシエイト．
1998年郵政省通信総合研究所入所．研究官．
自然言語処理，機械翻訳，情報検索の研究に従事．
言語処理学会，情報処理学会，人工知能学会，ACL，各会員．}
\bioauthor{馬 青}{
1983年北京航空航天大学自動制御学部卒業．
1987年筑波大学大学院理工学研究科修士課程修了．
1990年同大学院工学研究科博士課程修了．工学博士．
1990 $\sim$ 93年株式会社小野測器勤務．
1993年郵政省通信総合研究所入所，主任研究官． 
人工神経回路網モデル，知識表現，自然言語処理の研究に従事． 
日本神経回路学会，言語処理学会，電子情報通信学会，各会員．}
\bioauthor{関根 聡}{
1987年東京工業大学応用物理学科卒．同年松下電器東京研究所入社．
1990-1992年UMIST，CCL，Visiting Researcher．1992年MSc．
1994年からNew York University，Computer Science Department，
Assistant Research Scientist．1998年PhD．
同年からAssistant Research Professor．
自然言語処理の研究に従事．
情報処理学会，人工知能学会，言語処理学会，ACL会員．}
\bioauthor{井佐原 均}{
1978年京都大学工学部電気工学第二学科卒業．
1980年同大学院修士課程修了．博士（工学）．
同年通商産業省電子技術総合研究所入所．
1995年郵政省通信総合研究所
関西支所知的機能研究室室長．自然言語処理，機械翻訳の研究に従事．
言語処理学会，情報処理学会，人工知能学会，日本認知科学会，ACL，各会員．}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\end{document}
