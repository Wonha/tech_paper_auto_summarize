語順の学習と生成
\label{sec:learning_and_generation}

\subsection{学習モデル}
\label{sec:model}

この節ではどの語順が妥当であるかを確率として計算するためのモデル
について述べる．
モデルとしては，MEに基づく確率モデルを採用する．
まず，MEの基本について説明し，その後，MEに基づく確率モデルについて述べる．

\subsubsection{ME(最大エントロピー)モデル}
\label{sec:me_model}

一般に確率モデルでは，文脈(観測される情報のこと)とそのときに得られる出力値
との関係は既知のデータから推定される確率分布によって表される．
いろいろな状況に対してできるだけ正確に出力値を予測するためには
文脈を細かく定義する必要があるが，細かくしすぎると既知のデータにおいて
それぞれの文脈に対応する事例の数が少なくなりデータスパースネスの問題が生じる．

MEモデルでは，文脈は素性と呼ばれる個々の要素によって表され，
確率分布は素性を引数とした関数として表される．
そして，各々の素性はトレーニングデータにおける
確率分布のエントロピーが最大になるように重み付けされる．
このエントロピーを最大にするという操作によって，
既知データに観測されなかったような素性あるいは
まれにしか観測されなかった素性については，
それぞれの出力値に対して確率値が等確率になるように
あるいは近付くように重み付けされる．
このように未知のデータに対して考慮した重み付けがなされるため，
MEモデルは比較的データスパースネスに強いとされている．
このモデルは例えば言語現象などのように既知データにすべての現象が現れ得ない
ような現象を扱うのに適したモデルであると言える．

以上のような性質を持つMEモデルでは，
確率分布の式は以下のように求められる．
文脈の集合を$B$，出力値の集合を$A$とするとき，
文脈$b (\in$$B)$で出力値$a (\in$$A)$となる事象$(a,b)$の確率分布$p(a,b)$を
MEにより推定することを考える．
文脈$b$は$k$個の素性$f_j (1\leq j\leq k)$の集合で表す．
そして，文脈$b$において，素性$f_j$が観測され
かつ出力値が$a$となるときに1を返す以下のような関数を定義する．
\begin{eqnarray}
  \label{eq:f}
  g_{j}(a,b) & = & 
  \left\{
    \begin{array}[c]{l}
      1,\ {\rm if}\ exist(b,f_{j})=1 \ \& \ 出力値=a\\
      0,\ それ以外
    \end{array}
  \right.
\end{eqnarray}
これを素性関数と呼ぶ．
ここで，$exist(b,f_j)$は，文脈$b$において素性$f_j$が観測されるか否かによって
1あるいは0の値を返す関数とする．

次に，それぞれの素性が既知のデータ中に現れた割合は
未知のデータも含む全データ中においても変わらないとする制約を加える．
つまり，推定するべき確率分布$p(a,b)$による素性$f_j$の期待値と，
既知データにおける経験確率分布$\tilde{p}(a,b)$による
素性$f_j$の期待値が等しいと仮定する．これは以下の制約式で表せる．
\begin{eqnarray}
  \label{eq:constraint0}
  \sum_{a\in A,b\in B}p(a,b)g_{j}(a,b) 
  & = & \sum_{a\in A,b\in B}\tilde{p}(a,b)g_{j}(a,b)
  \q for\p \forall f_{j}\ (1\leq j \leq k)
\end{eqnarray}
この式で，
$p(a,b)=p(b)p(a|b)\approx\tilde{p}(b)p(a|b)$という近似を行ない以下の式を得る．
\begin{eqnarray}
  \label{eq:constraint}
  \sum_{a\in A,b\in B}\tilde{p}(b)p(a|b)g_{j}(a,b) 
  & = & \sum_{a\in A,b\in B}\tilde{p}(a,b)g_{j}(a,b)
  \q for\p \forall f_{j}\ (1\leq j \leq k) 
\end{eqnarray}
ここで，$\tilde{p}(b)$，$\tilde{p}(a,b)$は，
$freq(b)$，$freq(a,b)$をそれぞれ既知データにおける
事象$b$の出現頻度，出力値$a$と事象$b$の共起頻度として
以下のように推定する．
\begin{eqnarray}
  \tilde{p}(b) & = & 
  \frac{freq(b)}{\displaystyle\sum_{b\in B} freq(b)}\\
  \tilde{p}(a,b) & = & 
  \frac{freq(a,b)}{\displaystyle\sum_{a\in A,b\in B} freq(a,b)}
\end{eqnarray}

次に，式(\ref{eq:constraint})の制約を満たす確率分布$p(a,b)$のうち，
エントロピー
\begin{eqnarray}
  \label{eq:entropy}
  H(p) & = & -\sum_{a\in A,b\in B}\tilde{p}(b)p(a|b)\ log\left(p(a,b)\right)
\end{eqnarray}
を最大にする確率分布を推定するべき確率分布とする．
これは，式(\ref{eq:constraint})の制約を満たす確率分布のうちで
最も一様な分布となる．
このような確率分布は唯一存在し，以下の確率分布$p^{*}$として記述される．
\begin{eqnarray}
  \label{eq:p}
  p^{*}(a|b) & = & \frac{\prod_{j=1}^{k}\alpha_{a,j}^{g_{j}(a,b)}}
  {\sum_{a\in A} \prod_{j=1}^{k}\alpha_{a,j}^{g_{j}(a,b)}}
  \q (0\leq \alpha_{a,j}\leq \infty)
\end{eqnarray}
ただし，
\begin{eqnarray}
  \label{eq:alpha}
  \alpha_{a,j} & = & e^{\lambda_{a,j}}
\end{eqnarray}
であり，$\lambda_{a,j}$は素性関数$g_{j}(a,b)$の重みである．
この重みは文脈$b$のもとで出力値$a$となることを予測するのに
素性$f_{j}$がどれだけ重要な役割を果たすかを表している．
訓練集合が与えられたとき，$\lambda_{a,j}$の推定には
Improved Iterative Scaling(IIS)アルゴリズム
\cite{pietra95}
などが用いられる．
式(\ref{eq:p})の導出については文献
\cite{Jaynes:57,Jaynes:79}
を参照されたい．

\subsubsection{語順モデル}
\label{sec:word_order_model}

本節では語順を学習するためのMEモデルについて述べる．
ここで語順は，ある一つの文節に対しそれに係る文節(係り文節)が複数あるとき，
その係り文節の順序を語順と定義する．
係り文節の数はさまざまであるが，係り文節の数によらず二つずつ取り上げて
その順序を学習するモデルを提案する
\footnote{
  係り文節のうち二つずつではなく，三つあるいはそれ以上ずつ取り上げてその
  順序を学習するモデルを考えることもできる．しかし，データスパースネスの
  問題を考え，本論文では二つずつとりあげて順序を学習するモデルとした．
}．これを語順モデルと呼ぶ．
このモデルは前節のMEモデルにおける式(\ref{eq:p})を用いて以下のように
求められる．
ある文脈$b$において文節$B$に係る文節が二つあるときそれぞれを
文節$B_1$と文節$B_2$とすると，$B_1$の次に$B_2$という順序が適切である
確率$p^{*}(1|b)$は，
出力値$a$を二つの文節の順序が適切であるか否かの1, 0の二値とし，
$k$個の素性$f_j (1\leq j\leq k)$を考えるとき次の式で表される．
\begin{eqnarray}
  \label{eq:p1}
  p^{*}(1|b) & = & \frac{\prod_{j=1}^{k}\alpha_{1,j}^{g_{j}(1,b)}}
  {\prod_{j=1}^{k}\alpha_{1,j}^{g_{j}(1,b)} 
    + \prod_{j=1}^{k}\alpha_{0,j}^{g_{j}(0,b)}}
\end{eqnarray}
この式の$\alpha_{1,j}$，$\alpha_{0,j}$の値を学習するためのデータとしては，
形態素解析，構文解析済みのコーパスを用いる．
一般に係り文節が二つ以上あるときは次のようにする．
ある文脈$b$において文節$B$に係る文節が文節$B_1$，文節$B_2$，
$\ldots$，文節$B_n$ $(n\geq 2)$の$n$個あるとき，
その順序が適切である確率を$P(1|b)$とすると，
この確率は係り文節を二つずつ取り上げたときそれぞれの順序が適切である確率，
つまり，$P(\{W_{i,i+j}=1|1\leq i\leq n-1, 1\leq j\leq n-i\}|b)$で表される．
ここで，$W_{i,i+j}=1$は文節$i$と文節$(i+j)$の順序がこの順で
適切であることを表す．
このとき，$W_{i,i+j}$はそれぞれ独立であると仮定すると，
$P(1|b)$は次の式で表される．
\clearpage
\begin{eqnarray}
  \label{eq:p2}
  P(1|b) 
  & = & P(\{W_{i,i+j}=1|1\leq i\leq n-1, 1\leq j\leq n-i\}|b) \nonumber\\
  & \approx 
  & \prod_{i=1}^{n-1}\prod_{j=1}^{n-i}P(W_{i,i+j}=1|b_{i,i+j}) \nonumber\\
  & = & \prod_{i=1}^{n-1}\prod_{j=1}^{n-i}p^{*}(1|b_{i,i+j})
\end{eqnarray}
ここで，$b_{i,i+j}$は文節$B$とそれに係る文節$B_i$，文節$B_{i+j}$に
着目したときの文脈を表す．

例えば，コーパスに「昨日／太郎は／テニスを／した．」
(／は文節の区切りを表す．)という文があった場合を考える．
動詞「した」に係る文節は「昨日」，「太郎は」，「テニスを」の
三つである．語順モデルでは，このうち二文節ずつ，つまり
「昨日」と「太郎は」，「昨日」と「テニスを」，「太郎は」と「テニスを」
の三つのペアを取り上げ，それぞれこの語順が適切であると仮定して学習する．
素性としては文節の持つ属性などを考える．
例えば，「昨日／太郎は／した．」という関係からは「時相名詞」の方が
「固有名詞」より前に来るという情報，
「太郎は／テニスを／した．」という関係からは「は」格の方が
「を」格より前に来るという情報などを用いる．

\subsection{語順の生成}
\label{sec:generation}

本節では学習した語順モデルを用いて語順を生成するアルゴリズムについて説明する．
語順の生成とは，
ある文節に対し複数の係り文節があるものについて，その係り文節の順序を
決めることを言う．
入力は係り受け関係にある文節および素性の有無を判定するのに必要な情報であり，
出力は係り文節の並びである．
ただし，各文節を構成する語の語彙選択はすでになされており，
文節間の係り受け関係は決まっていると仮定する．
素性の有無を判定するのに必要な情報とは，
形態素情報，文節区切り情報，統語情報，文脈情報などである．
実際に実験で用いた情報については \ref{sec:exp}~章で述べる．

語順の生成は次の手順で行なう．

\underline{手順}

\begin{enumerate}
\item 係り文節について可能性のある並びをすべて考える．
\item それぞれの並びについて，
  その係り文節の順序が適切である確率を語順モデルを用いて求める．
\item 全体の確率が最大となる並びを解とする．
  全体の確率としては式(\ref{eq:p2})を用いる．
\end{enumerate}

例えば，再び「昨日／太郎は／テニスを／した．」という文を考えよう．
動詞「した」に係る文節は「昨日」，「太郎は」，「テニスを」の三つである．
この三つの係り文節の順序を以下の手順で決定する．
\begin{enumerate}
\item 二文節ずつ，つまり「昨日」と「太郎は」，「昨日」と「テニスを」，
  「太郎は」と「テニスを」の三つのペアを取り上げ，
  語順モデルの式(\ref{eq:p1})を用いてそれぞれこの語順が適切である確率
  $P_{昨日,太郎は}$，
  $P_{昨日,テニスを}$，
  $P_{太郎は,テニスを}$を求める．
  例えば，ある文脈においてそれぞれ0.6，0.8，0.7であったと仮定する．
\item 六つの語順の可能性すべてについて全体の確率を
  計算し(表~\ref{table:example})
  \footnote{
    式(\ref{eq:p2})を導出する際，二つの係り文節，文節$i$と文節$(i+j)$の順序
    $W_{i,i+j}$はそれぞれ独立であると仮定したため，
    式(\ref{eq:p2})は近似式となっている．
    したがって，式(\ref{eq:p2})により計算される確率の総和は
    必ずしも1にはならない．
    さらに，ここで例としてあげた確率
    $P_{昨日,太郎は}=0.6$，
    $P_{昨日,テニスを}=0.8$，
    $P_{太郎は,テニスを}=0.7$は適当に与えたものであるため，
    表~\ref{table:example} の六つの語順の可能性すべてについて
    全体の確率を計算し，その総和をとっても1にはならない．
    }，最も確率の高いもの
  「昨日／太郎は／テニスを／した．」が最も適切な語順であるとする．
\end{enumerate}

  \begin{table*}[htbp]
    \begin{center}
      \caption{係り文節の順序が適切である確率の計算例}
      \label{table:example}
      \leavevmode
      \renewcommand{\arraystretch}{}
      \begin{tabular}[c]{|l|p{6.5cm}|}
        \hline
        「昨日／太郎は／テニスを／した．」 
        & $P_{ 昨日,太郎は} 
        \times P_{昨日,テニスを} 
        \times P_{太郎は,テニスを}$
        $= 0.6 \times 0.8 \times 0.7 = 0.336$\\
        「昨日／テニスを／太郎は／した．」
        & $P_{ 昨日,太郎は} 
        \times P_{昨日,テニスを} 
        \times P_{テニスを,太郎は}$
        $= 0.6 \times 0.8 \times 0.3 = 0.144$\\
        「太郎は／昨日／テニスを／した．」
        & $P_{ 太郎は,昨日} 
        \times P_{昨日,テニスを} 
        \times P_{太郎は,テニスを}$
        $= 0.4 \times 0.8 \times 0.7 = 0.224$\\
        「太郎は／テニスを／昨日／した．」
        & $P_{ 太郎は,昨日} 
        \times P_{テニスを,昨日} 
        \times P_{太郎は,テニスを}$
        $= 0.4 \times 0.2 \times 0.7 = 0.056$\\
        「テニスを／昨日／太郎は／した．」
        & $P_{ 昨日,太郎は} 
        \times P_{テニスを,昨日} 
        \times P_{テニスを,太郎は}$
        $= 0.6 \times 0.2 \times 0.3 = 0.036$\\
        「テニスを／太郎は／昨日／した．」
        & $P_{ 太郎は,昨日} 
        \times P_{テニスを,昨日} 
        \times P_{テニスを,太郎は}$
        $= 0.4 \times 0.2 \times 0.3 = 0.024$\\
        \hline
      \end{tabular}
    \end{center}
  \end{table*}

\subsection{性能評価}
\label{sec:evaluation}

本節では語順モデルの性能つまりコーパスにおける語順をどの程度学習できたかを
評価する方法について述べる．
性能の評価は，コーパスから係り受け関係にある文節で
複数の係り文節を持つものを取り出し，
これを入力として\ref{sec:generation} 節で述べた方法で語順を生成し，
どの程度元の文における語順と一致するかを調べることによって行なう．
この一致する割合を一致率と呼ぶことにする．
このように元の文とどの程度一致するかを評価の尺度として用いることによって，
客観的な評価が可能となる．
また，一致率によって評価しておけば，学習したモデルがどの程度学習コーパスに
おける語順に近いものを生成できるかを知った上でそのモデルを使うことができる．

一致率の尺度としては以下の二種類のものを用いる．
\begin{description}
\item[二文節単位] 
  二つずつ係り文節を取りあげたとき，
  順序関係が元の文と一致しているものの割合．
  例えば，「昨日／太郎は／テニスを／した．」が元の文で，
  システムによる生成結果が「昨日／テニスを／太郎は／した．」のとき
  二つずつ係り文節を取り上げると，元の文ではそれぞれ
  「昨日／太郎は」，「昨日／テニスを」，「太郎は／テニスを」の
  順序，システムの結果ではそれぞれ
  「昨日／テニスを」，「昨日／太郎は」，「テニスを／太郎は」の
  順序となる．三つのうち二つの順序が等しいので一致率は$2/3$となる．  
\item[完全一致] 
  係り文節の順序が元の文と一致しているものの割合．
  普通の意味での一致の割合である．
\end{description}

