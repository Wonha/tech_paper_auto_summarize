実験の設定


\subsection{実験データ}

RIBES の有効性を示すため，NTCIR-7，NTCIR-9の特許翻訳タスク (PATMT) の
データを用いて評価実験（評価指標の評価なので，以降メタ評価と呼ぶ）を行った．
言語対は英日 (EJ)，日英 (JE)とした．
それぞれのデータセットの文数，1文あたりの参照翻訳の数，評価者の数，参加
システム数
を表\ref{data}に示す．なお，カッコ内の数字はルールベースシステムの数を示
す．

\begin{table}[b]
  \caption{実験データの詳細}
  \label{data}
\input{0985table02.txt}
\end{table}

NTCIRワークショップの事務局から公開されてい
るデータには，EJ，JEタスクとも1つの参照翻訳しか含まれていない．
そこで，NTCIR-7のデータに対してのみ，特許翻訳の専門家に依頼し，参照翻訳を
独自に追加した．
また，NTCIR-7のEJタスクに関しては，5システムだけにしか人間の主観
評価の結果が与えられていなかったため，特許に精通した被験者5名で再度 JE
タスクと同様，5段階評価で主観評価を行った．
さらに，評価対象とする翻訳システムに著者のグループの英日翻訳システム\cite{headfinal}を追加
し，計14システムで実験を行った．

全てのデータに対し，メタ評価の対象は翻訳の内容としての適切性 (adequacy) のみ
とした．これは，翻訳の流暢さよりも内容の適切性を
自動評価できた方がより良い翻訳システムの開発に貢献できると考えたからである．

なお，各システム翻訳文に対し複数の人間の評価スコアが与えられている場合に
は，その平均値を文に対する評価スコアとした．このように各システム翻訳
文に対して評価値を決定し，これを文集合全体での平均したものを人
間がシステムに与えた評価スコアとした．


\subsection{比較した自動評価手法}

比較評価には，Nグラム一致率に基づく評価手法として先に説明したBLEU，
大局的な単語列を考慮した評価法として同じく先に説明した
ROUGE-L \cite{ROUGEL}，
その改良版である IMPACT \cite{impact}を用いた．
IMPACTには，LCS の長さに応じた重みパラメタ，語順の入れ替えに応じた
重みパラメタがある．詳細については文献 \cite{impact} を参照されたい．
なお，ROUGE-L，IMPACTとも参
照翻訳
が複数ある場合には個々の参照翻訳を用いて求めたスコアの最大値を評価スコア
として採用した．BLEU の計算には {\tt mteval-v13a}，ROUGE-L 
には，\texttt{ROUGE-1.5.5}，IMPACT には \texttt{IMPACT version 4}を利用した．

また，LRscore \cite{birch,birch-wmt2010,birch-acl}も比較評価の対象とした．
LRscore は，参照翻訳とシステム翻訳との間の語順の近さを表すスコアとBLEUス
コアとの間の線形補間で評価スコアを決定する．
語順の近さを表す尺度としては，ハミング距離$d_h(h,r)$を利用する
ものと Kendall の$\tau$に基づく$d_k(h,r)$を利用するものがあるが，以降で
は，本稿との
関連が深い後者について述べる．
LRscore の定義を以下に示す．
\begin{equation}
 \mbox{LRscore}(\mathcal{H},\mathcal{R})=\gamma R(\mathcal{H},\mathcal{R})+(1-\gamma)\mbox{BLEU}(\mathcal{H},\mathcal{R})
\end{equation}

$R({\mathcal H},{\mathcal R})$は以下の式で定義される．
\begin{equation}
 R(\mathcal{H},\mathcal{R})=\frac{\displaystyle\mathop\sum_{h_i \in \mathcal{H}} d(h_i,r_i)\mbox{BP}_s(h_i,r_i)}{|\mathcal{H}|}
\end{equation}

$d_k(h,r)$は，文献\cite{birch-acl} に従うと
$d_k(h,r)=1-\sqrt{1-\mbox{NKT}(h,r)}$で定義されるが，それ以前の文献
\cite{birch,birch-wmt2010} では，$d_k(h,r)=\mbox{NKT}(h,r)$も用いられて
いる．以降，前者を${d_k}_1$，後者を${d_k}_2$とよぶ．
RIBES で $\alpha=0$, $\beta=1$と設定したときと，LRscore に ${d_k}_2$ を採用，
$\gamma=1$と設定したとき，これら2つの手法は一致する．しかし，LRscore
は日本語，英語のような大きな語順の入れ替えがある言語対を対象として考案
された手法ではなく，ヨーロッパ言語間，中英\footnote{
もちろん，英語，フランス語ほど語順が近くはないが，英語も中国語も SVO
型の言語であり，日本語，英語ほどの語順の違いはない．}翻訳という比較的語順が似た言語を対象として考案されたため，最終的には
${d_k}_1$を採用することで順位相関の低レンジスコアの感度を下げ，さらに語順の近い言語対を
対象としたときに実績のある BLEU \footnote{
実際，NTCIR-9 の中英翻訳タスクにおいて，BLEUは人間の評価結果との相関が
0.9以上の非常に高い値を記録している \cite{ntcir9}．} の恩恵を受けるため，それとの間の線形補間という定式化に至ったのであろう．
後述するが，英日，日英翻訳の評価では BLEU を利用するメリットは期待できな
い．さらに，NKT を${d_k}_1$によって非線形変換することで低レンジスコアの
感度をさらに下げるメリットも元々高いNKTを得ることが難しい英日，日英翻訳タ
スクでは期待できない．
以上より，LRscore は確かに RIBES と良く似た手法といえるが，
 BLEU を補うために派生した評価指標と捉えた方が自然であり， 
RIBES とはその根底にある研究の動機に大きな違いがある．
なお，LRscore には，参照翻訳とシステム翻訳との間の単語
アラインメントを決定する手段が提供されないため，
以降の実験では本稿での単語アラインメントを利用した．


\subsection{メタ評価の指標}

本稿では，メタ評価の指標として広く用いられている Pearson の積率相関係数，
Spearman の順位相関係数，Kendall の順位相関係数を用いた．Pearson の積率
相関係数は人間の評価と自動評価の結果がどの程度線形の関係にあるかを評価し，
Spearman，Kendall の相関係数は人間の評価と自動評価の結果の順位がどの程度
近いかを評価する．
Spearman と Kendall の違いは，先にも説明したように
順位の差に対して重みをどのように与えるかという点にある．


\subsection{実験の手順}

RIBES に対してはシステム翻訳の長さに対する重みパラメタと単語正解率に対
する重みパラメタ，IMPACT に対してはLCSに対する重みパラメタと語順の違いに
対する重みパラメタ，LRscore には順位相関係数とBLEUスコアの重みを調整する
パラメタがある．
これらの手法に対しては，以下の手順でパラメタの最適化を行い，メタ評価を行った．

\begin{enumerate}
 \item 文のIDをランダムに10個選択する．
 \item 選択したIDによる10文の集合を用いて，文集合全体での人間の評価スコアと自動
       評価スコアとの間の Spearman の順位相関係数が最大となるようパラメタを決定する．
 \item (2) で決定したパラメタを用いて (1) の残りの文集合全体を用いてメタ評
       価を行い，相関係数を記録する．
 \item (1) から (3) を100回繰り返し，相関係数の平均値を求める．
\end{enumerate}

なお，パラメタが存在しないBLEUとROUGE-Lに対しては，(2) をスキップし，同様
の手順でメタ評価を行った．


実験結果と考察

\subsection{NTCIR-7データ}

表 \ref{NTCIR7-single}にオーガナイザから配布された参照翻訳のみを用いた時
の相関係数の平均値，表 \ref{NTCIR7-multi}に複数参照翻訳を用いた時の相関係
数の平均値を示す．
平均値の差の
検定には，ペアワイズの比較に Wilcoxon の符号順位検定\cite{wilcox}を採用し，Holm
法 \cite{holm} による多重比較を用いた．

\begin{table}[t]
  \caption{メタ評価の結果（NTCIR-7データ，単一参照翻訳）}
  \label{NTCIR7-single}
\input{0985table03.txt}
\vspace{4pt}\small
表中，1はRIBES，2は
  LRscore(${d_k}_1$)，3はLRscore(${d_k}_2$)，4はROUGE-L，5はIMPACT，6はBLEUに対し，
  有意水準5\%で有意差があることを示す．
\par
\end{table}
\begin{table}[t]
  \caption{メタ評価の結果（NTCIR-7データ，複数参照翻訳）}
  \label{NTCIR7-multi}
\input{0985table04.txt}
\vspace{4pt}\small
表中，1はRIBES，2は
  LRscore(${d_k}_1$)，3はLRscore(${d_k}_2$)，4はROUGE-L，5はIMPACT，6はBLEUに対し，
  有意水準5\%で有意差があることを示す．
\par
\end{table}

表 \ref{NTCIR7-single}，\ref{NTCIR7-multi} より，
どの手法に対しても Spearman の順
位相関係数の方が Kendall の順位相関係数よりも高い．
Kendall の順位相関係数は，2つの順序列の間で一致する半順序関係の
数に基づき決定されるため，細かな順位の間違いに敏感である．
一方，Spearman の順位相関係数は順序列の間の
順位の差に基づき決定されるため，細かな順位の間違いには鈍感である．
本稿で用いたデータでは，
自動評価法にとって，明らかに良いシステムと悪いシステムの区別が容易であっ
たため，大きな差での順位の不一致が減少し，
Spearman の順位相関係数が
Kendall の順位相関係数よりも相対的に高い値を記録
したのであろう．ただし，全体の傾向としては両者の間に大きな違いはない．
Pearson の積率相関係数は順位相関係数より高い値を示しており，
BLEUではその差が特に大きい．
たとえば，表 \ref{NTCIR7-multi}の英日
タスクでは，Pearsonが0.9以上であることに対し，Spearmanは0.7程度でしかない．
これは，人間の評価との順位付けはやや強い程度相関でしか示していないにも関
わらず，
線形の相関は非常に強いことを意味する．
Pearson の積率相関係数は外れ値がある
場合，その値が過剰に高く見積もられるということが知られているが，その影響が
強く出ているのではないかと考える．よって，以降では主に順位相関に焦点をあ
てて議論する．

表\ref{NTCIR7-single}より，
日英翻訳に関しては，RIBES が他のすべての手法に対して統計的有意に優れてい
る．
英日翻訳に関しては，RIBES とLRscore(${d_k}_1$)が同程度で優れて
おり，十分強い相関である．
ROUGE-L，IMPACTが，RIBES ほどではないものの比較的よい相関を得ているこ
とに対し，
BLEUは双方の言語対において，相関係数の平均値が他のほぼ全ての手法に対し統
計的に有意な差で劣っ
ている．さらに，順位相関係数は弱い相関程度でしかない．
この結果は，英日，日英翻訳という大きな語順の入れ替えを必要とする言語対を対象
とした場合，Nグラム一致率で自動評価を行うことが不適切であることを示唆し
ている．

表 \ref{NTCIR7-multi}より，
参照翻訳の数が増えると相関係数の平均値は上昇する傾向にある．
BLEUの相関係数が他の手法よりも有意に劣っていることは単一参照翻訳の場合と同
様であるが，順位相関係数はやや強い相関程度にまで上昇している．
ROUGE-L，IMPACTの相関係数も上昇しており，ROUGE-Lは日英翻訳に関しては，
他のすべての手法に対して，英日翻訳に関しては，RIBES 以外の手法に対して
統計的有意に優れている．
RIBES は，日英翻訳ではROUGE-Lに次いでIMPACTと同程度，英日翻訳では
ROUGE-Lと同程度であるが，十分強い相関を示している．

BLEU，ROUGE-L，IMPACT の相関係数が複数参照翻訳が与えられた場合に顕著に改
善される理由は，語彙のバリエーションが増えたことであろう．
これらの手法は，N グラ
ム一致率，LCS 適合率，再現率を利用しているため，参照翻訳とシステム翻訳と
の間で一致しない単語も評価対象となる．よって，
語彙の一致判定を単に文字列としての一致だけで判定すると，意味的には一致す
るはずのものが一致せずに不当に低いスコアを得るという問題が起こる．
しかし，複数参照翻訳の場
合には，語彙のバリエーションが増えるためこうした問題は軽減される
のであろう．
一方，RIBES でもシステム翻訳と参照翻訳との間の単語一致率は利用するため，
一致しない単語を評価対象として用いているが，パラメタによりその影響を小さ
く抑えることができる．よって，単一参照翻訳でも複数参照翻訳でも安定して高い相関を示
すことができた．


\subsection{NTCIR-9データ}

表 \ref{NTCIR9}に相関係数の平均値を示す．
NTCIR-7 のデータとは異なり，どの手法も相関係数の平均値は大幅に低下してい
る．特にROUGE-L，IMPACT，BLEUは，非常に弱い相関，あるいは無相関と言える
ほどである．この原因は先の実験結果と同様，参照翻訳の数が1つであること
に加え，評価対象となる翻訳システムの中でのルールベースの翻訳システム（SMT
とのハイブリッドも含む）の占める
割合が増したことにある．NTCIR-7と比較すると，日英翻訳に関しては
ルールベースシステムは2システムから6システムへ，英日翻訳に関しては1シス
テムから5システムへと増えた．

\begin{table}[t]
  \caption{メタ評価の結果（NTCIR-9データ，単一参照翻訳）}
  \label{NTCIR9}
\input{0985table05.txt}
\vspace{4pt}\small
表中，1はRIBES，2は
  LRscore(${d_k}_1$)，3はLRscore(${d_k}_2$)，4はROUGE-L，5はIMPACT，6はBLEUに対し，
  有意水準5\%で有意差があることを示す．
\par
\end{table}
\begin{figure}[t]
   \begin{center}
\includegraphics{21-3ia985f2.eps}
   \end{center}
    \caption{ユニグラム適合率と人間のスコアの関係}
    \label{uniprec}
\end{figure}

図\ref{uniprec}にユニグラム適合率と人間のスコア
の関係を示す．図中四角のマーカ(RBMT-1〜6，JAPIO，TORI，EIWA)がルールベース
の翻訳システムである．図か
ら明らかなように，ルールベースの翻訳システムはユニグラム適合率が低いにも
関わらず人間の評価では高いスコアを獲得している．つまり，これらのシステム
の翻訳
は，参照翻訳と一致する単語の割合が少ないにも関わらず人間の評価では高いスコアを獲
得している．
ルールベースの翻訳システムは，
訓練データから訳語を推定するSMTシステムほど語彙の統制がとれていない．
よって，翻訳対象のドメインに合致した語彙，すなわち，特許特有の語彙を用い
て翻訳できるとは限らない．
しかし，SMTシステムにとって大きな問題となる語順の入れ替えに関しては，記述され
たルールに当てはまる限りは問題となり得ない．
よって，特許の訳語として多少おかしくとも文全体で意味が通る翻訳となり，そ
の結果，人間が高いスコアを与えたのであろう．
先にも説明した通り，
BLEUは，Nグラムの適合率，ROUGE-LとIMPACTはLCSの適合率・再現率に基づきスコア
を決定する．つまり，
参照翻訳と一致する単語の割合が大きいシステム翻訳にしか
高いスコアを与えることができない．よって，ルールベースシステム
に高いスコアを与えることはできず，それらの性能を低く見積もってしまったこ
とにより，相関が著しく低下したと考える．

一方，RIBES とLRscoreはこれらの手法とは異なり，単語正解率，BLEUスコアをパラメタで
軽減することで単語一致率の低いシステムであっても高いスコアを与
えること
ができるという特徴がある．
実際，日英，英日の双方においてROUGE-L，IMPACT，BLEUといった従来の自動評価法に
対し，統計的有意に高い相関係数を獲得していることがそれの有効性を示唆してい
る．

ユニグラム適合率が低いところでの自動評価法の性能をより詳細に調べるため，
ルール
ベースの翻訳システムのみを取り出し，同様の実験を行い相関係数の平均値を
求めたところ，表\ref{rbmt}を得た．翻訳システム数は日英翻訳タスクで6，英
日翻訳タスク
で5である．サンプル数が少ないため，相関係数の値に対する信頼性がこれまで
の実験よりも劣ることに注意されたい．表\ref{rbmt}より，RIBES は日英翻訳タス
クで ROUGE-L，IMPACT に劣るものの全体を通してみれば他の手法より良い相関
を得ている．参照翻訳が1つしかないという影響もあるが，英日翻訳タスクでは
ROUGE-L，IMPACT，BLEU は負の相関でしかない．さらに，先に示したとおり，
表\ref{NTCIR9}において RIBES がルールベースシステム，SMT システム双方
を含む場合でも良い相
関を得たこともふまえると，参照翻訳とシステム翻訳との間で一致す
る単語が少ない場合でも RIBES は有効であると考える．

\begin{table}[b]
  \caption{RBMTだけを用いたメタ評価の結果（NTCIR-9データ，単一参照翻訳）}
  \label{rbmt}
\input{0985table06.txt}
\vspace{4pt}\small
  表中，1はRIBES，2は
  LRscore(${d_k}_1$)，3はLRscore(${d_k}_2$)，4はROUGE-L，5はIMPACT，6はBLEUに対し，
  有意水準5\%で有意差があることを示す．
\par
\end{table}

以上より，BLEU，ROUGE-L，IMPACTといった単語の一致に強く依存する従来の自
動評価法は，単一参照翻訳時，評価対象としてルールベースシステムが多く混在
する場合には著しく信頼性が低下することを示した．特に，参照翻訳は常に
複数与えられるとは限らないため，自動評価法としては単一参照翻訳でも人間の
評価結果との間の相関が高いことが望ましい．
RIBES は，参照翻訳数，ルールベースシステムの数が変化した場合でも
安定して高い相関であることから，従来の自動評価法よりも優れていると考える．
ただし，RIBES には単語正解率と短い翻訳に対するペナルティを調整するため
の重みパラメタがある．
これらパラメタを最適化するためには，いわゆる教師データが必要となることか
ら，
それを必要としない BLEU，ROUGE-Lよりもコストのかかる手法とも言える．
しかし，実験結果より，
各システムに対し10文を人間が評価した結果を教師データとしてパラメタを最適
化できることを示した．よって，十分低いコストでパラメタの最適が可能である．


\subsection{獲得されたパラメタに関する考察}

 \begin{figure}[b]
   \begin{center}
\includegraphics{21-3ia985f3.eps}
   \end{center}
    \caption{獲得されたパラメタの分布(RIBES)}
    \label{parm}
\end{figure}

最後に RIBES と LRscore について獲得されたパラメタ，
$\alpha$, $\beta$, $\gamma$の違いから考察する．図 \ref{parm}に$\alpha$, $\beta$の
分布，図 \ref{lr-parm}に$\gamma$の分布を示す．

 \begin{figure}[t]
   \begin{center}
\includegraphics{21-3ia985f4.eps}
   \end{center}
    \caption{獲得されたパラメタの分布(LRscore)}
    \label{lr-parm}
\end{figure}

図 \ref{parm}より，パラメタ最適化のための訓練事例が10文と少ないことも影響
してか，獲得されたパラメタにばらつきがあるが，単一参照翻訳の場合には小
さな$\alpha$が選択されている割合が多く，4.3 節の仮説と一致する．また，
NTCIR-9 のように単一参照翻訳かつルールベースシステムの数が多い場合には，
$\alpha=0$の場合が非常に多い．一方，複数参照翻訳がある場合，比較的大きな
$\alpha$が選択されている．$\beta$に関しては，複数参照翻訳時には高い値が
選ばれている割合が高く，4.3 節の仮説と一致するが，単一参照翻訳時には必ず
しも低い値が選ばれておらず先の仮説と一致しない．
しかし，人間の
評価との間の相関をみる限り，RIBES は従来法よりも比較的高い相関を得てい
ることから，これは大きな問題ではないと考える．

図 \ref{lr-parm}より，LRscore の NTCIR-9 では，$\gamma=1$付近が多く選択
されており，
語順の相関に対する重みを上げ，
語彙の一致（BLEUスコア）に対する重みを下げるようにパラメタを選択しており，
RIBES と同様の傾向を示している．しかし，NTCIR-7 では，単一参照翻訳，複数参
照翻訳に関わらず0.3から0.6までの値が多く選ばれており，NTCIR-9 の場合ほど
BLEU スコアに対する重みを下げるようなパラメタが選択されていない．
NTCIR-7
では RIBES の相関が概ね LRscore の相関を上回っていたが，その原因はこうした語彙の
一致に対する重みの違いによると考える．
さらに RIBES は2つのパラメタ$\alpha$, $\beta$があることに対し，LRscoreは1つの
パラメタ$\gamma$しかない．よって，RIBES はLRscoreよりもより柔軟に
データにフィットできる点が双方の手法のパラメタ選択，相関係数の差に影響を
与えたとも考える．

5.2 節でも述べたが，$\alpha=0$, $\beta=1$，$\gamma=1$とした場合，RIBES も
LRscoreも語彙の一致スコアを考慮せず順位相関と短い翻訳に対するペナルティ
だけを考慮することになり，両者はほぼ一致する．図 \ref{lr-parm}より，
LRscore では$\gamma=1$が試行の半数近くで選択されているが，
図 \ref{parm}
の NTCIR-9 でそうした例がみられるものの全体に占める割合は決して多
くはない．
また，LRscore の語順相関の計算法として${d_k}_1$と${d_k}_2$の双方を試した
が，一貫して，${d_k}_1$の方がよい相関を示した．
こうしたことから，基本的には RIBES と LRscore は違うものと捉えて差し支え
ないだろう．


