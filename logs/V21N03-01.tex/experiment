\section{実験の設定}


\subsection{実験データ}

RIBES の有効性を示すため，NTCIR-7，NTCIR-9の特許翻訳タスク (PATMT) の
データを用いて評価実験（評価指標の評価なので，以降メタ評価と呼ぶ）を行った．
言語対は英日 (EJ)，日英 (JE)とした．
それぞれのデータセットの文数，1文あたりの参照翻訳の数，評価者の数，参加
システム数
を表\ref{data}に示す．なお，カッコ内の数字はルールベースシステムの数を示
す．

\begin{table}[b]
  \caption{実験データの詳細}
  \label{data}
\input{0985table02.txt}
\end{table}

NTCIRワークショップの事務局から公開されてい
るデータには，EJ，JEタスクとも1つの参照翻訳しか含まれていない．
そこで，NTCIR-7のデータに対してのみ，特許翻訳の専門家に依頼し，参照翻訳を
独自に追加した．
また，NTCIR-7のEJタスクに関しては，5システムだけにしか人間の主観
評価の結果が与えられていなかったため，特許に精通した被験者5名で再度 JE
タスクと同様，5段階評価で主観評価を行った．
さらに，評価対象とする翻訳システムに著者のグループの英日翻訳システム\cite{headfinal}を追加
し，計14システムで実験を行った．

全てのデータに対し，メタ評価の対象は翻訳の内容としての適切性 (adequacy) のみ
とした．これは，翻訳の流暢さよりも内容の適切性を
自動評価できた方がより良い翻訳システムの開発に貢献できると考えたからである．

なお，各システム翻訳文に対し複数の人間の評価スコアが与えられている場合に
は，その平均値を文に対する評価スコアとした．このように各システム翻訳
文に対して評価値を決定し，これを文集合全体での平均したものを人
間がシステムに与えた評価スコアとした．


\subsection{比較した自動評価手法}

比較評価には，Nグラム一致率に基づく評価手法として先に説明したBLEU，
大局的な単語列を考慮した評価法として同じく先に説明した
ROUGE-L \cite{ROUGEL}，
その改良版である IMPACT \cite{impact}を用いた．
IMPACTには，LCS の長さに応じた重みパラメタ，語順の入れ替えに応じた
重みパラメタがある．詳細については文献 \cite{impact} を参照されたい．
なお，ROUGE-L，IMPACTとも参
照翻訳
が複数ある場合には個々の参照翻訳を用いて求めたスコアの最大値を評価スコア
として採用した．BLEU の計算には {\tt mteval-v13a}，ROUGE-L 
には，\texttt{ROUGE-1.5.5}，IMPACT には \texttt{IMPACT version 4}を利用した．

また，LRscore \cite{birch,birch-wmt2010,birch-acl}も比較評価の対象とした．
LRscore は，参照翻訳とシステム翻訳との間の語順の近さを表すスコアとBLEUス
コアとの間の線形補間で評価スコアを決定する．
語順の近さを表す尺度としては，ハミング距離$d_h(h,r)$を利用する
ものと Kendall の$\tau$に基づく$d_k(h,r)$を利用するものがあるが，以降で
は，本稿との
関連が深い後者について述べる．
LRscore の定義を以下に示す．
\begin{equation}
 \mbox{LRscore}(\mathcal{H},\mathcal{R})=\gamma R(\mathcal{H},\mathcal{R})+(1-\gamma)\mbox{BLEU}(\mathcal{H},\mathcal{R})
\end{equation}

$R({\mathcal H},{\mathcal R})$は以下の式で定義される．
\begin{equation}
 R(\mathcal{H},\mathcal{R})=\frac{\displaystyle\mathop\sum_{h_i \in \mathcal{H}} d(h_i,r_i)\mbox{BP}_s(h_i,r_i)}{|\mathcal{H}|}
\end{equation}

$d_k(h,r)$は，文献\cite{birch-acl} に従うと
$d_k(h,r)=1-\sqrt{1-\mbox{NKT}(h,r)}$で定義されるが，それ以前の文献
\cite{birch,birch-wmt2010} では，$d_k(h,r)=\mbox{NKT}(h,r)$も用いられて
いる．以降，前者を${d_k}_1$，後者を${d_k}_2$とよぶ．
RIBES で $\alpha=0$, $\beta=1$と設定したときと，LRscore に ${d_k}_2$ を採用，
$\gamma=1$と設定したとき，これら2つの手法は一致する．しかし，LRscore
は日本語，英語のような大きな語順の入れ替えがある言語対を対象として考案
された手法ではなく，ヨーロッパ言語間，中英\footnote{
もちろん，英語，フランス語ほど語順が近くはないが，英語も中国語も SVO
型の言語であり，日本語，英語ほどの語順の違いはない．}翻訳という比較的語順が似た言語を対象として考案されたため，最終的には
${d_k}_1$を採用することで順位相関の低レンジスコアの感度を下げ，さらに語順の近い言語対を
対象としたときに実績のある BLEU \footnote{
実際，NTCIR-9 の中英翻訳タスクにおいて，BLEUは人間の評価結果との相関が
0.9以上の非常に高い値を記録している \cite{ntcir9}．} の恩恵を受けるため，それとの間の線形補間という定式化に至ったのであろう．
後述するが，英日，日英翻訳の評価では BLEU を利用するメリットは期待できな
い．さらに，NKT を${d_k}_1$によって非線形変換することで低レンジスコアの
感度をさらに下げるメリットも元々高いNKTを得ることが難しい英日，日英翻訳タ
スクでは期待できない．
以上より，LRscore は確かに RIBES と良く似た手法といえるが，
 BLEU を補うために派生した評価指標と捉えた方が自然であり， 
RIBES とはその根底にある研究の動機に大きな違いがある．
なお，LRscore には，参照翻訳とシステム翻訳との間の単語
アラインメントを決定する手段が提供されないため，
以降の実験では本稿での単語アラインメントを利用した．


\subsection{メタ評価の指標}

本稿では，メタ評価の指標として広く用いられている Pearson の積率相関係数，
Spearman の順位相関係数，Kendall の順位相関係数を用いた．Pearson の積率
相関係数は人間の評価と自動評価の結果がどの程度線形の関係にあるかを評価し，
Spearman，Kendall の相関係数は人間の評価と自動評価の結果の順位がどの程度
近いかを評価する．
Spearman と Kendall の違いは，先にも説明したように
順位の差に対して重みをどのように与えるかという点にある．


\subsection{実験の手順}

RIBES に対してはシステム翻訳の長さに対する重みパラメタと単語正解率に対
する重みパラメタ，IMPACT に対してはLCSに対する重みパラメタと語順の違いに
対する重みパラメタ，LRscore には順位相関係数とBLEUスコアの重みを調整する
パラメタがある．
これらの手法に対しては，以下の手順でパラメタの最適化を行い，メタ評価を行った．

\begin{enumerate}
 \item 文のIDをランダムに10個選択する．
 \item 選択したIDによる10文の集合を用いて，文集合全体での人間の評価スコアと自動
       評価スコアとの間の Spearman の順位相関係数が最大となるようパラメタを決定する．
 \item (2) で決定したパラメタを用いて (1) の残りの文集合全体を用いてメタ評
       価を行い，相関係数を記録する．
 \item (1) から (3) を100回繰り返し，相関係数の平均値を求める．
\end{enumerate}

なお，パラメタが存在しないBLEUとROUGE-Lに対しては，(2) をスキップし，同様
の手順でメタ評価を行った．


