単語グループに基づくWeb検索結果のクラスタリング手法 \label{sec:method}

\subsection{概要}\label{subsec:overview}

提案する手法の概要は以下の通りである．

\begin{enumerate}
\item ユーザの入力したクエリを受け取り，Googleによる検索結果の
  タイトルとスニペットを文書として取得する．
  本論文の以下では，各ページのタイトルとスニペットを
  ひとつの「文書」と呼ぶ．

\item 各文書に対して，茶筌 (http://chasen-legacy.sourceforge.jp/) を用いて形態素解析を行う．
  
\item 形態素解析で名詞・英字と判断された単語から，複合名詞を含む名詞を抽出する．

\item 抽出した名詞から，クラスタの話題を表すと考えられる
  互いに類似していない重要語を，指定された文書クラスタ数だけ抽出する．
        
\item 手順(3)で抽出されたすべての単語に対して，各重要語から単語グループを生成する．

\item 単語グループを用いて，文書クラスタを生成する．
\end{enumerate}

以下の\ref{sec_WM}節から\ref{sec_bun}節では，
上記の手順(3)から(6)の各処理の詳細を述べる．



\subsection{形態素解析結果からの名詞抽出} \label{sec_WM}

まず形態素解析により名詞及び英字と判断された単語を抽出する．
この際に，非自立の名詞や代名詞などは除き，英字の連続はひとつの名詞とする．
また，各単語$w_i$の文書頻度$df(w_i)$（$w_{i}$を含む文書数）を
検索結果の文書集合全体から計算し，一定値$C_W$以下の単語を除外する．
さらにクエリ及びクエリの一部となる単語は，ほぼ全ての文書に出現するため，
手順(4)の重要語の抽出に大きな影響を及ぼすので除外する．

次に，これらの単語から構成される名詞の$n$グラム（複合名詞）を，
重要語候補として抽出すべきかどうかを判断する．
例えば，文書集合中で「情報」や「検索」という名詞が，ほぼ「情報検索」という
複合名詞でしか用いられていない場合には，「情報検索」をひとつの単位
として抽出すべきである．
また，形態素解析が固有名詞と認識できないために不適切に分解されて
しまう固有名詞（例：「エースコック」）を適切に抽出することも意図
している．

以下の手法により，重要語の候補として抽出すべき（複合名詞を含む）名詞を決定する．
\begin{enumerate}
\item $\Sigma_1\leftarrow$（すべての単語の集合），$n \leftarrow 1$とする．

\item $\Sigma_{n+1} \leftarrow \phi$ とする．

\item 集合$\bigcup_{i=1}^n \Sigma_i$中の単語$w_{i}$と，集合$\Sigma_n$中の単語$w_{j}$の
  すべての組み合わせ（ただし$w_i\neq{w_j}$）に対して，以下の処理を行う．
  \begin{enumerate}
  \item 2つの単語をつなぎ合わせた語句$w_{i}w_{j}$，$w_{j}w_{i}$のうちで，
    全文書における出現頻度が高い方を合成候補$Str$とする．
    ただし，一方の単語がもう一方の単語を部分文字列として含む場合には，
    合成はせずに長いほうの単語を$Str$とする．

  \item 全文書における$w_{i}$，$w_{j}$及び$Str$の出現頻度
    （\ref{sec_weight}節の式(\ref{eqn:tf})で定義される）を
    それぞれ$tf(w_i)$，$tf(w_i)$，$tf(Str)$としたとき，
    次式で定義される値$WM$を計算する．
    \[
    WM = \frac{tf(Str)}{\max(tf(w_i)，tf(w_j))}
    \]
  \item 上記で計算した$WM$が閾値$C_{WM}(>0.5)$以上ならば，
    $\Sigma_{n+1} \leftarrow \Sigma_{n+1}\cup\{Str\}$，
    $\Sigma_1 \leftarrow \Sigma_1 - \{w_i\}$，
    $\Sigma_n \leftarrow \Sigma_n - \{w_j\}$とする．
    つまり，$w_{i}$，$w_{j}$の代わりに$Str$を複合名詞として用いることになる．
  \end{enumerate}

\item $\Sigma_{n+1}=\phi$ならば，$\Sigma = \bigcup_{i=1}^n \Sigma_i$を複合名詞
  （重要語候補）の集合として終了する．
  $\Sigma_{n+1}\neq\phi$ならば，$n$を1増やしてから手順(2)に戻る．
\end{enumerate}
閾値$C_{WM}$を適切に（0.5より大きく）設定することによって，
$w_{i}$や$w_{j}$が単独で出現するよりも複合名詞$Str$として
出現することが多い場合に，複合名詞として抽出することができる．

なお，本論文の以下では，複合名詞を含む重要語候補（$\Sigma$の要素）のことを
単に「名詞」や「単語」と表記する．



\subsection{重要語の抽出} \label{sec_weight}

前節で得られた名詞集合$\Sigma$から，
以下の手順を用いて，重要語を抽出する．
\begin{enumerate}
\item 抽出されたすべての名詞に対して，
  \ref{subsubsec:weight}節で述べる重み付け手法を用いて，ランク付けする．
  結果として得られた名詞のランク付きリストを$S$とする．
\item リスト$S$の中でランクの最上位にある名詞を取り出して，重要語とする．
\item 抽出した重要語との類似度（\ref{subsubsec:cosine}節参照）が
  基準値$C$以上のすべての名詞をリスト$S$から取り除く．
  なお，\ref{subsubsec:threshold}節で述べるように，
  基準値$C$は文書集合に応じて自動的に決定する．
\item 重要語の個数が指定されたクラスタ数 $n$ に満たない場合には，手順(2)に戻る．
\end{enumerate}
上記の手順(3)において，抽出された重要語と話題が類似する名詞を
重要語（クラスタのベースとなる語）としないことによって，
本手法は重要語どうしの類似度が低くなるように重要語を抽出する．
なお，\ref{sec:intro}章で述べた従来の手法は，手順(1)で得られるリスト$S$
のランク上位 $n$ 個をクラスタのベースとなる重要語として抽出することに
相当する．


\subsubsection{名詞の重み付け}\label{subsubsec:weight}

上記の手順(1)における名詞の重み付け手法としては，以下の基準が考えられる．
なお，\ref{sec:evaluation}章で述べる評価実験では，これらのどの基準を
用いても本手法のほうが優れていることを示す．

\begin{description}
\item[文書頻度 df] 名詞$w_i$の出現する文書数である$df(w_i)$の値が大きいほど，
  その名詞が重要であると考える．
  なお，計算に用いる文書は検索結果の文書集合全体である．
\item[出現頻度 tf] 次式で計算される文書集合中の総出現頻度$tf(w_i)$が高い名詞が
  重要であると考える．
  \begin{equation}
    tf(w_i) = \sum_{j=1}^{N} tf(w_i,d_j) \label{eqn:tf}
  \end{equation}
  
  ただし，$tf(w_i,d_j)$は文書$d_j$における単語$w_i$の出現数，
  $N$は文書数をそれぞれ表す．

\item[tfidf] 次式で定義される tfidf 値が高い（特定の文書に多く出現する）
  名詞が重要であると考える．
  \begin{align}
    tfidf(w_{i}) &= tf(w_i) \times idf(w_i)\\
    idf(w_i) &= log_{2} \left( \frac{N}{df(w_{i})} \right) + 1
  \end{align}

\item[SP, LP \cite{Narita03}] 次式で定義される$SP(w_i)$もしくは$LP(w_i)$が高い，
  つまり検索結果のランキング上位の文書に多く含まれる名詞ほど重要であるとする
  指標である．
  \begin{align}
    SP(w_{i}) &= \sum_{j=1}^{N} 
    \left[
      tf(w_{i},d_{j}) \times sin\left({\frac{\pi}{1+\sqrt{j}}}\right)
    \right] \times idf(w_{i}) \\
    LP(w_{i}) &= \sum_{j=1}^{N} 
    \left[
      tf(w_{i},d_{j}) \times log_{N}\left(\frac{N}{j}\right)
    \right] \times idf(w_{i})
  \end{align}
  
  ただし，$d_j$はサーチエンジン（本研究ではgoogle）の検索結果のランキングが$j$番目の文書を表す．

\item[TR \cite{Gelgi07}] $TR(w_i)$は単語をノード，共起の有無をエッジとするグラフの
  PageRankのように計算される値であり，$TR(w_i)$が高い（つまり重要な）単語と多く
  共起している単語は重要であると考える指標である．
  
  \begin{equation} 
    TR^{(t+1)}(w_i) = 
    \sum_{j = 0}^{N} \frac{TR^{(t)}(w_{j}) corres(w_i,w_j)}{\sum_{k = 0}^{N} corres(w_k,w_j)} 
    \label{eq:TR}
  \end{equation}
  
  ただし，{$corres(w_i,w_j)$}は{$w_i$}と{$w_j$}の共起回数，{$t$}は繰り返し計算回数を表し，
  {$TR^{(0)}(w_i)=tf(w_i)$}である．

\end{description}


\subsubsection{名詞どうしの類似度の計算}\label{subsubsec:cosine}

上記の手順(3)において，名詞どうしの類似度$sim(w_i,w_j)$
（抽出された重要語とリスト$S$に含まれる名詞との類似度）
には，次式のコサイン (cos) 類似度を用いる．
\begin{equation}
sim(w_i,w_j) = cos(V_{i},V_{j}) = 
\frac{\sum_{k=1}^{N} {tf(w_i,d_k) \cdot tf(w_j,d_k)}}
{\sqrt{\sum_{k=1}^{N} tf(w_i,d_k)^2} \sqrt{\sum_{k=1}^{N} tf(w_j,d_k)^2}} \label{eqn:cosine}
\end{equation}
つまり，名詞$w_i$を，文書$d_k$における出現頻度$tf(w_i,d_k)$を要素とする$N$次元ベクトル
で表現したときのコサイン類似度に相当する．


\subsubsection{基準値$C$の設定}\label{subsubsec:threshold}

基準値$C$が0.05〜0.5（0.05刻み）のいずれかの値をとるものとして，
それぞれの値で実際に文書クラスタリングを行い，最も多くの文書を分類できる
（つまりどのクラスタにも属さない文書が最も少ない）値を基準値$C$として採用する．
ただし抽出した重要語が指定したクラスタ数に満たなかった場合\footnote{
  \ref{sec_weight}節の手順(3)において，
  重要語と類似しているとして多くの単語が取り除かれる場合に，
  このようが現象が生じるときがある．}
には，指定したクラスタ数に最も近いものの中での最適値を基準値とする．

なお，この判定に用いる文書クラスタリング手法は，
\ref{subsec:word_clustering}節で述べる単語グループを用いた方法ではなく，
本節で述べた方法で抽出した重要語を含む文書をクラスタとする方法である．


\subsection{単語クラスタリング} \label{subsec:word_clustering}

前節の方法で得られた重要語に対して，以下のアルゴリズムを用いて
単語グループを生成する．
\begin{enumerate}
\item 各重要語$x_i$に対応する単語グループ$WG_i$を以下の方法で生成する．
  \begin{enumerate}
  \item 重要語$x_i$とのcos類似度（(\ref{eqn:cosine})式）が基準値$C'$以上
    の名詞（重要語は除く）をリスト$S$からすべて抽出する．
  \item 抽出した名詞集合に対し，重要語$x_i$とのcos類似度の平均値$M$を求める．
  \item 重要語$x_i$とのcos類似度が平均値$M$以上の名詞のみを，その重要語を
    核とした単語グループ$WG_i$に含める．
  \end{enumerate}
\item 複数の単語グループに含まれる名詞を，すべての単語グループから取り除く．
\end{enumerate}
基準値$C'$は0.05〜0.5（0.05刻み）のいずれかの値をとるものとし，
それぞれの値で実際に文書クラスタリングを行い，最も多くの文書を
分類できる値を基準値$C'$として採用する．
この判別に用いる文書クラスタリング手法は，\ref{sec_bun}節で述べる手法を用いる．


\subsection{単語グループからの文書クラスタリング} \label{sec_bun}

以下のアルゴリズムを用いて，単語グループから文書クラスタを生成する．
\begin{enumerate}
\item 単語グループ$WG_i\,(i=1,\cdots,n)$に対応する空の文書クラスタ
  $DC_i=\phi$を生成する．
\item 以下の方法で，各文書$d_j\,(j=1,\cdots,N)$がどの文書クラスタに
  含まれるかを決定する．
  \begin{enumerate}
  \item 文書$d_j$が単語グループ$WG_i$の核となった重要語$x_i$を含んでいれば，
    文書$d_j$を単語グループに対応する文書クラスタ$DC_i$に含める．
    （複数の重要語$x_i$を含んでいれば，複数の文書クラスタに属することになる．）

  \item 全ての単語グループ$WG_i$に対し，以下の式で定義される$S_i(d_j)$を計算する．
    
    \begin{align}
      S_i(d_j) &= \frac {\sum_{w_k\in{WG_i}}\delta(w_k,d_j)} {|WG_i|} \\
      \delta(w_k,d_j) &= \begin{cases}
        1 & \text{（名詞$w_k$が文書$d_j$に出現する場合）}\\
        0 & \text{（名詞$w_k$が文書$d_j$に出現しない場合）}
	\end{cases}  \nonumber 
    \end{align}
    
    そして，$S_i(d_j)$の値が以下の不等式を満たすならば，文書$d_j$を文書クラスタ$DC_i$に含める．
    \begin{equation}
      S_i(d_j) \geq \frac{1}{2}\sum_{k=1}^n S_k(d_j) > 0
    \end{equation}
    
    直観的に言うと，文書$d_j$が他の単語グループよりも単語グループ$CW_i$の単語を
    多く含んでいれば，その文書は$CW_i$に対応する文書クラスタ$DC_i$に分類される
    ことになる．
  \end{enumerate}
\end{enumerate}



