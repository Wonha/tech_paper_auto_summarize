評価実験
\label{sec:ev}

\subsection{提案手法と従来手法の比較}
\label{sec:comp_es_bleu}

提案手法では従来手法と比べ，下記の2点が異なる．
\begin{enumerate}
 \item 各感情コーパスにおける``入力文の形態素N-gram''の出現回数によって
       決まるペナルティFPの導入 \label{item:new1}
 \item 形態素N-gramの適合率の相加平均による類似度計算 \label{item:new2}
\end{enumerate}
評価実験では上記の2点によって感情推定の成功率がどの程度変化するのかを調
べる．


\subsubsection{実験設定}

実験には，\resp{基本的な感情であり，収集したコーパス中に比較的頻出
した}``喜び''，``怒
り''，``嫌悪''，``希望''の4種類の感情カテゴリを用いた．各感情コーパスに
は838文の発話文が含まれる．発話文はWeb上の掲示板
から8名の作業者によって収集した．
発話文の分類先とな
る感情コーパスは，作業者の主観によって決定した．また発話文の分類先は複
数選ぶことを許容した．

入力となる文とその感情は次の手順で決定した．
まず，感情コーパスに含まれない，別途掲示板から収集した文を無作為な順番で被験者4名に提示し，
被験者に文の感情を判定させる．このとき，
判定結果としての感情を``喜び''，``怒り''，``嫌悪''，``希望''の中から0個以上を選ばせる．
被験者4名のうち3名以上の判定結果が一致した文を各感情ごとに51文ず
つ用意し，入力文として用いる．なお，この予備実験で入力文に割り振られた感
情の数はすべて1つとなった．

感情推定の成功条件として，出力として得られる4つの感情類似度のうち，最も
値が大きい類似度の感情と，入力文の感情が一致すれば成功とした．

上記の(\ref{item:new1})，(\ref{item:new2})の効果を確かめるため
に，実験に用いた類似度計算式は\resp{三品らの方法（式 ({\ref{eq:bleu}})）}と
\resp{RECARE（式({\ref{eq:recare}})）}に加え，
BLEUにFPのみを導入した\resp{{$sim_{\rm BLEUFP^{+}}$}（式({\ref{eq:bleu_with_fp_pi}})）}と，\respeqn{RECARE}からFPを除い
た$\respeqn{sim_{\rm RECAREFP^{-}}}$を用いた．$\respeqn{sim_{\rm RECAREFP^{-}}}$を次のとおりに定義する．
\begin{equation}
 \respeqn{sim_{\rm RECAREFP^{-}}(x,s)} = {\rm BP} \cdot \frac{1}{\rm N}\sum^{\rm N}_{n=1}
 p_{n}(x,s) \label{eq:es_without_fp}
\end{equation}


\subsubsection{実験結果と考察}

類似度の計算に用いる$\respeqn{\ngram}$の$n$の値を変化させながら，推定成功率を計算
した．
図\ref{fig:success_ratios}に感情推定の成功率を示す．三品らの方法で最も推定成
功率が良好だったのは$\rm N=2$を用いたときの60.3\%であり，提案手法では$\rm N=3$を
用いたときの\resp{81.8\%}であった．ここでは，まず(\ref{item:new1})のFPを導入したことによる成
功率の影響について考察する．図\ref{fig:success_ratios}より，FP無しの
$\respeqn{sim_{\rm RECAREFP^{-}}}$で最も良好だった成功率57.8\%(N=3)と比
べて，FP有りの提案手法$\respeqn{sim_{\rm RECARE}}$の
成功率\resp{81.8\%}が大きく上回っていることがわかる．同様に，FP無しの従来手法
$\respeqn{sim_{\rm BLEU}}$の
成功率60.3\%と比べて，FP有りの$\respeqn{sim_{\rm BLEUFP^{+}}}$で最も良好
だった成功率\resp{77.9\%(N=1)}が大きく上回ってい
る．これらのことから，
形態素N-gramの適合率の平均の求め方に関わらず，FPの
導入が成功率の向上に寄与していることがわかる．

次に(\ref{item:new2})の，類似度計算に形態素N-gramの適合率の相加平均を用
いたことによる成功率の影響を考察する．
$\rm N=2$以降で$\rm N$が増加するにつれて，三品らの方法の成功率は減少して
\resp{いるが，}
提案手法においては\resp{成功率の減少は認められなかった．}このこと
から，形態素N-gramの適合率の相加平均を類似度計算に用いることは，高次の
$\rm N$を用いたときの成功率の改善に効果があることがわかる．

\begin{figure}[t]
 \begin{center}
  \includegraphics{17-4ia6f2.eps}
 \end{center}
 \caption{感情推定成功率}
 \label{fig:success_ratios}
\end{figure}

\resp{なお，類似度計算に相乗平均を用い，FPを導入した方法 ({$sim_{\mathrm{BLEUFP}^{+}}$}) は，Nを増加させると急激にその性能を落としていた．この原因は，Nが大きくなる
と共通の形態素N-gramがコーパス中に存在しなくなる割合が増加する
ため，式 ({\ref{eq:wn}}) において，1文中のすべての{$w_{n}$}で{$\freq{C_{e}}w_{n}$}が0になる，という場合が増加したためであった．
提案方法においては，類似度計算に相加
平均を用いることでこの問題を解決し，Nが大きい場合においても高い性能を維持
していることがわかった．}

\resp{このような場合は，共通形態素N-gramが存在しないため，FPと
同様に形態素N-gram適合率 ({$p_{n}(x,s)$}) 
も0になる．そこで，{$sim_{\mathrm{BLEUFP}^{+}}$}だけではなく，同じように相乗平均
を利用している従来方法 ({$sim_{\mathrm{BLEU}}$}) も影響を受けると考えられる．
本実験においては，従来方法における類似度計算に式 ({\ref{eq:bleu}}) を用いて
いる．この式では{$p_{n}(x,s)$}の対数をとっているために，実装上，もし
{$p_{n}(x,s)=0$}であった場合は，非常に小さな正の値にフロアリングした上で
対数を求めていた．そのため，{$sim_{\mathrm{BLEUFP}^{+}}$}のように急激に性能を落とす
ことはなかったと考えられる．このことを確認するため，式 ({\ref{eq:bleu}}) を式
 ({\ref{eq:bleu_with_fp_pi}}) と同様，対数を用いない形に変形した上で，
フロアリングをせずに実験を行ったところ，{$sim_{\mathrm{BLEU}}$}も{$sim_{\mathrm{BLEUFP}^{+}}$}
と同様，Nが大きくなるとその性能を急激に落とす結果となった．}


\subsubsection{\resp{感情推定に有効な形態素N-gram}}

\resp{提案方法（や三品らの方法）においては，各感情コーパス中に含まれる形態素
N-gramのうち，感情ごとに出現頻度に偏りがあるものが，感情推定において重要
な意味を持つ．そこで，どのような形態素N-gramが提案方法にとって有効に働いた
のか，といったことについて調査を行った．}

\begin{table}[b]
  \caption{喜びの文の感情推定に寄与していた形態素N-gramのコーパス別出現回数の一部}
\label{tbl:freq1}
\input{06table03.txt}
\end{table}
\begin{table}[b]
 \caption{怒りの文の感情推定に寄与していた形態素N-gramのコーパス別出現回数の一部}
\input{06table04.txt}
\end{table}

\resp{感情推定実験において，三品らの方法では感情判定に失敗し，提案方法で
成功した入力サンプルを抽出し，その中に含まれるすべての形態素N-gramにつ
いて，各感情コーパス内での出現頻度を調べた．特に正解感情の
コーパスに偏って頻出している形態素N-gramを表{\ref{tbl:freq1}}〜
{\ref{tbl:freq4}}に示す．
これを見ると，「喜び」の``ありがとう／感動詞''や``♪／名詞''，
「怒り」の``むかつく／動詞''，
「希望」の``たい／助動詞，です／助動詞''等，感情を表現するであろうと思われる
形態素N-gramが感情推定に寄与していたことがわかる．}

\begin{table}[t]
 \caption{嫌悪の文の感情推定に寄与していた形態素N-gramのコーパス別出現回数の一部}
\input{06table05.txt}
\end{table}
\begin{table}[t]
 \caption{希望の文の感情推定に寄与していた形態素N-gramのコーパス別出現回数の一部}
\label{tbl:freq4}
\input{06table06.txt}
\end{table}

\resp{一方，「喜び」の``でし／助動詞，た／助動詞''や「嫌悪」の``顔／名詞''等，
一見すると感情とは無関係と思われる形態素N-gramも存在した．
これらについては，今後別の角度からの検証（例えば，Web掲示板において
喜びを表現する時は，「〜でした」のような丁寧な表現が用いられること
が多い，といった仮説を立て，統計的に検証する）を行う必要があるが，
今まで発見されていなかった事実を暗示するものである可能性がある．
また，「怒り」に``職場／名詞''，``仕事／名詞''，
「嫌悪」に``上司／名詞''，``会社／名詞''
がはいっていることも，「Webの掲示板においては，仕事に対する不満や
愚痴等が多い」といった事実を暗示しているのかもしれない．}



\subsection{\resp{FPの導入効果の検証}}

\resp{FPはコーパス中に含まれる以下のふたつの文の影響を低減する目的で
提案された．}
\begin{enumerate}
 \item \resp{感情が異なっていても，たまたま表現や文型が類似している文}
 \item \resp{コーパスを構築する際に誤って分類された文}
\end{enumerate}
\resp{FPがこうした文に対して，どの程度頑健性を持っているかを検証するため，
感情コーパス内に存在するこうした文を増減させ，その時の性能を評価した．
また，FPのかわりにTF-IDFを用いた時の性能についても評価を行った．}


\subsubsection{\resp{表現や文型が類似している文の影響}}

\resp{``入力文と感情が異なっているが，たまたま表現や文型が類似している文''の
影響について，こうした文を各感情コーパスから削除した時の性能を評価する
ことで検証を行った．}

\resp{
``たまたま表現や文型が類似している文''
として``入力文とは感情が異なるコーパス中において，BLEUスコアが高い文''と
定義し，各感情コーパスにおいて，このような文をBLEUスコア順に上位から{$n$}文
削除した．また条件を揃えるため，正解となる感情コーパスからは
乱数で{$n$}文削除した．}

\begin{figure}[b]
 \begin{center}
  \includegraphics{17-4ia6f3.eps}
 \end{center}
 \caption{表現等が類似している文の影響}
 \label{fig:similar}
\end{figure}

\resp{
感情の推定成功率を図{\ref{fig:similar}}に示す．
ここで横軸は，コーパス1つあたりの除去した文数を表す．
この結果を見ると，三品らの
方法，提案方法どちらも``たまたま表現や文型が類似している文''を削除する
ことで性能が向上していることから，これらの文の影響を受けていたことがわかる．
しかし，提案方法では文を一切削除しなかった場合においても
比較的性能の低下が抑えられていることから，
その目的である``たまたま表現や文型が類似している文''による影響を抑える
ことができていることがわかった．}



\subsubsection{\resp{感情分類を誤った文の影響}}

\resp{
次に，各感情コーパス中で感情分類を誤った文を意図的に増加
させ，その時の性能を評価した．
具体的には，各感情コーパスから{$n$}文を乱数で抽出し，それら
を他の感情コーパスへと均等に混入させることで，感情分類を誤った文を
増加させた．}

\begin{figure}[b]
 \begin{center}
  \includegraphics{17-4ia6f4.eps}
 \end{center}
 \caption{感情分類を誤った文の影響}
 \label{fig:shuffle}
\end{figure}
\begin{table}[b]
\caption{感情分類を誤った文に対する性能差}
\label{tbl:shuffle-diff}
\input{06table07.txt}
\end{table}

\resp{
感情推定成功率を図{\ref{fig:shuffle}}に示す．
ここで横軸は，コーパス1つあたりの混入させた文数を表す．
この結果を見ると，混入する
文が増加するに従って両方法ともに性能が低下し，感情分類を誤った文の影響を
大きく受けていることがわかる．しかし，両者の性能差
（表{\ref{tbl:shuffle-diff}}）はわずかではあるが
拡がっており，提案方法の方が若干ではあるが，こうした
文の影響を低減できていることがわかった．}


\subsubsection{\resp{TF-IDFの導入との比較}}

\resp{FPの有効性を確認するため，FPのかわりにTF-IDFを用いた実験を行った．
ここでは，TF-IDFをFPと同様，0から1の範囲の値とするため，
{\it tf}値として拡大正規化索引語頻度{\cite{kita2}}を用い，また{\it idf}値も
通常の{\it idf}値{\cite{jones}}を式 ({\ref{eq:idf-norm}}) を用いて正規化し，
下記のような計算式によるTF-IDF値を用いた．}
\begin{align}
\respeqn{
sim_{\rm TFIDF}(x,s)} & \respeqn{=} \respeqn{{\rm BP} \cdot \frac{1}{\rm N} \sum^{\rm N}_{n=1}{tfidf}_{n} \cdot p_{n}(x,s)} \\
\respeqn{
tfidf_{n} } & \respeqn{=} \respeqn{\frac{1}{\left|G_{n}(x)\right|}\sum_{\ngram \in
  G_{n}(x)}{tf_{n} \cdot idf_{n}} }\\
\respeqn{
tf_{n}} & \respeqn{=} \respeqn{ \left\{
	  \begin{array}{lll}
	   0.5 + 0.5 \cdot \frac{\displaystyle freq_{C_{e}(w_{n})}}{\displaystyle
	    \max_{c \in C}freq_{c}(w_{n})} & {\rm if} &
	    freq_{C_{e}}(w_{n}) > 0 \\
	   0 & {\rm if} & freq_{C_{e}}(w_{n}) = 0  
	 \end{array} \right. } \\
\respeqn{
idf_{n} } & \respeqn{=} \respeqn{ \left\{
	   \begin{array}{lll}
	    \frac{\log \frac{|C|}{D(w_{n})} + 1}{\log |C| + 1} & {\rm
	     if} & D(w_{n}) > 0 \\
	    0 & {\rm if} & D(w_{n}) = 0
	   \end{array}
\right . \label{eq:idf-norm}} 
\end{align}
\resp{ここで{$D(w_{n})$}は{$w_{n}$}を含むコーパスの数を返す関数である．}

\begin{figure}[b]
 \begin{center}
  \includegraphics{17-4ia6f5.eps}
 \end{center}
 \caption{TF-IDFを用いた感情推定結果}
 \label{fig:tf-idf}
\end{figure}


\resp{TF-IDFを用いた感情推定実験の結果を図{\ref{fig:tf-idf}}に示す．
この結果を見ると，すべてのNにおいてFPのほうが2.5ポイント〜8ポイント
程度上まわっており，FPの有効性が認められた．}




\subsection{提案手法とSVMの比較}
三品らの方法とは異なる従来手法の一つとして，良好なクラス分類が可能なSVM (Support
Vector Machine) による感情推定を行い，提案手法との比較を行った．
本稿では学習，分類を行うプログラムとして，${\rm SVM}^{light}$\footnote{http://svmlight.joachims.org/}を用いた．


\subsubsection{特徴ベクトルの生成\label{sec:sent2vec}}

SVMを用いて感情推定を行うために，まず感情コーパスの
各発話文から特徴ベクトルを生成する必要がある．今回は特徴ベクトルと
して，1文中に出現する$\ngram$の出現回数をベクトルして表現したものを用い
た．
特徴ベクトルを生成するために，
まず考慮する$\ngram$の最高次数Nを決め，それ以下の各$n$について，感情コー
パスから得られる\NGRAM すべてに通し番号を振った（このとき，
\NGRAM が低次であるほど若い番号を振ることとした）．次に，感情コーパスから
取り出した一つの発話文
$s$の\NGRAM を$m$，$s$における$m$の出現回数を$f$，それ以外の次元の値を0とす
る特徴ベクトルを生成した．
例えば最高次数を${\rm N}_{max} = 2$と
した場合，形態素unigramと形態素bigramを用いて発話文から特徴ベクト
ルを生成する．
この時，特徴ベクトルの次元数は，コーパス中に出現するすべてのunigramと
bigramの種類数となる．
${\rm N}_{max} = 1$とした特徴ベクトルの例として，``ありがとうございまし
た！！''という発話文の場合，
各形態素unigramに割り振られる番号を表\ref{table:mresult}のとおりとすると，
``ありがとうございました！！''の特徴ベクトル$v$
は以下のように表現される．
\begin{equation}
 v = (0, 1, 0, 1, 1, 0, 0, 1, 2, 0, ..., 0) \label{eq:sent_vec}
\end{equation}

\begin{table}[b]
  \caption{形態素unigramに対応する番号}
  \label{table:mresult}
\input{06table08.txt}
\end{table}



\subsubsection{クラス分類モデルの構築}

クラス分類モデルは感情コーパスの数と同じ数だけ構築する．
例えば，``怒り''の分類モデルを構築する場合，ポジティブデータを``怒り''のコーパ
スに含まれる発話文から生成した特徴ベクトル，ネガティブデータを
``怒り''以外の感情コーパスに含まれる発話文から生成
した特徴ベクトルと定義し，学習を行う．

本稿で学習に用いるポジティブデータの量はネガティブデータの量に比べて少
なく，デフォルト値では良好な分類性能が得られない．そこでcost factor
 ($C_{+}/C_{-}$) の計
算には，Morikらが定義した次の式を用いた\cite{morik}．
\begin{equation}
 \frac{C_{+}}{C_{-}} 
  = \frac{\rm number\ of\ negative\ training\
  examples}{\rm number\ of\ positive\ training\ examples}
\end{equation}
cost factor以外の学習パラメータはデフォルト値を用いた．
また学習パラメータで与えるカーネルのタイプもデフォルトである線形カーネル
を用いた．


\subsubsection{実験設定}

実験に用いた感情コーパスは，
\ref{sec:comp_es_bleu}節と同様に``喜び''，``怒り''，``嫌悪''，``希望''の各838文で
あり，入力文も\ref{sec:comp_es_bleu}節と同様の文を用いた．
成功条件は，入力文の感情と出力感情が一致すれば推定成功とした．
 


\subsubsection{実験結果と考察}

表\ref{table:svm_success_ratios}に推定成功率を示す．
SVMで最も高かった推定成功率が$\rm N=2$を用いたときの80.4\%であり，
提案手法の中で最も高かった\resp{成功率81.8\%と比べると1.4ポイント程度の
差になっている．このことから，発話文の感情推定において，適切な N を選択
すれば，SVMと提案手法は同程度の感情推定成功率が得られることが示された．}

\begin{table}[b]
 \caption{SVMと提案手法の感情推定成功率}
  \label{table:svm_success_ratios}
\input{06table09.txt}
\end{table}

\resp{しかし，表{\ref{table:svm_success_ratios}}を見ると，
{${\rm N}_{max} > 2$}を用いた場合のSVMによる推定精
度が急激に減少していることがわかる．}これは``感情コーパス中で出現回数が少ない高次の\NGRAM ''を素性として利用してい
る事例に対する過学習が原因であると考えられる．
\resp{一方提案方法においては，N を増加させていってもその性能にはほとんど
差がなく，``出現回数が少ない高次{\NGRAM}''の影響をほとんど受けていないこと
がわかる．これは，RECAREの計算を相加平均で行ったことの
効果であると考えられる．}

\resp{一般に感情コーパス中の文数などによって最適な N は異なることが考え
られ，SVMの場合は，実際の応用に際し評価実験を通して最適値を探索することが
必要である．
一方RECAREであれば，十分大きな N を設定しておくことで，（計算量や
記憶容量等の問題を除けば）常に最適な推定精度を得ることが可能となる．}

\resp{更に，例えば6形態素からなるある特定の文末表現がある特定の感情に
数多く出現する，といったことがあった場合，SVMであれば，全体として
考慮すべき形態素の長さを決定する必要があるが，RECAREならば，その特定
の文末表現を利用するためだけに $\mathrm{N} = 6$と設定してしまっても，悪影響を
ほとんど及ぼさない．こうしたことから，SVMに比べてRECAREが有効である
ことが示された．}




