    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.3}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{udline}
\setulminsep{1.2ex}{0.2ex}
\let\underline

\def\resp#1{}
\def\respeqn#1{}
\def\bleu{}
\def\es{}
\def\step#1{}
\def\freq#1#2{}
\def\ngram{}
\def\NGRAM{}
    \newcommand{\argmax}{}




\Volume{17}
\Number{4}
\Month{July}
\Year{2010}



\received{2009}{11}{14}
\revised{2010}{2}{28}
\accepted{2010}{3}{17}

\setcounter{page}{91}

\jtitle{コーパスごとの類似度を考慮した用例に基づく\\
	感情推定手法の改善}
\jauthor{三品　賢一\affiref{Author_1} \and 土屋　誠司\affiref{Author_2} \and 鈴木　基之\affiref{Author_3} \and 任　　福継\affiref{Author_3}}
\jabstract{
発話文を感情ごとに分類したコーパスを構築し，入力文と最も類似度が高
い発話文を含むコーパスの感情を推定結果として出力する用例ベースの感
情推定手法が提案されている．従来手法ではコーパスを構築する際，発話テキス
トの収集者が個人個人で発話文の分類先を決定しているため，分類先を決
定する基準が個々によってぶれてしまう．これにより，例えば``希望''のコーパスの中に喜
びの発話文が混じるといったことが起こり，推定成功率を下げてしまう．
本稿ではこの問題を解決するため，コーパスごとにおける入力文の形態素列の出
現回数を用いて，入力文とコーパスの類似度を定義する．そしてこの類似度を従
来手法に導入した新たな類似度計算式を提案する．これにより，誤って分
類されてしまった発話文の影響を緩和することができる．評価実験では従
来手法と比べて成功率が\resp{21.5}ポイント向上し，提案手法の有効性が確認できた．
}
\jkeywords{感性情報処理，類似度計算，用例ベース，N-gram}

\etitle{An Improvement of Example-based Emotion Estimation Using Similarity between Sentence and each Corpus}
\eauthor{Kenichi Mishina\affiref{Author_1} \and Seiji Tsuchiya\affiref{Author_2} \and Motoyuki Suzuki\affiref{Author_3} \and Fuji Ren\affiref{Author_3}}
\eabstract{
Example-based emotion estimators need an emotion corpus in which each
sentences are assigned with emotion tags. It is difficult to determine
emotion tags for the sentence consistently because of ambiguity of emotion.
As a result, there are several wrong tags in a corpus. It causes decrease in
the performance of an emotion estimation. 

In order to solve the problem, a
new similarity between input sentence and emotion corpus is proposed. This
similarity is based on frequencies of morpheme N-gram of the both input
sentence and corpus. Experimental results show that the proposed method
improves emotion precision \resp{from 60.3{\%} to 81.8{\%}}.
}
\ekeywords{Affective computing, Similar calculation, Example-based, N-gram}

\headauthor{三品，土屋，鈴木，任}
\headtitle{コーパスごとの類似度を考慮した用例に基づく感情推定手法の改善}

\affilabel{Author_1}{徳島大学大学院先端技術科学教育部}{Graduate School of Advanced Technology and Science, The University of Tokushima}
\affilabel{Author_2}{同志社大学理工学部}{Faculty of Science and Engineering, Doshisha University}
\affilabel{Author_3}{徳島大学大学院ソシオテクノサイエンス研究部}{Institute of Technology and Science, The University of Tokushima}



\begin{document}
\maketitle


\section{BLEUを類似度計算に用いた用例に基づく感情推定手法}
\label{sec:conventional}

\subsection{概要}

\begin{figure}[b]
 \begin{center}
  \includegraphics{17-4ia6f1.eps}
 \end{center}
 \caption{用例に基づく感情推定}
 \label{fig:estimation}
\end{figure}

\resp{三品らによって提案された類似度計算を用いた用例に基づく
感情推定アルゴリズムは次の式で定式化される．}
\pagebreak
\begin{equation}
\respeqn{E(x) = \argmax_{e} sim(x, s) \hspace{1em} (s \in C_{e})}
\end{equation}
\resp{ここで{$x$}を入力文，{$E(x)$}を推定結果となる感情，{$C_{e}$}を感情{$e$}の
コーパス，{$s$}を{$C_{e}$}に含まれる文，
{$sim(x, s)$}を{$x$}と{$s$}の類似度を返す
関数とする．三品らは{$sim(\cdot)$}にBLEU{\cite{bleu}}を用いている．}
この手法は，発話文を発話者の感情別に分類して構築した感情コーパスを
用いることで感情推定を行う用例ベースの手法である．
発話文の分類先は，
発話文の収集者が発話者の感情を判定することで決定する．図\ref{fig:estimation}に
発話文からの感情推定の流れを示す．まず発話者の
感情によって分類された感情コーパスを用意しておく．次に発話者の感情を推定する対象と
なる発話文を入力とする．そして，各感情コ
ーパスに含まれる発話文と入力文との類似度を求める．最後に各感情コ
ーパス別に，得られた類似度の最大値を求める．この類似度が入力文が表現している感
情のスコアとなる．スコアは0から1までの値をとり，値が大きいほどその感情を
表しているという意味になる．得られた類似度の中で最も値が大きい類似度
の感情を，感情推定結果として出力する．
この方法では図\ref{fig:estimation}における類似度の計算にBLEUを用いている．

用例ベースではない手法では，単語や文末表現への感情属性の付与
や，単語や文末表現の組み合わせから感情を導出するルールを作成する必要が出
てくるため，作業コストが非常に高いと考えられる．しかし従来手法のような用例ベース
のシステムを構築する際には，発話文を集め，発話者の感情ごとに発話文を分類して
コーパスを構築すればよく，用例ベースではない手法と比べて作業コストが低いと考
えられる．


\subsection{BLEU}

BLEUは機械翻訳システムが出力
した複数の翻訳候補文から，システムの翻訳精度を評価するための尺度である．
BLEUは次のとおりに定義されている\cite{bleu}．
\begin{equation}
	\respeqn{{\rm BLEU}(x,y) = {\rm BP} \cdot \exp\left( \sum^{\rm
					      N}_{n=1}\frac{1}{\rm N}\log 
					      p_{n}(x,y) \right) }
		\label{eq:bleu}
\end{equation}
なお，\cite{bleu}では${\rm N} = 4$を用
いている．\resp{{$p_{n}(x,y)$}}は機械翻訳文\resp{{$x$}}と人による翻訳文
\resp{{$y$}}に
おける共通\NGRAM  数の適合率（\NGRAM 
適合率）\resp{を返す関数}であり，$\rm BP$は機械翻訳文が人による翻訳文に比べて
簡潔すぎることによる適合率のペナルティ
である．三品らの方法では，BLEUにおける機械翻訳文をコーパス中の1文，人に
よる翻訳文を入力文と変更して，類似度計算に用いている\resp{（以下，式({\ref{eq:bleu}})を{$sim_{\bleu}$}と表記する）}．


\subsubsection{\NGRAM  適合率}

\NGRAM  適合率\resp{{$p_{n}(x,s)$}}は，入力文\respeqn{$x$}と感情コーパス中の1文\respeqn{$s$}の間で共通な\NGRAM  が
多く存在するかを表す値である．共通な\NGRAM  が多いほど\resp{{$p_{n}(\cdot)$}}
は大きくなる．
\respeqn{$x$}と\respeqn{$s$}を用いて，\resp{{$p_{n}(x,s)$}}は
次のとおりに定義されている．
\begin{gather}
p_{n}(x,s) = \frac{\displaystyle \sum_{\ngram \in 
	G_{n}(s)}Count^{*}_{n}(x, s, \ngram)}{\displaystyle
	\sum_{\ngram \in G_{n}(s)}Count_{n}(s, \ngram)}  \\
Count^{*}_{n}(x, s, \ngram) = \min\left\{Count_{n}(x, \ngram),
	 Count_{n}(s, \ngram)\right\}
	\label{eq:pn}
\end{gather}
\resp{ここで{$G_{n}(s)$}を{$s$}に含まれる``連続する{$n$}個の形態素から作られる形
態素N-gram''の集合を返す関数とする．{$Count_{n}(x, \ngram)$}は，{$x$}中の{$\ngram$}の出現
数を返す}\footnote{\resp{形態素同士の比較は，形態素の文字列と形態素の品詞を用いて行う．こ
れらが一致していれば，二つの形態素は等しいとする．}}．\respeqn{$s$}が
\respeqn{$x$}と共通な\NGRAM  を持っていなければ\resp{{$Count^{*}_{n}(\cdot)$}}
は\resp{すべての{$w_{n}$}で}0になるため，
\resp{{$p_{n}(\cdot)$}}は入力文と感情コーパス中の1文がどれほど共通な\NGRAM  を持っているかの指
標となる．


\resp{{$p_{n}(\cdot)$}}を求める例として，
形態素unigramの適合率\resp{{$p_{1}(x,s)$}}と形態素bigramの適合率\resp{{$p_{2}(x,s)$}}を計算する．
\respeqn{$x$}を``明日からの旅行が楽しみです''，\respeqn{$s$}を``明日がすごく楽しみで
す''とする．このときの形態素unigramの\resp{{$Count_{1}(\cdot)$}}と
\resp{{$Count^{*}_{1}(\cdot)$}}の値を
表\ref{table:count1}に示す\footnote{\resp{本稿では形態素を``形態素の文字列
/品詞''の形式で表記する．}}．表\ref{table:count1}と式(\ref{eq:pn})より，
\resp{{$p_{1}(x,s)$}}は$1/2$であることがわかる．また形態素
bigramの\resp{{$Count_{2}(\cdot)$}}と\resp{{$Count^{*}_{2}(\cdot)$}}の
値を表\ref{table:count2}に示す．表
\ref{table:count2}と式(\ref{eq:pn})より，\resp{{$p_{2}(x,s)$}}は
$2/5$であることがわかる．なお，形態素bigramには文頭や文末を表す記号は
\cite{bleu}と同様に用いていない．

 \begin{table}[t]
 \caption{$Count_{1}とCount^{*}_{1}$の例}
 \label{table:count1}
\input{06table01.txt}
\end{table}
\begin{table}[t]
 \caption{$Count_{2}とCount^{*}_{2}$の例}
 \label{table:count2}
\input{06table02.txt}
\end{table}


\subsubsection{適合率のペナルティ}

ここでは感情コーパス中の1文\respeqn{$s$}が入力文\respeqn{$x$}に比べて簡潔すぎることによる適合率のペナルティ
BPについて説明する．
\respeqn{$s$}が\respeqn{$x$}に比べて簡潔すぎる場合，\respeqn{$s$}に含まれるほとんどの形態
素を\respeqn{$x$}が含んでいる可能性がある．この場合は$p_{n}$が大きくな
り，BLEUスコアが高くなってしまう．つまり，簡潔な文を数多く含んで
いる感情コーパスのほうが，入力文との類似度が高くなりやすくなってしまうの
で，これを防ぐためにBPが用いられる．
\resp{{$g(x)$}}を\resp{{$x$}}の形態素数を返す関数として，
${\rm BP}$は次のとおりに定義される．
\begin{equation}
{\rm BP} = \left\{
	\begin{array}{lll}
	 1 & {\rm if} & \respeqn{g(x) < g(s)} \\
	 e^{(1 - \respeqn{g(x)} / \respeqn{g(s)})} & & \respeqn{{\rm otherwise}} \\
	\end{array}
	\right. \label{eq:bp}
\end{equation}
\respeqn{$x$}を``明日からの旅行が楽しみです''，\respeqn{$s$}を``明日がすごく楽しみで
す''としたとき，$\respeqn{g(x)} = 7$，$\respeqn{g(s)} = 6$となるため，${\rm BP} = e^{1 - 7/6} \approx 0.846$
となる．これは\respeqn{$s$}が短かすぎるため，適合率へペナルティが課せられることを意味する．



\section{提案手法}
\label{sec:proposed_method}

\subsection{\respeqn{コーパスごとの出現回数を考慮したペナルティ}}

三品らの方法には次のような文の影響を受け，感情推定に失敗する問題点がある．
\begin{enumerate}
 \item 感情が異なっていても，たまたま表現や文型が類似している文
 \item コーパスを構築する際に誤って分類された文
\end{enumerate}
これらの文が影響を及ぼしてしまう原因として，1文対1文の類似度の
みを用いて感情推定を行っている点があげられる．これが原因で，例えば``喜び''の文を入力し
たとしても，この入力文と表現や文型が類似している文が``希望''のコーパスに存在
していれば，感情推定結果として``希望''が出力される可能性が非常に高くなる．ま
た，``喜び''の文を``希望''のコーパスに分類されてしまっていた場合，``喜び''の文を入力
した時に``希望''が出力される可能性もある．

\resp{この問題を解決するために，形態素N-gram適合率に対して新たなペナルティFPを
導入する．
FPは入力文の形態素列が各感情コーパスにどの程度偏って存在す
るかを表す指標であり，各感情コーパス中の出現頻度から計算される．
この指標を用いる理由は，入力文に含まれ
る形態素列が，他の感情コーパスに比べて相対的に数多く出現している
感情コーパスの感情を，入力文は表現している可能性が
高いのではないか，と考えたためである．入力文{$x$}において，
感情{$e$}に対するFPを次のとおりに定義する．
}
\begin{equation}
\respeqn{
 {\rm FP}_{n} =
  \frac{1}{\left|G_{n}(x)\right|}\sum_{\ngram \in
  G_{n}(x)}{\frac{\freq{C_{e}}{\ngram}}{\displaystyle \sum_{c \in
  C}{\freq{c}{\ngram}}}} \label{eq:wn} }
\end{equation}
\resp{
ここで{$C$}をすべ
ての感情コーパス，{${freq_{\it C_{e}}}$}を感情コーパス{$C_{e}$}における{$\ngram$}の
出現回数を返す関数とする．
FPは，たまたま1文対1文の類似度が高かったとしても，類似度計算
に用いている文を含むコーパスにおいて入力文の形態素列の出現回数が少なけれ
ば，求められる類似度を低く押さえる効果を持つ．これにより，(1)と(2)の文に
よる影響を改善する．}

\resp{一般に，一部の文書に偏って存在している単語を表す指標として，TF-IDFがよく
用いられている．その意味では，FPのかわりにTF-IDFを用いる方法が考えられる．
しかしTF-IDFは，ある感情コーパス中の絶対的な出現頻度（{\it tf}値）を，
他の感情コーパスにも出現しているかどうか，という形態素N-gramの一般性を
示す値（{\it idf}値）を用いて修正したものであり，ある感情コーパス
中での出現頻度が低い形態素N-gramであれば，たとえその感情コーパスに
偏って存在していたとしても，その値は低いものとなる．そのため，TF-IDF
を類似度計算に導入したとしても，FPよりその効果は薄いものとなることが
予想される．}



\subsection{\respeqn{RECARE}}

従来手法で用いられていた\resp{{$sim_{\rm BLEU}$}に{${\rm FP}_{n}$}を導入し，
\pagebreak
更に式を適切に変更する
ことで，``類似しているが感情が異なる文''や，``コーパス構築時に誤って分類さ
れてしまった文''を含むコーパスに対して頑健な感情推定を行うための新たな類似
度計算式RECARE}\footnote{{\underline R}obust {\underline E}motion 
{\underline C}{\underline A}tegorization with {\underline R}ough {\underline E}motion 
Corpora}\resp{を定義する．RECAREは二つの文に類似した表現が含まれており，
かつ二つの文が同じ感情を表しているかどうかを表すスコアとなる．}

\resp{まず，式（{\ref{eq:bleu}}）で定義される{$sim_{\rm BLEU}$}に対し，
前節で定義したFPを導入する（これを{$sim_{\rm BLEUFP^{+}}$}と
表記する）．}
\begin{align}
 \respeqn{sim_{\rm BLEUFP^{+}}(x,s)} & \respeqn{=} \respeqn{{\rm BP} \cdot \exp
	\left\{ \sum^{\rm N}_{n=1}\frac{1}{\rm N}\log     
	\left({\rm FP}_{n} \cdot p_{n}(x,s) \right)\right\}} \nonumber\\
  & \respeqn{=} \respeqn{{\rm BP} \cdot \exp\left\{ \frac{1}{\rm N}\log \prod^{\rm N}_{n=1} 
	\left({\rm FP}_{n} \cdot p_{n}(x,s) \right)\right\}} \nonumber\\
  & \respeqn{=} \respeqn{{\rm BP} \cdot \exp\left\{ \log \left\{ \prod^{\rm N}_{n=1} 
	\left({\rm FP}_{n} \cdot p_{n}(x,s) \right)\right\}^{\frac{1}{\rm N}}\right\}} \nonumber\\
  & \respeqn{=} \respeqn{{\rm BP} \cdot \left(\prod^{\rm N}_{n=1} 
	\left({\rm FP}_{n} \cdot p_{n}(x,s)\right)
	\right)^{\frac{1}{\rm N}}}\label{eq:bleu_with_fp_pi}
\end{align}

式(\ref{eq:bleu_with_fp_pi})では\resp{{${\rm FP}_{n}\cdot p_{n}(\cdot)$}}の相乗平均を求めている
ことになるが，
\resp{{$p_{n}(x,s) = 0$}}となる$n$が存在したとき，$\respeqn{sim_{\rm BLEUFP^{+}}}= 0$となる．
これは$n$が高次になるほど起こりやすくなると考えら
れる（$n$が高次になるほど入力文とコーパス中の1文との共通な形態素N-gramが
現れにくくなると考えられるためである）．このままでは，低次の\resp{{$p_{n}(\cdot)$}}の情報も失われてしまう．しかし，単語や文末表現の組み合わせによって発話文の感情が決まるとする
ならば，``隣接する形態素同士の組み合わせからなる低次の形態素N-gram''の適合
率は積極的に利用すべきである．高次の
形態素N-gramの適合率が0になったとしても，低次の形態素N-gramの適合率を破
棄する理由はない．以上のことから，形態素N-gramの適合率の相乗平均を求めることは，用例ベースの感
情推定には不向きであると考え，提案する新たな類似度計算式では形態素N-gramの適合率の
相加平均を求めることとした．以上のことから，\respeqn{RECARE}を次のとおりに定義する．
\begin{equation}
\respeqn{sim_{\rm RECARE}(x,s)} = {\rm BP} \cdot \frac{1}{\rm N} \sum^{\rm N}_{n=1} {\rm FP}_{n}
 \cdot p_{n}(x,s) \label{eq:recare}
\end{equation}




\end{document}
