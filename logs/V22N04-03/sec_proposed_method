今まで数多くの単語アラインメント手法が提案されてきており，それらは，生成モデル（例えば[CITE]）と識別モデル（例えば[CITE]）に大別できる．
[REF_sect:SWA]節では生成モデルを概観し，[REF_sect:FFNN]節では識別モデルの一例として，提案手法のベースラインとなるFFNNに基づくモデル[CITE]を説明する．
生成モデルでは，[MATH]単語から構成される原言語の文を[MATH]，それに対応する[MATH]単語で構成される目的言語の文を[MATH]とすると，[MATH]は[MATH]からアラインメント[MATH]を通じて生成されると考える．
ここで，各[MATH]は，原言語の単語[MATH]が目的言語の単語[MATH]に対応する事を示す隠れ変数である．
通常，目的言語の文には単語「null」([MATH])が加えられ，[MATH]が目的言語のどの単語にも対応しない場合，[MATH]となる．
そして，[MATH]が[MATH]から生成される生成確率は，次の通り，[MATH]が生成する全アラインメントとの生成確率の総和で定義される：
IBMモデル1，2やHMMに基づくモデルでは，式([REF_eqn:base1])中の特定アラインメント[MATH]との生成確率[MATH]をアラインメント確率[MATH]と語彙翻訳確率[MATH]で定義する：
この3つのモデルでは，アラインメント確率の定義が異なる．
例えば，HMMに基づくモデルでは単純マルコフ性を持つアラインメント確率を用いる：[MATH]．
また，目的言語の各単語に対する稔性(fertility)や歪み(distortion)を考慮するIBMモデル3-5も提案されている．
これらのモデルは，EMアルゴリズム[CITE]により，単語単位のアラインメントが付与されていない対訳文の集合（ラベルなし学習データ）から学習される．
また，ある対訳文([MATH], [MATH])の単語アラインメントを解析する際は，学習したモデルを用いて，次式([REF_eqn:viterbi_alignment])を満たすアラインメント（ビタビアラインメント）[MATH]を求める：
例えば，HMMに基づくモデルは，ビタビアルゴリズム[CITE]によりビタビアラインメントを求めることができる．
FFNNは，非線形関数を持つ隠れ層を備えることにより，入力データから多層的に非線形な素性を自動的に学習することができ，入力データの複雑な特徴を捉えることができる．
近年，その特長を活かし，音声認識[CITE]，統計的機械翻訳[CITE]やその他の自然言語処理[CITE]等，多くの分野で成果をあげている．
Yangらは，FFNNの一種であるCD-DNN-HMM [CITE]をHMMに基づくアラインメントモデルに適用したモデルを提案した[CITE]．
本節では，提案手法のベースラインとなる，このFFNNに基づく単語アラインメントモデルを説明する．
FFNNに基づくモデルは，式([REF_eqn:base2])のアラインメント確率[MATH]及び語彙翻訳確率[MATH]をFFNNにより計算する：
ただし，全単語にわたる正規化は計算量が膨大となるため，確率の代わりにスコアを用いる．
[MATH]と[MATH]は，それぞれ，アラインメントスコアと語彙翻訳スコアであり，[MATH]と[MATH]に対応する．
また，[MATH]はアラインメント[MATH]のスコアであり，「[MATH]」は単語[MATH]の文脈を表す．
ビタビアラインメントは，典型的なHMMに基づくアラインメントモデル同様，ビタビアルゴリズムにより求める．
アラインメントスコアは直前のアラインメント[MATH]に依存しているため，FFNNに基づくモデルも単純マルコフ過程に従う．
図[REF_fig:FFNN]に，語彙翻訳スコア[MATH]を計算するネットワーク構造（語彙翻訳モデル）を示す．
このネットワークは，lookup層（入力層），1層の隠れ層，出力層から構成され，各層は，それぞれ，重み行列[MATH]，[MATH]，[MATH]を持つ．
[MATH]はword embedding行列であり，各単語を特徴付ける低次元の実ベクトルとして，単語の統語的，意味的特性を表す[CITE]．
原言語の単語集合を[MATH]，目的言語の単語集合を[MATH]，word embeddingの長さを[MATH]とすると，[MATH]は[MATH]行列である．
ただし，[MATH]と[MATH]には，それぞれ，未知語を表す[MATH]と単語「null」を表す[MATH]を追加する．
この語彙翻訳モデルは，入力として，計算対象である原言語の単語[MATH]と目的言語の単語[MATH]と共に，それらの文脈単語を受け付ける．
文脈単語とは，予め定めたサイズの窓内に存在する単語であり，図[REF_fig:FFNN]は窓幅が3の場合を示している．
まず，lookup層が，入力の各単語に対して行列[MATH]から対応する列を見つけ，word embeddingを割り当てる．
そして，それらを結合させた実ベクトル[MATH]を隠れ層に送る．
次に，隠れ層がlookup層の出力[MATH]を受け取り，[MATH]の非線形な特徴を捉える．
最後に，出力層が隠れ層の出力[MATH]を受け取り，語彙翻訳スコアを計算して出力する．
隠れ層，出力層が行う具体的な計算は次の通りである：
z_{1} & =f(H \times z_{0} + B_{H}),
t_{t} & =O \times z_{1} + B_{O}.
ここで，[MATH]，[MATH]，[MATH]，[MATH]は，それぞれ，[MATH]，[MATH]，[MATH]，[MATH]行列である．
また，[MATH]は非線形活性化関数であり，本論文の実験では，[CITE]に倣い，htanh[MATH]を用いた．
アラインメントスコア[MATH]を計算するアラインメントモデルも，語彙翻訳モデルと同様に構成できる．
語彙翻訳モデル及びアラインメントモデルの学習では，次式([REF_eqn:FFNN3])のランキング損失を最小化するように，各層の重み行列を最適化する．
最適化は，サンプル毎に勾配を計算してパラメータを更新する確率的勾配降下法(SGD)で行い，各重みの勾配は，誤差逆伝播法[CITE]で計算する．
ここで，[MATH]は最適化するパラメータ（重み行列の重み），[MATH]は学習データ，[MATH]はパラメータ[MATH]のモデルによる[MATH]のスコア（式([REF_eqn:FFNN])参照），[MATH]は正解アラインメント，[MATH]はパラメータ[MATH]のモデルでスコアが最も高い不正解アラインメントである．
本節では，アラインメント[MATH]のスコアをRNNにより計算する単語アラインメントモデルを提案する：
ここで，[MATH]はアラインメント[MATH]のスコアであり，FFNNに基づくモデルと異なり，直前のアラインメント[MATH]だけでなく，[MATH]個の全てのアラインメントの履歴[MATH]に依存している．
また，本モデルにおいても，FFNNに基づくモデルと同様，確率ではなくスコアを用いる．
図[REF_fig:RNN]にRNNに基づくモデルのネットワーク構造を示す．
このネットワークは，lookup層（入力層），隠れ層，出力層から構成され，各層は，それぞれ，重み行列[MATH]，[MATH]，[MATH]を持つ．
隠れ層の重み行列[MATH]，[MATH]，[MATH]は，直前のアラインメント[MATH]からの距離[MATH] ([MATH])毎に定義される．
本論文の実験では，8より大きい距離及び[MATH]より小さい距離は，それぞれ，「[MATH]」と「[MATH]」にまとめた．
つまり，隠れ層は，直前のアラインメントからの距離[MATH]に対応した重み行列[MATH]を用いて[MATH]を算出する．
ビタビアラインメントは，FFNNに基づくモデルと同様に，図[REF_fig:RNN]のモデルを[MATH]から[MATH]に順番に適用して求める．
ただし，アラインメント[MATH]のスコアは，[MATH]を通じて[MATH]から[MATH]の全てに依存しているため，動的計画法に基づくビタビアルゴリズムは適用できない．
そこで，実験では，ビームサーチにより近似的にビタビアラインメントを求める．
図[REF_fig:RNN]のモデルにより[MATH]と[MATH]のアラインメントのスコアを計算する流れを説明する．
まず，[MATH]と[MATH]の2単語がlookup層へ入力される．
そして，lookup層が2単語それぞれをword embeddingに変換し，そのword embeddingを結合させた実ベクトル[MATH]を隠れ層に送る．
このlookup層が行う処理は，FFNNに基づくモデルのlookup層と同じである．
次に，隠れ層は，lookup層の出力[MATH]と直前のステップ[MATH]の隠れ層の出力[MATH]を受け取り，それらの間の非線形な特徴を捉える．
この時に用いる重み行列[MATH]，[MATH]，[MATH]は，直前のアラインメント[MATH]との距離[MATH]により区別されている．
隠れ層の出力[MATH]は，出力層と次のステップ[MATH]の隠れ層に送られる．
そして最後に，出力層が，隠れ層の出力[MATH]に基づいて[MATH]と[MATH]のアラインメントのスコア[MATH]を計算して出力する．
隠れ層，出力層が行う具体的な計算は次の通りである：
& y_{j}=f(H^{d} \times x_{j} + R^{d} \times y_{j-1} + B^{d}_{H}),
& t_\mathit{RNN}=O \times y_{j} + B_{O}.
ここで，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]は，それぞれ，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]行列である．
ただし，[MATH]である．
また，[MATH]は非線形活性化関数であり，[CITE]と同様に，本論文ではhtanh(x)を用いる．
前述の通り，FFNNに基づくモデルは，語彙翻訳スコア用とアラインメントスコア用の2つのモデルから構成される．
一方で，RNNに基づくモデルは，直前のアラインメントとの距離[MATH]に依存した重み行列を隠れ層で使うことで，アラインメントと語彙翻訳の両者を考慮する1つのモデルで単語アラインメントをモデル化する．
また，RNNに基づくモデルは再帰的な構造をした隠れ層を持つ．
このため，過去のアラインメント履歴全体をこの隠れ層の入出力[MATH]にコンパクトに埋め込むことで，直前のアラインメント履歴のみに依存する従来のFFNNに基づくモデルよりも長いアラインメント履歴を活用して単語アラインメントを行うことができる．
提案モデルの学習では，特定の目的関数に従い，各層の重み行列（つまり，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]）を最適化する．
最適化は，単純なSGD（バッチサイズ[MATH]）よりも収束が早いミニバッチSGDにより行う．
また，各重みの勾配は，通時的誤差逆伝播法[CITE]で計算する．
通時的誤差逆伝播法は，時系列（提案モデルにおける[MATH]）でネットワークを展開し，時間ステップ上で誤差逆伝播法により勾配を計算する手法である．
提案モデルは，FFNNに基づくモデル同様，式([REF_eqn:FFNN3])で定義されるランキング損失に基づいて教師あり学習することができる（[REF_sect:FFNN]節参照）．
しかし，この学習法は正解の単語アラインメントが必要であるという問題がある．
この問題を解決するため，次の[REF_sect:usv]節で，ラベルなし学習データから提案モデルを学習する教師なし学習法を提案する．
本節で提案する教師なし学習は，Dyerらにより提案されたcontrastive estimation (CE) [CITE]に基づく教師なし単語アラインメントモデル[CITE]を拡張した手法である．
CEとは，観測データの近傍データを疑似負例と捉え，観測データとその近傍データを識別するモデルを学習する手法である．
Dyerらは，ラベルなし学習データ中の対訳文[MATH]において考えられる全ての単語アラインメントを観測データ，目的言語側を単語空間[MATH]全体とした単語アラインメント，つまり，対訳文[MATH]中の原言語の各単語と[MATH]中の各単語との全単語対を近傍データとしてCEを適用した．
提案する学習法は，この考え方を目的関数のランキング損失に導入する：
ここで，[MATH]は対訳文[MATH]に対する全ての単語アラインメントの集合，E[MATH]は[MATH]におけるスコア[MATH]の期待値を表す．
[MATH]は対訳文[MATH]中の目的言語の各単語を[MATH]全体とした対訳対集合である．
したがって，[MATH]は学習データ[MATH]中の目的言語の文であり，[MATH]は[MATH]個の目的言語の単語で構成される疑似の文である([MATH])．
一つ目の期待値が観測データ，二つ目の期待値が近傍データに関する項である．
しかしながら，式([REF_eqn:usv1])中の[MATH]に対する期待値の計算量は膨大となる．
そこで，計算量を削減するため，Noise Contrastive Estimation [CITE]に基づくNegative Sampling [CITE]のように，近傍データ空間からサンプリングした空間を用いる．
つまり，各原言語の文[MATH]に対する[MATH]として，[MATH]個の目的言語の単語で構成される全ての文ではなく，サンプリングしたN文を使う．
さらに，ビーム幅[MATH]のビームサーチにより期待値を計算することで，スコアが低いアラインメントを切り捨て計算量を削減する：
式([REF_eqn:usv2])において，[MATH]は学習データ内で[MATH]の対訳となっている目的言語の文([MATH])であり，[MATH]は無作為に抽出された長さ[MATH]の疑似の目的言語の文である．
つまり，[MATH]である．
そして，[MATH]は，各原言語の文[MATH]に対して抽出する疑似の目的言語の文の数である．
GENは，ビームサーチにより探索される単語アラインメント空間であり，全ての単語アラインメント空間[MATH]の部分集合である．
各[MATH]は，無作為に抽出した[MATH]個の目的言語の単語を順番に並べることで生成する．
学習に効果的な負例を生成するために，[MATH]の各単語は，[MATH]から抽出する代わりに，[MATH]正則化付きIBMモデル1 [CITE]によって対訳文中で[MATH]との共起確率が[MATH]以上と判定された目的言語の単語集合から抽出する．
[MATH]正則化付きIBMモデル1は，単純なIBMモデル1と比較して，より疎なアラインメントを生成するため，疑似翻訳[MATH]の候補の範囲を制限することが可能となる．
FFNNに基づくモデルとRNNに基づくモデルは，共に方向性を持つモデルである．
すなわち，[MATH]に対する[MATH]のアラインメントモデルにより，単語[MATH]に対して[MATH]との1対多アラインメントを表す．
通常，方向性を持つモデルは方向毎に独立に学習され，両方向のアラインメント結果をヒューリスティックに結合し決定される．
Yangらの研究においても，FFNNに基づくモデルは独立に学習されている[CITE]．
一方で，各方向のモデルの合意を取るように同時に学習することで，アラインメント精度を改善できることが示されている．
例えば，MatusovらやLiangらは，目的関数を「[MATH]」と「[MATH]」の2つのモデルのパラメータで定義し，2つのモデルを同時に学習している[CITE]．
また，GanchevらやGra\c{c}aらは，EMアルゴリズムのEステップ内で，各方向のモデルが合意するような制約をモデルパラメータの事後分布に課している[CITE]．
そこで，提案モデルの学習においても両方向の合意制約を導入し，それぞれのモデルの特定方向への過学習を防ぎ，双方で大域的な最適化を可能とする．
具体的には，各方向のword embeddingが一致するようにモデルを学習する．
これを実現するために，各方向のword embeddingの差を表すペナルティ項を目的関数に導入し，その目的関数に基づいて各方向のモデルを同時に学習する：
ここで，[MATH]と[MATH]は，それぞれ，[MATH]と[MATH]のアラインメントモデルのパラメータ，[MATH]はlookup層のパラメータ（[MATH]の重みでありword embeddingを表す），[MATH]は合意制約の強さを制御するパラメータ，[MATH]は[MATH]のノルムである．
実験では2-ノルムを用いた．
この合意制約は，教師あり学習と教師なし学習の両方に導入可能である．
教師あり学習の場合は，式([REF_eqn:agreement])の[MATH]として式([REF_eqn:FFNN3])を用い，教師なし学習の場合は式([REF_eqn:usv2])を用いる．
両方向の合意制約を導入した教師なし学習の手順をアルゴリズム1にまとめる．
ステップ2では，学習データTからバッチサイズ分のD個の対訳文[MATH]を無作為に抽出する．
ステップ3-1と3-2では，それぞれ，各[MATH]と[MATH]に対して，[MATH]正則化付きIBMモデル1 ([MATH])が特定した翻訳候補の単語集合から無作為に単語をサンプリングすることにより，負例となる対訳文を[MATH]個([MATH]と[MATH])生成する（[REF_sect:usv]節参照）．
ステップ4-1と4-2では，特定の目的関数に従い，SGDにより各層の重み行列を更新する（[REF_sect:usv]節と[REF_sect:agreement]参照）．
このステップでは，[MATH]と[MATH]の更新は同時に行われ，各方向のword embeddingを一致させるために，[MATH]は[MATH]の更新に，[MATH]は[MATH]の更新に制約を課している．
今まで数多くの単語アラインメント手法が提案されてきており，それらは，生成モデル（例えば[CITE]）と識別モデル（例えば[CITE]）に大別できる．
[REF_sect:SWA]節では生成モデルを概観し，[REF_sect:FFNN]節では識別モデルの一例として，提案手法のベースラインとなるFFNNに基づくモデル[CITE]を説明する．
生成モデルでは，[MATH]単語から構成される原言語の文を[MATH]，それに対応する[MATH]単語で構成される目的言語の文を[MATH]とすると，[MATH]は[MATH]からアラインメント[MATH]を通じて生成されると考える．
ここで，各[MATH]は，原言語の単語[MATH]が目的言語の単語[MATH]に対応する事を示す隠れ変数である．
通常，目的言語の文には単語「null」([MATH])が加えられ，[MATH]が目的言語のどの単語にも対応しない場合，[MATH]となる．
そして，[MATH]が[MATH]から生成される生成確率は，次の通り，[MATH]が生成する全アラインメントとの生成確率の総和で定義される：
IBMモデル1，2やHMMに基づくモデルでは，式([REF_eqn:base1])中の特定アラインメント[MATH]との生成確率[MATH]をアラインメント確率[MATH]と語彙翻訳確率[MATH]で定義する：
この3つのモデルでは，アラインメント確率の定義が異なる．
例えば，HMMに基づくモデルでは単純マルコフ性を持つアラインメント確率を用いる：[MATH]．
また，目的言語の各単語に対する稔性(fertility)や歪み(distortion)を考慮するIBMモデル3-5も提案されている．
これらのモデルは，EMアルゴリズム[CITE]により，単語単位のアラインメントが付与されていない対訳文の集合（ラベルなし学習データ）から学習される．
また，ある対訳文([MATH], [MATH])の単語アラインメントを解析する際は，学習したモデルを用いて，次式([REF_eqn:viterbi_alignment])を満たすアラインメント（ビタビアラインメント）[MATH]を求める：
例えば，HMMに基づくモデルは，ビタビアルゴリズム[CITE]によりビタビアラインメントを求めることができる．
FFNNは，非線形関数を持つ隠れ層を備えることにより，入力データから多層的に非線形な素性を自動的に学習することができ，入力データの複雑な特徴を捉えることができる．
近年，その特長を活かし，音声認識[CITE]，統計的機械翻訳[CITE]やその他の自然言語処理[CITE]等，多くの分野で成果をあげている．
Yangらは，FFNNの一種であるCD-DNN-HMM [CITE]をHMMに基づくアラインメントモデルに適用したモデルを提案した[CITE]．
本節では，提案手法のベースラインとなる，このFFNNに基づく単語アラインメントモデルを説明する．
FFNNに基づくモデルは，式([REF_eqn:base2])のアラインメント確率[MATH]及び語彙翻訳確率[MATH]をFFNNにより計算する：
ただし，全単語にわたる正規化は計算量が膨大となるため，確率の代わりにスコアを用いる．
[MATH]と[MATH]は，それぞれ，アラインメントスコアと語彙翻訳スコアであり，[MATH]と[MATH]に対応する．
また，[MATH]はアラインメント[MATH]のスコアであり，「[MATH]」は単語[MATH]の文脈を表す．
ビタビアラインメントは，典型的なHMMに基づくアラインメントモデル同様，ビタビアルゴリズムにより求める．
アラインメントスコアは直前のアラインメント[MATH]に依存しているため，FFNNに基づくモデルも単純マルコフ過程に従う．
図[REF_fig:FFNN]に，語彙翻訳スコア[MATH]を計算するネットワーク構造（語彙翻訳モデル）を示す．
このネットワークは，lookup層（入力層），1層の隠れ層，出力層から構成され，各層は，それぞれ，重み行列[MATH]，[MATH]，[MATH]を持つ．
[MATH]はword embedding行列であり，各単語を特徴付ける低次元の実ベクトルとして，単語の統語的，意味的特性を表す[CITE]．
原言語の単語集合を[MATH]，目的言語の単語集合を[MATH]，word embeddingの長さを[MATH]とすると，[MATH]は[MATH]行列である．
ただし，[MATH]と[MATH]には，それぞれ，未知語を表す[MATH]と単語「null」を表す[MATH]を追加する．
この語彙翻訳モデルは，入力として，計算対象である原言語の単語[MATH]と目的言語の単語[MATH]と共に，それらの文脈単語を受け付ける．
文脈単語とは，予め定めたサイズの窓内に存在する単語であり，図[REF_fig:FFNN]は窓幅が3の場合を示している．
まず，lookup層が，入力の各単語に対して行列[MATH]から対応する列を見つけ，word embeddingを割り当てる．
そして，それらを結合させた実ベクトル[MATH]を隠れ層に送る．
次に，隠れ層がlookup層の出力[MATH]を受け取り，[MATH]の非線形な特徴を捉える．
最後に，出力層が隠れ層の出力[MATH]を受け取り，語彙翻訳スコアを計算して出力する．
隠れ層，出力層が行う具体的な計算は次の通りである：
z_{1} & =f(H \times z_{0} + B_{H}),
t_{t} & =O \times z_{1} + B_{O}.
ここで，[MATH]，[MATH]，[MATH]，[MATH]は，それぞれ，[MATH]，[MATH]，[MATH]，[MATH]行列である．
また，[MATH]は非線形活性化関数であり，本論文の実験では，[CITE]に倣い，htanh[MATH]を用いた．
アラインメントスコア[MATH]を計算するアラインメントモデルも，語彙翻訳モデルと同様に構成できる．
語彙翻訳モデル及びアラインメントモデルの学習では，次式([REF_eqn:FFNN3])のランキング損失を最小化するように，各層の重み行列を最適化する．
最適化は，サンプル毎に勾配を計算してパラメータを更新する確率的勾配降下法(SGD)で行い，各重みの勾配は，誤差逆伝播法[CITE]で計算する．
ここで，[MATH]は最適化するパラメータ（重み行列の重み），[MATH]は学習データ，[MATH]はパラメータ[MATH]のモデルによる[MATH]のスコア（式([REF_eqn:FFNN])参照），[MATH]は正解アラインメント，[MATH]はパラメータ[MATH]のモデルでスコアが最も高い不正解アラインメントである．
本節では，アラインメント[MATH]のスコアをRNNにより計算する単語アラインメントモデルを提案する：
ここで，[MATH]はアラインメント[MATH]のスコアであり，FFNNに基づくモデルと異なり，直前のアラインメント[MATH]だけでなく，[MATH]個の全てのアラインメントの履歴[MATH]に依存している．
また，本モデルにおいても，FFNNに基づくモデルと同様，確率ではなくスコアを用いる．
図[REF_fig:RNN]にRNNに基づくモデルのネットワーク構造を示す．
このネットワークは，lookup層（入力層），隠れ層，出力層から構成され，各層は，それぞれ，重み行列[MATH]，[MATH]，[MATH]を持つ．
隠れ層の重み行列[MATH]，[MATH]，[MATH]は，直前のアラインメント[MATH]からの距離[MATH] ([MATH])毎に定義される．
本論文の実験では，8より大きい距離及び[MATH]より小さい距離は，それぞれ，「[MATH]」と「[MATH]」にまとめた．
つまり，隠れ層は，直前のアラインメントからの距離[MATH]に対応した重み行列[MATH]を用いて[MATH]を算出する．
ビタビアラインメントは，FFNNに基づくモデルと同様に，図[REF_fig:RNN]のモデルを[MATH]から[MATH]に順番に適用して求める．
ただし，アラインメント[MATH]のスコアは，[MATH]を通じて[MATH]から[MATH]の全てに依存しているため，動的計画法に基づくビタビアルゴリズムは適用できない．
そこで，実験では，ビームサーチにより近似的にビタビアラインメントを求める．
図[REF_fig:RNN]のモデルにより[MATH]と[MATH]のアラインメントのスコアを計算する流れを説明する．
まず，[MATH]と[MATH]の2単語がlookup層へ入力される．
そして，lookup層が2単語それぞれをword embeddingに変換し，そのword embeddingを結合させた実ベクトル[MATH]を隠れ層に送る．
このlookup層が行う処理は，FFNNに基づくモデルのlookup層と同じである．
次に，隠れ層は，lookup層の出力[MATH]と直前のステップ[MATH]の隠れ層の出力[MATH]を受け取り，それらの間の非線形な特徴を捉える．
この時に用いる重み行列[MATH]，[MATH]，[MATH]は，直前のアラインメント[MATH]との距離[MATH]により区別されている．
隠れ層の出力[MATH]は，出力層と次のステップ[MATH]の隠れ層に送られる．
そして最後に，出力層が，隠れ層の出力[MATH]に基づいて[MATH]と[MATH]のアラインメントのスコア[MATH]を計算して出力する．
隠れ層，出力層が行う具体的な計算は次の通りである：
& y_{j}=f(H^{d} \times x_{j} + R^{d} \times y_{j-1} + B^{d}_{H}),
& t_\mathit{RNN}=O \times y_{j} + B_{O}.
ここで，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]は，それぞれ，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]行列である．
ただし，[MATH]である．
また，[MATH]は非線形活性化関数であり，[CITE]と同様に，本論文ではhtanh(x)を用いる．
前述の通り，FFNNに基づくモデルは，語彙翻訳スコア用とアラインメントスコア用の2つのモデルから構成される．
一方で，RNNに基づくモデルは，直前のアラインメントとの距離[MATH]に依存した重み行列を隠れ層で使うことで，アラインメントと語彙翻訳の両者を考慮する1つのモデルで単語アラインメントをモデル化する．
また，RNNに基づくモデルは再帰的な構造をした隠れ層を持つ．
このため，過去のアラインメント履歴全体をこの隠れ層の入出力[MATH]にコンパクトに埋め込むことで，直前のアラインメント履歴のみに依存する従来のFFNNに基づくモデルよりも長いアラインメント履歴を活用して単語アラインメントを行うことができる．
提案モデルの学習では，特定の目的関数に従い，各層の重み行列（つまり，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]）を最適化する．
最適化は，単純なSGD（バッチサイズ[MATH]）よりも収束が早いミニバッチSGDにより行う．
また，各重みの勾配は，通時的誤差逆伝播法[CITE]で計算する．
通時的誤差逆伝播法は，時系列（提案モデルにおける[MATH]）でネットワークを展開し，時間ステップ上で誤差逆伝播法により勾配を計算する手法である．
提案モデルは，FFNNに基づくモデル同様，式([REF_eqn:FFNN3])で定義されるランキング損失に基づいて教師あり学習することができる（[REF_sect:FFNN]節参照）．
しかし，この学習法は正解の単語アラインメントが必要であるという問題がある．
この問題を解決するため，次の[REF_sect:usv]節で，ラベルなし学習データから提案モデルを学習する教師なし学習法を提案する．
本節で提案する教師なし学習は，Dyerらにより提案されたcontrastive estimation (CE) [CITE]に基づく教師なし単語アラインメントモデル[CITE]を拡張した手法である．
CEとは，観測データの近傍データを疑似負例と捉え，観測データとその近傍データを識別するモデルを学習する手法である．
Dyerらは，ラベルなし学習データ中の対訳文[MATH]において考えられる全ての単語アラインメントを観測データ，目的言語側を単語空間[MATH]全体とした単語アラインメント，つまり，対訳文[MATH]中の原言語の各単語と[MATH]中の各単語との全単語対を近傍データとしてCEを適用した．
提案する学習法は，この考え方を目的関数のランキング損失に導入する：
ここで，[MATH]は対訳文[MATH]に対する全ての単語アラインメントの集合，E[MATH]は[MATH]におけるスコア[MATH]の期待値を表す．
[MATH]は対訳文[MATH]中の目的言語の各単語を[MATH]全体とした対訳対集合である．
したがって，[MATH]は学習データ[MATH]中の目的言語の文であり，[MATH]は[MATH]個の目的言語の単語で構成される疑似の文である([MATH])．
一つ目の期待値が観測データ，二つ目の期待値が近傍データに関する項である．
しかしながら，式([REF_eqn:usv1])中の[MATH]に対する期待値の計算量は膨大となる．
そこで，計算量を削減するため，Noise Contrastive Estimation [CITE]に基づくNegative Sampling [CITE]のように，近傍データ空間からサンプリングした空間を用いる．
つまり，各原言語の文[MATH]に対する[MATH]として，[MATH]個の目的言語の単語で構成される全ての文ではなく，サンプリングしたN文を使う．
さらに，ビーム幅[MATH]のビームサーチにより期待値を計算することで，スコアが低いアラインメントを切り捨て計算量を削減する：
式([REF_eqn:usv2])において，[MATH]は学習データ内で[MATH]の対訳となっている目的言語の文([MATH])であり，[MATH]は無作為に抽出された長さ[MATH]の疑似の目的言語の文である．
つまり，[MATH]である．
そして，[MATH]は，各原言語の文[MATH]に対して抽出する疑似の目的言語の文の数である．
GENは，ビームサーチにより探索される単語アラインメント空間であり，全ての単語アラインメント空間[MATH]の部分集合である．
各[MATH]は，無作為に抽出した[MATH]個の目的言語の単語を順番に並べることで生成する．
学習に効果的な負例を生成するために，[MATH]の各単語は，[MATH]から抽出する代わりに，[MATH]正則化付きIBMモデル1 [CITE]によって対訳文中で[MATH]との共起確率が[MATH]以上と判定された目的言語の単語集合から抽出する．
[MATH]正則化付きIBMモデル1は，単純なIBMモデル1と比較して，より疎なアラインメントを生成するため，疑似翻訳[MATH]の候補の範囲を制限することが可能となる．
FFNNに基づくモデルとRNNに基づくモデルは，共に方向性を持つモデルである．
すなわち，[MATH]に対する[MATH]のアラインメントモデルにより，単語[MATH]に対して[MATH]との1対多アラインメントを表す．
通常，方向性を持つモデルは方向毎に独立に学習され，両方向のアラインメント結果をヒューリスティックに結合し決定される．
Yangらの研究においても，FFNNに基づくモデルは独立に学習されている[CITE]．
一方で，各方向のモデルの合意を取るように同時に学習することで，アラインメント精度を改善できることが示されている．
例えば，MatusovらやLiangらは，目的関数を「[MATH]」と「[MATH]」の2つのモデルのパラメータで定義し，2つのモデルを同時に学習している[CITE]．
また，GanchevらやGra\c{c}aらは，EMアルゴリズムのEステップ内で，各方向のモデルが合意するような制約をモデルパラメータの事後分布に課している[CITE]．
そこで，提案モデルの学習においても両方向の合意制約を導入し，それぞれのモデルの特定方向への過学習を防ぎ，双方で大域的な最適化を可能とする．
具体的には，各方向のword embeddingが一致するようにモデルを学習する．
これを実現するために，各方向のword embeddingの差を表すペナルティ項を目的関数に導入し，その目的関数に基づいて各方向のモデルを同時に学習する：
ここで，[MATH]と[MATH]は，それぞれ，[MATH]と[MATH]のアラインメントモデルのパラメータ，[MATH]はlookup層のパラメータ（[MATH]の重みでありword embeddingを表す），[MATH]は合意制約の強さを制御するパラメータ，[MATH]は[MATH]のノルムである．
実験では2-ノルムを用いた．
この合意制約は，教師あり学習と教師なし学習の両方に導入可能である．
教師あり学習の場合は，式([REF_eqn:agreement])の[MATH]として式([REF_eqn:FFNN3])を用い，教師なし学習の場合は式([REF_eqn:usv2])を用いる．
両方向の合意制約を導入した教師なし学習の手順をアルゴリズム1にまとめる．
ステップ2では，学習データTからバッチサイズ分のD個の対訳文[MATH]を無作為に抽出する．
ステップ3-1と3-2では，それぞれ，各[MATH]と[MATH]に対して，[MATH]正則化付きIBMモデル1 ([MATH])が特定した翻訳候補の単語集合から無作為に単語をサンプリングすることにより，負例となる対訳文を[MATH]個([MATH]と[MATH])生成する（[REF_sect:usv]節参照）．
ステップ4-1と4-2では，特定の目的関数に従い，SGDにより各層の重み行列を更新する（[REF_sect:usv]節と[REF_sect:agreement]参照）．
このステップでは，[MATH]と[MATH]の更新は同時に行われ，各方向のword embeddingを一致させるために，[MATH]は[MATH]の更新に，[MATH]は[MATH]の更新に制約を課している．
