図[REF_fig:wa]に[MATH]及び[MATH]で解析した単語アラインメントの具体例を示す．
三角が[MATH]の解析結果，丸が[MATH]の解析結果，四角が正しい単語アラインメントを表す．
図[REF_fig:wa]より，[MATH]は[MATH]と比較して，複雑なアラインメント（例えば，図[REF_fig:wa](a)中の「have you been」に対するギザギザのアラインメント）を特定できていることが分かる．
これは，[MATH]は直前のアラインメント履歴しか利用しないが，[MATH]は長いアラインメント履歴に基づいてアラインメントのパス（例えば，フレーズ単位のアラインメント）を捉えられることを示唆している．
[REF_sect:res_alignment]節で述べた通り，RNNに基づくモデルの効果は，日英アラインメント([MATH])と比べて仏英アラインメント(Hansards)に対して少ない．
これは，英語とフランス語は語順が似ていて，日英に比べて1対1アラインメントが多く（図[REF_fig:wa]参照），仏英単語アラインメントは局所的な手がかりで捉えられる場合が多いためであると考えられる．
図[REF_fig:wa](b)は，このような単純な単語アラインメントは，[MATH]と[MATH]の両モデルで正しく解析できることを示している．
BTECにおける日英アラインメントタスクにおいて様々なサイズの学習データを使った時のアラインメント精度を表[REF_tbl:size]に示す．
「40 K」，「9 K」，「1 K」は，それぞれ，IWSLTの全学習データ，[MATH]の全学習データ，[MATH]の全学習データから無作為に抽出した1,000文対を学習データとした時の，[MATH]のテストデータに対するアラインメント精度である．
「9 K」及び「1 K」はラベルあり学習データ，「40 K」はラベルなし学習データである．
そのため，教師ありモデル(REF)の「40 K」に対する実験は実施していない．
表[REF_tbl:size]より，「1 K」の[MATH](REF)と「9 K」の[MATH]は「40 K」のIBM4より性能がよいことが分かる．
すなわち，RNNに基づくモデルは，IBM4の学習データの22.5%以下(9,000/40,000)のデータから同等の精度を持つモデルを学習できたことが分かる．
その結果，表[REF_tbl:res_mt]が示す通り，学習データの一部を使った[MATH]に基づくSMTシステムが，全学習データを用いた[MATH]に基づくSMTシステムと同等の精度を達成できる場合がある．
表[REF_tbl:size]より，HMMに基づくモデルに導入したLiangらの両方向の合意制約は学習データが小規模なほど効果があることが分かる．
一方で，提案の合意制約は，Liangらの合意制約と比較すると精度の改善幅は小さいが，どのテータサイズにおいても同等の効果を発揮することが確認できる．
また，各データサイズで[REF_sect:res_alignment]節と同様の手法の比較を行うと，教師ラベルを使わない場合は[MATH]，使う場合は[MATH](REF)が最も性能が良い．
そして，本論文で提案した，RNNの利用，教師なし学習，合意制約の個別の有効性も確認できることから，データサイズに依らず提案手法が有効であることが分かる．
本論文では，RNNに基づく単語アラインメントモデルを提案した．
提案モデルは，隠れ層の再帰的な構造を利用し，長いアラインメント履歴に基づいてアラインメントのパス（例えば，フレーズ単位のアラインメント）を捉えることができる．
また，RNNに基づくモデルの学習法として，Dyerらの教師なし単語アラインメント[CITE]を拡張して人工的に作成した負例を利用する教師なし学習法を提案した．
そして，更なる精度向上のために，学習過程に各方向のword embeddingを一致させる合意制約を導入した．
複数の単語アラインメントタスクと翻訳タスクの実験を通じて，RNNに基づくモデルは従来のFFNNに基づくモデル[CITE]よりアラインメント精度及び翻訳精度が良いことを示した．
また，提案した教師なし学習や合意制約により，アラインメント精度を更に改善できることを確認した．
提案モデルでは，アラインメント対象の文脈をアラインメント履歴([MATH])に暗示的に埋め込み利用しているが，今後は，FFNNに基づくモデルのように周辺単語の入力（[MATH]や[MATH]）として明示的に利用することも検討したい．
また，Yangらは複数の隠れ層を用いることでFFNNに基づくモデルの精度を改善している[CITE]．
これに倣って提案モデルでも各隠れ層を複数にするなど，提案モデルの改良を行う予定である．
さらに，本論文では提案モデルにより特定したアラインメントに基づいて翻訳モデルを学習したが，翻訳モデル学習時の素性としてアラインメントモデルが算出するスコアを使用したり，Watanabeら[CITE]のように翻訳候補のリランキングの中で使ったりするなど，提案モデルのSMTシステムへの効果的な組み込み方に関しても検討したい．
図[REF_fig:wa]に[MATH]及び[MATH]で解析した単語アラインメントの具体例を示す．
三角が[MATH]の解析結果，丸が[MATH]の解析結果，四角が正しい単語アラインメントを表す．
図[REF_fig:wa]より，[MATH]は[MATH]と比較して，複雑なアラインメント（例えば，図[REF_fig:wa](a)中の「have you been」に対するギザギザのアラインメント）を特定できていることが分かる．
これは，[MATH]は直前のアラインメント履歴しか利用しないが，[MATH]は長いアラインメント履歴に基づいてアラインメントのパス（例えば，フレーズ単位のアラインメント）を捉えられることを示唆している．
[REF_sect:res_alignment]節で述べた通り，RNNに基づくモデルの効果は，日英アラインメント([MATH])と比べて仏英アラインメント(Hansards)に対して少ない．
これは，英語とフランス語は語順が似ていて，日英に比べて1対1アラインメントが多く（図[REF_fig:wa]参照），仏英単語アラインメントは局所的な手がかりで捉えられる場合が多いためであると考えられる．
図[REF_fig:wa](b)は，このような単純な単語アラインメントは，[MATH]と[MATH]の両モデルで正しく解析できることを示している．
BTECにおける日英アラインメントタスクにおいて様々なサイズの学習データを使った時のアラインメント精度を表[REF_tbl:size]に示す．
「40 K」，「9 K」，「1 K」は，それぞれ，IWSLTの全学習データ，[MATH]の全学習データ，[MATH]の全学習データから無作為に抽出した1,000文対を学習データとした時の，[MATH]のテストデータに対するアラインメント精度である．
「9 K」及び「1 K」はラベルあり学習データ，「40 K」はラベルなし学習データである．
そのため，教師ありモデル(REF)の「40 K」に対する実験は実施していない．
表[REF_tbl:size]より，「1 K」の[MATH](REF)と「9 K」の[MATH]は「40 K」のIBM4より性能がよいことが分かる．
すなわち，RNNに基づくモデルは，IBM4の学習データの22.5%以下(9,000/40,000)のデータから同等の精度を持つモデルを学習できたことが分かる．
その結果，表[REF_tbl:res_mt]が示す通り，学習データの一部を使った[MATH]に基づくSMTシステムが，全学習データを用いた[MATH]に基づくSMTシステムと同等の精度を達成できる場合がある．
表[REF_tbl:size]より，HMMに基づくモデルに導入したLiangらの両方向の合意制約は学習データが小規模なほど効果があることが分かる．
一方で，提案の合意制約は，Liangらの合意制約と比較すると精度の改善幅は小さいが，どのテータサイズにおいても同等の効果を発揮することが確認できる．
また，各データサイズで[REF_sect:res_alignment]節と同様の手法の比較を行うと，教師ラベルを使わない場合は[MATH]，使う場合は[MATH](REF)が最も性能が良い．
そして，本論文で提案した，RNNの利用，教師なし学習，合意制約の個別の有効性も確認できることから，データサイズに依らず提案手法が有効であることが分かる．
本論文では，RNNに基づく単語アラインメントモデルを提案した．
提案モデルは，隠れ層の再帰的な構造を利用し，長いアラインメント履歴に基づいてアラインメントのパス（例えば，フレーズ単位のアラインメント）を捉えることができる．
また，RNNに基づくモデルの学習法として，Dyerらの教師なし単語アラインメント[CITE]を拡張して人工的に作成した負例を利用する教師なし学習法を提案した．
そして，更なる精度向上のために，学習過程に各方向のword embeddingを一致させる合意制約を導入した．
複数の単語アラインメントタスクと翻訳タスクの実験を通じて，RNNに基づくモデルは従来のFFNNに基づくモデル[CITE]よりアラインメント精度及び翻訳精度が良いことを示した．
また，提案した教師なし学習や合意制約により，アラインメント精度を更に改善できることを確認した．
提案モデルでは，アラインメント対象の文脈をアラインメント履歴([MATH])に暗示的に埋め込み利用しているが，今後は，FFNNに基づくモデルのように周辺単語の入力（[MATH]や[MATH]）として明示的に利用することも検討したい．
また，Yangらは複数の隠れ層を用いることでFFNNに基づくモデルの精度を改善している[CITE]．
これに倣って提案モデルでも各隠れ層を複数にするなど，提案モデルの改良を行う予定である．
さらに，本論文では提案モデルにより特定したアラインメントに基づいて翻訳モデルを学習したが，翻訳モデル学習時の素性としてアラインメントモデルが算出するスコアを使用したり，Watanabeら[CITE]のように翻訳候補のリランキングの中で使ったりするなど，提案モデルのSMTシステムへの効果的な組み込み方に関しても検討したい．
