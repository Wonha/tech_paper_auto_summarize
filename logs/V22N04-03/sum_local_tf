================================================================
  [ type  : abstract ]
  [ title : abstract ]
ニューラルネットワークに基づくモデルでは，従来，教師あり学習が行われてきたが，本論文では，本モデルの学習法として，Dyerらの教師なし単語アラインメント[CITE]を拡張して人工的に作成した負例を利用する教師なし学習法を提案する．  [score 3194]

================================================================
  [ type  : intro ]
  [ title : はじめに ]
ここで，学習される特徴は方向毎に異なり，それらは相補的であるとの考えに基づき，各方向の合意を取るようにモデルを学習することによりアラインメント精度が向上することが示されている(Matusov, Zens, and Ney 2004; Liang, Taskar, and Klein 2006; Gra\c{c}a, Ganchev, and Taskar 2008; Ganchev, Gra\c{c}a, and Taskar 2008)．  [score 3039]

================================================================
  [ type  : proposed_method ]
  [ title : 従来の単語アラインメントモデル ]
[REF_sect:SWA]節では生成モデルを概観し，[REF_sect:FFNN]節では識別モデルの一例として，提案手法のベースラインとなるFFNNに基づくモデル[CITE]を説明する．  [score 2685]
------------------------------------------------------------
  [ title : 生成モデル ]
IBMモデル1，2やHMMに基づくモデルでは，式([REF_eqn:base1])中の特定アラインメント[MATH]との生成確率[MATH]をアラインメント確率[MATH]と語彙翻訳確率[MATH]で定義する：  [score 2953]
------------------------------------------------------------
  [ title : FFNNに基づく単語アラインメントモデル ]
FFNNに基づくモデルは，式([REF_eqn:base2])のアラインメント確率[MATH]及び語彙翻訳確率[MATH]をFFNNにより計算する：  [score 2570]

================================================================
  [ type  : proposed_method ]
  [ title : RNNに基づく単語アラインメントモデル ]
一方で，RNNに基づくモデルは，直前のアラインメントとの距離[MATH]に依存した重み行列を隠れ層で使うことで，アラインメントと語彙翻訳の両者を考慮する1つのモデルで単語アラインメントをモデル化する．  [score 3259]

================================================================
  [ type  : proposed_method ]
  [ title : モデルの学習 ]
提案モデルの学習では，特定の目的関数に従い，各層の重み行列（つまり，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]，[MATH]）を最適化する．  [score 2280]
------------------------------------------------------------
  [ title : 教師なし学習 ]
学習に効果的な負例を生成するために，[MATH]の各単語は，[MATH]から抽出する代わりに，[MATH]正則化付きIBMモデル1 [CITE]によって対訳文中で[MATH]との共起確率が[MATH]以上と判定された目的言語の単語集合から抽出する．  [score 3394]
------------------------------------------------------------
  [ title : 両方向の合意制約 ]
ステップ3-1と3-2では，それぞれ，各[MATH]と[MATH]に対して，[MATH]正則化付きIBMモデル1 ([MATH])が特定した翻訳候補の単語集合から無作為に単語をサンプリングすることにより，負例となる対訳文を[MATH]個([MATH]と[MATH])生成する（[REF_sect:usv]節参照）．  [score 3532]

================================================================
  [ type  : experiment_result ]
  [ title : 評価実験 ]
  [score ]
------------------------------------------------------------
  [ title : 実験データ ]
また，NAACL 2003のshared taskオリジナルの学習データの総数は約110万文対あるが，今回のHansardsの実験では，学習時の計算量を削減するため，無作為にサンプリングした10万文対を学習データとして用いた．  [score 2592]
------------------------------------------------------------
  [ title : 実験対象 ]
このFFNNに基づくモデルは，[CITE]に倣って[REF_sect:FFNN]節の教師あり手法により学習したモデル[MATH]に加えて，[REF_sect:usv]節と[REF_sect:agreement]節で提案した教師なし学習や合意制約の効果を確かめるため，[MATH]，[MATH]，[MATH]のモデルを評価した．  [score 3434]
------------------------------------------------------------
  [ title : 実験結果（単語アラインメント） ]
NNに基づく教師ありモデルに対しては，学習データに付与されている正しい単語アラインメントを学習したモデル(REF)と，IBM4で特定した単語アラインメントを学習したモデル(IBM4)の2種類の精度を示す．  [score 3190]
------------------------------------------------------------
  [ title : 実験結果（機械翻訳） ]
また，NTCIR-9とFBISでは，提案モデルは学習データの一部から学習したが，学習データ全てから学習した[MATH]と同等の精度を達成している．  [score 2872]

================================================================
  [ type  : conclusion ]
  [ title : 考察 ]
  [score ]
------------------------------------------------------------
  [ title : RNNに基づくモデルの効果 ]
図[REF_fig:wa](b)は，このような単純な単語アラインメントは，[MATH]と[MATH]の両モデルで正しく解析できることを示している．  [score 2639]
------------------------------------------------------------
  [ title : 学習データ量の影響 ]
「40 K」，「9 K」，「1 K」は，それぞれ，IWSLTの全学習データ，[MATH]の全学習データ，[MATH]の全学習データから無作為に抽出した1,000文対を学習データとした時の，[MATH]のテストデータに対するアラインメント精度である．  [score 2937]

================================================================
  [ type  : conclusion ]
  [ title : まとめ ]
提案モデルでは，アラインメント対象の文脈をアラインメント履歴([MATH])に暗示的に埋め込み利用しているが，今後は，FFNNに基づくモデルのように周辺単語の入力（[MATH]や[MATH]）として明示的に利用することも検討したい．  [score 3195]

