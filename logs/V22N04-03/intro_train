0 対訳文中 の 単語 の 対応関係 を 解析 する 単語アラインメント は 統計的機械翻訳 に 欠か せ ない 重要 な 処理 の 一つ で あり 研究 が 盛ん に 行わ れ て いる 
0 その 中 で 生成モデル で ある IBMモデル15CITE や HMM に 基づく モデルCITE は 最も 有名 な 手法 で あり それら を 拡張 し た 手法 が 数多く 提案 さ れ て いる 
1 近年 で は Yangら が フィードフォワードニューラルネットワークFFNN の 一種 で ある ContextDependentDeepNeuralNetworkforHMMCDDNNHMMCITE を HMM に 基づく モデル に 適用 し た 手法 を 提案 し 中英アラインメントタスク において IBMモデル4 や HMM に 基づく モデル より も 高い 精度 を 達成 し て いる 
0 この FFNNHMMアラインメントモデル は 単語アラインメント に 単純マルコフ性 を 仮定 し た モデル で あり アラインメント履歴 として 一つ前 の 単語アラインメント結果 を 考慮 する 
0 一方 で ニューラルネットワークNN の 一種 に フィードバック結合 を 持つ リカレントニューラルネットワークRNN が ある 
0 RNN の 隠れ層 は 再帰的 な 構造 を 持ち 自身 の 信号 を 次 の ステップ の 隠れ層 へ と 伝達 する 
0 この 再帰的 な 構造 により 過去 の 入力データ の 情報 を 隠れ 層 で 保持 できる ため入力データ に 内在 する 長距離 の 依存関係 を 捉える こと が できる 
0 この よう な 特長 を 持つ RNN に 基づく モデル は 近年多く の タスク で 成果 を あげ て おり FFNN に 基づく モデル の 性能 を 凌駕 し て いる 
0 例えば 言語モデルCITE や 翻訳モデルCITE の 構築 で 効果 を 発揮 し て いる 
0 一方 で 単語アラインメントタスク において RNN を 活用 し た モデル は 提案 さ れ て い ない 
1 本 論文 で は 単語アラインメント において 過去 の アラインメント の 情報 を 保持 し て 活用 する こと は 有効 で ある と 考え RNN に 基づく 単語アラインメントモデル を 提案 する 
0 前述 の 通り従来 の FFNN に 基づく モデル は 直前 の アラインメント履歴 しか 考慮 し ない 
1 一方 で RNN に 基づく モデル は 隠れ層 の 再帰的 な 構造 として アラインメント の 情報 を 埋め込む こと で FFNN に 基づく モデル より も 長い 文頭 から 直前 の 単語アラインメント の 情報 つまり 過去 の アラインメント履歴全体 を 考慮 できる 
0 NN に 基づく モデル の 学習 に は 通常教師データ が 必要 で ある 
0 しかし 単語単位 の 対応関係 が 付与 さ れ た 対訳文 を 大量 に 用意 する こと は 容易 で は ない 
0 この 状況 に対して Yangら は 従来 の 教師 なし 単語アラインメントモデルIBMモデルHMM に 基づく モデル により 生成 し た 単語アラインメント を 疑似 の 正解データ として 使い モデル を 学習 し た 
0 しかし この 方法 で は 疑似正解データ の 作成段階 で 生み出さ れ た 誤っ た 単語アラインメント が 正しい アラインメント として 学習 さ れ て しまう 可能性 が ある 
1 これら の 状況 を 踏まえ て 本 論文 で は 正解 の 単語アラインメント や 疑似 の 正解データ を 用意 せ ず に RNN に 基づく モデル を 学習 する 教師 なし 学習法 を 提案 する 
1 本 学習法 で は Dyerら の 教師 なし 単語アラインメントCITE を 拡張 し 正しい 対訳文 における 単語対 と 語彙空間全体 における 単語対 を 識別 する よう に モデル を 学習 する 
0 具体的 に は まず 語彙空間全体 から の サンプリング により 偽 の 対訳文 を 人工的 に 生成 する 
0 その後 正しい 対訳文 における アラインメントスコア の 期待値 が 偽 の 対訳文 における アラインメントスコア の 期待値 より 高く なる よう に モデル を 学習 する 
1 RNN に 基づく モデル は 多く の アラインメントモデル と 同様 に 方向性原言語MATH目的言語MATH 又は MATH を 持ち 各 方向 の モデル は 独立 に 学習使用 さ れる 
1 ここ で 学習 さ れる 特徴 は 方向毎 に 異なり それら は 相補的 で ある と の 考え に 基づき 各 方向 の 合意 を 取る よう に モデル を 学習 する こと により アラインメント精度 が 向上 する こと が 示さ れ て いる 
0 CITEそこ で 提案手法 において も MATH と MATH の 2 つ の RNN に 基づく モデル の 合意 を 取る よう に それら の モデル を 同時に 学習 する 
1 両方向 の 合意 は 各 方向 の モデル の wordembedding が 一致 する よう に モデル を 学習 する こと で 実現 する 
1 具体的 に は 各 方向 の wordembedding の 差 を 表す ペナルティ項 を 目的関数 に 導入 し その 目的関数 に したがっ て モデル を 学習 する 
0 この 制約 により それぞれ の モデル の 特定方向 へ の 過 学習 を 防ぎ 双方 で 大域的 な 最適化 が 可能 と なる 
0 提案手法 の 評価 は 日英 及び 仏英単語アラインメント実験 と 日英 及び 中英翻訳実験 で 行う 
1 評価実験 を通じて 前記提案全て を 含む 合意制約付き教師 なし 学習法 で 学習 し た RNN に 基づく モデル は FFNN に 基づく モデル や IBMモデル4 より も 単語アラインメント精度 が 高い こと を 示す 
0 また 機械翻訳実験 を通じて 学習データ量 が 同じ 場合 に は FFNN に 基づく モデル や IBMモデル4 を 用い た 場合 より も 高い 翻訳精度 を 実現 できる こと を 示す 
0 具体的 に は アラインメント精度 は FFNN に 基づく モデル より 最大00792F1値IBMモデル4 より 最大00703F1値翻訳精度 は FFNN に 基づく モデル より 最大074BLEUIBMモデル4 より 最大058BLEU 上回っ た 
0 また 各 提案RNN の 利用教師 なし 学習法合意制約個別 の 有効性 も 検証 し 機械翻訳 において は 一部 の 設定 における 精度改善 に とどまる が 単語アラインメント において は 各 提案 により 精度 が 改善 できる こと を 示す 
0 以降REF_sectrelated節 で 従来 の 単語アラインメントモデル を 説明 し REF_sectRNN節 で RNN に 基づく 単語アラインメントモデル を 提案 する 
0 そして REF_sectlearning節 で RNN に 基づく モデル の 学習法 を 提案 する 
0 REF_sectexperiment節 で は 提案手法 の 評価実験 を 行い REF_sectdiscuss節 で 提案手法 の 効果 や 性質 について の 考察 を 行う 
0 最後 に REF_sectconclusion節 で 本 論文 の まとめ を 行う 
