提案手法は，分割確率最大化という観点からテキスト分割を定式化した．
これに類似の手法として，訓練データを利用したテキスト分割では，[CITE]が隠れマルコフモデルに基づいて，複数ニュースを個々のニュースに分割しているが，訓練データを利用しないテキスト分割では，類似の研究はない．
また，[CITE]についても，彼等は，テキストの分割確率を直接扱っているのではなく，各単語を生起させるようなトピックを単語毎に求め，同一トピックの単語が連続する部分を同一トピックとする，という間接的アプローチをとっている．
そのため，彼等のアプローチでは，たとえば，トピックの平均の長さなどを直接取り込むことが難しい．
一方，我々のアプローチでは，このことは素直に表現できる．
たとえば，[CITE]と同様に，トピックの長さ[MATH]が，平均長[MATH]，標準偏差[MATH]の正規分布[MATH]に従うと仮定すると，単純な拡張としては，([REF_eq:cS_i])式を，[MATH]として，以下のようにすれば，トピックの長さが平均と同じくなるような分割が優先される．
更に，彼等の手法と我々の手法との大きな違いは，彼等が単語の確率を訓練データから推定しているのに対して，我々は，単語の確率を分割対象のテキストから推定している点である．
なお，訓練データが利用可能な場合に，彼等の手法と我々の手法とを比較することは興味深いであろう．
その場合には，上式で示したような，トピックの長さをコスト関数として取り込むことや，種々の手がかり表現をコスト関数に取り込むことも検討したい．
次に，提案手法のテキスト分割における特徴としては，[REF_eq:rec]節で述べたように，長い文章でも短かい文章でも，分割数が，大幅には変動しないというものがある．
これは，短かい文章は，細かい粒度で分割し，長い文章は大雑把な粒度で分割するということである．
この性質は，我々がテキスト分割をする目的が要約のため，という観点からは適した性質である．
なぜなら，要約では，文章の長さに関わらず，それを適当に少ないトピックにまとめる必要があるので，分割の結果得られる区間数は，文章の長さに，それほど影響されない方が望ましいからである．
しかし，応用によっては，任意に指定した粒度の分割が望ましい場合もあると考えられる．
そのために，我々は，本稿では，大域的な最小コスト解よりも細かい分割が必要な場合には，再帰的な分割を適用し，それは有効ではあったが，より有効な分割方法を考えることは今後の課題としたい．
そのための見込みのある方法の一つは，[CITE]で提案されているように，分割したい粒度に応じて窓の大きさを設定し，その窓内を一つの文章としてテキストを分割することである．
最後に，提案手法によると，テキストの分割の結果として，テキストの各区間における単語の確率(密度)が自然に求まる．
このような密度は，重要単語の抽出[CITE]や，重要説明箇所の特定[CITE]に有用であることが知られている．
提案手法を，このようなアプリケーションに対して適用することも興味深い．
我々は，本稿において，分割確率最大化という観点から，テキスト内の情報のみを用いて，テキストを分割する手法を提案した．
提案手法は，従来の手法と比べて，同等以上の精度でテキストを分割することができた．
このことは提案手法がテキストの分割に有用であることを示している．
我々は，今後，実際の応用におけるテキスト分割の有効性を調べることを考えている．
\appendix
# # perl npaa-div.pl (chapter|section) < file.sgm # #ファイルの第1部(part)のchapterまたはsectionに相当する部分を#抜き出して，区切り(================)を入れるプログラム．
#
[MATH]type&i){ print "================\n"; while(<>){ last if m&</[MATH]type&i; } } last; } } print "================\n";
