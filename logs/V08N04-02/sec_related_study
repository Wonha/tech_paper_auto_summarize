表[REF_tab:secnorec]は，再帰をせずに，分割数を指定して分割したときの精度を示している．
このとき，±1の精度の平均が0.31であり，±0の精度の平均が0.14である．
一方，再帰的分割をしたときには，表[REF_tab:secrec]に示すように，±1の精度の平均が0.51であり，±0の精度の平均が0.34である．
これから分かるように，最小コスト解よりも粒度の細かい分割が必要なときには，再帰的分割をした方が精度良く分割ができる．
なお，[CITE]では，ベースラインが0.035のときにF値が0.29であるので，再帰的分割による方法は，[CITE]と比べて，少なくとも同等程度に節の区切れを再現していると考える．
提案手法は，分割確率最大化という観点からテキスト分割を定式化した．
これに類似の手法として，訓練データを利用したテキスト分割では，[CITE]が隠れマルコフモデルに基づいて，複数ニュースを個々のニュースに分割しているが，訓練データを利用しないテキスト分割では，類似の研究はない．
また，[CITE]についても，彼等は，テキストの分割確率を直接扱っているのではなく，各単語を生起させるようなトピックを単語毎に求め，同一トピックの単語が連続する部分を同一トピックとする，という間接的アプローチをとっている．
そのため，彼等のアプローチでは，たとえば，トピックの平均の長さなどを直接取り込むことが難しい．
一方，我々のアプローチでは，このことは素直に表現できる．
たとえば，[CITE]と同様に，トピックの長さ[MATH]が，平均長[MATH]，標準偏差[MATH]の正規分布[MATH]に従うと仮定すると，単純な拡張としては，([REF_eq:cS_i])式を，[MATH]として，以下のようにすれば，トピックの長さが平均と同じくなるような分割が優先される．
更に，彼等の手法と我々の手法との大きな違いは，彼等が単語の確率を訓練データから推定しているのに対して，我々は，単語の確率を分割対象のテキストから推定している点である．
なお，訓練データが利用可能な場合に，彼等の手法と我々の手法とを比較することは興味深いであろう．
その場合には，上式で示したような，トピックの長さをコスト関数として取り込むことや，種々の手がかり表現をコスト関数に取り込むことも検討したい．
