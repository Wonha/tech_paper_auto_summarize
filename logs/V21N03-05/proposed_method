解析対象
\label{sec:target}

本章では，まず，解析対象である絵本データベースの紹介を行う（\ref{sec:ehon-db}節）．
次に，新聞などの一般向けテキストと絵本のテキストを比較し，違いを調査する（\ref{sec:mojisyu}節）．
また，評価，実験用に形態素情報を付与した絵本のラベルありデータ（フルアノテーションデータ）を紹介する（\ref{sec:full-ano}節）．


\subsection{絵本データベース}
\label{sec:ehon-db}

本稿では構築中の絵本データベースを解析対象とする
\cite{Taira:Fujita:Kobayashi:2012j}．絵本データベースは，発達心理学におけ
る研究や，子供の興味や発達に応じた絵本リコメンデーションを目的として構築
されている．


含まれる絵本は，2010年度の紀伊国屋書店グルー
プの売上冊数が上位のファーストブック（以下，\first{}）と絵本（以下，\ehon）\footnote{
	絵本とファーストブックの分類は紀伊国屋書店による．絵本には，大人向けと見られる絵本も一部含まれていた．}
計 1,010冊，および，福音館書店の月刊誌（以下，\kodomo）190冊，合計 1,200冊である\footnote{含まれる絵本のリストは http://www.kecl.ntt.co.jp/icl/lirg/members/sanae/ehon-list.html で閲覧可能である．}．
これらの選定理由は，
前者は多くの子供に読まれていると考えられること，
後者は対象年齢が比較的はっきりしていることである．
後者の対象年齢は0・1・2歳向け（以下，\kod{012}），年少（3歳児）向け（以下，\kod{3}），年中（4歳児）向け（以下，\kod{4}），
年長（5歳児）向け（以下，\kod{5}）とわかれている．
本稿では，これらをまとめて絵本と呼ぶこととする．
なお，\kodomo 以外で対象年齢が記載されていた絵本は，463冊 (45.8\%) にとどまり，
その記載方法も「3歳から小学校初級むき」「乳児から」「4才から」のように多様で，
\kodomo のように 1 歳単位で対象年齢が設定されている絵本は少ない．

\begin{table}[b]
 \caption{絵本データベースのサイズ}\label{tb:size}
\input{1008table02.txt}
\end{table}

本稿では，絵本の本文のテキストを解析対象とする．
本文のテキストは人手で入力されている\footnote{当初，既存OCRによる自動的
な文字認識を試したが，絵と文字の判別が難しく，高精度な自動認識は困難
だった．}．
また文や文節の途中での改行など元のページのレイアウトも
忠実に再現されている
（例\refs{ex-org}）．
なお，絵本データベースの 1,200冊のサイズは
表~\ref{tb:size}の通りである．
\begin{exe}
 \ex \label{s:ex-org}
もう　いつ　はるが　きて、なつが　きたのか、いつが\\
あきで、いつが　ふゆなのか、わかりません。\\
\small （バージニア・リー・バートン，石井桃子・訳「ちいさいおうち」p.~24（1954，岩波書店））
\end{exe}


\subsection{絵本と他のコーパスの比較}
\label{sec:mojisyu}

絵本のテキストの特徴を調べるため，絵本と一般的なコーパスにおける
文字種の割合を比較する．
表~\ref{tb:mojisyu}に，
絵本 1,200冊（表 \ref{tb:size}）における
文字種と，
現代日本語書き言葉均衡コーパス\footnote{http://www.ninjal.ac.jp/kotonoha/}（以下，\bccwj），
京都大学テキストコーパス\footnote{http://nlp.ist.i.kyoto-u.ac.jp/index.php}（以下，京大コーパス）
，および，基本語データベース\cite{Lexeed:2004j} （以下，\lxd）
の定義文，例文に出現する文字種の数と割合を示す．

\begin{table}[t]
\caption{文字種毎の数と割合：絵本と他のコーパスの比較}
\label{tb:mojisyu}
\input{1008table03.txt}
\end{table}

表 \ref{tb:mojisyu}から，他のコーパスに比べ，
絵本の場合，ひらがなと空白が占める割合が圧倒的に高いことがわかる．
また逆に，漢字が占める割合は非常に低い．
表~\ref{tb:mojisyu}には，参考として，
一文に含まれる平均文字数，および，平均形態素数も記載した．但し，絵本の場合は，一行に含まれる平均文字
数を記載しており，必ずしも文単位ではない．また，平均形態素数について，絵本は未知であり，
\bccwj は品詞体系が異なるため記載していない．


\subsection{絵本のフルアノテーションデータ}
\label{sec:full-ano}

精度評価
のために，絵本の一部に正解の形態素区切り，IPA品詞，読み，できるだけ漢字表記にした原形を
付与したフルアノテーションデータ（ラベルありデータ）を作成した．
ただし，活用型と活用形は付与していない．
付与自体が難しいことと，
作業量が増えるためにコストと時間がかかること，
これらの情報を今後利用する予定がないことが理由である．

絵本に出現した文\refs{eva-org}に対するフルアノテーションデータを
\refs{ehon-full}に示す．
ただし\refs{ehon-full}では，形態素区切りは\jpn[,]{}で示し，
形態素は\jpn[出現形/品詞/読み/原形]{}の形で示し，
漢字表記にした原形には\ul{下線}を引いた（以降の例でも同様）．

\begin{exe}
 \ex \label{s:eva-org}
めには、いちごの　あかい　みを　いれました。\\
\small （舟崎靖子「もりのおかしやさん」p.~11（1979，偕成社））
\end{exe} 

\begin{exe}
 \ex \label{s:ehon-full}
め/名詞-一般/メ/\ul{目},
に/助詞-格助詞-一般/ニ/に,
は/助詞-係助詞/ハ/は,
、/記号-読点/、/、,
いちご/名詞-一般/イチゴ/\ul{苺},
の/助詞-連体化/ノ/の,
　/記号-空白/　/　,
あかい/形容詞-自立/アカイ/\ul{赤い},
　/記号-空白/　/　,
み/名詞-一般/ミ/\ul{実},
を/助詞-格助詞-一般/ヲ/を,
　/記号-空白/　/　,
いれ/動詞-自立/イレ/\ul{入れる},
まし/助動詞/マシ/ます,
た/助動詞/タ/た,
。/記号-句点/。/。
 \end{exe}

アノテーションは，言語学者や研究者ではない一般の作業者によって行ったが，
特に活用語に対するアノテーションは難しく，既存のラベルありデータを参照しながら作業を行った．
また，作業者による不一致や判断のゆれをなくすため，一定の作業の後には
同じ出現形の形態素に異なる品詞や原形が振られたもの\footnote{例えば，\jpn[ごしごし]{}を\pos{名詞-サ変接続}にするか，\pos{副詞-一般}にするか，といった判断のゆれが多かった．}をリストアップし，
統一的に確認，修正を行う作業を繰り返した．
なお，実際の作業では，アノテーションしたデータを順次学習データに追加することで，
解析精度自体を高めながら作業を進めた（\ref{sec:exp-add-ehon}章参照）．


フルアノテーションを行う
対象データは 2 通りの方法で選んだ．まず，
対象年齢がはっきりしている\kodomo\ 190 冊を対象とした．
また，それ以外の\first, \ehon の中から，絵本をランダムに選び，さらにラン
ダムに 1 ページずつ選んで対象とした（以下，\random）．サイズは表~\ref{tb:test-size}の通りである．
フルアノテーションデータは，\ref{sec:exp-adult}章の
教師なし分野適応実験の評価用データとして利用するほか，
\ref{sec:exp-add-ehon}章の
教師あり分野適応実験の学習，評価用データとして利用する．

\begin{table}[t]
\caption{絵本のフルアノテーションデータのサイズ}
\label{tb:test-size}
\input{1008table04.txt}
\end{table}


形態素解析器
\label{sec:morph-kytea}

本稿では，既存の辞書やラベルありデータを，対象分野である絵本の特徴にあ
わせて自動的に変換する手法を提案する．学習器は学習データと独立に選ぶこ
とができるが，本稿では，京都テキスト解析ツールキッ
ト\kytea\ \cite{Mori:Nakata:Graham:Kawahara:2011j} の学習機能を利用する．

\kytea では，点予測を採用しており，分類器の素性として，周囲の単語境界や品詞等の推定値を利用せ
ずに，周囲の文字列の情報のみを利用する．
そのため，柔軟に言語資源を利用することができ，分野適応が容易だという
特徴がある\cite{Mori:Nakata:Graham:Kawahara:2011j}．


\kytea のモデル学習時には，フルアノテーションデータ，部分アノテーショ
ンデータ，辞書などの言語資源が利用できる．これらの言語資源は，それぞれ
複数利用することができる．また，辞書と部分アノテーションデータはなくて
もよい．

ここで，フルアノテーションデー
タとは，文全体に形態素情報が付与されたデータである（\ref{sec:full-ano}節，
例\refs{ehon-full}）．また，部分アノテーションデータとは，文の一部に
だけ単語境界や形態素情報が付与されたデータである．
例えば，例\refs{ehon-part}のように，文\refs{eva-org}の\jpn[め]{}と
\jpn[み]{}にだけ形態素情報をアノテーションしたデータを，部分アノ
テーションデータとして利用することができる．
誤りやすい語や分野特有の語にだけ集中的にアノテーションを付与して利用
できるため，能動学習や分野適応に有効である．
 \begin{exe}
 \ex \label{s:ehon-part}
め/名詞-一般/メ/\ul{目},に は 、 い ち ご の 　 あ か い 　,み/名詞-一般/ミ/\ul{実},を 　 い れ ま し た 。
 \end{exe}

なお，\kytea の配布版モデルでは，単語分割とUniDicの品詞大分類，読みの付与を行っているが，
他の種類の品詞や情報を推定するモデルの構築も可能である．
本稿では，既存言語資源との整合性を考慮し，品詞はIPA品詞体系に準拠した．
さらに，元の漢字表記の推定も同時に行う．つまり，
単語分割，IPA品詞体系の品詞，読み，漢字表記による原形推定を出力とするモデルを構築する．

本稿では，フルアノテーションデータとして，
コーパス\hinoki\ \cite{Bond:Fujita:Tanaka:2006}を用いる．\hinoki に
は，\lxd の定義文，例文，京大コーパスの全文\footnote{但し，IPA品詞体系で解析
しなおしてある．}が含まれている．さらに教師あり分野適応
の実験（\ref{sec:exp-add-ehon}章）では，絵本のフルアノテーションデータも利用する．

辞書には，
\naistj \footnote{http://sourceforge.jp/projects/naist-jdic/} （以下，
\ntj），\lxd，および，日本語語彙大系\cite{GoiTaikeij}の固有名詞，および，動植物名\footnote{
具体的には，日本語語彙大系の日本語辞書のうち，\izj{543:生物}配
下の意味クラスが付与されている語を追加した．}を利用する．但し，\lxd と日
本語語彙大系は，本来IPA品詞体系ではないため，自動的に品詞を変換した．


絵本を対象とした形態素解析における問題分析
\label{sec:bunseki}

本章では，絵本を形態素解析するときに起こる精度低下の原因を調査する．

\ref{sec:mojisyu}節では，一般的なテキストと比べて，絵本のテキストでは，
空白，ひらがなが圧倒的に多く，漢字が非常に少ないことを示した．これらの
違いのうち，直感的には，ひらがなによる曖昧性の増加が精度低下の主要因で
あり，空白は解析の手がかりとなるように感じられる．しかしこれまで，この
直感が正しいかどうか，また，実際にどの程度精度への影響があるのかを調査
した研究はない．そこで本章では，ひらがなと空白の形態素解析への影響を調
査する．


\subsection{実験用解析対象文の作成}
\label{sec:bunseki-bun}

調査用の評価データとして，絵本の\kodomo の
フルアノテーションデータをいくつかのルールに沿って自動的に変換したデータを作成する．
つまり，
絵本に出現した文\refs{eva-org} （\ref{sec:full-ano}節）
から空白を削除したもの（文\refs{eva-del}），
空白を読点に変換したもの（文\refs{eva-punc}），
ひらがなをできるだけ漢字に変換したもの（文\refs{eva-han}），
漢字に変換し，かつ，空白を削除したもの（文\refs{eva-handel}），
漢字に変換し，かつ，空白を読点に変換したもの（文\refs{eva-hanpunc}）
を作成した．

 \begin{exe}
 \ex \label{s:eva-del}
めには、いちごのあかいみをいれました。
 \ex \label{s:eva-punc}
めには、いちごの、あかい、みを、いれました。
 \ex \label{s:eva-han}
目には、苺の　赤い　実を　入れました。
  \ex \label{s:eva-handel}
目には、苺の赤い実を入れました。
  \ex \label{s:eva-hanpunc}
目には、苺の、赤い、実を、入れました。
\end{exe} 


\subsection{実験と結果}
\label{sec:bunseki-exp}

調査のため，
\hinoki コーパスと\naistj などの辞書（\ref{sec:morph-kytea}章）をそのま
ま学習に利用したモデル（以下，\kytea（\Def））を構築する．これは，一般的
な形態素解析モデルと同じような学習条件に相当する．
また，表~\ref{tb:morph-ex} （\ref{sec:introduction}章）で利用した
既存の形態素解析モデルの中で最も誤りの少なかった \mecab も利用する．

\begin{table}[b]
\caption{評価結果: 形態素区切り，および，品詞が一致した数と割合 (\kodomo)}
\label{tb:res-bunseki}
\input{1008table05.txt}
\par\vspace{4pt}
\small
ただし，\refs{eva-org}から\refs{eva-handel}は，対応する評価用データの例の番号を示している．
\par
\end{table}

表~\ref{tb:res-bunseki}に，評価用データ（文\refs{eva-org}，および，
文\refs{eva-del}から文\refs{eva-hanpunc}）のそれぞれに対し，形態素解析を
実行し，形態素区切りと品詞一致精度を調べた結果を示す．



\subsection{分析}
\label{sec:ana-sphan}


表~\ref{tb:res-bunseki}の\pos{\Org \refs{eva-org}}の列が，
絵本のテキストをそのまま解析した場合の精度であり，
\kytea（\Def）では 63.0\%，\mecab では 83.2\%だった．
\mecab は
ひらがなのままの評価データの場合でも，ひらがなを考慮しない一般的な学習条件で学習し
た\kytea（\Def）よりも精度が高い．しかし，
新聞である京大コーパスを対象とした場合
98\% 以上の精度が報告されているのに比べると\footnote{\mecab\ ver.0.90 の場合．http://mecab.googlecode.com/svn/trunk/mecab/doc/feature.htmlより．}，
はるかに低い精度である．

ここで，空白の影響を分析する．\kytea（\Def）では，空白を削除す
ると精度が向上する．また，空白を読点に変更すると精度はさらに向上する．
これは，学習データに空白が出現しないため，学習できていないためだと考えられる．
空白をただ削除するよりも，読点に変更した方が精度が高くなることから，
空白の働きをうまく学習することができれば，区切りの判別の手がかりとして有効に働く
だろうことが予想できる．

実際，\mecab の場合，空白は区切りの判別のための手がかりと
して有効に利用されているようであり，空白を削除するとむしろ精度は低下す
る．また，空白を読点に変更した場合と空白のままの場合の精度は同程度で
あり，空白が読点の代わりを果たしていることが伺える．


特に，\refs{err-del}のように，擬音語や擬態語が連なる場合，
空白を削除すると，解析が非常に困難になっており，
空白の有無が形態素の判別に有効な手がかりであることがわかる．
 \begin{exe} 
 \ex \label{s:err-del}
  「こちょ　こちょ　こちょ　こちょ \\
 {\small （豊田一彦 「こちょこちょももんちゃん」p.~24（2010，童心社））}\\
  COR:「,\ul{こちょ,　,こちょ,　,こちょ,　,こちょ}\\
  RES:「,\ul{こ,ちょこ,ちょこちょこ,ちょ}\\
 \small （ただし，COR: は正解，RES: は空白を削除した場合の結果）
 \end{exe} 

次に，ひらがなが多いことによる影響を分析する．
評価データ中のひらがなを漢字に変換した場合，
\kytea（\Def）でも\mecab でも，ひらがなのままの評価データより高い精度
が得られる．空白を読点に変換した場合の精度
（表~\ref{tb:res-bunseki}の
\pos{\Sp\ \refs{eva-punc}}と
\pos{\HanSp\ \refs{eva-hanpunc}}）で
比較すると，
\kytea（\Def）では $+11.4$\%，\mecab では $+8.2$\% 精度が向上しており，
漢字は大きな手がかりとなっていることがわかる．
つまり，一般的なテキストとの大きな違いのうち，
ひらがなによる曖昧性の増大が解析精度の低下の主な要因だといえる．


なお，元データのままだと解析に失敗するが，漢字に変換すると正解する例には，
\refs{err-org}などがあった．
 \begin{exe}
 \ex \label{s:err-org}
  みずを　のみにきた　うしさんに \hfill
{\small （たちもとみちこ「おほしさま」p.~10（2006，教育画劇））}\\
COR: みず,を,　,のみ,に,き,た,　,うし,さん,に,\\
RES: みず,を,　,のみ,に,\ul{きた},　,\ul{うしさん},に\\
RES2: 水,を,　,飲み,に,\ul{来,た},　,\ul{牛,さん},に\\
\small （ただし，COR: は正解，RES: は結果，RES2: は漢字に変換した場合の結果）
 \end{exe} 


提案手法
\label{sec:morph}

 本章では，絵本の特徴に合わせたラベルありデータと辞書の変換方法を提案す
 る（\ref{sec:train-data}，\ref{sec:dic}節）．また，ラベルありデータと辞
 書の変換と追加の必要性について議論する（\ref{sec:comp-kudo}節）．


\subsection{ラベルありデータの変換方法}
\label{sec:train-data}


\ref{sec:bunseki}章で示したように，絵本の解析では，空白の働きを学習することと，
ひらがなが多い文でも解析できることが必要である．
そこで，既存のラベルありデータである\hinoki コーパスを 3 通りの方法で自動的に変換する．
例えば，文\refs{lxdex-org}は，\lxd での見出し語\jpn[きしめん]{}に付与さ
れた例文である．
この文に，まず，句読点の直後を除く文節毎に空白を挿入する（文\refs{lxdex-sp}）．
また，すべての漢字をひらがなの読みに変換する（文\refs{lxdex-hira}）．
句読点の直後を除く文節毎に空白を挿入し，かつ，ひらがなに変換する
（文\refs{lxdex-hirasp}）．このように，元の文に対して 3 通りの変換を行い，ラベルありデータデータを作成する．

 \begin{exe} 
 \ex \label{s:lxdex-org}
寄せ鍋,に,きしめん,を,入れる,。
 \ex \label{s:lxdex-sp}
寄せ鍋,に,　,きしめん,を,　,入れる,。
 \ex \label{s:lxdex-hira}
よせなべ,に,きしめん,を,いれる,。
 \ex \label{s:lxdex-hirasp}
よせなべ,に,　,きしめん,を,　,いれる,。
 \end{exe}

 さらに，元の漢字表記の推定も同時に行うため，元の漢字表記による原形を利
 用する．つまり，文\refs{lxdex-hira}や\refs{lxdex-hirasp}のようにひらが
 なに変換した場合でも，原形は漢字表記を利用する．そのため，例え
 ば\refs{lxdex-hira}は，実際には\refs{lxdex-hira-full}のような形で与えられる．

 \begin{exe}
 \ex \label{s:lxdex-hira-full}
よせなべ/名詞-一般/ヨセナベ/\ul{寄せ鍋},に/助詞-格助詞-一般/ニ/に,きしめん/名詞-一般/キシメン/きしめん,を/助詞-格助詞-一般/ヲ/を,いれる/動詞-自立/イレル/\ul{入れる},。/記号-句点/。/。
 \end{exe}

\ref{sec:exp-adult}章では，ラベルありデータの変換方法毎の
 効果を検証するため，これらの組み合わせを変えて利用した場合の精度評価を
 行う．

なお，空白の挿入に利用した文節区切りや，ひらがなへの変換に利用した読みは，
元々コーパスに付与されていたものであり，自動的に変換することができる．
本稿では\hinoki コーパスを利用したが，京大コーパスでも文節
情報や読みは付与されているため，同様の変換ができる．また\bccwj 
にも読みは付与されている．文節情報は付与されていないが，形態素情
報は付与されているため，助詞と自立語が連続する箇所に空白をいれるなどの
簡単なルールによって，同様の自動的変換が可能である．


\subsection{辞書の変換方法}
\label{sec:dic}

\ref{sec:morph-kytea}章で紹介した通り，辞書には
\ntj，\lxd，日本語語彙大系の固有名詞，および，動植物名を利用しており，
これらを絵本の特徴にあわせて変換する．

まず，\ntj と\lxd の漢字やカタカナのエントリ
をひらがなに変換したエントリも作成し，辞書に追加する．
固有名詞や動植物名は，カタカナで表記されることも多いため，
カタカナ，ひらがなの両方に変換したエントリも作成し，辞書に追加する．
このとき，原形には漢字やカタカナの表記を用いる．
例えば，
\jpn[伊予柑]{}の場合，元の見出し語か
ら得られる辞書エントリは\refs{iyokan-org}となるが，ひらがなのエントリ
\refs{iyokan-hira}とカタカナのエントリ\refs{iyokan-kata}も追加した．
しかし，人名の固有名詞だけは，カタカナはカタカナのまま，
ひらがなはひらがなのまま原形とした．
これは，ひらがなで出てくる人名の漢字表記が何かは決められないためである．
最終的に利用した辞書サイズは，表~\ref{tb:dic-size}の通りで
ある．

\begin{exe}
  \ex \label{s:iyokan-org}
  伊予柑/名詞-一般/イヨカン/伊予柑
  \ex \label{s:iyokan-hira}
  いよかん/名詞-一般/イヨカン/伊予柑
  \ex \label{s:iyokan-kata}
  イヨカン/名詞-一般/イヨカン/伊予柑
\end{exe}

\begin{table}[b]
\caption{辞書サイズ：ひらがなやカタカナに展開済み}
\label{tb:dic-size}
\input{1008table06.txt}
\end{table}


\subsection{辞書と学習データの追加の必要性についての議論}
\label{sec:comp-kudo}

\citeA{Kudo:Ichikawa:Talbot:Kazawa:2012j}は，
Web上のひらがな交じり文に対する形態素解析手法の提案にあたり，
次のように述べている．
\begin{quote}
ひらがな交じりの解析も，通常の日本語の文の解析であ
ることには変わりがないため，
以下のような一般的に用いられている既存手法で解析精度を向上させること
が可能である．\\
1. ひらがな単語のユーザ辞書への追加\\
2. ひらがな交じり文を含む学習データを人手で作成し，再学習\\
1. は簡単な手法であるが，ひらがなは日本語の機能語に用いられているた
め，むやみにひらがな語を追加すると副作用により精度が低下する可能性
がある．2. の方法は学習データの作成が必要なためコストが高い．
\end{quote}

これらの理由によって，\citeA{Kudo:Ichikawa:Talbot:Kazawa:2012j}では，
辞書への追加や学習データの追加は行われていない．
\citeA{Kudo:Ichikawa:Talbot:Kazawa:2012j}の手法は，広い分野に対して安定
的に比較的高い精度で解析を行える．しかし，特定の分野における実用を考え
た場合，対象分野においてより高い精度を得ることが重要である．
確かに，1. に関して，ひらがな語を多く追加することによる副作用の可能性は
否定できないが，絵本の場合，いずれの語でもひらがなで記述される可能性が
あるため，すべてのエントリをひらがなにする必要がある．また，2. に関して
は，提案手法では自動的に学習データを作成するので問題ない．

本稿では，提案手法で変換・作成した
辞書と学習データを学習に用いることで，絵本に対しては
既存モデルより高い精度が得られることを示す（\ref{sec:exp-adult}章）．
ただし，本提案手法で得られる精度
は，既存モデルよりは高いが，実用的にはまだ改良の必要がある．
そのため，さらなる精度向上のためには，能動学習や対象分野のラベルありデータの構築が必要となるが，
その際も，ベースとなるモデルの精度がより高い方がより効率的である．


