================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[279] 本論文では，ある対象を評価している文章（レビュー）が与えられた時，対象物に対する評価が「良い」か「悪い」かでレビューを二値分類するのではなく，どの程度「良い」か「悪い」かの指標（sentiment polarity score（SP score））をレビューに与える新しいタスクを提案する．

================================================================
[section type  : proposed_method]
[section title : Introduction]
================================================================
[1731] In addition, our system determines the subjectivity of each sentence using a Naive Bayes classifier, since a review includes many irrelevant sentences. This approach performs well when training data includes many irrelevant sentences, but may lead to a reduction in the classifier's accuracy. This is because Naive Bayes classification cannot correctly classify subjective sentences completely, while objective sentences can contain information that is useful in determining SP scores. We show that this problem can be overcome, however, simply by adding a constant factor to the Naive Bayes estimation.

================================================================
[section type  : proposed_method]
[section title : Related Work]
================================================================
[2289] Independently of and in parallel with our work, two other papers consider the degree of polarity for the purposes of sentiment classification. Koppel [CITE] exploited a neutral class and applied a regression method similar to that of the present study. Pang [CITE] applied a metric labeling method for the task in which similar reviews tend to have same polarities. Our work differs from these two studies in several respects. In the present study evaluation was carried out by exploiting square errors rather than precision errors, with a five-point scoring scale used in the experiments, in contrast to Koppel [CITE], who used three (``good'', ``bad'', ``neutral''), and Pang [CITE], who used three/four point scores. Therefore we use regression which minimize not a precision error but a square error. We argue that the precision errors are not enough to capture the task. Because if we use the precision errors mistakes of assigning [MATH] SP score to a review whose correct SP score is [MATH] can occur many times, which becomes unacceptable problem in real applications. We also examine various features to capture the characteristics of reviews, which are found to be effective in experiences.

================================================================
[section type  : proposed_method]
[section title : Analyzing Reviews with Polarity Scores]
================================================================
[1319] In this section we present a novel task setting where we predict the degree of sentiment polarity of a review. We first define SP scores and the task of assigning them to review documents. We then describe the present evaluation data set. Using this data set, we examined the performance of human classifiers on this task, to clarify the difficulty of quantifying polarity.
-----------------------------------------------------
  [subsection title : Sentiment Polarity Scores]
-----------------------------------------------------
  [1409] The task is to assign correct SP scores to unseen reviews as accurately as possible. Let [MATH] be the predicted SP score and [MATH] be the SP score assigned by the reviewer. We measure the performance of an estimator with the mean square error,
-----------------------------------------------------
  [subsection title : Evaluation Data]
-----------------------------------------------------
  [1822] It was easier to predict SP scores for Corpus A than Corpus B because Corpus A books have a smaller vocabulary and each review was about twice as large. To create a data set with a uniform score distribution (the effect of skewed class distributions is out of the scope of this paper), we selected 330 reviews per SP score for Corpus A and 280 reviews per SP score for Corpus B. Table [REF_hoge] shows the number of words and sentences in the corpora. There is no significant difference in the average number of words/sentences among different SP scores.
-----------------------------------------------------
  [subsection title : Preliminary Experiments: Human Performance for Assigning SP scores]
-----------------------------------------------------
  [1970] We asked two computational linguists to independently assign an SP score to each review from Corpus A. These two linguists first learned the relationship between reviews and SP scores using 20 reviews, and were then given 100 reviews with a uniform SP score distribution as test data. Table [REF_humantest_abssq] shows the results given in terms of the mean square error. The Random row shows the performance achieved by random assignment, and the All3 row shows the performance achieved by assigning [MATH] to all the reviews. These results suggest that SP scores would be estimated solely from the contents of reviews with a square error of [MATH].

================================================================
[section type  : proposed_method]
[section title : Assigning SP scores to Reviews]
================================================================
[1926] This section describes a machine learning approach to predict the SP scores of review documents. Our method consists of the following two steps: extraction of feature vectors from reviews, and estimation of SP scores from these feature vectors. The first step basically uses existing techniques for document classification. In contrast, the prediction of SP scores is different from previous studies because we consider ordered multi-class classification, that is, each SP score has its own class and the classes are ordered. Unlike usual multi-class classification, large mistakes in terms of the order should have large penalties. In this paper, we discuss two methods of estimating SP scores: pSVMs and SVR.
-----------------------------------------------------
  [subsection title : Review Representation]
-----------------------------------------------------
  [1330] We represent a review as a feature vector. Although this representation ignores the syntactic structure, word positions, and the order of words, it is known to work reasonably well for many tasks such as information retrieval and document classification. We use binary, tf, and tf-idf as feature weighting methods [CITE].
-----------------------------------------------------
  [subsection title : Support Vector Regression]
-----------------------------------------------------
  [2275] This approach captures the order of classes and does not suffer from data sparseness. While we could use conventional linear regression instead of SVR [CITE], in the present study we use SVR because it can exploit the kernel-trick and avoid over-training. Another good characteristic of SVR is that we can identify the features contributing to determining the SP scores by examining the coefficients ([MATH] in ([REF_svr])), while pSVMs do not give such information, because multiple classifiers are involved in determining final results. A difficulty associated with the present approach, however, is that it is difficult to learn non-linear regression by SVR. For example, when given training data is [MATH], SVR cannot perform regression correctly without adjusting the input space (feature values) so that the output plane becomes linear-one. Note that this problem does not occur in classification problems, but in regression problems. We can solve this problem by choosing an appropriate kernel for the task, but this selection is not straightforward.
-----------------------------------------------------
  [subsection title : Pairwise Support Vector Machines]
-----------------------------------------------------
  [1538] We apply a multi-class classification approach to estimating SP scores. pSVMs [CITE] consider each SP score as a unique class, ignoring the order among the classes. Given reviews with SP scores [MATH], we construct [MATH] SVM classifiers for all the pairs of possible values of SP scores. The classifier for an SP score pair ([MATH] vs [MATH]) assigns the SP score to a review with [MATH] or [MATH].
-----------------------------------------------------
  [subsection title : Features beyond Bag-of-Words]
-----------------------------------------------------
  [1706] Previous studies [CITE] suggested that complex features do not work as expected because data becomes sparse when such features are used, and a bag-of-words approach is sufficient to capture the information in most reviews. Nevertheless, we observed that reviews include many chunks of words such as ``very good'' or ``must buy'' that are useful for estimating the degree of polarity. We confirmed this observation by using n-grams.
-----------------------------------------------------
  [subsection title : Identification of Subjectivity Sentences]
-----------------------------------------------------
  [1829] The feature values in [MATH] and [MATH] can be considered to be the expected feature values in a subjective sentence. We could alternatively adopt an approach [CITE] whereby first the objective sentences are eliminated, and then we solve the problem as before. This approach would be faster than our approach since the training data is smaller than the original training data set. However, the results of this approach would be less accurate than our approach using the feature vectors [MATH] and [MATH], since these vectors can be seen as an approximation of feature values. However, clearly we can tradeoff the speed and accuracy of the classifier by the choice of approach, and we plan to investigate this further as part of future work.

================================================================
[section type  : proposed_method]
[section title : Experiments]
================================================================
[1245] We divided the data into ten disjoint subsets, maintaining the uniform class distribution. All the results reported below are the averages of ten-fold cross-validation. In SVMs and SVR, we used SVMlight with the quadratic polynomial kernel [MATH] and set the control parameter [MATH] to [MATH] in all the experiments.
-----------------------------------------------------
  [subsection title : Comparison of pSVMs and SVR]
-----------------------------------------------------
  [1972] We compared pSVMs and SVR to see differences in the properties of the regression approach compared with those of the classification approach. Both pSVMs and SVR used unigram/tf-idf to represent reviews. Table [REF_hikaku] shows the square error results for SVM, SVR and a simple regression (least square error) method for Corpus A/B. These results indicate that SVR outperformed SVM in terms of the square error and suggests that regression methods avoid large mistakes by taking account of the fact that SP scores are ordered, while pSVMs does not. We also note that the result of a simple regression method is close to the result of SVR with a linear kernel.
-----------------------------------------------------
  [subsection title : Comparison of Different Features]
-----------------------------------------------------
  [2086] Table [REF_tab:feature4] summarizes the comparison results for different features. For Corpus A, unigram + bigram and unigram + trigram achieved high performance. The performance of unigram + inbook does not achieve as good a performance as expected, contrary to our intuitive belief that the words around the target object are more important than others. However, for Corpus B, the results are less accurate, that is, n-gram features were less able to accurately predict the SP scores. This is because the variety of words/phrases was much larger than in Corpus A, and n-gram features may have suffered from a data sparseness problem. We should note that these feature settings are too simple, and we cannot accept the result of reference or target object (inbook/aroundbook) directly.
-----------------------------------------------------
  [subsection title : Using Naive Bayes Classifier to Subjectivity Detection]
-----------------------------------------------------
  [2134] The results indicate that NB is better than baseline when the training data and test data are different, especially when the training data is corpus B and test data is corpus A. We suspect that when we use reviews taken from various themes as training data, some proper nouns have polarity and these words cause the classifier to be misled to the wrong polarity. In contrast, when we use reviews on a specific theme as training data, proper nouns tend to occur uniformly through all SP scores, and the effects of proper nouns on polarity scores are not overestimated. The decline of accuracy by NB in corpora A and B is probably caused by the inadequate performance of Naive Bayes classifiers or loss of useful information in objective sentences. NB with C performs well in each case, suggesting that NB with C has the advantages of objective sentence elimination without suffering any significant decline due to the loss of information in objective sentences.
-----------------------------------------------------
  [subsection title : Learning Curve]
-----------------------------------------------------
  [1320] We generated learning curves to examine the effect of the size of training data on performance. Figure [REF_fig:lc] shows the results of a classification task using unigram/tf-idf to represent reviews. The results suggest that performance can be improved further by increasing the training data.

================================================================
[section type  : proposed_method]
[section title : Conclusion]
================================================================
[1502] This result agrees with our intuition that pSVMs do not consider the order of SP scores, while SVR captures the order of SP scores and avoids high penalty mistakes. With SVR, SP scores can be estimated with a square error of [MATH], which is very close to the square error achieved by human classifiers ([MATH]).

