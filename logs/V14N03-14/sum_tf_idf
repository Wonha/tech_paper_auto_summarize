================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:0, score:0.41233] 本論文では，ある対象を評価している文章（レビュー）が与えられた時，対象物に対する評価が「良い」か「悪い」かでレビューを二値分類するのではなく，どの程度「良い」か「悪い」かの指標（sentiment polarity score（SP score））をレビューに与える新しいタスクを提案する．
[i:1, score:0.31244] SP scoreはレビューの簡潔な要約であり，単純な「良い」か「悪い」かの二値分類より詳細な情報を与える．
[i:3, score:0.40598] 本稿ではsupport vector regressionを用いてSP scoreを求める方法を提案する．

================================================================
[section type  : proposed_method]
[section title : Introduction]
================================================================
[i:12, score:0.99129] In this paper, we describe a machine learning method for this task. Our system uses support vector regression (SVR) [CITE] to determine the SP scores of reviews. This method enables us to annotate SP scores to arbitrary reviews, such as comments in bulletin board systems or blog systems. We explore several types of features beyond a bag-of-words to capture key phrases to determine SP scores: n-grams and references (the words around the reviewed object).
[i:13, score:0.99613] In addition, our system determines the subjectivity of each sentence using a Naive Bayes classifier, since a review includes many irrelevant sentences. This approach performs well when training data includes many irrelevant sentences, but may lead to a reduction in the classifier's accuracy. This is because Naive Bayes classification cannot correctly classify subjective sentences completely, while objective sentences can contain information that is useful in determining SP scores. We show that this problem can be overcome, however, simply by adding a constant factor to the Naive Bayes estimation.
[i:14, score:0.98822] We conducted experiments with book reviews from amazon.com, each of which had a five-point scale rating along with text. We compared pairwise support vector machines (pSVMs) and SVR and found that SVR outperformed better than pSVMs by about 30% in terms of the squared error, which is close to human performance. We also demonstrated that the detection of sentence subjectivity by using a Naive Bayes classifier improved the robustness of the classifier.

================================================================
[section type  : proposed_method]
[section title : Related Work]
================================================================
[i:17, score:0.96680] There have been many attempts to analyze reviews to a deeper level in order to improve accuracy. Mullen [CITE] used features from various information sources such as references to the ``work'' or ``artist'', which were annotated by hand, and showed that these features have the potential to improve accuracy. We use reference features given by the words around the fixed review target word (``book'').
[i:18, score:0.94088] Turney [CITE] used semantic orientation, which measures the distance from phrases to ``excellent'' or ``poor'' by using search engine results and gives the word polarity. Kudo [CITE] used decision stumps to capture substructures embedded in text (such as word-based dependency), and suggested that subtree features are important for opinion/modality classification.
[i:19, score:0.99945] Independently of and in parallel with our work, two other papers consider the degree of polarity for the purposes of sentiment classification. Koppel [CITE] exploited a neutral class and applied a regression method similar to that of the present study. Pang [CITE] applied a metric labeling method for the task in which similar reviews tend to have same polarities. Our work differs from these two studies in several respects. In the present study evaluation was carried out by exploiting square errors rather than precision errors, with a five-point scoring scale used in the experiments, in contrast to Koppel [CITE], who used three (``good'', ``bad'', ``neutral''), and Pang [CITE], who used three/four point scores. Therefore we use regression which minimize not a precision error but a square error. We argue that the precision errors are not enough to capture the task. Because if we use the precision errors mistakes of assigning [MATH] SP score to a review whose correct SP score is [MATH] can occur many times, which becomes unacceptable problem in real applications. We also examine various features to capture the characteristics of reviews, which are found to be effective in experiences.

================================================================
[section type  : proposed_method]
[section title : Analyzing Reviews with Polarity Scores]
================================================================
[i:20, score:0.97682] In this section we present a novel task setting where we predict the degree of sentiment polarity of a review. We first define SP scores and the task of assigning them to review documents. We then describe the present evaluation data set. Using this data set, we examined the performance of human classifiers on this task, to clarify the difficulty of quantifying polarity.
-----------------------------------------------------
  [subsection title : Sentiment Polarity Scores]
-----------------------------------------------------
  [i:lead, score:0.96255] We extend the sentiment classification task to the more challenging task of assigning rating scores to reviews. We call this score the SP score. Examples of SP scores include five-star scales and scores out of 100. Let SP scores take discrete values in a closed interval [MATH].
.....
  [i:21, score:0.96255] We extend the sentiment classification task to the more challenging task of assigning rating scores to reviews. We call this score the SP score. Examples of SP scores include five-star scales and scores out of 100. Let SP scores take discrete values in a closed interval [MATH].
  [i:22, score:0.97645] The task is to assign correct SP scores to unseen reviews as accurately as possible. Let [MATH] be the predicted SP score and [MATH] be the SP score assigned by the reviewer. We measure the performance of an estimator with the mean square error,
  [i:23, score:0.94943] where [MATH] is the test set of reviews. In contrast to conventional multi-class classification, which gives equal penalties to all mistakes, penalties for the present estimator are larger when the mistake in predicted SP score is large.
-----------------------------------------------------
  [subsection title : Evaluation Data]
-----------------------------------------------------
  [i:lead, score:0.47372] We used book reviews on amazon.com for evaluation data[MATH].
.....
  [i:27, score:0.94367] Each review has stars assigned by the reviewer, with the number of stars ranging from one to five, where one is the worst score, while five is the best. We converted the number of stars into SP scores [MATH].
  [i:29, score:0.84184] From these data, we made two data sets. The first was a set of reviews for books in the Harry Potter series (Corpus A).
  [i:31, score:0.99249] It was easier to predict SP scores for Corpus A than Corpus B because Corpus A books have a smaller vocabulary and each review was about twice as large. To create a data set with a uniform score distribution (the effect of skewed class distributions is out of the scope of this paper), we selected 330 reviews per SP score for Corpus A and 280 reviews per SP score for Corpus B. Table [REF_hoge] shows the number of words and sentences in the corpora. There is no significant difference in the average number of words/sentences among different SP scores.
-----------------------------------------------------
  [subsection title : Preliminary Experiments: Human Performance for Assigning SP scores]
-----------------------------------------------------
  [i:lead, score:0.95689] We treat the SP scores assigned by the reviewers as correct answers. However, the content of a review and its SP score may not be related. Moreover, SP scores may vary depending on the reviewers. Accordingly, we examined the universality of the SP score.
.....
  [i:33, score:0.99500] We asked two computational linguists to independently assign an SP score to each review from Corpus A. These two linguists first learned the relationship between reviews and SP scores using 20 reviews, and were then given 100 reviews with a uniform SP score distribution as test data. Table [REF_humantest_abssq] shows the results given in terms of the mean square error. The Random row shows the performance achieved by random assignment, and the All3 row shows the performance achieved by assigning [MATH] to all the reviews. These results suggest that SP scores would be estimated solely from the contents of reviews with a square error of [MATH].
  [i:34, score:0.98192] Table [REF_humantest_a] shows the distribution of the estimated SP scores and correct SP scores. In the table we can observe the difficulty of this task; the precise quantification of SP scores. For example, it can be seen from the table that human B tended to overestimate SP scores for reviews whose correct scores were in the range between 2 and 4, assigning a [MATH] or [MATH].
  [i:35, score:0.97137] We should note that if we consider this task as binary classification by treating the reviews whose SP scores are [MATH] and [MATH] as positive examples and those with [MATH] and [MATH] as negative examples (ignoring the reviews whose SP scores are [MATH]), the classification precisions by humans A and B are 95% and 96% respectively.

================================================================
[section type  : proposed_method]
[section title : Assigning SP scores to Reviews]
================================================================
[i:44, score:0.99813] This section describes a machine learning approach to predict the SP scores of review documents. Our method consists of the following two steps: extraction of feature vectors from reviews, and estimation of SP scores from these feature vectors. The first step basically uses existing techniques for document classification. In contrast, the prediction of SP scores is different from previous studies because we consider ordered multi-class classification, that is, each SP score has its own class and the classes are ordered. Unlike usual multi-class classification, large mistakes in terms of the order should have large penalties. In this paper, we discuss two methods of estimating SP scores: pSVMs and SVR.
-----------------------------------------------------
  [subsection title : Review Representation]
-----------------------------------------------------
  [i:lead, score:0.95089] We represent a review as a feature vector. Although this representation ignores the syntactic structure, word positions, and the order of words, it is known to work reasonably well for many tasks such as information retrieval and document classification. We use binary, tf, and tf-idf as feature weighting methods [CITE].
.....
  [i:46, score:0.32569] The feature vectors are normalized to have [MATH] norm [MATH].
-----------------------------------------------------
  [subsection title : Support Vector Regression]
-----------------------------------------------------
  [i:lead, score:0.66231] Support vector regression (SVR) is a method of regression that follows a similar underlying idea to that of SVM [CITE].
.....
  [i:49, score:0.96959] where [MATH] is the predicted SP score, [MATH] is the feature vector of a review, [MATH] and [MATH] are parameters of SVR. SVR uses an [MATH]-insensitive loss function. This loss function means that all errors inside an [MATH] cube are ignored. This allows SVR to require only a few support vectors, and gives a generalization ability. Given a training set, [MATH], parameters [MATH] and [MATH] are determined by solving the following problem,
  [i:50, score:0.95366] The factor [MATH] is a parameter that controls the trade-off between training error minimization and margin maximization. The loss in training data increases as [MATH] becomes smaller, while generalization is lost as [MATH] becomes larger. Moreover, we can apply a kernel-trick to SVR, as in the case for SVMs, by using a kernel function.
  [i:51, score:0.99914] This approach captures the order of classes and does not suffer from data sparseness. While we could use conventional linear regression instead of SVR [CITE], in the present study we use SVR because it can exploit the kernel-trick and avoid over-training. Another good characteristic of SVR is that we can identify the features contributing to determining the SP scores by examining the coefficients ([MATH] in ([REF_svr])), while pSVMs do not give such information, because multiple classifiers are involved in determining final results. A difficulty associated with the present approach, however, is that it is difficult to learn non-linear regression by SVR. For example, when given training data is [MATH], SVR cannot perform regression correctly without adjusting the input space (feature values) so that the output plane becomes linear-one. Note that this problem does not occur in classification problems, but in regression problems. We can solve this problem by choosing an appropriate kernel for the task, but this selection is not straightforward.
-----------------------------------------------------
  [subsection title : Pairwise Support Vector Machines]
-----------------------------------------------------
  [i:lead, score:0.98721] We apply a multi-class classification approach to estimating SP scores. pSVMs [CITE] consider each SP score as a unique class, ignoring the order among the classes. Given reviews with SP scores [MATH], we construct [MATH] SVM classifiers for all the pairs of possible values of SP scores. The classifier for an SP score pair ([MATH] vs [MATH]) assigns the SP score to a review with [MATH] or [MATH].
.....
  [i:52, score:0.98721] We apply a multi-class classification approach to estimating SP scores. pSVMs [CITE] consider each SP score as a unique class, ignoring the order among the classes. Given reviews with SP scores [MATH], we construct [MATH] SVM classifiers for all the pairs of possible values of SP scores. The classifier for an SP score pair ([MATH] vs [MATH]) assigns the SP score to a review with [MATH] or [MATH].
  [i:53, score:0.91834] The class label of a document is determined by majority voting of the classifiers. Any ties in the voting are resolved by choosing the class that is closest to the neutral SP score (i.e, [MATH]).
  [i:54, score:0.98294] This approach ignores the fact that SP scores are ordered, which causes the following two problems: First, it allows large mistakes. Second, when the number of possible values of the SP score is large (e.g, [MATH]), this approach suffers from a data sparseness problem. This is because pSVMs cannot employ examples that have close SP scores (e.g, SP score = [MATH]) for the classification of other SP scores (e.g, the classifier for a SP score pair [MATH]).
-----------------------------------------------------
  [subsection title : Features beyond Bag-of-Words]
-----------------------------------------------------
  [i:lead, score:0.99007] Previous studies [CITE] suggested that complex features do not work as expected because data becomes sparse when such features are used, and a bag-of-words approach is sufficient to capture the information in most reviews. Nevertheless, we observed that reviews include many chunks of words such as ``very good'' or ``must buy'' that are useful for estimating the degree of polarity. We confirmed this observation by using n-grams.
.....
  [i:55, score:0.99007] Previous studies [CITE] suggested that complex features do not work as expected because data becomes sparse when such features are used, and a bag-of-words approach is sufficient to capture the information in most reviews. Nevertheless, we observed that reviews include many chunks of words such as ``very good'' or ``must buy'' that are useful for estimating the degree of polarity. We confirmed this observation by using n-grams.
  [i:56, score:0.97746] Since the words around the review target might be expected to influence the overall SP score more than other words, we use these words as features. We call these features reference. We assume the review target is only the word ``book'', and we use ``inbook'' and ``aroundbook'' features. The ``inbook'' feature are the words appearing in the sentence which includes the word ``book''.
  [i:57, score:0.72959] The ``around book'' feature is given by the words lying within two places either side of the word ``book''.
-----------------------------------------------------
  [subsection title : Identification of Subjectivity Sentences]
-----------------------------------------------------
  [i:lead, score:0.93682] A review document includes many sentences that are irrelevant to sentiment polarity of the document, such as explanation of a reviewed object or objective sentences. There exist some methods for detecting subjective sentences by a knowledge-based approach or machine learning [CITE].
.....
  [i:81, score:0.96587] Using the probability [MATH], we recalculate the value of each feature. The disadvantage of this approach is that the information of the training data becomes small in comparison with the original data, because the estimation of Naive Bayes classification tends to overestimate the probability. For example, even if the true probability is close to [MATH], the result of NB would be [MATH] or [MATH].
  [i:82, score:0.96200] Futhermore, the objective sentences may have information that could be useful in determining the SP scores. We therefore introduce a smoothing factor [MATH], which means that we assignes probability from [MATH] to [MATH] (by ignoring the normalization factor).
  [i:88, score:0.99205] The feature values in [MATH] and [MATH] can be considered to be the expected feature values in a subjective sentence. We could alternatively adopt an approach [CITE] whereby first the objective sentences are eliminated, and then we solve the problem as before. This approach would be faster than our approach since the training data is smaller than the original training data set. However, the results of this approach would be less accurate than our approach using the feature vectors [MATH] and [MATH], since these vectors can be seen as an approximation of feature values. However, clearly we can tradeoff the speed and accuracy of the classifier by the choice of approach, and we plan to investigate this further as part of future work.

================================================================
[section type  : proposed_method]
[section title : Experiments]
================================================================
[i:89, score:0.94340] We performed two series of experiments. First, we compared pSVMs and SVR and examined the performance of various features and weighting methods. Second, we compared the method using sentence subjectivity detection with the method which does not.
[i:90, score:0.86378] The corpora A and B introduced in Section [REF_eval] were used as the experimental data. We first removed all HTML tags and punctuation marks, and then applied the Porter stemming method [CITE] to the reviews.
[i:91, score:0.93772] We divided the data into ten disjoint subsets, maintaining the uniform class distribution. All the results reported below are the averages of ten-fold cross-validation. In SVMs and SVR, we used SVMlight with the quadratic polynomial kernel [MATH] and set the control parameter [MATH] to [MATH] in all the experiments.
-----------------------------------------------------
  [subsection title : Comparison of pSVMs and SVR]
-----------------------------------------------------
  [i:lead, score:0.99717] We compared pSVMs and SVR to see differences in the properties of the regression approach compared with those of the classification approach. Both pSVMs and SVR used unigram/tf-idf to represent reviews. Table [REF_hikaku] shows the square error results for SVM, SVR and a simple regression (least square error) method for Corpus A/B. These results indicate that SVR outperformed SVM in terms of the square error and suggests that regression methods avoid large mistakes by taking account of the fact that SP scores are ordered, while pSVMs does not. We also note that the result of a simple regression method is close to the result of SVR with a linear kernel.
.....
  [i:93, score:0.99717] We compared pSVMs and SVR to see differences in the properties of the regression approach compared with those of the classification approach. Both pSVMs and SVR used unigram/tf-idf to represent reviews. Table [REF_hikaku] shows the square error results for SVM, SVR and a simple regression (least square error) method for Corpus A/B. These results indicate that SVR outperformed SVM in terms of the square error and suggests that regression methods avoid large mistakes by taking account of the fact that SP scores are ordered, while pSVMs does not. We also note that the result of a simple regression method is close to the result of SVR with a linear kernel.
  [i:94, score:0.78594] Figure [REF_fig:hresult] shows the distribution of estimation results for humans (top left: human A, top right: human B), pSVMs (below left), and SVR (below right).
  [i:95, score:0.97210] In all the plots the horizontal axes show the estimated SP scores, the vertical axes show the correct SP scores, while shading indicates the number of reviews. These figures suggest that pSVMs and SVR were able to capture the gradualness of SP scores better than the human classifiers. They also show that pSVMs cannot predict neutral SP scores well, whereas SVR accurately predicts these scores.
-----------------------------------------------------
  [subsection title : Comparison of Different Features]
-----------------------------------------------------
  [i:lead, score:0.87476] We compared the different features presented in Section [REF_step1] and feature weighting methods. First we compared different weighting methods, using only unigram features for this comparison. We then compared different features, using only tf-idf weighting methods for this comparison.
.....
  [i:106, score:0.99730] Table [REF_tab:feature4] summarizes the comparison results for different features. For Corpus A, unigram + bigram and unigram + trigram achieved high performance. The performance of unigram + inbook does not achieve as good a performance as expected, contrary to our intuitive belief that the words around the target object are more important than others. However, for Corpus B, the results are less accurate, that is, n-gram features were less able to accurately predict the SP scores. This is because the variety of words/phrases was much larger than in Corpus A, and n-gram features may have suffered from a data sparseness problem. We should note that these feature settings are too simple, and we cannot accept the result of reference or target object (inbook/aroundbook) directly.
  [i:107, score:0.98227] Note that the data used in the preliminary experiments described in Section [REF_SP score] are a part of Corpus A, and so we can compare the results obtained from the human classifiers with those for Corpus A in this experiment. The best result by the machine learning approach (0.89) was close to the human results (0.78).
  [i:108, score:0.99552] To analyze the influence of n-gram features, we used the linear kernel [MATH] in SVR training. We used tf-idf as feature weighting, and examined each coefficient of regression. Since we used the linear kernel, the coefficient value of SVR showed the polarity of a single feature, that is, this value expressed how much the occurrence of a particular feature affected the SP score. Tables [REF_tab:feature_unib], [REF_tab:feature_uniw], [REF_tab:feature_bib], [REF_tab:feature_biw], [REF_tab:feature_trib] and [REF_tab:feature_triw] show the coefficients resulting from the training of SVR. These results show that phrases such as ``all ag (age)'', ``can't wait'' ``on (one) star'' and ``not interest'' have strong polarity even if the word which constitutes these phrases does not have strong polarity.
-----------------------------------------------------
  [subsection title : Using Naive Bayes Classifier to Subjectivity Detection]
-----------------------------------------------------
  [i:lead, score:0.79120] We examined the effectiveness of subjectivity detection using the Naive Bayes classifier (NB) proposed in Section [REF_idenSub].
.....
  [i:110, score:0.99431] First, we examined the performance of NB itself by using Pang's sentence corpus version 1.0. The result of ten-fold cross-validation was [MATH] accuracy. A review, however, includes both subjective and objective sentences. Moreover, we have to examine whether the information of subjectivity contributes to the polarity detection. We asked two computer linguists to select the sentence which is most influential on the SP score in each review. The test data is the same as the test data used in Section [REF_SP score].
  [i:111, score:0.97639] We then assigned subjectivity for each sentence using the NB. Table [REF_tab:sub_human] shows the average subjectivity of all sentences and also of the most influential sentences as selected by the human classifiers. It is almost certain that subjectivity is correlated with the sentence that is most influential on the SP score.
  [i:123, score:0.99892] The results indicate that NB is better than baseline when the training data and test data are different, especially when the training data is corpus B and test data is corpus A. We suspect that when we use reviews taken from various themes as training data, some proper nouns have polarity and these words cause the classifier to be misled to the wrong polarity. In contrast, when we use reviews on a specific theme as training data, proper nouns tend to occur uniformly through all SP scores, and the effects of proper nouns on polarity scores are not overestimated. The decline of accuracy by NB in corpora A and B is probably caused by the inadequate performance of Naive Bayes classifiers or loss of useful information in objective sentences. NB with C performs well in each case, suggesting that NB with C has the advantages of objective sentence elimination without suffering any significant decline due to the loss of information in objective sentences.
-----------------------------------------------------
  [subsection title : Learning Curve]
-----------------------------------------------------
  [i:lead, score:0.95498] We generated learning curves to examine the effect of the size of training data on performance. Figure [REF_fig:lc] shows the results of a classification task using unigram/tf-idf to represent reviews. The results suggest that performance can be improved further by increasing the training data.
.....
  [i:124, score:0.95498] We generated learning curves to examine the effect of the size of training data on performance. Figure [REF_fig:lc] shows the results of a classification task using unigram/tf-idf to represent reviews. The results suggest that performance can be improved further by increasing the training data.

================================================================
[section type  : proposed_method]
[section title : Conclusion]
================================================================
[i:126, score:0.97759] We compared two methods for estimating SP scores: pSVMs and SVR. Experimental results for book reviews showed that SVR performed better in terms of the square error than pSVMs by about [MATH]%.
[i:127, score:0.98072] This result agrees with our intuition that pSVMs do not consider the order of SP scores, while SVR captures the order of SP scores and avoids high penalty mistakes. With SVR, SP scores can be estimated with a square error of [MATH], which is very close to the square error achieved by human classifiers ([MATH]).
[i:131, score:0.97789] We plan to develop a classifier specialized for ordered multi-class classification using recent studies on machine learning for structured output spaces [CITE] or ordinal regression [CITE], since our experiments suggest that pSVMs and SVR have both advantages and disadvantages. We will develop a more efficient classifier that outperforms pSVMs and SVR by combining these ideas. We also examine whether or not our task setting is appropriate to summarize the review.

