In recent years, discussion groups, online shops, and blog systems on the Internet have gained popularity, and the number of documents, such as reviews, is growing dramatically. Sentiment classification refers to classifying reviews not by their topics but by the polarity of their sentiment (e.g, positive or negative).
It is useful for recommendation systems, fine-grained information retrieval systems, and business applications that collect opinions about a commercial product.
Recently, sentiment classification has been actively studied and experimental results have shown that machine learning approaches perform well [CITE].
The present study asserts, however, that the polarity of reviews can be estimated more precisely than is possible with existing classification techniques. For example, both reviews A and B in Table [REF_tab:reviewSample] would be classified simply as positive in binary classification, but clearly this classification loses information concerning the difference in the degree of polarity that is apparent in the review texts.
We propose a novel type of document classification task where we evaluate reviews with scores, such as selecting from a scale of one to five stars for example. We call this score the sentiment polarity score (SP score).
If, for example, the range of the score is from one to five, we could give five to review A and four to review B. This task, namely, ordered multi-class classification, is considered as an extension of binary sentiment classification. In ordered multi-class classification, the classes are not independent, but are ordered. While it is possible to treat this problem as a multi-class classification task ignoring the order information, the performance of the classifier can be improved by incorporating this information into the classifier.
In this paper, we describe a machine learning method for this task. Our system uses support vector regression (SVR) [CITE] to determine the SP scores of reviews. This method enables us to annotate SP scores to arbitrary reviews, such as comments in bulletin board systems or blog systems. We explore several types of features beyond a bag-of-words to capture key phrases to determine SP scores: n-grams and references (the words around the reviewed object).
In addition, our system determines the subjectivity of each sentence using a Naive Bayes classifier, since a review includes many irrelevant sentences. This approach performs well when training data includes many irrelevant sentences, but may lead to a reduction in the classifier's accuracy. This is because Naive Bayes classification cannot correctly classify subjective sentences completely, while objective sentences can contain information that is useful in determining SP scores. We show that this problem can be overcome, however, simply by adding a constant factor to the Naive Bayes estimation.
We conducted experiments with book reviews from amazon.com, each of which had a five-point scale rating along with text. We compared pairwise support vector machines (pSVMs) and SVR and found that SVR outperformed better than pSVMs by about 30% in terms of the squared error, which is close to human performance. We also demonstrated that the detection of sentence subjectivity by using a Naive Bayes classifier improved the robustness of the classifier.
Recent studies on sentiment classification focused on machine learning approaches. Pang [CITE] represents a review as a feature vector and estimates polarity using SVM, which is almost the same method as those used for topic classification [CITE].
This paper essentially follows this work, but we extend this task to an ordered multi-class classification task.
There have been many attempts to analyze reviews to a deeper level in order to improve accuracy. Mullen [CITE] used features from various information sources such as references to the ``work'' or ``artist'', which were annotated by hand, and showed that these features have the potential to improve accuracy. We use reference features given by the words around the fixed review target word (``book'').
Turney [CITE] used semantic orientation, which measures the distance from phrases to ``excellent'' or ``poor'' by using search engine results and gives the word polarity. Kudo [CITE] used decision stumps to capture substructures embedded in text (such as word-based dependency), and suggested that subtree features are important for opinion/modality classification.
Independently of and in parallel with our work, two other papers consider the degree of polarity for the purposes of sentiment classification. Koppel [CITE] exploited a neutral class and applied a regression method similar to that of the present study. Pang [CITE] applied a metric labeling method for the task in which similar reviews tend to have same polarities. Our work differs from these two studies in several respects. In the present study evaluation was carried out by exploiting square errors rather than precision errors, with a five-point scoring scale used in the experiments, in contrast to Koppel [CITE], who used three (``good'', ``bad'', ``neutral''), and Pang [CITE], who used three/four point scores. Therefore we use regression which minimize not a precision error but a square error. We argue that the precision errors are not enough to capture the task. Because if we use the precision errors mistakes of assigning [MATH] SP score to a review whose correct SP score is [MATH] can occur many times, which becomes unacceptable problem in real applications. We also examine various features to capture the characteristics of reviews, which are found to be effective in experiences.
In this section we present a novel task setting where we predict the degree of sentiment polarity of a review. We first define SP scores and the task of assigning them to review documents. We then describe the present evaluation data set. Using this data set, we examined the performance of human classifiers on this task, to clarify the difficulty of quantifying polarity.
We extend the sentiment classification task to the more challenging task of assigning rating scores to reviews. We call this score the SP score. Examples of SP scores include five-star scales and scores out of 100. Let SP scores take discrete values in a closed interval [MATH].
The task is to assign correct SP scores to unseen reviews as accurately as possible. Let [MATH] be the predicted SP score and [MATH] be the SP score assigned by the reviewer. We measure the performance of an estimator with the mean square error,
where [MATH] is the test set of reviews. In contrast to conventional multi-class classification, which gives equal penalties to all mistakes, penalties for the present estimator are larger when the mistake in predicted SP score is large.
Ordinal regression [CITE] is another framework to predict variables of ordinal scale. Since our task setting gives a large penalty for large mistake in SP score, the regression approach is more suitable for the task than ordinal regression which considers only mistakes for order of SP score [CITE].
We also note the efficiency of training. In several methods of ordinal regression [CITE] the problem size is a quadratic function of the training data size which is not accptable for trining large data. Our method uses well-studied formulations (SVMs, SVR) and can employ efficient algorithms and softwares.
We used book reviews on amazon.com for evaluation data[MATH].
Each review has stars assigned by the reviewer, with the number of stars ranging from one to five, where one is the worst score, while five is the best. We converted the number of stars into SP scores [MATH].
Although each review may include several paragraphs, we did not exploit paragraph information.
From these data, we made two data sets. The first was a set of reviews for books in the Harry Potter series (Corpus A).
The second was a set of reviews for books of arbitrary kinds (Corpus B).
It was easier to predict SP scores for Corpus A than Corpus B because Corpus A books have a smaller vocabulary and each review was about twice as large. To create a data set with a uniform score distribution (the effect of skewed class distributions is out of the scope of this paper), we selected 330 reviews per SP score for Corpus A and 280 reviews per SP score for Corpus B. Table [REF_hoge] shows the number of words and sentences in the corpora. There is no significant difference in the average number of words/sentences among different SP scores.
We treat the SP scores assigned by the reviewers as correct answers. However, the content of a review and its SP score may not be related. Moreover, SP scores may vary depending on the reviewers. Accordingly, we examined the universality of the SP score.
We asked two computational linguists to independently assign an SP score to each review from Corpus A. These two linguists first learned the relationship between reviews and SP scores using 20 reviews, and were then given 100 reviews with a uniform SP score distribution as test data. Table [REF_humantest_abssq] shows the results given in terms of the mean square error. The Random row shows the performance achieved by random assignment, and the All3 row shows the performance achieved by assigning [MATH] to all the reviews. These results suggest that SP scores would be estimated solely from the contents of reviews with a square error of [MATH].
Table [REF_humantest_a] shows the distribution of the estimated SP scores and correct SP scores. In the table we can observe the difficulty of this task; the precise quantification of SP scores. For example, it can be seen from the table that human B tended to overestimate SP scores for reviews whose correct scores were in the range between 2 and 4, assigning a [MATH] or [MATH].
We should note that if we consider this task as binary classification by treating the reviews whose SP scores are [MATH] and [MATH] as positive examples and those with [MATH] and [MATH] as negative examples (ignoring the reviews whose SP scores are [MATH]), the classification precisions by humans A and B are 95% and 96% respectively.
[t] \hangcaption{Human performance of SP score estimation. Test data: 100 reviews of Corpus A with 1,2,3,4,5 SP scores.}
{|ll|r|} \hline & & Square error
\hline &Human A & 0.77
&Human B & 0.79
\hline &Human average & 0.78
\hline cf. & Random & 3.20
& All3 & 2.00
\hline
This section describes a machine learning approach to predict the SP scores of review documents. Our method consists of the following two steps: extraction of feature vectors from reviews, and estimation of SP scores from these feature vectors. The first step basically uses existing techniques for document classification. In contrast, the prediction of SP scores is different from previous studies because we consider ordered multi-class classification, that is, each SP score has its own class and the classes are ordered. Unlike usual multi-class classification, large mistakes in terms of the order should have large penalties. In this paper, we discuss two methods of estimating SP scores: pSVMs and SVR.
We represent a review as a feature vector. Although this representation ignores the syntactic structure, word positions, and the order of words, it is known to work reasonably well for many tasks such as information retrieval and document classification. We use binary, tf, and tf-idf as feature weighting methods [CITE].
The feature vectors are normalized to have [MATH] norm [MATH].
Support vector regression (SVR) is a method of regression that follows a similar underlying idea to that of SVM [CITE].
SVR predicts the SP score of a review by the following regression:
where [MATH] is the predicted SP score, [MATH] is the feature vector of a review, [MATH] and [MATH] are parameters of SVR. SVR uses an [MATH]-insensitive loss function. This loss function means that all errors inside an [MATH] cube are ignored. This allows SVR to require only a few support vectors, and gives a generalization ability. Given a training set, [MATH], parameters [MATH] and [MATH] are determined by solving the following problem,
The factor [MATH] is a parameter that controls the trade-off between training error minimization and margin maximization. The loss in training data increases as [MATH] becomes smaller, while generalization is lost as [MATH] becomes larger. Moreover, we can apply a kernel-trick to SVR, as in the case for SVMs, by using a kernel function.
This approach captures the order of classes and does not suffer from data sparseness. While we could use conventional linear regression instead of SVR [CITE], in the present study we use SVR because it can exploit the kernel-trick and avoid over-training. Another good characteristic of SVR is that we can identify the features contributing to determining the SP scores by examining the coefficients ([MATH] in ([REF_svr])), while pSVMs do not give such information, because multiple classifiers are involved in determining final results. A difficulty associated with the present approach, however, is that it is difficult to learn non-linear regression by SVR. For example, when given training data is [MATH], SVR cannot perform regression correctly without adjusting the input space (feature values) so that the output plane becomes linear-one. Note that this problem does not occur in classification problems, but in regression problems. We can solve this problem by choosing an appropriate kernel for the task, but this selection is not straightforward.
We apply a multi-class classification approach to estimating SP scores. pSVMs [CITE] consider each SP score as a unique class, ignoring the order among the classes. Given reviews with SP scores [MATH], we construct [MATH] SVM classifiers for all the pairs of possible values of SP scores. The classifier for an SP score pair ([MATH] vs [MATH]) assigns the SP score to a review with [MATH] or [MATH].
The class label of a document is determined by majority voting of the classifiers. Any ties in the voting are resolved by choosing the class that is closest to the neutral SP score (i.e, [MATH]).
This approach ignores the fact that SP scores are ordered, which causes the following two problems: First, it allows large mistakes. Second, when the number of possible values of the SP score is large (e.g, [MATH]), this approach suffers from a data sparseness problem. This is because pSVMs cannot employ examples that have close SP scores (e.g, SP score = [MATH]) for the classification of other SP scores (e.g, the classifier for a SP score pair [MATH]).
Previous studies [CITE] suggested that complex features do not work as expected because data becomes sparse when such features are used, and a bag-of-words approach is sufficient to capture the information in most reviews. Nevertheless, we observed that reviews include many chunks of words such as ``very good'' or ``must buy'' that are useful for estimating the degree of polarity. We confirmed this observation by using n-grams.
Since the words around the review target might be expected to influence the overall SP score more than other words, we use these words as features. We call these features reference. We assume the review target is only the word ``book'', and we use ``inbook'' and ``aroundbook'' features. The ``inbook'' feature are the words appearing in the sentence which includes the word ``book''.
The ``around book'' feature is given by the words lying within two places either side of the word ``book''.
Table [REF_tab:features] summarizes the list of features for the experiments.
A review document includes many sentences that are irrelevant to sentiment polarity of the document, such as explanation of a reviewed object or objective sentences. There exist some methods for detecting subjective sentences by a knowledge-based approach or machine learning [CITE].
Here, we propose a method for estimating the probability that a given sentence is subjective using Naive Bayes classifiers (further details and other Naive Bayes models can be found in [CITE]).
Figure [REF_fig:sNB] shows an overview of our approach, that is, we assign the probability that a given sentence in review is subjective, and then weight each feature using this probability.
Although it is hard to obtain a corpus in which individual sentences are annotated with whether the sentence is objective or subjective, we can obtain documents consisting of subjective or objective sentences only [CITE].
Using these documents, we construct a Naive Bayes classifier to estimate the probability of each sentence's subjectivity.
We assign a class which may be either subjective (sub) or objective (obj) to each sentence ([MATH]).
We use a multinomial Naive Bayes Classier to estimate the probability sentence subjectivity [MATH],
p(sub|s_i) & = \frac{p(sub) p(s_i|sub)}{p(s_i)} \nonumber
& = \frac{p(sub) p(s_i|sub)}{p(sub) p(s_i | sub) + p(obj)p(s_i| obj)} .
We decompose [MATH] and [MATH] into the probability of [MATH] and [MATH],
p(s_i|sub) & = C \prod_{t=1}^{|s_i|} p(w_t|sub),
p(s_i|obj) & = C \prod_{t=1}^{|s_i|} p(w_t|obj),
C & = P(|s_i|)|s_i|! \prod_{t=1}^{|V|} \frac{1}{|{w_j|w_j = w_t, w_j \in s_i}\|!}.
We substitute expressions ([REF_NB1]) and ([REF_NB2]) into equation ([REF_NB0]), and obtain
We estimate [MATH], [MATH], [MATH], [MATH] from training data. The training data consists of subjective sentences [MATH] and objective sentences [MATH].
Let [MATH] be the frequency of the word [MATH] in the subjective corpus, and [MATH] be the frequency of the word [MATH] in the objective corpus, then the above parameters are given by
p(w_t|sub) & = \frac{p(w_t)p(sub|w_t)}{p(sub)}
& = \frac{1 + c_{sub}(w_t)}|V| +\sum_{s=1}^{V}(c_{sub}(w_s)),
p(w_t|obj) & = \frac{1 + c_{obj}(w_t)}|V| +\sum_{s=1}^{V}(c_{obj}(w_s)),
p(sub) & = p(obj) = \frac{1}{2}.
We use Laplacian smoothing for estimating [MATH].
Using [MATH], we weight each sentence.
Using the probability [MATH], we recalculate the value of each feature. The disadvantage of this approach is that the information of the training data becomes small in comparison with the original data, because the estimation of Naive Bayes classification tends to overestimate the probability. For example, even if the true probability is close to [MATH], the result of NB would be [MATH] or [MATH].
Futhermore, the objective sentences may have information that could be useful in determining the SP scores. We therefore introduce a smoothing factor [MATH], which means that we assignes probability from [MATH] to [MATH] (by ignoring the normalization factor).
The feature vectors for a review are calculated as follows: the first ([MATH]) is the original feature vector, the second [MATH] is a feature vector weighted by the probability that the particular sentence is subjective, while the third ([MATH]) is a feature vector which is smoothed by using the pre-defined smoothing factor ([MATH]),
x & := \sum_{i=1}^{m}(f(s_i)),
x_{NB} & := \sum_{i=1}^{m}(p(sub|s_i)f(s_i)),
x_{NBS} & := \sum_{i=1}^{m}((p(sub|s_i) + \alpha)f(s_i)) ,
where [MATH] is the feature vector for the sentence [MATH].
The feature values in [MATH] and [MATH] can be considered to be the expected feature values in a subjective sentence. We could alternatively adopt an approach [CITE] whereby first the objective sentences are eliminated, and then we solve the problem as before. This approach would be faster than our approach since the training data is smaller than the original training data set. However, the results of this approach would be less accurate than our approach using the feature vectors [MATH] and [MATH], since these vectors can be seen as an approximation of feature values. However, clearly we can tradeoff the speed and accuracy of the classifier by the choice of approach, and we plan to investigate this further as part of future work.
We performed two series of experiments. First, we compared pSVMs and SVR and examined the performance of various features and weighting methods. Second, we compared the method using sentence subjectivity detection with the method which does not.
The corpora A and B introduced in Section [REF_eval] were used as the experimental data. We first removed all HTML tags and punctuation marks, and then applied the Porter stemming method [CITE] to the reviews.
We divided the data into ten disjoint subsets, maintaining the uniform class distribution. All the results reported below are the averages of ten-fold cross-validation. In SVMs and SVR, we used SVMlight with the quadratic polynomial kernel [MATH] and set the control parameter [MATH] to [MATH] in all the experiments.
For sentence subjectivity detection, we used Pang's sentence corpus version 1.0 [CITE].
We compared pSVMs and SVR to see differences in the properties of the regression approach compared with those of the classification approach. Both pSVMs and SVR used unigram/tf-idf to represent reviews. Table [REF_hikaku] shows the square error results for SVM, SVR and a simple regression (least square error) method for Corpus A/B. These results indicate that SVR outperformed SVM in terms of the square error and suggests that regression methods avoid large mistakes by taking account of the fact that SP scores are ordered, while pSVMs does not. We also note that the result of a simple regression method is close to the result of SVR with a linear kernel.
Figure [REF_fig:hresult] shows the distribution of estimation results for humans (top left: human A, top right: human B), pSVMs (below left), and SVR (below right).
In all the plots the horizontal axes show the estimated SP scores, the vertical axes show the correct SP scores, while shading indicates the number of reviews. These figures suggest that pSVMs and SVR were able to capture the gradualness of SP scores better than the human classifiers. They also show that pSVMs cannot predict neutral SP scores well, whereas SVR accurately predicts these scores.
[b]
{|l|rr|} \hline & \multicolumn{2}{|c|}{Mean Square error}
\hline Method & Corpus A & Corpus B
\hline pSVMs & 1.32 & 2.13
simple regression & 1.05 & 1.49
SVR (linear kernel) & 1.01 & 1.46
SVR (polynomial kernel [MATH]) & 0.94 & 1.38
\hline
We compared the different features presented in Section [REF_step1] and feature weighting methods. First we compared different weighting methods, using only unigram features for this comparison. We then compared different features, using only tf-idf weighting methods for this comparison.
Table [REF_tab:feature3] summarizes the comparison results of different feature weighting methods. The results show that tf-idf performed well on both test corpora. We should note that simple representation methods, such as binary or tf, give comparable results to tf-idf, which indicates that we can add more complex features without considering the scale of feature values. For example, when we add word-based dependency features, we have some difficulty in adjusting these feature values to those of unigrams. However, we could use these features together in binary weighting methods.
Table [REF_tab:feature4] summarizes the comparison results for different features. For Corpus A, unigram + bigram and unigram + trigram achieved high performance. The performance of unigram + inbook does not achieve as good a performance as expected, contrary to our intuitive belief that the words around the target object are more important than others. However, for Corpus B, the results are less accurate, that is, n-gram features were less able to accurately predict the SP scores. This is because the variety of words/phrases was much larger than in Corpus A, and n-gram features may have suffered from a data sparseness problem. We should note that these feature settings are too simple, and we cannot accept the result of reference or target object (inbook/aroundbook) directly.
Note that the data used in the preliminary experiments described in Section [REF_SP score] are a part of Corpus A, and so we can compare the results obtained from the human classifiers with those for Corpus A in this experiment. The best result by the machine learning approach (0.89) was close to the human results (0.78).
To analyze the influence of n-gram features, we used the linear kernel [MATH] in SVR training. We used tf-idf as feature weighting, and examined each coefficient of regression. Since we used the linear kernel, the coefficient value of SVR showed the polarity of a single feature, that is, this value expressed how much the occurrence of a particular feature affected the SP score. Tables [REF_tab:feature_unib], [REF_tab:feature_uniw], [REF_tab:feature_bib], [REF_tab:feature_biw], [REF_tab:feature_trib] and [REF_tab:feature_triw] show the coefficients resulting from the training of SVR. These results show that phrases such as ``all ag (age)'', ``can't wait'' ``on (one) star'' and ``not interest'' have strong polarity even if the word which constitutes these phrases does not have strong polarity.
We examined the effectiveness of subjectivity detection using the Naive Bayes classifier (NB) proposed in Section [REF_idenSub].
First, we examined the performance of NB itself by using Pang's sentence corpus version 1.0. The result of ten-fold cross-validation was [MATH] accuracy. A review, however, includes both subjective and objective sentences. Moreover, we have to examine whether the information of subjectivity contributes to the polarity detection. We asked two computer linguists to select the sentence which is most influential on the SP score in each review. The test data is the same as the test data used in Section [REF_SP score].
We then assigned subjectivity for each sentence using the NB. Table [REF_tab:sub_human] shows the average subjectivity of all sentences and also of the most influential sentences as selected by the human classifiers. It is almost certain that subjectivity is correlated with the sentence that is most influential on the SP score.
Figure [REF_fig:subsen] shows examples of the results of sentence subjectivity detection by the NB. The results suggest that the NB analyzes the subjectivity of sentences well. For instance, explanations of the plot of the novel (lines 2 in review 1) are assigned [MATH] subjectivity.
[t] \hangcaption{The results of classification with/without Naive Bayes subjectivity detection presented in terms of average square errors. The notation [MATH] means training data is [MATH] and test data is [MATH].
The Square error column lists the average square errors.}
{|l|r|r|r|r|} \hline
& \multicolumn{4}{|c|}{Square error}
\hline Methods & Corpus A & Corpus B & Corpus A [MATH] B & Corpus B [MATH] A
\hline baseline & 0.94 & 1.38 & 1.83 & 1.93
NB & 1.04 & 1.46 & 1.81 & 1.72
NB with C & 0.95 & 1.39 & 1.80 & 1.72
\hline
Second, we evaluated the performance of classification using NB sentence subjectivity detection. Table [REF_tab:sub2] shows the results by baseline(SVR + tfidf + unigram), NB(baseline + NB sentence subjectivity detection (Eq. [REF_NBD])) and NB with C (baseline + NB sentence subjectivity detection with added constant [MATH] (Eq. [REF_NBD-added])).
The results indicate that NB is better than baseline when the training data and test data are different, especially when the training data is corpus B and test data is corpus A. We suspect that when we use reviews taken from various themes as training data, some proper nouns have polarity and these words cause the classifier to be misled to the wrong polarity. In contrast, when we use reviews on a specific theme as training data, proper nouns tend to occur uniformly through all SP scores, and the effects of proper nouns on polarity scores are not overestimated. The decline of accuracy by NB in corpora A and B is probably caused by the inadequate performance of Naive Bayes classifiers or loss of useful information in objective sentences. NB with C performs well in each case, suggesting that NB with C has the advantages of objective sentence elimination without suffering any significant decline due to the loss of information in objective sentences.
We generated learning curves to examine the effect of the size of training data on performance. Figure [REF_fig:lc] shows the results of a classification task using unigram/tf-idf to represent reviews. The results suggest that performance can be improved further by increasing the training data.
In this paper, we described a novel task setting in which we predicted SP scores---degree of polarity---of reviews. We proposed a machine learning method using SVR to predict SP scores.
We compared two methods for estimating SP scores: pSVMs and SVR. Experimental results for book reviews showed that SVR performed better in terms of the square error than pSVMs by about [MATH]%.
This result agrees with our intuition that pSVMs do not consider the order of SP scores, while SVR captures the order of SP scores and avoids high penalty mistakes. With SVR, SP scores can be estimated with a square error of [MATH], which is very close to the square error achieved by human classifiers ([MATH]).
We examined the effectiveness of features beyond a bag-of-words and reference features (the words around the reviewed objects.) The results suggest that n-gram features and reference features contribute to improve accuracy.
The experimental results for sentence subjectivity detection using Naive Bayes classifiers showed that this approach can improve the robustness of a classifier, which may be improved further by adding a constant to the result of Naive Bayes classifiers. This is because the noise from objective sentences is eliminated.
As the next step in our research, we plan to exploit parsing results such as predicate argument structures for detecting precise reference information. As well as attitude, we will also capture other types of polarity, such as modality and writing position [CITE], and we will consider the estimation of these types of polarity.
We plan to develop a classifier specialized for ordered multi-class classification using recent studies on machine learning for structured output spaces [CITE] or ordinal regression [CITE], since our experiments suggest that pSVMs and SVR have both advantages and disadvantages. We will develop a more efficient classifier that outperforms pSVMs and SVR by combining these ideas. We also examine whether or not our task setting is appropriate to summarize the review.
