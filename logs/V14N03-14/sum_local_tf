================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[276] 本論文では，ある対象を評価している文章（レビュー）が与えられた時，対象物に対する評価が「良い」か「悪い」かでレビューを二値分類するのではなく，どの程度「良い」か「悪い」かの指標（sentiment polarity score（SP score））をレビューに与える新しいタスクを提案する．

================================================================
[section type  : proposed_method]
[section title : Introduction]
================================================================
[2169] If, for example, the range of the score is from one to five, we could give five to review A and four to review B. This task, namely, ordered multi-class classification, is considered as an extension of binary sentiment classification. In ordered multi-class classification, the classes are not independent, but are ordered. While it is possible to treat this problem as a multi-class classification task ignoring the order information, the performance of the classifier can be improved by incorporating this information into the classifier. In this paper, we describe a machine learning method for this task. Our system uses support vector regression (SVR) [CITE] to determine the SP scores of reviews. This method enables us to annotate SP scores to arbitrary reviews, such as comments in bulletin board systems or blog systems. We explore several types of features beyond a bag-of-words to capture key phrases to determine SP scores: n-grams and references (the words around the reviewed object).

================================================================
[section type  : proposed_method]
[section title : Related Work]
================================================================
[2433] Turney [CITE] used semantic orientation, which measures the distance from phrases to ``excellent'' or ``poor'' by using search engine results and gives the word polarity. Kudo [CITE] used decision stumps to capture substructures embedded in text (such as word-based dependency), and suggested that subtree features are important for opinion/modality classification. Independently of and in parallel with our work, two other papers consider the degree of polarity for the purposes of sentiment classification. Koppel [CITE] exploited a neutral class and applied a regression method similar to that of the present study. Pang [CITE] applied a metric labeling method for the task in which similar reviews tend to have same polarities. Our work differs from these two studies in several respects. In the present study evaluation was carried out by exploiting square errors rather than precision errors, with a five-point scoring scale used in the experiments, in contrast to Koppel [CITE], who used three (``good'', ``bad'', ``neutral''), and Pang [CITE], who used three/four point scores. Therefore we use regression which minimize not a precision error but a square error. We argue that the precision errors are not enough to capture the task. Because if we use the precision errors mistakes of assigning [MATH] SP score to a review whose correct SP score is [MATH] can occur many times, which becomes unacceptable problem in real applications. We also examine various features to capture the characteristics of reviews, which are found to be effective in experiences.

================================================================
[section type  : proposed_method]
[section title : Analyzing Reviews with Polarity Scores]
================================================================
[1306] In this section we present a novel task setting where we predict the degree of sentiment polarity of a review. We first define SP scores and the task of assigning them to review documents. We then describe the present evaluation data set. Using this data set, we examined the performance of human classifiers on this task, to clarify the difficulty of quantifying polarity.
-----------------------------------------------------
  [subsection title : Sentiment Polarity Scores]
-----------------------------------------------------
  [1482] where [MATH] is the test set of reviews. In contrast to conventional multi-class classification, which gives equal penalties to all mistakes, penalties for the present estimator are larger when the mistake in predicted SP score is large. Ordinal regression [CITE] is another framework to predict variables of ordinal scale. Since our task setting gives a large penalty for large mistake in SP score, the regression approach is more suitable for the task than ordinal regression which considers only mistakes for order of SP score [CITE].
-----------------------------------------------------
  [subsection title : Evaluation Data]
-----------------------------------------------------
  [1781] It was easier to predict SP scores for Corpus A than Corpus B because Corpus A books have a smaller vocabulary and each review was about twice as large. To create a data set with a uniform score distribution (the effect of skewed class distributions is out of the scope of this paper), we selected 330 reviews per SP score for Corpus A and 280 reviews per SP score for Corpus B. Table [REF_hoge] shows the number of words and sentences in the corpora. There is no significant difference in the average number of words/sentences among different SP scores.
-----------------------------------------------------
  [subsection title : Preliminary Experiments: Human Performance for Assigning SP scores]
-----------------------------------------------------
  [2052] We treat the SP scores assigned by the reviewers as correct answers. However, the content of a review and its SP score may not be related. Moreover, SP scores may vary depending on the reviewers. Accordingly, we examined the universality of the SP score. We asked two computational linguists to independently assign an SP score to each review from Corpus A. These two linguists first learned the relationship between reviews and SP scores using 20 reviews, and were then given 100 reviews with a uniform SP score distribution as test data. Table [REF_humantest_abssq] shows the results given in terms of the mean square error. The Random row shows the performance achieved by random assignment, and the All3 row shows the performance achieved by assigning [MATH] to all the reviews. These results suggest that SP scores would be estimated solely from the contents of reviews with a square error of [MATH].

================================================================
[section type  : proposed_method]
[section title : Assigning SP scores to Reviews]
================================================================
[1903] This section describes a machine learning approach to predict the SP scores of review documents. Our method consists of the following two steps: extraction of feature vectors from reviews, and estimation of SP scores from these feature vectors. The first step basically uses existing techniques for document classification. In contrast, the prediction of SP scores is different from previous studies because we consider ordered multi-class classification, that is, each SP score has its own class and the classes are ordered. Unlike usual multi-class classification, large mistakes in terms of the order should have large penalties. In this paper, we discuss two methods of estimating SP scores: pSVMs and SVR.
-----------------------------------------------------
  [subsection title : Review Representation]
-----------------------------------------------------
  [1321] We represent a review as a feature vector. Although this representation ignores the syntactic structure, word positions, and the order of words, it is known to work reasonably well for many tasks such as information retrieval and document classification. We use binary, tf, and tf-idf as feature weighting methods [CITE].
-----------------------------------------------------
  [subsection title : Support Vector Regression]
-----------------------------------------------------
  [2478] The factor [MATH] is a parameter that controls the trade-off between training error minimization and margin maximization. The loss in training data increases as [MATH] becomes smaller, while generalization is lost as [MATH] becomes larger. Moreover, we can apply a kernel-trick to SVR, as in the case for SVMs, by using a kernel function. This approach captures the order of classes and does not suffer from data sparseness. While we could use conventional linear regression instead of SVR [CITE], in the present study we use SVR because it can exploit the kernel-trick and avoid over-training. Another good characteristic of SVR is that we can identify the features contributing to determining the SP scores by examining the coefficients ([MATH] in ([REF_svr])), while pSVMs do not give such information, because multiple classifiers are involved in determining final results. A difficulty associated with the present approach, however, is that it is difficult to learn non-linear regression by SVR. For example, when given training data is [MATH], SVR cannot perform regression correctly without adjusting the input space (feature values) so that the output plane becomes linear-one. Note that this problem does not occur in classification problems, but in regression problems. We can solve this problem by choosing an appropriate kernel for the task, but this selection is not straightforward.
-----------------------------------------------------
  [subsection title : Pairwise Support Vector Machines]
-----------------------------------------------------
  [1514] We apply a multi-class classification approach to estimating SP scores. pSVMs [CITE] consider each SP score as a unique class, ignoring the order among the classes. Given reviews with SP scores [MATH], we construct [MATH] SVM classifiers for all the pairs of possible values of SP scores. The classifier for an SP score pair ([MATH] vs [MATH]) assigns the SP score to a review with [MATH] or [MATH].
-----------------------------------------------------
  [subsection title : Features beyond Bag-of-Words]
-----------------------------------------------------
  [2107] Previous studies [CITE] suggested that complex features do not work as expected because data becomes sparse when such features are used, and a bag-of-words approach is sufficient to capture the information in most reviews. Nevertheless, we observed that reviews include many chunks of words such as ``very good'' or ``must buy'' that are useful for estimating the degree of polarity. We confirmed this observation by using n-grams. Since the words around the review target might be expected to influence the overall SP score more than other words, we use these words as features. We call these features reference. We assume the review target is only the word ``book'', and we use ``inbook'' and ``aroundbook'' features. The ``inbook'' feature are the words appearing in the sentence which includes the word ``book''.
-----------------------------------------------------
  [subsection title : Identification of Subjectivity Sentences]
-----------------------------------------------------
  [1806] The feature values in [MATH] and [MATH] can be considered to be the expected feature values in a subjective sentence. We could alternatively adopt an approach [CITE] whereby first the objective sentences are eliminated, and then we solve the problem as before. This approach would be faster than our approach since the training data is smaller than the original training data set. However, the results of this approach would be less accurate than our approach using the feature vectors [MATH] and [MATH], since these vectors can be seen as an approximation of feature values. However, clearly we can tradeoff the speed and accuracy of the classifier by the choice of approach, and we plan to investigate this further as part of future work.

================================================================
[section type  : proposed_method]
[section title : Experiments]
================================================================
[1881] We performed two series of experiments. First, we compared pSVMs and SVR and examined the performance of various features and weighting methods. Second, we compared the method using sentence subjectivity detection with the method which does not. The corpora A and B introduced in Section [REF_eval] were used as the experimental data. We first removed all HTML tags and punctuation marks, and then applied the Porter stemming method [CITE] to the reviews. We divided the data into ten disjoint subsets, maintaining the uniform class distribution. All the results reported below are the averages of ten-fold cross-validation. In SVMs and SVR, we used SVMlight with the quadratic polynomial kernel [MATH] and set the control parameter [MATH] to [MATH] in all the experiments. For sentence subjectivity detection, we used Pang's sentence corpus version 1.0 [CITE].
-----------------------------------------------------
  [subsection title : Comparison of pSVMs and SVR]
-----------------------------------------------------
  [1962] We compared pSVMs and SVR to see differences in the properties of the regression approach compared with those of the classification approach. Both pSVMs and SVR used unigram/tf-idf to represent reviews. Table [REF_hikaku] shows the square error results for SVM, SVR and a simple regression (least square error) method for Corpus A/B. These results indicate that SVR outperformed SVM in terms of the square error and suggests that regression methods avoid large mistakes by taking account of the fact that SP scores are ordered, while pSVMs does not. We also note that the result of a simple regression method is close to the result of SVR with a linear kernel. Figure [REF_fig:hresult] shows the distribution of estimation results for humans (top left: human A, top right: human B), pSVMs (below left), and SVR (below right).
-----------------------------------------------------
  [subsection title : Comparison of Different Features]
-----------------------------------------------------
  [2342] Table [REF_tab:feature4] summarizes the comparison results for different features. For Corpus A, unigram + bigram and unigram + trigram achieved high performance. The performance of unigram + inbook does not achieve as good a performance as expected, contrary to our intuitive belief that the words around the target object are more important than others. However, for Corpus B, the results are less accurate, that is, n-gram features were less able to accurately predict the SP scores. This is because the variety of words/phrases was much larger than in Corpus A, and n-gram features may have suffered from a data sparseness problem. We should note that these feature settings are too simple, and we cannot accept the result of reference or target object (inbook/aroundbook) directly. Note that the data used in the preliminary experiments described in Section [REF_SP score] are a part of Corpus A, and so we can compare the results obtained from the human classifiers with those for Corpus A in this experiment. The best result by the machine learning approach (0.89) was close to the human results (0.78).
-----------------------------------------------------
  [subsection title : Using Naive Bayes Classifier to Subjectivity Detection]
-----------------------------------------------------
  [2090] The results indicate that NB is better than baseline when the training data and test data are different, especially when the training data is corpus B and test data is corpus A. We suspect that when we use reviews taken from various themes as training data, some proper nouns have polarity and these words cause the classifier to be misled to the wrong polarity. In contrast, when we use reviews on a specific theme as training data, proper nouns tend to occur uniformly through all SP scores, and the effects of proper nouns on polarity scores are not overestimated. The decline of accuracy by NB in corpora A and B is probably caused by the inadequate performance of Naive Bayes classifiers or loss of useful information in objective sentences. NB with C performs well in each case, suggesting that NB with C has the advantages of objective sentence elimination without suffering any significant decline due to the loss of information in objective sentences.
-----------------------------------------------------
  [subsection title : Learning Curve]
-----------------------------------------------------
  [1304] We generated learning curves to examine the effect of the size of training data on performance. Figure [REF_fig:lc] shows the results of a classification task using unigram/tf-idf to represent reviews. The results suggest that performance can be improved further by increasing the training data.

================================================================
[section type  : proposed_method]
[section title : Conclusion]
================================================================
[2278] We examined the effectiveness of features beyond a bag-of-words and reference features (the words around the reviewed objects.) The results suggest that n-gram features and reference features contribute to improve accuracy. The experimental results for sentence subjectivity detection using Naive Bayes classifiers showed that this approach can improve the robustness of a classifier, which may be improved further by adding a constant to the result of Naive Bayes classifiers. This is because the noise from objective sentences is eliminated. As the next step in our research, we plan to exploit parsing results such as predicate argument structures for detecting precise reference information. As well as attitude, we will also capture other types of polarity, such as modality and writing position [CITE], and we will consider the estimation of these types of polarity. We plan to develop a classifier specialized for ordered multi-class classification using recent studies on machine learning for structured output spaces [CITE] or ordinal regression [CITE], since our experiments suggest that pSVMs and SVR have both advantages and disadvantages. We will develop a more efficient classifier that outperforms pSVMs and SVR by combining these ideas. We also examine whether or not our task setting is appropriate to summarize the review.

