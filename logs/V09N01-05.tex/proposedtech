




\documentstyle[jnlpbbl,epsf]{jnlp_j}


\setcounter{page}{101}
\setcounter{巻数}{9}
\setcounter{号数}{1}
\setcounter{年}{2002} 
\setcounter{月}{1}
\受付{2001}{7}{16}
\再受付{2001}{9}{19}
\採録{2001}{9}{28}

\setcounter{secnumdepth}{2}

\title{最大エントロピー法を用いた対訳単語対の抽出}
\author{佐藤 健吾\affiref{KEIOCS} \and 斎藤 博昭\affiref{KEIOICS}}

\headauthor{佐藤,斎藤}

\headtitle{最大エントロピー法を用いた対訳単語対の抽出}

\affilabel{KEIOCS}{慶應義塾大学大学院理工学研究科計算機科学専攻}{Department of Computer Science,Keio University}

\affilabel{KEIOICS}{慶應義塾大学理工学部情報工学科}{Department of Information and Computer Science,Keio University}

\jabstract{
機械翻訳などの多言語間自然言語処理で用いられる対訳辞書は現在，人手によっ
て作成されることが多い．しかし，人手による作成には一貫性・網羅性などの
点で限界があることから対訳コーパスから自動的に対訳辞書を作成しようとす
る研究が近年盛んに行われている．
本論文では，最大エントロピー法を用いて対訳コーパス上に対訳関係の確率モ
デルを推定し，自動的に対訳単語対を抽出する手法を提案する．
素性関数として共起情報を用いるモデルと品詞情報を用いるモデルを定義した．
共起情報により対訳関係にある単語の意味を制約し，品詞情報により対訳関係
にある単語の品詞を制約する．
本手法の有効性を示すために日英対訳コーパスを用いた対訳単語対の抽出実験
を行い，本論文で提案した手法が従来の手法よりも精度・再現率において優れ
た結果となり，また，テストコーパスによる実験では学習コーパスに出現しな
かった単語対に関しても学習データに現れたものとほぼ同等の精度・再現率で
抽出できることを示した．}

\jkeywords{対訳辞書，機械翻訳，最大エントロピー法}

\etitle{Extracting Bilingual Word Pairs \\ with Maximum Entropy Modeling}
\eauthor{Kengo SATO\affiref{KEIOCS} \and Hiroaki SAITO\affiref{KEIOICS}}

\eabstract{
  Translation dictionaries used in multilingual natural
  language processing such as machine translation have been made
  manually, but a great deal of labor is required for this work and it
  is difficult to keep the description of the dictionaries
  consistent. Therefore, researches of extracting bilingual word pairs
  from parallel corpora automatically become active recently.
  In this paper, we propose a learning and extracting method of
  bilingual word pairs from aligned parallel corpora with the maximum
  entropy modeling.
  We define a probabilistic model of bilingual word pairs and four
  types of feature functions which express statistical and linguistic
  properties such as co-occurrence information and morphlogical
  information. Co-occurrence information restricts the sense of
  words. Morphlogical information restricts the part-of-speech of
  words.
  Experiment results in which Japanese and English parallel corpora
  are used show that our method performs better than the previous
  methods and can extract the bilingual word pairs which do not appear
  in the training corpus with almost the same accuracy as the appeared
  pairs due to the property of the maximum entropy modeling.}


\ekeywords{
  translation dictionary,
  machine translation,
  maximum entropy modeling
}


\begin{document}

\thispagestyle{plain}
\maketitle

\section{最大エントロピー法}
\label{sec:ME_method}

一般に確率モデルは，履歴とその時の出力の関係を既知のデータから推定され
る確率分布によって表す．この際，履歴の種類を多くすればより正確に出力を
予測することができるが，履歴の種類を多くしすぎるとそれぞれの履歴におけ
る既知データの数が少なくなり，データスパースネスに陥ってしまう．

最大エントロピー法では履歴と出力の関係を素性関数で表し，それぞれの素性
関数に関して既知データにおける確率分布の期待値と推定すべき確率分布の期
待値が等しくなるという制約のもとで，確率分布のエントロピーが最大となる
ようなモデルを推定する．この操作は，既知データにおいて出現しなかったも
の，あるいは稀であったものに対しても一様分布に近づいていくということを
意味しており，このため最大エントロピー法はデータスパースネスに対して比
較的強いとされている．この性質は，言語モデルのように既知データにおいて
全ての事象を扱うことが難しい現象を扱うのに適したものであると言える．

最大エントロピー法では，以下のように確率分布を推定する．
$X$ を履歴の集合，$Y$ を出力の集合とする時，既知データの集合
$\{(x_i,y_i)\ |\  x_i \in X, y_i \in Y\}$ から確率分布 $P(y|x)$ を推定
することを考える．
まず，求めたい確率モデルの統計的特性 (素性) によって集合 $X \times Y$
を二つの集合に分割する 2 値関数 $f: X \times Y \rightarrow \{0,1\}$ を
定義する．このような関数は素性関数と呼ばれる．

この時，既知データにおける確率分布 $\tilde{P}(x,y)$ に関する $f$ の期待
値と推定すべき確率分布 $P(y|x)$ に関する $f$ の期待値が等しくなるとい
う制約を与える．
\begin{equation}
  \label{eq:constraint}
  \sum_{x,y} \tilde{P}(x,y) f(x,y) = \sum_{x,y} P(x) P(y|x) f(x,y)
\end{equation}
計算量の観点から右辺の $P(x)$ の代わりに $\tilde{P}(x)$ で近似すること
が多い．
ここで，$\tilde{P}(x,y)$, $\tilde{P}(x)$ は既知データにおける $(x,y)$,
$x$ の出現頻度 $c(x,y)$, $c(x)$ から得られる相対頻度を用いる．
\[ \tilde{P}(x,y) = \frac{c(x,y)}{\sum_{v,w} c(v,w)} \]
\[ \tilde{P}(x) = \frac{c(x)}{\sum_v c(v)} \]

モデル化の過程において重要であると思われる $n$ 個の素性関数 $f_i$ があ
る時，すべての $f_i$ について式(\ref{eq:constraint})を満たすような確率
分布は一般的には複数存在するため，これらの中から最も一様な確率分布を選
択するのが自然である．条件付き確率分布 $P(y|x)$ の一様性の数学的な尺度
としては条件付きエントロピーがよく用いられ，これを最大とする確率分布が
求めるべき確率分布となる．
\begin{equation}
  \label{eq:cond_entropy}
  H(P) = - \sum_{x,y} \tilde{P}(x) P(y|x) \log P(y|x)
\end{equation}

このような確率分布は唯一存在し，以下のような $\lambda_i$ をパラメータ
とする形式で表すことができる．
\begin{eqnarray}
  \label{eq:gibbs}
  P_\lambda(y|x) = \frac{1}{Z_\lambda(x)} \exp \left( \sum_i \lambda_i
  f_i(x,y) \right)\\
  Z_\lambda(x) = \sum_y \exp \left( \sum_i \lambda_i f_i(x,y) \right)
  \nonumber
\end{eqnarray}
パラメータ $\lambda_i$ の推定には Improved Iterative Scaling アルゴリ
ズム\cite{berger_96}などが用いられる．

\section{最大エントロピー法による対訳単語対の抽出}
\label{sec:MEdict}

本節では，最大エントロピー法を対訳単語対の抽出に適用する手法を述べる．
まず，確率分布の事象の定義を行い，次に対訳単語対の確率分布を推定する際
に用いる素性関数を定義する．最後に，得られた確率分布を用いて対訳単語対
を抽出する手法を述べる．

\subsection{事象の定義}
\label{sec:problem_setting}

原言語のコーパス $X$, 目的言語のコーパス $Y$ が対訳となっており，それ
らの間で単語間の対訳関係
が観測されたと
する．この時，観測値から得られる同時出現確率は以下の式で表される．
\begin{equation}
  \label{eq:em_joint}
  \tilde{P}(x,y) = \frac{c(x,y)}{\sum_{v \in X, w \in Y} c(v,w)}
\end{equation}
ここで $c(x,y)$ は単語 $x$ と $y$ が対訳関係で出現した回数である．

しかし実際には対訳コーパスから単語間の対訳関係を計数することは膨大な労
力が必要であるため，文対応があらかじめ
付いている対訳コーパスを用いた場合は対訳文の単語数に応じて出現回数を均等
に割り振り，式 (\ref{eq:article_joint}) のように出現回数を近似する．
\begin{equation}
  \label{eq:article_joint}
  c(x,y) = \sum_i \frac{c'_i(x,y)}{|X_i| |Y_i|}
\end{equation}
ここで，$X_i$ は原言語のコーパス $X$ の $i$ 番目の文，$Y_i$ は目的言語
のコーパス $Y$ の $i$ 番目の文を表す．すなわち $X_i$ と $Y_i$ は対訳関
係にあるものとする．また，$|X_i|$,$|Y_i|$ はそれぞれ $X_i$,$Y_i$ の文
中に含まれる単語数を表し，$c'_i(x,y)$ は $i$ 番目の文において $x$ と
$y$ が出現した回数である．

このようにして観測値から得られた $\tilde{P}(x,y)$ から，原言語の中に 
$x$ が出現した時に目的言語において $x$ が $y$ に翻訳される確率
$P(y|x)$ を推定する．

\subsection{素性関数の定義}
\label{sec:def_feature}

どのような素性関数を定義するかという問題は最大エントロピー法によるモデ
ル化において最も重要である．本論文では以下の 4 種類のモデルの素性関数を定
義した．

\subsubsection{対訳文中に現れる単語対の情報を用いた素性関数 (素性タイ
  プ 1)}
\label{sec:model1}

対訳コーパスにおいて対応する文で出現したことのある単語対 $x,y$ は対訳
関係にある可能性がある．これを確率モデルに反映させるために以下のような
素性関数を定義する．
\begin{equation}
  \label{eq:model1}	
  f_x(x,y) = \left\{
    \begin{array}{ll}
      1 & \left( \parbox{4.5cm}{
          \begin{flushleft}
            $x \in X_i$, $ y \in Y_i$\\
            を満たすような $i$ が存在する
          \end{flushleft}}
        \right)\\
      0 & {\rm (それ以外)}
    \end{array} \right.
\end{equation}

\subsubsection{原言語における共起情報を用いた素性関数 (素性タイプ 2)}
\label{sec:model2}

一般に，単語はそれと共起する単語によってある程度意味を限定することができ
る．このことを利用し，原言語のコーパス $X$ における単語の共起情報を用
いて素性関数を定義する．
\begin{equation}
  \label{eq:model2}
  f_w(x,y) = \left\{
    \begin{array}{ll}
      1 & \left( \parbox{5cm}{
          \begin{flushleft}
            $x,w \in X_i$, $y \in Y_i$, $x \in W(d,w)$\\
            を満たすような $i$ が存在する
          \end{flushleft}}
        \right)\\
      0 & {\rm (それ以外)}
    \end{array} \right.
\end{equation}
ただし $W(d,w)$ はコーパス中で $w \in X$ から $d$ 語以内に出現する単語
の集合である．今回の実験では $d=5$ とした．$f_w(x,y)$ は $x$ が $y$ に翻
訳されることに対して $x$ と共起関係にある $w$ が予測力を持っているかどう
かということを表す (図 \ref{fig:cooccurance})．
\begin{figure}[htbp]
  \begin{center}
\atari(88,37)
    \caption{原言語における共起情報を用いた素性関数}
    \label{fig:cooccurance}
  \end{center}
\end{figure}

\subsubsection{原言語と目的言語における共起情報を用いた素性関数 (素性
  タイプ 3)}
\label{sec:model3}

\ref{sec:model2}節で述べた素性関数に目的言語のコーパス $Y$ における共
起情報を付け加えたものを定義する．
\begin{equation}
  \label{eq:model3}
  f_{w,v}(x,y) = \left\{
    \begin{array}{ll}
      1 & \left( \parbox{7.2cm}{
          \begin{flushleft}
            $x,w \in X_i$, $y,v \in Y_i$, $x \in W(d,w)$, $y \in W(d,v)$\\
            を満たすような $i$ が存在する
          \end{flushleft}}
        \right)\\
      0 & {\rm (それ以外)}
    \end{array} \right.
\end{equation}
$f_{w,v}(x,y)$ は $x$ が $y$ に翻訳されることに対して $x$ と共起関係に
ある $w$ と $y$ と共起関係にある$v$ が予測力を持っているかどうかという
ことを表す (図 \ref{fig:cooccurance2})．
\begin{figure}[htbp]
\begin{center}
\atari(89,37)
  \caption{原言語と目的言語における共起情報を用いた素性関数}
  \label{fig:cooccurance2}
\end{center}
\end{figure}

\subsubsection{品詞情報を用いた素性関数 (素性タイプ 4)}
\label{sec:model4}

対訳文において対訳関係にある単語同士は同じような形態素的意味を持つ品詞で
あることが望ましい．しかし，それぞれの言語における形態素解析器の品詞タグ
セットが全く同じであることは稀である．そこで本論文では各言語の形態素解析
器が出力する品詞タグ情報をそのまま使用し，その組み合わせで素性関数を定
義する．
\begin{equation}
  \label{eq:model4}
  f_{t,s}(x,y) = \left\{
    \begin{array}{ll}
      1 & \left( \parbox{6.5cm}{
          \begin{flushleft}
            $x \in X_i, y \in Y_i$, $POS(x) = t$, $POS(y) = s$\\
            を満たすような $i$ が存在する
          \end{flushleft}} \right)\\
      0 & {\rm (それ以外)}
    \end{array} \right.
\end{equation}
ここで $POS(x)$ は言語 $X$ における単語 $x$ の品詞タグ，$POS(y)$ は言語
$Y$ における単語 $y$ の品詞タグである．$f_{t,s}(x,y)$ は $x$ が $y$
に翻訳されることに対して$x$ に割り当てられた品詞 $t$ と $y$ に割り当てら
れた品詞 $s$ が予測力を持っているかどうかということを表す
(図 \ref{fig:morphological})．
\begin{figure}[htbp]
  \begin{center}
\atari(88,42)
    \caption{品詞情報を用いた素性関数}
    \label{fig:morphological}
  \end{center}
\end{figure}

\subsection{対訳単語対の抽出アルゴリズム}
\label{sec:extracting}

本節では，前節までに述べた手法によって得られた確率モデルを用いて対訳単
語対を抽出する手法を述べる．本手法では 1 単語対 1 単語の対訳関係を仮定
し，Competitive Linking Algorithm \cite{melamed_97} と類似した抽出アル
ゴリズム\footnote{Competitive Linking Algorithm とは対訳単語対の対応度
  の計算方法が異なる点を除き，本質的には同じアルゴリズムである．}
を採用する．
  \begin{enumerate}
  \item[1.]
    閾値 $th \in [0,1]$ を決める．
  \item[2.]
    すべての $(x,y) \in X \times Y$ について $P(y|x)$ を計算し，
    $P(y|x) \geq th$ となる $(x,y)$ をリストに保持する．
  \item[3.]
    リストを $P(y|x)$ について降順にソートする．
  \item[4.]\label{enum:rep}
    $P(y|x)$ が最大となる (すなわちリストの先頭にある) $(x',y')$ を対
    訳単語対として抽出する．
  \item[5.]
    本手法では 1 単語対 1 単語の対訳関係を仮定しているので，$x'$ や
    $y'$ を含む単語対は二度と抽出されない．したがって $\left\{(x',v) |
      v \in Y \right\}$ や $\left\{(w,y') |  w \in X \right\}$ に含ま
    れるような単語対をリストから削除する．
  \item[6.]
    抽出すべき単語対がまだ存在すれば 4. へ戻る．
  \end{enumerate}


\end{document}

