\section{実験と考察}
\label{sec:experiment_discussion}
本節では本論文で述べた手法による実験結果を示し，それに対する考察を述べ
る．最後に関連研究との比較を行う．
\subsection{実験結果}
\label{sec:result}

本論文でここまで述べた手法を用いて英語 $\cdot$ 日本語間の対訳単語対を抽
出する実験を行った．今回の実験では対訳コーパスとして通産省電子技術総合
研究所\footnote{現在は独立行政法人産業技術総合研究所}において電子化さ
れた講談社和英辞典に含まれる例文のうち 30,287 文を用い，原言語のコーパ
ス $X$ として英語文，目的言語のコーパス $Y$ として日本語文を使用した．
そのうち，学習コーパスとして 27,258 文，テストコーパスとして 3,029 文
を用いた．日英対訳例文に対して
日本語文は茶筌\footnote{{\tt http://chasen.aist-nara.ac.jp/}}
\cite{chasen20}，
英語文は Brill
Tagger\footnote{{\tt http://www.cs.jhu.edu/$\sim$brill/code.html}}
\cite{brill_94}
を用いて形態素解析を行った．
助詞，助動詞などの機能語，出現回数が極めて多い単語 (出現回数 1,000 回
以上)，出現回数が極めて少ない単語 (出現回数 3 回以下) を推定から除外し
た．その結果今回の実験の対象となる単語の語彙数は表\ref{tab:corpus}の通
りとなった．
\begin{table}[htbp]
  \centering
  \caption{コーパスの語彙数}
  \label{tab:corpus}
  \begin{tabular}{l|r|r|r} \hline
    & \multicolumn{1}{c|}{文数} &
    \multicolumn{1}{c|}{英語語彙数} &
    \multicolumn{1}{c}{日本語語彙数} \\ \hline
    学習コーパス & 27,258 & 4,664 & 6,796 \\
    テストコーパス & 3,029 & 3,540 & 5,753 \\ \hline
  \end{tabular}
\end{table}

学習コーパス中で観測された素性の総数はおよそ 4,350 万個であった．その
うち確率モデルの推定には学習コーパスで 5 回以上観測された素性 12,368
個を用いた．表\ref{tab:feature} にその内訳を示す．
\begin{table}[htbp]
  \centering
  \caption{学習に使用した素性}
  \label{tab:feature}
  \begin{tabular}{c|l|r} \hline
    \multicolumn{1}{c|}{素性タイプ} &
    \multicolumn{1}{c|}{素性の種類} &
    \multicolumn{1}{c}{個数} \\ \hline

    1 &
    対訳文中に現れる単語対 (\ref{sec:model1}節) &
    4,086 \\

    2 &
    原言語における共起情報 (\ref{sec:model2}節) &
    3,112 \\

    3 &
    両言語における共起情報 (\ref{sec:model3}節) &
    5,011 \\

    4 &
    品詞情報 (\ref{sec:model4}節) &
    159 \\ \hline
  \end{tabular}
\end{table}
これらの素性を用いて最大エントロピー法により対訳単語対の確率分布の推定を
行った．その際のパラメータ推定には Improved Iterative Scaling を採用し，
その反復回数は 400 回に設定した．

表\ref{tab:ME_result}に本手法による対訳単語対抽出の実験結果を示す．
対訳単語対の抽出の際に用いる閾値 $th$ は $0.5$ からはじめ，$0.1$ 刻み
に $0.1$ まで下げた．閾値の段階別に対訳単語対として抽出された総抽出数，
そのうちの正解数，精度 ($=\rm{正解数} /\rm{総抽出数}$)，再現率
($=\rm{正解数} / \rm{日本語語彙数}$) を記す．
対訳単語対が正解であるかどうかは既存の辞書を用いて人手により判定した．
表の左側は学習コーパスにおける抽出結果である．
右側は学習コーパスで学習したパラメータを使用してテストコーパスに対して
抽出アルゴリズムを適用したときの結果である．
\begin{table}[htbp]
\begin{center}
  \caption{最大エントロピー法による抽出結果}
  \label{tab:ME_result}
  \begin{tabular}{r|rr|rr||rr|rr} \hline
    &
    \multicolumn{4}{c||}{学習コーパス} &
    \multicolumn{4}{c}{テストコーパス} \\ \hline
    \multicolumn{1}{c|}{閾値} &
    \multicolumn{1}{c}{正解数} &
    \multicolumn{1}{c|}{総抽出数} &
    \multicolumn{1}{c}{精度(\%)} &
    \multicolumn{1}{c||}{再現率(\%)} &
    \multicolumn{1}{c}{正解数} &
    \multicolumn{1}{c|}{総抽出数} &
    \multicolumn{1}{c}{精度(\%)} &
    \multicolumn{1}{c}{再現率(\%)} \\ \hline

    0.5 &
    4 & 5 & 80.00 & 0.06 &
    33 & 51 & 64.71 & 0.57 \\

    0.4 &
    24 & 29 & 82.76 & 0.35 &
    55 & 90 & 61.11 & 0.96 \\

    0.3 &
    154 & 181 & 85.08 & 2.27 &
    252 & 387 & 65.12 & 4.38 \\

    0.2 &
    486 & 604 & 80.46 & 7.15 &
    759 & 1,224 & 62.01 & 13.19 \\

    0.1 &
    1,481 & 2,011 & 73.64 & 21.79 &
    1,397 & 2,329 & 59.98 & 24.28 \\ \hline
  \end{tabular}
\end{center}
\end{table}
表\ref{tab:correct}に本手法による対訳単語対抽出の実験によって抽出され
た対訳単語対の正解例を示す．
\begin{table}[htbp]
  \centering
  \caption{本手法で抽出した対訳単語対の正解例}
  \label{tab:correct}
  \begin{tabular}{l|l|l||l|l|l} \hline
    \multicolumn{1}{c|}{英語 $x$} &
    \multicolumn{1}{c|}{日本語 $y$} &
    \multicolumn{1}{c||}{$P(y|x)$} &
    \multicolumn{1}{c|}{英語 $x$} &
    \multicolumn{1}{c|}{日本語 $y$} &
    \multicolumn{1}{c}{$P(y|x)$} \\ \hline

    sturdy & たくましい & 0.6194 &
    hat & 帽子 & 0.4400 \\

    snore & ねむれる & 0.5128 &
    chair & いす & 0.4304 \\

    trouser & ズボン & 0.4907 &
    ambassador & 大使 & 0.4236 \\

    battery & バッテリー & 0.4808 &
    library & 図書館 & 0.4211 \\

    negotiation & 交渉 & 0.4634 &
    explanation & 説明 & 0.4209 \\

    taxi & タクシー & 0.4599 &
    film & 映画 & 0.4197 \\

    fish & 魚 & 0.4515 &
    pneumonia & 肺炎 & 0.4195 \\

    rally & たちなおる & 0.4195 &
    baseball & 野球 & 0.4068 \\

    dog & 犬 & 0.4134 &
    music & 音楽 & 0.4035 \\

    criminal & 犯人 & 0.4100 &
    fuel & 燃料 & 0.4028 \\

    pistol & ピストル & 0.4087 &
    toy & おもちゃ & 0.3975 \\ \hline
  \end{tabular}
\end{table}
\vspace{1em}
\subsection{未知語と解析精度}
\label{sec:unknown}

本論文で提案した手法によるテストコーパスにおける抽出実験において，テスト
コーパスにのみ現れた未知の日本語単語と学習コーパスにも現れた日本語単語
に分けて精度と再現率を計算した結果を表 \ref{tab:test_corpus} に示す．
抽出アルゴリズムにおける閾値 $th$ は $0.1$ を用いた．未知語の場合でも，
既知の単語とほぼ同等の精度と再現率が得られることが分かる．最大エントロピー
法では履歴と出力の関係を素性で表すため，学習データにおいて現れなかった
事象でも素性さえ観測できれば確率値を計算することができる．したがって，
学習コーパスに特化しない素性を用いてパラメータ推定を行い，その結果を使っ
て対訳単語対の抽出を行えばテストコーパスにおいてもほぼ同等の抽出結果を
得ることができるということをこの結果は示している．
\begin{table}[tbp]
  \centering
  \caption{テストコーパスにおける抽出結果の内訳}
  \label{tab:test_corpus}
  \begin{tabular}{l|r|rr|rr} \hline
    &
    \multicolumn{1}{c|}{\small{語彙数}} &
    \multicolumn{1}{c}{\small{正解数}} &
    \multicolumn{1}{c|}{\small{総抽出数}} &
    \multicolumn{1}{c}{\small{精度(\%)}} &
    \multicolumn{1}{c}{\small{再現率(\%)}} \\ \hline
    
    テストコーパスのみ現れた単語 &
    2,536 &
    595 & 1,010 &
    58.91 & 23.46 \\
    
    学習コーパスにも現れた単語 &
    3,217 &
    802 & 1,319 &
    60.80 & 24.93 \\ \hline

    合計 &
    5,737 &
    1,397 & 2,329 &
    59.98 & 24.28 \\ \hline
  \end{tabular}
\end{table}

\subsection{抽出誤りについて}
\label{sec:correspondance_j_e}

学習コーパスでの対訳単語対の抽出結果において誤りとなったものから無作為に
100 個を選び，それぞれについて誤りとなった原因を調べた．
抽出アルゴリズムにおける閾値 $th$ は $0.1$ を用いた．
以下は誤った単語対の英単語側に着目した時の誤った原因の内訳である．

\begin{enumerate}
\item[1.] 訳語が抽出対象コーパスに一回も出現しない $\cdots$ 10 個
\item[2.] 訳語が抽出対象コーパスには存在するが，抽出対象には含まれない
  $\cdots$ 49 個
\item[3.] 形態素の区切りが異なる $\cdots$ 15 個
\item[4.] 形態素解析誤り $\cdots$ 4 個
\item[5.] その他 $\cdots$ 22 個
\end{enumerate}

1. はある英単語に対応する日本語単語が抽出対象コーパス中に一回も出現し
ていない場合である．対訳文を見ると，以下のようないわゆる「意訳」に近い
ものであったり慣用句的な表現であった．
\begin{flushleft}
  \begin{quote}
    $ \left\{
      \begin{array}{l}
        ${\rm The taxi driver demanded an unreasonable fare.}$\\
        \rm{タクシーに乗ったら法外な料金をとられた．}\\
      \end{array}
    \right. $
  \end{quote}
\end{flushleft}
この例の場合では ``driver'' の訳として ``運転手'' あるいは ``ドライバー
'' を推定するためには意味解析などのより高度な解析が必要になることから，
現時点ではこのような対訳文だけから正しい対訳単語対を抽出することは難し
いと思われる．

2. はある英単語に対応する日本語単語が抽出対象コーパス中には出現するが，出
現回数の制限により抽出対象である $6,796$ 語に含まれない場合である．対
訳文を見ると，日本語単語の表記の揺れにより出現回数が分散してしまうこと
が原因であった．例えば ``solemnly'' の訳語候補として ``厳粛に'', ``粛々
と'', ``荘厳に'' のように複数の正しい訳語が対訳文中に現れているが，
出現回数がそれぞれに分散し出現回数の閾値の下限を下回り実験対象
に含まれない．これに対しては，日本語のシソーラスを利用して複数の訳語を
同一視することで抽出が可能になると思われる．

3. は英単語と日本語単語の形態素の区切りが異なる場合である．
``shoulder'' に対する訳語は ``肩'' となるべきであるが，日本語コーパス
の形態素解析の結果には ``肩'' という単語は含まれていなかったため，``右
肩'' という単語が出力される結果となった．本手法が 1 単語対 1 単語の
対訳関係を仮定していることが原因であると考えられる．
\cite{kumiko_SIGNL97,yamamoto_00} では統計情報から得られる連語や係り受
け解析結果から得られる文節を単位として対訳関係を推定することにより精度
を上げており，統計的 $\cdot$ 意味的にまとまりのある単位で対応付けを行
う方が良いことを示している．対訳文においては，形態素単位では対応が取れ
ない場合であっても統計的 $\cdot$ 意味的にまとまりのある単位では対応が
取れることが多いと考えられる．したがって本手法でも，単語単位でなく連語
や文節を単位として推定を行うことで精度が向上すると思われる．

\subsection{素性と解析精度}
\label{sec:feature_accuracy}

表 \ref{tab:feature2} に，それぞれの素性を削除したことによる精
度と再現率の増減を示した．括弧内は素性一個あたりの増減である．抽出アル
ゴリズムにおける閾値 $th$ は $0.1$ を用いた．
\begin{table}[htbp]
  \centering
  \caption{素性を削除したときの精度と再現率の増減}
  \label{tab:feature2}
  \begin{tabular}{c|r|rr|rr} \hline
    \multicolumn{1}{c|}{素性タイプ} &
    \multicolumn{1}{c|}{個数} &
    \multicolumn{2}{c|}{精度 (\%)} &
    \multicolumn{2}{c}{再現率 (\%)} \\ \hline
    
    1 & 4,086 &
    $-1.55$ & $(-3.79 \times 10^{-4})$ &
    $-0.85$ & $(-2.08 \times 10^{-4})$ \\
    
    2 & 3,112 &
    $-5.03$ & $(-1.62 \times 10^{-3})$ &
    $-7.93$ & $(-2.54 \times 10^{-3})$ \\
    
    3 & 5,011 &
    $-4.66$ & $(-9.30 \times 10^{-4})$ &
    $-16.23$ & $(-3.24 \times 10^{-3})$ \\
    
    4 & 159 &
    $-0.50$ & $(-3.14 \times 10^{-3})$ &
    $-3.15$ & $(-1.98 \times 10^{-2})$ \\ \hline
  \end{tabular}
\end{table}

素性一個あたりでもっとも抽出結果に影響していると考えられるのは，品詞情
報を用いた素性関数 (素性タイプ 4) であることがこの表からわかる．このこ
とは翻訳候補を選択する際には品詞情報が重要であるという直感的な判断と合
致する．逆にもっとも影響していないと考えられるのは，対訳文中に現れる単
語対のみを用いた素性 (素性タイプ 1) である．他の素性は共起情報や品詞情
報によって単語対に対して制約を与えているのに対して，タイプ 1 の素性は
対訳文中に現れたことがあるかどうかということのみを扱うため，翻訳候補を
選択するには制約が弱いと考えられる．このことから，係り受け情報やシソー
ラスの情報などによる制約を素性として表し，それらを加えていくことにより
抽出結果の改善を期待することができる．ただし，必要以上に素性を増やして
いくことは過学習の危険があるので注意が必要である．

\subsection{関連研究との比較}
\label{sec:related}

本手法と同様に対訳文の文対応が既に付いていることを前提にしている研究に
は \cite{gale_91,kitamura_96,melamed_97} があげられる．
\cite{gale_91} は単語対の対応度として $\phi^2$ 統計を用い，相互情報量
より有用であることを示している．
\cite{kitamura_96} は Dice 係数\cite{kay_93} に対して単語対の出現頻度
の対数で重み付けする重み付き Dice 係数を提案し，これを単語対の対応度と
して採用した．
\cite{melamed_97} は Competitive Linking Algorithm と 2 つのパラメータ
に対する山登り法を組み合わせて単語対の対応度を求める手法を提案した．
これらの研究に共通する特徴は，単語対の対応度の計算手法において単語対の
共起頻度がベースになっているために未知語に弱いということである．学習コー
パスと異なるコーパスでは未知語が出現するために単語対の対応度を計算する
ことは不可能となり，対訳単語対を抽出するためには新たに学習しなおさなけ
ればならない．
これに対して本論文で提案している手法では \ref{sec:unknown} 節で述べた
ように，注目している単語対の前後の文脈や品詞情報を用いた素性を用いてい
るため，未知語の場合にも既知の単語と同様に対応度の計算を行うことができ
る．

本手法と異なり，対訳文の文対応が付いていることを前提としない代わりに，
既存の対訳辞書を用いている研究には \cite{kaji_96,fung_97} があげられる．
これらの手法は一方の言語で共起する単語の訳語は他方の言語でも共起するこ
とを仮定している．
\cite{kaji_96} では既存の辞書に含まれる単語との共起集合間の共通部分の
大きさで対応度を計算している．
\cite{fung_97} では既存の辞書に含まれる単語との重み付き相互情報量を要
素とするベクトルを計算し，その内積を対応度として採用している．
現状では文対応の付いた対訳コーパスはあまり多くないため，文対応を前提と
しないこれらの手法は適用できる範囲は広いが，文対応が付いたコーパスを用
いた手法よりも精度が劣る．一方，本手法の前提となって
いる文対応済の対訳コーパスは，原文に忠実に翻訳した対訳コーパスであれば，
\cite{kay_93,utsuro_94,sukehiro_95} で提案されている手法
により作成することができる．対応する文がなかったり，1 つの文が複数の文
に対応している場合には人手による後編集が必要になるが，その労力は全て人
手による対応付けに比べて比較にならないほど少ないと考えられる．

学習コーパスを用いて相対頻度\footnote{
  相対頻度は学習コーパス中における対訳単語対の出現頻度 (式
  (\ref{eq:article_joint})) から
  \[ \tilde{P}(y|x) = \frac{c(x,y)}{\sum_{v \in Y} c(x,v)} \]
  と計算し，抽出アルゴリズムにおいて $P(y|x)$ の代わりに
  $\tilde{P}(y|x)$ を使用して対訳単語対の抽出を行った．}
と重み付き Dice 係数\footnote{
  重み付き Dice 係数は以下のように計算される．
  \[
  sim(x,y) = \left(\log_2 f(x,y)\right) \frac{2 f(x,y)}{f_X(x)+f_Y(y)}
  \]
  ここで $f(x,y)$ は $x$ と $y$ が対訳文中に出現した回数，$f_X(x)$,
  $f_Y(y)$ はそれぞれコーパス $X$, $Y$ 内で $x$, $y$ が出現した回数で
  ある．抽出アルゴリズムにおいて $P(y|x)$ の代わりに $sim(x,y)$ を使用
  して対訳単語対の抽出を行った．\cite{kitamura_96} で使用されている
  対訳単語対抽出アルゴリズムは本論文で示した手法とは異なるが，比較のた
  めに本論文で示した手法と同じ抽出アルゴリズムを用いた．
}
による比較実験を行った．図\ref{fig:compare}
\begin{figure}[tbp]
\begin{center}
\begin{tabular}{cc}
\framebox(160,115){} & \framebox(160,115){}
\vspace*{1em}
\end{tabular}
  \caption{各手法における抽出結果の比較}
  \label{fig:compare}
\end{center}
\end{figure}
は，抽出アルゴリズムの閾値
を徐々に下げていきながら対訳単語対を抽出した時の，抽出された対訳単語対
100 個ごとの精度 (左) と再現率 (右) を示したグラフである．重み付き
Dice 係数は単語対の共起回数が 2 回以下の場合は対応度が 0 になってしま
うため，2,700 個までしか抽出することができなかった．このグラフを見ると
大体の場合において本手法がもっとも良い結果となっていることがわかる．相
対頻度による抽出は本手法や重み付き Dice 係数に比べて悪い結果となってい
る．対訳単語対 $(x,y)$ について考えた時，$x$ とよく共起する単語 $x'$
が存在すると $(x',y)$ の対応度も高くなり $(x',y)$ が抽出されてしまう誤
りが多く見られ，これが原因であると考えられる．この問題に対して，重み付
き Dice 係数による抽出では単語対の出現回数の対数によって重み付けし，
$(x,y)$ と $(x',y)$ の対応度の差をより広げることによって対処している．
本論文で提案した手法では，共起情報や品詞情報を素性として用いてより多く
の制約を与えることによりこの問題による誤りを減少させていると考えられる．

