本研究の文節まとめあげの手法\label{sec:文節まとめあげ}

\subsection{複数決定リストの順次適用による文節まとめあげ}\label{subsec:複数決定リスト}

本研究では，従来手法の問題点を解決するために次の点に着目した．

\begin{itemize}
\item 学習が容易で用いる情報の数が少ないこと
\item 学習結果を利用して文節をまとめる方法が従来手法より簡明であること
\item 精度が従来手法と同程度かそれ以上となること
\end{itemize}
これらを実現するために，{\bold 複数決定リストの順次適用による文節まとめあげ}という新しい手法を考案した．

機械学習を用いる従来手法では，大規模コーパスから得られた様々なn-gram(主に2-gramから4-gram)が利用されている．
本手法では，1つの形態素の隙間に対して6種類のn-gramのそれぞれの決定リストだけを考慮するという非常に簡明な方法を用いる．
具体的には，品詞，単語表記，品詞細分類，単語表記＋品詞の4種類の形態素2-gramと，品詞，単語表記の2種類の形態素3-gramを要素とする決定リストを利用する
\footnote{
  品詞細分類の3-gramと単語表記＋品詞の3-gramは，予備実験を行ったところ結果に変化がなかったため用いなかった．
  }
(図\ref{fig:本手法})．
以下では，これらのn-gramを要素とする決定リストを，n-gramリストと呼ぶ．
文節まとめあげの処理は，村田らと同様に形態素解析済みのテキストに対して行い，形態素の隙間ごとにその前後の形態素の情報からn-gramリストを調べて文節を区切るか区切らないかを決定する処理とした．

\begin{figure}
  \begin{center}
    \begin{tabular}{l|ccccc}
      &
      二つ前   &     & 一つ前         &     & 一つ後\\
      \hline
      単語表記 2-gram &
      ~        &     & 単語表記       & --- & 単語表記\\
      品詞 2-gram &
      ~        &     & 品詞           & --- & 品詞\\
      品詞細分類 2-gram &
      ~        &     & 品詞細分類     & --- & 品詞細分類\\
      単語表記＋品詞 2-gram &
      ~        &     & 単語表記＋品詞 & --- & 単語表記＋品詞\\
      単語表記 3-gram &
      単語表記 & --- & 単語表記       & --- & 単語表記\\
      品詞 3-gram &
      品詞     & --- & 品詞           & --- & 品詞\\
    \end{tabular}
    \caption{本手法で用いる情報}
    
    
    
    \label{fig:本手法}
  \end{center}
\end{figure}

例えば，2-gramの場合には$Y$，$Z$という連続する2つの単語の$Y$と$Z$の間，3-gramの場合には$X$，$Y$，$Z$という連続する3つの単語の$Y$と$Z$の間に注目し，その間の文節区切りを次のように決定する
\footnote{
  3-gramを考慮する場合，$X$，$Y$，$Z$という3つの単語の$X$と$Y$の間も考慮する必要があると思われるが，本手法は処理が簡明であることを最大の目標としたため，ここでは考慮しない．
  }
．

\begin{enumerate}
\item $X$，$Y$，$Z$の形態素を得る．
\item 図\ref{fig:本手法}の6種類のn-gramリストを順番に調べる
  \footnote{
    n-gramリストの適用順は\ref{sec:実験}章の実験で最適化を行う．
    }
  ．
  \begin{enumerate}
  \item n-gramリスト中に規則が見つかり，文節に区切る数が区切らない数よりも多い場合には区切りを入れ，少ない場合には区切らないこととする．
    この段階で$Y$と$Z$の文節区切りを確定し，処理を終了する．
  \item n-gramリスト中に規則が見つからない場合，または文節に区切る数と区切らない数が等しい場合には，次のn-gramリストを調べる．
  \end{enumerate}
\item 6種類すべてのn-gramリストを調べた結果，文節に区切るか区切らないか確定しない場合，デフォルト処理として文節に区切るものとする
\footnote{
  デフォルト処理を文節に区切らないとした場合は，予備実験により精度が下がることがわかったため，文節に区切るものとした．
  }
．
\end{enumerate}

本手法の最大の特徴は，このように{\bold 6種類のn-gramリストを順番に調べるだけで文節まとめあげを行う}，という非常に簡明な点である．
村田らの手法では\ref{subsec:機械学習手法}節で示したように，1つの形態素の隙間に対して約3500回もの処理をしなければならない．
しかし，本手法では最大で$6\times23=138回$の処理でよいため，村田らの手法と比べて約$\dfrac{1}{25}$の処理量で文節まとめあげを行うことができる．


\subsection{n-gramリストの取得方法}\label{subsec:n-gramリスト取得}

各n-gramリストの要素は，大規模コーパスから機械学習によって得る．
本研究では，学習コーパスとして京大コーパス
\cite{KyotoCorpus}
を利用した．
京大コーパスにはあらかじめ詳細な形態素の情報と文節区切りの情報が付与されているので，形態素の隙間ごとに文節に区切る数と区切らない数を数えて，それを確率の高い順に並べて保持する．
ただし，確率には文節に区切る確率か文節に区切らない確率の2種類があるが，高い方の確率を基準としてリストに並べた．
つまり，リストの最下位は確率50\%となる．

以上のようにして得られた品詞2-gramの学習結果の決定リストの例を図\ref{tab:学習結果例}に示す．

\begin{figure}
  \begin{center}
    \begin{tabular}{rr@{ }c@{ }lc}
      確率 & \multicolumn{3}{c}{規則} & 区切り\\
      \hline
      100\% & 「連体詞」 & ＋ & 「連体詞」 & 区切る\\
      100\% & 「連体詞」 & ＋ & 「名詞」   & 区切る\\
      $\vdots$ & & $\vdots$ & & $\vdots$\\
      55\% & 「特殊」 & ＋ & 「特殊」   & 区切る\\
      52\% & 「特殊」 & ＋ & 「接続詞」 & 区切らない\\
    \end{tabular}
    \caption{品詞2-gramの決定リスト}
    
    
    
    \label{tab:学習結果例}
  \end{center}
\end{figure}



おわりに

本研究で提案した文節まとめあげの手法は，車載情報機器の求める条件を満たすよう考案したものであり，複数の決定リストを順次適用して文節の区切りを行うだけ非常に簡明かつ高速である．
それにもかかわらず，従来の手法と比較してより高い精度を得られることが示された．
また，本手法は非常に簡明であるため，他の手法の長所のみを導入することが容易である．
そのことを1-gramや排反な規則を組み合わせることにより示した．

今後は，本手法を係り受け解析の技術と融合させ，より高精度な係り受け解析の技術に応用し，音声合成の品質の向上に貢献しようと考えている．



\bibliographystyle{jnlpbbl}
\bibliography{jpaper}

\begin{biography}
\biotitle{略歴}
\bioauthor{白木 伸征}{
  1997年京都大学工学部電気系学科卒業．
  1999年同大学院修士課程修了．
  同年，株式会社豊田中央研究所に入社，現在に至る．
  自然言語処理，ヒューマンインタフェースの研究に従事．
}
\bioauthor{梅村 祥之（正会員）}{
  1979年岐阜大学工学部電子工学科卒業．
  1981年名古屋大学大学院工学研究科修士課程修了．
  同年，東京芝浦電気（株）入社．
  1988年（株）豊田中央研究所入社．
  自然言語処理，音響・音声処理，画像処理の研究に従事．
}
\bioauthor{原田 義久}{
  1973年名古屋工業大学計測工学科卒業，
  1975年東京工業大学制御工学専攻修士課程修了，
  工学博士（京都大学），
  同年（株）豊田中央研究所入社，
  2000年名古屋商科大学教授，IEEE ICCD'84優秀論文賞，
  IJCNN Best Presentation Award受賞．
}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

