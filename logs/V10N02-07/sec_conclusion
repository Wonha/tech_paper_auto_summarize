SVDPACKCが扱える行列の大きさについて述べておく．
las2.c からメモリ割り当ての関数mallocの部分を抜き出してみると，las2 は[MATH]の行列の特異値分解を行うのに，大ざっぱに見積もって，[MATH]バイト強のメモリを必要としていることがわかる．
この点から考えると，必要メモリが約200Mバイトとなる[MATH]位の大きさが現実的な最大サイズだと思われる．
確認のために，非ゼロ要素の密度が1 %であり，平均2のポアソン分布に従って，非ゼロ要素の整数値（1〜6）が配置されるような[MATH]の行列[MATH]を人工的に作成し，その行列に対してlas2で特異値分解を行ってみた．
Pentium-4 1.5GHzメモリ512MバイトのLinux環境での実行時間は227秒，要したメモリは228Mバイトであった．
この程度の大きさの行列であれば，実行時間は大きな問題にはならないと思われる．
ただし，行列の大きさを変更して同じ条件で試してみると表[REF_sokudo]の結果が得られた．
メモリは行列のサイズにほぼ比例するが実行時間は指数関数的に増加しているので，実行時間の面からも，この程度の大きさの行列がSVDPACKCで扱える限度だと思われる．
ちなみに[MATH]の行列ではメモリ不足で実行できなかった．
ただし実験で用いたマシンにスワップは設定されていないことを注記しておく．
スワップを利用すれば，更に大きな行列も扱えるが，その場合は実行時間の方で問題が生じるであろう．
実際に情報検索で用いられる索引語ベクトルの次元数は少なくとも数十万単位になり，検索対象の文書も100万文書以上となるであろう．
その場合の索引語文書行列[MATH]の大きさは巨大なスパース行列である．
このような巨大な行列になると，SVDPACKCによって一気に特異値分解を行うのは不可能である．
この問題に対しては最初に小さな行列で特異値分解を行い，その後に文書や索引語の追加に従って特異値分解の更新を行うfolding-inとよばれる手法や大規模な文書集合から文書をランダムサンプルし，そこから特異値分解を行う手法などが提案されている[CITE]．
あるいは概念ベクトルの選択に特異値分解以外の手法を使うアプローチもある（[CITE]など）．
最近では言語横断検索にもLSIが利用されているが[CITE]，そこでも大規模な行列の特異値分解をどう行うかが問題点として上がっている[CITE]．
結局，現実の情報検索で現れるような大規模な行列に対しては，SVDPACKCを直接利用することはできない．
しかしアイデアを試すための中規模の実験であれば，十分にその役割を果たせる．
実験では圧縮する次元の数を交差検定では75に，実際の評価では100に固定している．
この値は適当である．
最適な次元数については様々な議論があるが，ここではSVDPACKCの利用例として紹介した実験であるため，最適な次元数を推定する処理は行わなかった．
ちなみに実際の評価における次元数を100から増減させた場合の，実験結果を表[REF_jigen]に示す．
次元数を100に圧縮するといっても行列のランク数がそれ以下であれば，100よりも小さい数になるので，100のときに圧縮された次元数を基準に[MATH]20, [MATH]10, +10, +20と次元数を変更させて実験を行った．
また表中にMAXとあるのは，行列のランク数で圧縮した場合を示す．
これが圧縮できる次元の最大値である．
大まかな傾向としては次元数が多い方が精度は高いようである．
ただし最高精度を記録する次元数は個々の単語によって異っており，最適な次元数は問題に依存すると言える．
LSIのアイデア自体は情報検索以外にも適用できる．
ここでは語義判別問題への利用を試みた．
他にも文書分類への応用が報告されている[CITE]．
このような教師付き学習のタイプでは，訓練事例数が大規模なものになることはないため，SVDPACKCが利用できる．
また素性ベクトルの次元圧縮という手法は，統計学では主成分分析，パターン認識ではKarhunen-Lo\`{e}ve展開や線形判別法[CITE]として知られている手法である．
またデータマイニングの分野ではデータ数が非常に大きいために，現実的には機械学習手法を直接適用できないという問題がある．
そのために類似する事例集合を抽象表現として表される事例に変換し，変換後の事例に対して機械学習手法を適用するData Squashingという手法が使われる[CITE]．
これは索引語ベクトルではなく文書ベクトルに対する次元圧縮の手法に対応する．
このように次元圧縮の手法は様々な分野で重要であり，新しい手法が次々と提案されている（例えば[CITE]など）．
次元圧縮の手法として，特異値分解は古典的と言えるが，ベースとなる手法として容易に試すことのできる意味でもSVDPACKCは有用であろう．
最後にLSIを分類問題に利用する場合の注意を述べておく．
次元圧縮を行う手法は種々あるが，それらは2つに大別できる．
１つは「表現のための次元圧縮」であり，もう１つは「判別のための次元圧縮」である[CITE]．
「表現のための次元圧縮」は素性ベクトルの分布全体のもつ情報をできるだけ反映できるように次元を圧縮する．
一方，「判別のための次元圧縮」はクラスをできるだけ明確に分離できるように次元を圧縮する．
主成分分析やKarhunen-Lo\`{e}ve展開は前者であり，線形判別法は後者である．
そして特異値分解も「表現のための次元圧縮」に属する手法である．
このため，特異値分解を行ったからといって必ずしも判別精度が高まることは保証されない．
「表現のための次元圧縮」が判別精度向上に寄与できる問題は，非常に高次元のベクトルを扱う問題（例えば情報検索や音声・画像認識）だと思われる．
このような場合，「表現のための次元圧縮」は``次元の呪い''に対抗できる可能性がある．
あるいは次元数が多くなったときに素性間に共起性（依存関係）が生じる傾向があり，それが精度向上に悪影響を及ぼすが，そのような依存関係を解消できる可能性ももつ．
特異値分解による次元圧縮が判別精度の向上に寄与できるかどうかは未知である．
本研究では交差検定を行うことで，精度向上に寄与できそうな問題を選別しておくというアプローチをとった．
しかし，LSI法は平均的には他の学習手法よりも精度が低かった．
LSI法を語義判別問題に利用するためには，また別の工夫が必要になるだろう．
１つの利用可能性としてはbagging手法[CITE]の１つの学習器として使うことが考えられる．
実際に，LSI法は数個の単語に関しては他の学習手法よりも精度が高かった．
またNN法まで含めるとその数は更に増える．
SENSEVAL2の辞書タスクでは様々な学習手法を融合して用いる手法が最も良い成績を納めた[CITE]．
そこでは，決定リスト，Naive Bayes，SVMの学習手法を用意し，交差検定の結果から単語毎に利用する学習手法を設定している．
ここで用いたNN法やLSI法も１つの学習手法としてエントリーさせておけばよい．
今回の実験では，多くの単語に対してLSI法はNN法よりも正解率が低かった．
特に，語義判別の場合，素性ベクトルの次元数[MATH]に比べ，訓練事例数[MATH]が小さい．
特異値分解で圧縮する次元数の最大値は[MATH]なので，この点でかなり制約があった．
交差検定を用いることでNN法の精度を高めることができたが，他の学習手法と比べると精度の面ではまだ十分ではない．
今後はNN法やLSI法が他の学習手法よりも正解率が高かった単語について，その原因を調査する．
これによってLSIを語義判別問題のような分類問題に利用する方法を探ってゆく．
またLSIが利用可能な他の問題を調べゆく．
本論文ではフリーの特異値分解ツールSVDPACKCを紹介した．
その利用方法を解説し，利用事例として語義判別問題を扱った．
SENSEVAL2の辞書タスクの動詞50単語を対象に実験を行ったところ，交差検定を合わせて用いることで，NN法を改良できた．
またNN法やLSI法は，一部の単語に対して決定リストやNaive Bayes以上の正解率が得られることも確認できた．
特異値分解は，情報検索のLSIだけではなく，高次元の特徴ベクトルを重要な低次元のベクトルに射影する手法で必要とされる．
このために様々な応用が期待される．
今後はここでの実験の結果を詳しく調査し，LSIが利用可能な問題を調べてゆきたい．
