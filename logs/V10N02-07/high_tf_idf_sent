================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:3, score:0.29941] LSIでは高次元の索引語ベクトルを低次元の潜在的な概念のベクトルに射影することで，ベクトル空間モデルの問題点である同義語や多義語の問題に対処する．
[i:4, score:0.33710] そして概念のベクトルを構築するために，索引語文書行列に対して特異値分解を行う．
[i:5, score:0.43332] SVDPACKCは索引語文書行列のような高次元かつスパースな行列に対して特異値分解を行うツールである．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[i:22, score:0.38900] 具体的には索引語文書行列[MATH]に対して特異値分解を行い，その左特異ベクトル（[MATH]の固有ベクトル）を固有値の大きい順に適当な数[MATH]だけ取りだし，それらを潜在的な概念に対応するベクトルとする[CITE]．
[i:24, score:0.35202] 低次元の特異値分解のプログラムは比較的簡単に作成できるが，現実の問題においては，高次元かつスパースな行列を扱わなくてはならない．
[i:27, score:0.44201] このツールによって高次元かつスパースな行列に対する特異値分解が行え，簡単にLSIを試すことができる．

================================================================
[section type  : proposed_method]
[section title : LSI と 特異値分解]
================================================================
[i:33, score:0.27867] 検索対象の文書群が[MATH]個ある場合，各文書[MATH]に対する[MATH]次元の索引語ベクトルが[MATH]個並ぶので，[MATH]の索引語文書行列[MATH]ができる．
[i:38, score:0.32436] 行列[MATH]の特異値分解を行ったとき，[MATH]は[MATH]の列ベクトルが張る空間の正規直交基底となっている．
[i:45, score:0.41232] 以上より，情報検索にLSIを利用するためには，索引語文書行列[MATH]の特異値分解から得られる行列[MATH]が求まれば良いことがわかる．

================================================================
[section type  : proposed_method]
[section title : 特異値分解ツール SVDPACKC]
================================================================
-----------------------------------------------------
  [subsection title : 入手とコンパイル]
-----------------------------------------------------
  [i:lead, score:0.32461] 行列[MATH]の特異値分解を行うツールがSVDPACKCである．
.....
  [i:46, score:0.32461] 行列[MATH]の特異値分解を行うツールがSVDPACKCである．
  [i:51, score:0.31293] 単に特異値分解の結果だけを得たいのであればlas2を利用すれば良く，他のプログラムをコンパイルする必要はない．
  [i:55, score:0.30868] これによってlas2による特異値分解の結果がファイルに保存される．
-----------------------------------------------------
  [subsection title : 利用方法]
-----------------------------------------------------
  [i:lead, score:0.16721] las2は内部で２つのファイルを読む込む．
.....
  [i:71, score:0.34118] １つは特異値分解を行いたい対象の行列が記述されたファイルmatrixであり，もう１つはパラメータを記述したファイルlap2である．
  [i:83, score:0.34482] lao2には特異値分解したときの式[REF_siki1]の[MATH]，つまり特異値の列とその他の情報（行列の大きさや実行時間等）が保存される．
  [i:85, score:0.37703] 特異値分解の対象となる行列[MATH]は，ハーウェル・ボーイング形式（Harwell-Boeing format）と呼ばれる列方向の圧縮形式を用いてファイルmatrixに記述する．
-----------------------------------------------------
  [subsection title : 出力結果の利用]
-----------------------------------------------------
  [i:lead, score:0.36076] las2による特異値分解の結果はlav2とlao2というファイルに保存される．
.....
  [i:145, score:0.36076] las2による特異値分解の結果はlav2とlao2というファイルに保存される．
  [i:151, score:0.35178] lav2はバイナリファイルであり，las2のソースをみて出力形式を確認すれば，特異値分解結果の[MATH]や[MATH]を得ることが可能である．
  [i:176, score:0.22704] fp_out3 = fopen("V-matrix", "w"); /*行列Vのファイル名はV-matrixとする．

================================================================
[section type  : proposed_method]
[section title : 語義判別問題への利用]
================================================================
[i:185, score:0.05042] SENSEVAL2の日本語辞書タスクで課題として出された動詞50単語を実験の対象とする．
-----------------------------------------------------
  [subsection title : 最近傍法の利用]
-----------------------------------------------------
  [i:lead, score:0.04217] 単語[MATH]は[MATH]個の語義を持つとし，各語義を[MATH]  ([MATH])で表す．
.....
  [i:190, score:0.14951] NN法は与えられた素性ベクトルと最も距離が近い訓練事例中の素性ベクトルを選び，そのクラスを出力とする手法である．
  [i:193, score:0.14669] 実際の語義判別は，単語[MATH]の現れた文脈を素性ベクトル[MATH]で表し，以下の式で求められる訓練事例[MATH]のクラスを返すことで行える．
  [i:195, score:0.38081] また行列[MATH]を特異値分解し，式[REF_siki2]を利用して[MATH]を定義したものをLSI法と呼ぶことにする．
-----------------------------------------------------
  [subsection title : 素性の設定]
-----------------------------------------------------
  [i:lead, score:0.05162] ここでは語義判別の手がかりとなる属性として以下のものを設定した．
.....
  [i:197, score:0.07883] e1直前の単語e2直後の単語e3前方の内容語２つまでe4後方の内容語２つまでe5 e3の分類語彙表の番号e6 e5の分類語彙表の番号
  [i:201, score:0.07897] 次に，「出す」の前方の内容語は「短い」と「コメント」なので， `e3=短い'，`e3=コメント'の２つが作られる．
  [i:217, score:0.20023] 以上より，上記例文に対する素性ベクトルは第21次元目，第60次元目，第134次元目，第302次元目，第379次元目，第406次元目，第789次元目，第790次元目の各要素が1であり，その他の要素がすべて0の978次元のベクトルとなる．
-----------------------------------------------------
  [subsection title : 交差検定の利用]
-----------------------------------------------------
  [i:lead, score:0.18276] LSI法を利用した場合，NN法と比較して，必ずしも精度が向上するわけではなく，逆に精度が悪化する場合もある．
.....
  [i:225, score:0.27382] また特異値分解を使って圧縮する次元数は75とした．
  [i:228, score:0.33566] そこではSENSEVAL2の日本語辞書タスクの動詞50単語の各単語に対する行列[MATH]の大きさ，非ゼロ要素の密度，圧縮した次元数，次元圧縮に要したメモリと時間が記されている．
  [i:230, score:0.32969] また次元圧縮に要したメモリと時間はlas2の出力ファイルlao2から得ている．
-----------------------------------------------------
  [subsection title : 特異値分解を用いた語義判別実験]
-----------------------------------------------------
  [i:lead, score:0.16558] 実際は選出した14単語のみに対してLSI法を行えば良いが，交差検定の効果も示すために，すべての単語に対してLSI法を試みた．
.....
  [i:239, score:0.33566] そこではSENSEVAL2の日本語辞書タスクの動詞50単語の各単語に対する行列[MATH]の大きさ，非ゼロ要素の密度，圧縮した次元数，次元圧縮に要したメモリと時間が記されている．
  [i:240, score:0.32969] また次元圧縮に要したメモリと時間はlas2の出力ファイルlao2から得ている．
  [i:250, score:0.22850] ほとんどの単語で，決定リストやNaive BayesはNN法やLSI法よりも良い結果を出しているが，一部ではNN法やLSI法の方が良い値を出している．

================================================================
[section type  : conclusion]
[section title : 考察]
================================================================
[i:253, score:0.41717] las2.c からメモリ割り当ての関数mallocの部分を抜き出してみると，las2 は[MATH]の行列の特異値分解を行うのに，大ざっぱに見積もって，[MATH]バイト強のメモリを必要としていることがわかる．
[i:255, score:0.40797] 確認のために，非ゼロ要素の密度が1 %であり，平均2のポアソン分布に従って，非ゼロ要素の整数値（1〜6）が配置されるような[MATH]の行列[MATH]を人工的に作成し，その行列に対してlas2で特異値分解を行ってみた．
[i:268, score:0.38104] 最近では言語横断検索にもLSIが利用されているが[CITE]，そこでも大規模な行列の特異値分解をどう行うかが問題点として上がっている[CITE]．

================================================================
[section type  : conclusion]
[section title : おわりに]
================================================================
[i:318, score:0.23888] 本論文ではフリーの特異値分解ツールSVDPACKCを紹介した．
[i:321, score:0.22070] またNN法やLSI法は，一部の単語に対して決定リストやNaive Bayes以上の正解率が得られることも確認できた．
[i:322, score:0.38812] 特異値分解は，情報検索のLSIだけではなく，高次元の特徴ベクトルを重要な低次元のベクトルに射影する手法で必要とされる．

