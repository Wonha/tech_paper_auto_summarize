考察


SVDPACKC が扱える行列の大きさについて述べておく．
\verb| las2.c | からメモリ割り当ての関数\verb| malloc |の部分を抜き出して
みると，\verb| las2 | は\( m \times n \)の行列の特異値分解を行うのに，
大ざっぱに見積もって，\(  8mn \)バイト強の
メモリを必要としていることがわかる\footnote{{\tt sizeof(double)} の \( mn \)倍である．
{\tt sizeof(double) = 8} として\( 8mn \)を得ている．}．
この点から考えると，必要メモリが約200Mバイトとなる\( 25000 \times 1000 \) 位の
大きさが現実的な最大サイズだと思われる\footnote{これは個人的な感覚である．}．
確認のために，非ゼロ要素の密度が 1\,\% であり，平均 2 のポアソン分布に従って，
非ゼロ要素の整数値（1 〜 6）が配置されるような\( 25000 \times 1000 \)の行列\( A \)を人工的に作成し
\footnote{実際の索引語文書行列に似せるよう考慮している．}，
その行列に対して{\tt las2 }で特異値分解を行ってみた．Pentium-4 1.5GHz メモリ 512Mバイトの Linux 環境での
実行時間は 227秒，要したメモリは 228M バイトであった．
この程度の大きさの行列であれば，実行時間は大きな問題にはならないと思われる．

ただし，行列の大きさを変更して同じ条件で試してみると\mbox{表\ref{sokudo}}の結果が得られた．
メモリは行列のサイズにほぼ比例するが実行時間は指数関数的に増加しているので，
実行時間の面からも，この程度の大きさの行列が SVDPACKC で扱える限度だと思われる．
ちなみに\( 25000 \times 2000 \)の行列ではメモリ不足で実行できなかった．
ただし実験で用いたマシンにスワップは設定されていないことを注記しておく．
スワップを利用すれば，更に大きな行列も扱えるが，その場合は実行時間の方で
問題が生じるであろう．

\begin{table}[htbp]
  \begin{center}
    \leavevmode
    \caption{行列の大きさと速度・メモリの関係}\label{sokudo}
    \begin{tabular}{|c|c|c|c|c|} \hline
                &   \( 25000 \times 125 \) & \( 25000 \times 250 \) & \( 25000 \times 500 \) & \( 25000 \times 1000 \) \\  \hline
使用メモリ(MB)  &    26.2      &    52.6    &    108    &   228      \\
実行時間（秒）  &     7.49     &    16.8    &   53.5    &   227      \\ \hline
    \end{tabular}
  \end{center}
\end{table}

実際に情報検索で用いられる索引語ベクトルの次元数は少なくとも数十万単位になり，
検索対象の文書も 100万文書以上となるであろう．
その場合の索引語文書行列\( A \)の大きさは巨大なスパース行列である．
このような巨大な行列になると，SVDPACKC によって一気に特異値分解を行うのは不可能である．
この問題に対しては最初に小さな行列で特異値分解を行い，その後に
文書や索引語の追加に従って特異値分解の更新を行う folding-in とよばれる手法や
大規模な文書集合から文書をランダムサンプルし，そこから特異値分解を
行う手法などが提案されている\cite{kita-ir}．あるいは概念ベクトルの選択に
特異値分解以外の手法を使うアプローチもある（\cite{sasaki}など）．
最近では言語横断検索にも LSI が利用されているが\cite{dumais}，
そこでも大規模な行列の特異値分解をどう行うかが問題点として上がっている\cite{mori}．
結局，現実の情報検索で現れるような大規模な行列に対しては，
SVDPACKC を直接利用することはできない．
しかしアイデアを試すための中規模の実験であれば，十分にその役割を果たせる．

実験では圧縮する次元の数を交差検定では 75 に，実際の評価では 100 に固定している．
この値は適当である．最適な次元数については様々な議論があるが，
ここでは SVDPACKC の利用例として紹介した実験であるため，
最適な次元数を推定する処理は行わなかった．
ちなみに実際の評価における次元数を 100 から増減させた場合の，実験結果を\mbox{表\ref{jigen}}に示す．
次元数を 100 に圧縮するといっても行列のランク数がそれ以下であれば，100よりも
小さい数になるので，100のときに圧縮された次元数を基準に $-$20, $-$10, +10, +20 と
次元数を変更させて実験を行った．また表中に MAX とあるのは，行列のランク数で圧縮した場合を示す．
これが圧縮できる次元の最大値である．
大まかな傾向としては次元数が多い方が精度は高いようである．
ただし最高精度を記録する次元数は個々の単語によって異っており，
最適な次元数は問題に依存すると言える．

\begin{table}[htbp]
  \begin{center}
    \leavevmode
    \caption{圧縮次元数と精度の関係}\label{jigen}
    \begin{tabular}{|c|c|c|c|c|c|c|} \hline
                &   $-$20  &  $-$10    & 100      & +10       & +20      & MAX   \\  \hline
判別精度        &  0.7030 &  0.7168 & 0.7202  &  0.7212   &  0.7202  & 0.7203 \\ \hline
    \end{tabular}
  \end{center}
\end{table}

LSI のアイデア自体は情報検索以外にも適用できる．
ここでは語義判別問題への利用を試みた．
他にも文書分類への応用が報告されている\cite{zelikovitz}．
このような教師付き学習のタイプでは，訓練事例数が大規模なものになることはないため，
SVDPACKC が利用できる．
また素性ベクトルの次元圧縮という手法は，統計学では主成分分析，パターン認識では
Karhunen-Lo\`{e}ve 展開や線形判別法\cite{ishii}として知られている手法である．
またデータマイニングの分野ではデータ数が非常に大きいために，
現実的には機械学習手法を直接適用できないという問題がある．
そのために類似する事例集合を抽象表現として表される事例に変換し，
変換後の事例に対して機械学習手法を適用する Data Squashing という
手法が使われる\cite{suzuki}．これは索引語ベクトルではなく
文書ベクトルに対する次元圧縮の手法に対応する．
このように次元圧縮の手法は様々な分野で重要であり，
新しい手法が次々と提案されている（例えば\cite{suenaga}など）．
次元圧縮の手法として，特異値分解は古典的と言えるが，ベースとなる手法として
容易に試すことのできる意味でも SVDPACKC は有用であろう．

最後に LSI を分類問題に利用する場合の注意を述べておく．
次元圧縮を行う手法は種々あるが，それらは2つに大別できる．
１つは「表現のための次元圧縮」であり，もう１つは「判別のための次元圧縮」である\cite{ishii}．
「表現のための次元圧縮」は素性ベクトルの分布全体のもつ情報をできるだけ
反映できるように次元を圧縮する．一方，「判別のための次元圧縮」は
クラスをできるだけ明確に分離できるように次元を圧縮する．
主成分分析やKarhunen-Lo\`{e}ve 展開は前者であり，線形判別法は後者である．
そして特異値分解も「表現のための次元圧縮」に属する手法である．
このため，特異値分解を行ったからといって必ずしも判別精度が高まることは保証されない．
「表現のための次元圧縮」が判別精度向上に寄与できる問題は，
非常に高次元のベクトルを扱う問題（例えば情報検索や音声・画像認識）だと思われる．
このような場合，「表現のための次元圧縮」は ``次元の呪い''に対抗できる可能性がある．
あるいは次元数が多くなったときに素性間に共起性（依存関係）が生じる
傾向があり，それが精度向上に悪影響を及ぼすが，そのような依存関係を解消できる可能性ももつ．

特異値分解による次元圧縮が判別精度の向上に寄与できるかどうかは未知である．
本研究では交差検定を行うことで，精度向上に寄与できそうな問題を選別しておく
というアプローチをとった．しかし，LSI法は平均的には他の学習手法よりも
精度が低かった．LSI 法を語義判別問題に利用するためには，また別の工夫が必要になるだろう．
１つの利用可能性としてはbagging 手法\cite{breiman96}の１つの学習器として使うことが
考えられる．
実際に，LSI 法は数個の単語に関しては他の学習手法よりも精度が高かった．
また NN法まで含めるとその数は更に増える．
SENSEVAL2 の辞書タスクでは様々な学習手法を融合して用いる
手法が最も良い成績を納めた\cite{murata-sen2}．
そこでは，決定リスト，Naive Bayes，SVM の学習手法を用意し，交差検定の結果から
単語毎に利用する学習手法を設定している．
ここで用いた NN法 や LSI法も１つの学習手法としてエントリーさせておけばよい．

今回の実験では，多くの単語に対して LSI法は NN法よりも正解率が低かった．
特に，語義判別の場合，素性ベクトルの次元数\( m \)に比べ，訓練事例数\( n \)が小さい．
特異値分解で圧縮する次元数の最大値は\( n \)なので，
この点でかなり制約があった．交差検定を用いることで NN 法の精度を高めることができたが，
他の学習手法と比べると精度の面ではまだ十分ではない．
今後は NN法や LSI法が他の学習手法よりも正解率が高かった単語について，その原因を調査する．
これによって LSI を語義判別問題のような分類問題に利用する方法を
探ってゆく．またLSI が利用可能な他の問題を調べゆく．





おわりに


本論文ではフリーの特異値分解ツール SVDPACKC を紹介した．
その利用方法を解説し，利用事例として語義判別問題を扱った．
SENSEVAL2 の辞書タスクの動詞 50単語を対象に実験を行ったところ，
交差検定を合わせて用いることで，NN法を改良できた．
また NN法や LSI法は，一部の単語に対して
決定リストや Naive Bayes 以上の正解率が得られることも確認できた．
特異値分解は，情報検索の LSI だけではなく，高次元の特徴ベクトルを重要な低次元の
ベクトルに射影する手法で必要とされる．
このために様々な応用が期待される．
今後はここでの実験の結果を詳しく調査し，LSI が利用可能な問題を
調べてゆきたい．




\bibliographystyle{jnlpbbl}
\bibliography{408}


\newpage
