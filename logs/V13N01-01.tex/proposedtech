



\documentstyle[epsf,jnlpbbl]{jnlp_j}

\setcounter{page}{3}
\setcounter{巻数}{2}
\setcounter{号数}{3}
\setcounter{年}{1995}
\setcounter{月}{7}
\受付{1995}{5}{6}
\再受付{1995}{7}{8}
\採録{1995}{9}{10}

\setcounter{secnumdepth}{2}

\title{言語資源を活用した実用的な対訳表現抽出}
\author{北村 美穂子\affiref{OKI} \and 松本 裕治\affiref{NAIST}}

\headauthor{北村 美穂子・松本 裕治}
\headtitle{言語資源を活用した実用的な対訳表現抽出}

\affilabel{OKI}{沖電気工業株式会社 研究開発本部}
{Corporate Research \& Development Center, Oki Electric Industry Co., Ltd.}

\affilabel{NAIST}{奈良先端科学技術大学院大学 }
{Nara Institute of Science and Technology}

\jabstract{
  高精度の機械翻訳システムや言語横断検索システムを構築するためには，大規模な
  対訳辞書が必要である．文対応済みの対訳文書に出現する原言語と目的言語の単語
  列の共起頻度に基づいて対訳表現を自動抽出する試みは，対訳辞書を自動的に作成
  する方法として精度が高く有効な手法の一つである．本稿はこの手法をベースにし，
  文節区切り情報や対訳辞書などの言語知識を利用したり，抽出結果を人間が確認す
  る工程を設けたりすることにより，高精度で，かつ，カバレッジの高い対訳表現抽
  出方法を提案する．また，抽出にかかる時間を削減するために，対訳文書を分割し，
  抽出対象とする文書量を徐々に増やしながら確からしい対訳表現から段階的に抽出
  していくという手法についても検討する．
  8,000文の対訳文書による実験では，従来手法は精度40\%，カバレッジ79\%であった
  のに対し，言語知識を利用した提案手法では，精度89\%，カバレッジ85\%と向上した．
  さらに人手による確認工程を設けることにより，精度が96\%，カバレッジが85\%と向
  上した．また，16,000文の対訳文書による実験では，対訳文書を分割しない方法では
  抽出時間が約16時間であったのに対し，文書を4分割する方法では，約9時間に短縮さ
  れたことを確認した． }

\jkeywords{対訳表現抽出, 対訳文書, 機械翻訳}

\etitle{Practical Translation Pattern Acquisition \\
with Utilizing Language Resources}
\eauthor{Mihoko Kitamura\affiref{OKI} \and Yuji Matsumoto\affiref{NAIST}} 

\eabstract{ High-quality MT systems and cross-lingual information
  retrieval systems need large-sized translation dictionaries.
  Automatic extraction of translation patterns from parallel corpora
  is an efficient and accurate way to automatically develop
  translation dictionaries, and various approaches have been proposed
  to achieve this. This paper presents a practical translation pattern
  extraction method where translation patterns based on co-occurrence
  frequency of word sequences between English and Japanese can be
  greedily extracted, and manual confirmation or extra linguistic
  resources, such as chunking information and translation
  dictionaries, can be also effectively combined with.  　This paper
  examines the method of extracting probable translation patterns in
  incremental steps by gradually enlarging a unit of segmentalized
  corpus, in order to reduce the time spent on pattern extraction.
  　Our experiments using 8,000 sentences showed that the proposed
  method achieved an accuracy of 89\% for coverage of 85\% while the
  existing method achieved only an accuracy of 40\% for coverage of
  79\%, and this was further improved to an accuracy of 96\% for
  coverage of 85\% when combined with manual confirmation. Our
  experiments using 16,000 sentences showed that the method of
  dividing a corpus in quarters could reduce the extraction time to 9
  hours while the nondividing method required 16 hours.  }

\ekeywords{translation pattern extraction, parallel corpus, machine translation}

\begin{document}
\maketitle


\section{従来手法}
\label{従来}

\subsection{連続単語列間の対応度計算方法}
\label{連続単語列間の対応度計算方法}

原言語単語列 $ w_{o} $ と目的言語単語列 $ w_{t} $ の対応関係の強さを
示す尺度として，対応度$sim(w_{o},w_{t})$を定義する．対応度は，原言語の単語列の出現回数，目的言語の
単語列の出現回数，両者が同時に対訳文に同時に出現する回数で求められ，いくつかの
計算方法が提案されている\cite{Matsumoto-Utsuro:2000}．

従来手法では重み付きDice係数が用いられている．Dice係数はXとYの事象に
おいて，Xが発生する回数とYが発生する回数の和に対してXとYの事象が同時に出現する
回数の割合で表す．さらに，同時出現回数の重みを与えたものを重み付きDice係数と呼び，
これはXとYの相関関係だけでなく，出現回数も考慮に入れることができる．日本語単語列を
$ w_{J} $，英語単語列を $ w_{E} $，$ w_{J} $の日本語文書中の出現回数を
$ f_{j} $，$ w_{E} $の英語文書中の出現回数を$ f_{e} $とし，
$ w_{J} $と$ w_{E} $が対訳文に同時に出現する回数を$ f_{je} $とすると，
重み付きDice係数を用いた対応度は，以下の式で定義される\cite{北村97}．
  
\[ sim(w_{J},w_{E})=(\log_{2}f_{je}) \cdot \frac{2f_{je}}{f_{j}+f_{e}}  \]

\subsection{抽出アルゴリズム}
\label{抽出アルゴリズム}

従来手法の基本的な考え方は，原言語文書と目的言語文書から抽出される
連続単語列集合の全ての組合せに対して，\ref{連続単語列間の対応度計算方法}節に
述べた対応度を計算し，対応度の高い連続単語列ペアから順に対訳表現として抽出する
という手法である．図\ref{基本アルゴリズム}の流れ図に従って，各処理を説明
する\footnote{この手法で対象とする対訳文書は言語に依存しないが，以下では
日本語と英語の対訳文書を対象にして説明する．}．

\begin{figure}[t]
  \begin{center}
    \epsfxsize=9cm \epsfbox{./figure/figure1.ps}
  \end{center}
  \caption{基本アルゴリズムの流れ図}
  \label{基本アルゴリズム}
\end{figure}

\vspace{5mm}

\begin{description}
\item[(1)形態素解析:] 対訳文書 $(E,J)$ を構成する日本語文書及び英語文書中の
各対訳文を形態素解析する．形態素解析された対訳文書 $(E,J)$ 中の各対訳文を 
$(es,js)$とする．

\item[(2)連続単語列の抽出:] 各対訳文 $(es,js)$ に対して英語連続単語列集合 $EWS$ と
日本語連続単語列集合 $JWS$ を抽出し，$ (EWS,JWS) $ を連続単語列データベースに登録する．
ここでの連続単語列とは，連続単語列の構成単語数の最大値を $l_{max} $とした場合の
$ n \leq l_{max} $ の条件を満たす単語n-gramである．\ref{実験}節の実験では
$ l_{max}=10 $を用いた．

\item[(3)出現回数閾値設定:] (4)から(6)の処理で抽出対象とする連続単語列の出現回数の
最低値を閾値 $ Th $ とし，その初期値である $ f_{max} $ を代入する．4節の実験では，
$ f_{max} $ は，抽出された英語及び日本語連続単語列の出現回数の最高値の1/2の値に
設定した．なお，(4)から(6)の処理は，対訳表現が抽出されなくなるまで閾値 $Th$ を
変えずに繰り返される．対訳表現が抽出されなくなれば(7)で $Th$ を下げ，再び(4)から
(6)が繰り返される．

\item[(4)出現回数の数え上げ:] 上記(2)の全ての $(EWS, JWS)$ に対して，閾値$Th$回以上
出現する英語連続単語列($ews$ とする)と日本語連続単語列($jws$ とする)を抽出する．

\item[(5)対応度の計算・対訳表現の抽出:] (4)で抽出された全ての$ews$，$jws$に対して，
\ref{連続単語列間の対応度計算方法}節の評価式にしたがって対応度を計算し，
以下のa),b)の処理を行う．

\begin{description}
\item[a)] $ews$において最大の対応度をもつ $jws'$ を探す．$jws$ において最大の対応度を
もつ $ews'$を探す．
\item[b)] $ews$と$ews'$及び$jws$と$jws'$が同じで，かつ，$jws$と$ews$の対応度が\\
$ sim(jws,ews)\geq \log_{2}Th $ の条件を満たすならば，連続単語列ペア $(ews,jws)$ を
対訳表現とみなして，対訳表現データベースに登録する．
\end{description}

b)の条件の右辺は評価式(ここでは $(\log_{2}f_{je})\cdot \frac{2f_{je}}{f_{j}+f_{e}}$) に
$Th=f_{je}=f_{j}=f_{e}$を代入することにより求められる．
出現回数が閾値 $Th$ の場合，対応度が最高となるのは $Th=f_{je}=f_{j}=f_{e}$ の時である．
したがって，出現回数が閾値 $Th$ より小さい場合，$Th=f_{je}=f_{j}=f_{e}$ の時の
対応度の最高値を超えることはない．この条件を課すことで，閾値 $Th$ を下げた次の段階で
得られる対応度の最高値は，前段階で抽出される対訳表現の対応度の値を越えないことが
保証される．

\item[(6)連続単語列データベースにおける候補の削減:] 以降の処理で新たな対訳表現を
抽出候補としたいため，既に抽出した対訳表現，つまり，(5)で「対訳表現データベース」に
登録された対訳表現 $(ews,jws)$ に関連する連続単語列を連続単語列データベースから
削除する．削除の方法は，連続単語列データベース内の $(EWS,JWS)$ に同時に出現する 
$ews$ と同一の部分をもつ英語連続単語列 $ews$-$ex$ ($ews$と$ews$-$ex$が同一である
場合も含む)と，$jws$と同一の部分をもつ日本語連続単語列 $jws$-$ex$ ( $jws$と
$jws$-$ex$が同一である場合も含む)を$(EWS,JWS)$から削除する．$ews$でなく$ews$と
同一の部分をもつ英語連続単語列 $ews$-$ex$を削除する理由は，$ews$-$ex$は$ews$を
元にして抽出された連続単語列であるためである．$jws$-$ex$を削除する理由も同様である．
(5)において対訳表現が抽出されたならば (4)の処理に戻る．抽出されなければ (7)に進む．

\item[(7)閾値低下による候補拡大:] $ Th > f_{min}$であれば閾値$Th$の値を1つ減らして
(4)から(7)の処理を繰り返す．$Th=f_{min}$ であれば処理を終了する．
$f_{min}$ は抽出対象とする連続単語列の最低出現回数であり，4節の実験では $f_{min}=2$又は
$f_{min}=1$に設定した．

\end{description}

\vspace{5mm}

\noindent
この手法の特徴は，連続単語列ペアの出現回数に対する閾値を設け，その閾値を満足する連
続単語列ペアを対象にして対訳表現を抽出し，閾値を満足する連続単語列ペアがなくなれば，
その閾値を徐々に下げていくという点にある．対応度と連続単語列ペアの出現回数は
相関関係をもつように設定されているので，出現回数に対する閾値を設けて，その値を徐々に
小さくしていくことで，対応度の高い連続単語列ペア，つまり確からしい連続単語列ペアから
順に対訳表現を抽出することができる．また，閾値を下げていき，精度が保証されなくなる
段階で，処理を終了することもできる．

\section{提案手法}
\label{提案}

我々は\ref{従来}節の従来手法に対して， (A)文節区切り情報や品詞情報の利用，
(B)対訳辞書の利用，(C)複数候補の対訳表現が得られた場合の人手による選択，
(D)多対多の対応数を考慮に入れた対応度評価式，(E)対訳文書の分割による漸進的な抽出，
の5つの改良を行った．これらを改良した提案手法の処理の流れ図を
図\ref{提案アルゴリズム}に示す．ステップの番号のカッコ内の数字は，
\ref{抽出アルゴリズム}の抽出アルゴリズムの各ステップの番号に対応している．
図\ref{提案アルゴリズム}の四角の枠で囲まれたステップは，本提案で改良された
ステップである．ステップ(1)-2では，形態素解析と同時に文節区切り処理を行い，
ステップ(2)-1では，その結果を用いて文節を超えないように連続単語列を抽出する．
ステップ(5)-1の対応度の計算では，改良された対応度の評価式を用いる．
ステップ(1)-1，(5)-2，(5)-3-aは，辞書を参照する場合に適用されるステップである．
辞書参照だけでなく，人手による確認も行う場合はステップ(5)-3-aではなく，ステップ
(5)-3-bを用いる．対訳文書の分割による漸進的な抽出は，ステップ(1)-3で対象となる
対訳文書をあらかじめ決められた単位に分割し，ステップ(7)-2で1単位ずつ追加する
ことによって対象とする文書範囲を拡大していくという手法をとる．
以下に，提案アルゴリズムの各処理を説明する．

\begin{figure}[t]
  \begin{center}
    \epsfxsize=9cm \epsfbox{./figure/figure2.ps}
  \end{center}
  \caption{対訳表現抽出方法の流れ図}
  \label{提案アルゴリズム}
\end{figure}

\vspace{5mm}

\begin{description}
\item[(1)-1 対訳辞書の登録:] 既存の対訳辞書を「対訳辞書データベース」に登録する．

\item[(1)-2 形態素解析・文節区切り解析:] 対訳文書 $(E,J)$ を構成する日本語文書及び
英語文書の各対訳文において形態素解析及び文節区切り解析\footnote{英語には「文節」と
いう単位がないため，それに相当する単位を導入する．\ref{文節情報利用}節を参照のこと．}
を行う．形態素解析及び文節区切り解析された各対訳文を$(es,js)$とする． 

\item[(1)-3 対訳文書抽出対象の分割・設定:] 対訳文書 $(E,J)$ を$n$個に分割し，\\
$(E_{1},J_{1}),(E_{2},J_{2}),...,(E_{n},J_{n})$とする．
現時点での分割文書番号 $x$に $1$ をセットする．

\item[(2)-1連続単語列の抽出:] 対訳文書$(E_{x},J_{x})$ 中の各対訳文 $(es,js)$ に対して，
(1)-2の文節区切り解析結果を利用して，文節の境界を超えない範囲で英語単語列の集合 
$EWS$ と日本語単語列の集合 $JWS$ を抽出する．各対訳文 $(es,js)$における$(EWS,JWS)$を
連続単語列データベースに追加する．

\item[(2)-2連続単語列ＤＢにおける候補の削減:] $x=1$であれば，(3)の処理に進む．\\
$x > 1 $であれば，現時点で「対訳表現データベース」に登録されている対訳表現$(ews,jws)$に
関連する連続単語列を連続単語列データベースから削除する．削除の方法は，連続単語列
データベース内の$(ews,jws)$に同時に出現する $ews$ と同一の部分をもつ英語連続単語列 
$ews$-$ex$($ews$と$ews$-$ex$が同一である場合も含む)と，$jws$と同一の部分をもつ
日本語連続単語列$jws$-$ex$($jws$と$jws$-$ex$が同一である場合も含む)を$(EWS,JWS)$から
削除する．(これは，ステップ(6)と同じ処理である．)

\item[(3) 出現回数の閾値の設定:] (4)から(6)の処理で抽出対象となる連続単語列の
出現回数の最低値を閾値$Th$とし，その初期値 $f_{max}$を代入する．
\ref{実験}節の実験での$f_{max}$の値は\ref{抽出アルゴリズム}節の
抽出アルゴリズムに準ずる． 

\item[(4) 出現回数の数え上げ:] 上記(2)-2の全ての $(EWS,JWS)$ に対して，$Th$ 回以上
出現する英語連続単語列($ews$ とする)と日本語連続単語列($jws$ とする)を抽出する． 

\item[(5)-1対応度の計算: ] 抽出された全ての $ews$，$jws$に対して，
\ref{多対多評価式}節の計算方法にしたがって対応度を計算し，以下のa),b)の処理を行う．

  \begin{description}
  \item[a)] $ews$において最大の対応度をもつ$jws'$を探す．
$jws$において最大の対応度をもつ$ews'$を探す．
  \item[b)] $ews$ と$ews'$及び$jws$と$jws'$が同じであり，以下の条件を満たしており，
かつ，連続単語列ペア$(ews,jws)$が「対訳表現除外データベース」に登録されていない
ならば，連続単語列ペア$(ews,jws)$を対訳表現候補とする．\\

{\bf 対応度にDice係数を用いる場合:} $sim(jws,ews) \leq \log_{2} Th $

{\bf 対応度にLog-Likelihoodを用いる場合:}
\[ sim(jws,ews) \geq f_{all}\log f_{all}-(f_{all}-Th)\log(f_{all}-Th)-Th\log Th \]
  \end{description}
             
上記の条件を課す理由は，\ref{従来}節の従来手法の抽出アルゴリズムの(5)と同じである．
本処理は閾値$Th$において複数回繰り返されるが，閾値 において1回目の処理であれば，
(5)-2に進む．2回目以降の処理であれば，(5)-3-aまたは(5)-3-bに進む．

\item[(5)-2 対訳辞書参照による対訳表現の抽出(1回目の処理):] (5)-1で候補とされた
連続単語列ペア$(ews,jws)$において，$ews$を構成する英語自立語単語の集合と，$jws$を
構成する日本語自立語単語の集合を取り出し，両集合間での組合せにおいて，少なくとも
1つの組合せが「対訳辞書データベース」に登録されているならば，連続単語列ペア
$(ews,jws)$を対訳表現と認定し「対訳表現データベース」に登録する．

\item[(5)-3-a 対訳表現の抽出(2回目以降の処理):](5)-1で候補とされた対訳表現候補$(ews,jws)$を
全て対訳表現と認定し「対訳表現データベース」に登録する．

\item[(5)-3-b 人手による対訳表現の確認・選択(2回目以降の処理):] (5)-1で候補とされた
対訳表現候補を対訳表現候補$(ews,jws)$を対応度の高いものから順に並べ替える．
それをファイルに格納し，そのファイルを作業者に提示する．作業者は各対訳表現候補を確認し，
間違った対訳表現候補をファイルから削除する．全ての確認を終了し，ファイルが保存されると，
ファイルに残された候補は「対訳表現データベース」に登録され，ファイルから削除された候補は
「対訳表現除外データベース」に登録される．

\item[(6) 連続単語列DBにおける候補の削減:] (5)-2,(5)-3-a,(5)-3-bで「対訳表現データベース」に
登録された全ての対訳表現 $(ews,jws)$に関連する連続単語列を連続単語列データベースから削除する．
削除の方法は，連続単語列データベース内の$(EWS,JWS)$に同時に出現する$ews$と同一の部分をもつ
英語連続単語列$ews$-$ex$($ews$と$ews$-$ex$が同一である場合も含む)と，$jws$と同一の部分を
もつ日本語連続単語列$jws$-$ex$($jws$と$jws$-$ex$が同一である場合も含む)を$(EWS,JWS)$から
削除する．(5)において対訳表現が抽出されたならば(4)の処理に戻る．抽出されなければ，
ステップ(7)-1に進む．

\item[(7)-1 閾値低下による候補の拡大:] $Th > f_{merge} $であれば，閾値$Th$の値を
1つ下げ，(4)から(7)の処理を繰り返す．
$Th > f_{merge} $でなければ，ステップ(7)-2に進む．
$f_{merge}$は100\%の精度が得られる出現回数の最低値であり，\ref{実験}節の実験では
予備実験の結果から$ f_{merge}=3 $に設定した．

\item[(7)-2 対訳文書対象範囲の拡大:] $ x \neq n $，つまり，抽出対象が全対訳文書で
ないならば，$x$に $1$ を加え，(2)から(7)の処理を繰り返す．一方，$x=n$ かつ 
$Th > f_{min}$であれば，閾値の値を下げて(4)から(7)の処理を繰り返す．$x=n$かつ
$Th=f_{min}$であれば，全処理を終了する．$f_{min}$ は抽出対象とする連続単語列の
最低出現回数であり，\ref{実験}節の実験では，従来手法と同様，$f_{min}=2$又は
$f_{min}=1$を用いた．

\end{description}

\subsection{文節区切り情報や品詞情報の利用}
\label{文節情報利用}

\ref{連続単語列間の対応度計算方法}節の従来手法による抽出結果を
分析すると\footnote{実験データは，先行研究\cite{北村97}で
使用された「取引条件表現辞典例文\cite{石上:1992}を利用した．
\ref{文書性質の実験}節の実験結果の分析も同様である．}「中心に多くの:will become 
the target」「その保有する膨大な:vast inventory」のように，構成する一部の単語の
対応は正しいが，全体では間違っているという対訳表現が数多くみられた．このような
例の多くは，英語や日本語の表現として意味をなさない不適切な単位であることが
多かった．この課題を解決するため，不適切な対訳表現候補を生成しない工夫を施す．
具体的には，以下の処理を行う．

\begin{description}

\item[(a)] 候補となる連続単語列を抽出する時(ステップ(2)-1)，文節区切り情報を用いて，
文節境界の範囲を超える連続単語列候補を生成しない．なお，英語には，文節という単位が
ないため，動詞句，名詞句，前置詞句，副詞句の4つの文の構成成分を文節とする．

\item[(b)] あらかじめ設定された連続単語列内の位置及び品詞の条件をもつ連続単語列は
生成しない．これは，その条件を規則として表現し，その規則に適合する連続単語列は
生成しないことで実現している．現時点での品詞レベルの規則数は英語25規則，日本語45規則，
単語(見出し)レベルの規則数は，英語201規則，日本語309規則である．
\end{description}


\begin{figure}[t]
  \begin{center}
    \epsfxsize=9cm \epsfbox{./figure/figure3.ps}
  \end{center}
  \caption{文節区切りを利用した連続単語列の抽出}
  \label{文節区切り利用連続単語列抽出}
\end{figure}


\noindent
以下に具体例を示す．図\ref{文節区切り利用連続単語列抽出}は日本語文における文節区切り
結果とそれに基づく連続単語列抽出の例である．形態素解析ツールによって区切られた形態素の
区切りを ``/''，係り受け解析ツールによって区切られた文節の区切りを ``//''で表す．
文節区切り情報を利用しない場合「する安全」「決議の諸」などの対訳表現として不適切な
日本語表現が候補となるが，提案手法では文節境界の範囲を超える連続単語列を生成しない
ため，これらの表現は候補とならない．また，(b)の条件を課することによって，文節内の
不適切な表現も生成されない．例えば「の」や「れる」のような一単語のみからなる
助詞や助動詞は生成されないが「決議\_の」や「満たす\_れる」のように，助詞が名詞の
後ろに位置する場合や，助動詞が動詞の後ろに位置する連続単語列は生成される．

上記の処理をすることにより，提案手法では意味的にまとまりをなしていない文字列を除外する
ことができる．

\subsection{対訳辞書の利用}

対訳辞書は，抽出精度向上のための有効な知識である．しかし，対訳辞書を手がかりとして
抽出すると，対訳辞書に登録されていない専門用語などの表現が抽出されなくなる可能性が
ある．そこで，我々は，従来手法のアルゴリズムで抽出された対訳表現に対して，
対訳辞書を参照することによって適切な対訳表現を選り出し，それらを優先的に抽出すると
いう改良を行った．

閾値$Th$において対訳表現が抽出される限り，ステップ(4)から(6)の処理が繰り返されるが，
何回目の処理かによって処理内容を変える．閾値$Th$での処理が1回目の場合，対訳辞書を
参照し，その候補が対訳関係にあると認められれば抽出する．対訳関係か否かの判断は，
連続単語列を構成する英語と日本語の自立語単語の組合せにおいて1つでも対訳辞書登録語が
あれば，対訳関係にあると認定する．この理由は，対訳辞書登録語との完全一致する
連続単語列ペアのみを対訳関係にあると認定すると，対訳関係にある連続単語列ペアは
わずかとなり，辞書参照の効果が得られないためである．

ステップ(6)では，抽出された対訳表現に関する連続単語列候補は削除される．閾値での処理が
2回目以降では，この削除された状態で，対訳辞書を参照せずに対訳表現を抽出する．

このように，対訳辞書の利用は適切な対訳表現を選り出す働きだけでなく，不適切な連続単語列
候補を除外する役割も果たすことができる．

\subsection{複数候補の対訳表現が得られた場合の人手による確認・選択}

次に人手による確認・選択を考える．人手による確認・選択は，作業効率に見合った効果が
得られるかどうかが重要である．その作業が時間や手間がかかるものであれば，最終結果を
人手で取捨選択する作業と変わらない．我々は，ここでも従来手法の抽出アルゴリズムの性質を
利用して，繰り返し処理の途中に人手による確認・選択作業を施す．

具体的には，出現回数の閾値での1回目の処理では，辞書参照による対訳表現抽出を行い，2回目
以降の処理において，ステップ(5)-1で候補とされた対訳表現候補全てに対して，人間が正しいか
どうかを確認する．正しいと判断された候補は「対訳表現データベース」に登録され，一方，
残りの対訳表現は「対訳表現除外データベース」に登録される．

「対訳表現データベース」に登録された連続単語列ペアは，ステップ(6)の連続単語列候補の
削減に利用される．一方「対訳表現除外データベース」に登録された連続単語列ペアは，
ステップ(5)-2で対訳表現を抽出する時に参照され，対訳表現候補から必ず除外される．
　
\subsection{多対多の対応数を考慮した対応度評価式}
\label{多対多評価式}

従来手法では対応度を評価する式として重み付きDice係数が用いられたが，提案手法では，
重み付きDice係数と同様に，原言語と目的言語の単語列の同時出現回数と相関がある
Log-Likelihoodを用いる．さらに，重み付きDice係数やLog-likelihoodに対して，多対多の
対応数を考慮した改良を行う．

Log-Likelihood\cite{Dunning:1993,Matsumoto-Utsuro:2000}は，ある表現Xの出現が
別の表現Yの出現にどの程度強く依存するかを調べるための確率論に基づいた尺度である．
実際の出現事例においてXの出現がYに依存しないという仮説とYの出現/非出現に依存する
という仮説の尤度比で表す．\ref{連続単語列間の対応度計算方法}節の
$w_{J}$,$w_{E}$,$f_{j}$,$f_{e}$.$f_{je}$の前提に加えて，対訳文書が有する
文数を$f_{all}$とすると，以下の式で定義される．

\begin{small}
\begin{eqnarray*}
sim(w_{J},w_{E})&=& f_{je} \log f_{je} + (f_{e}-f_{je}) \log(f_{e}-f_{je}) + (f_{j}-f_{je})\log(f_{j}-f_{je})\\
                 &&+(f_{all}+f_{je}-f_{e}-f_{j})\log(f_{all}+f_{je}-f_{e}-f_{j}) \\
                 &&- f_{e}\log f_{e}-f_{j}\log f_{j}- (f_{all}-f_{j})\log(f_{all}-f_{j})-(f_{all}-f_{e})\log(f_{all}-f_{e})\\
                 &&+(f_{all})\log(f_{all})
\end{eqnarray*}
\end{small}


\begin{figure}[t]
  \begin{center}
    \epsfxsize=9cm \epsfbox{./figure/figure4.ps}
  \end{center}
  \caption{基本アルゴリズムによる抽出結果の例}
  \label{抽出結果の例}
\end{figure}

\noindent
次に，多対多の対応数を考慮した対応度評価式について説明する．\ref{従来}節の
従来手法を用いて抽出した対訳表現を分析した結果，図\ref{抽出結果の例}(a)のように，
日本語連続単語列と英語連続単語列が，対応度が同じで，かつ，多対多の関係で対応付け
られている場合に誤りが多かった．
一方，(b)の例のように，同じ対応度では1対1の対応関係しか持たない場合，その大半は
正しかった．図\ref{抽出結果の例}(a)の現象は，一部の単語が異なり，残りの単語は全て
共通である対訳文が複数存在した場合に起こる．その共通部分において組み合わされる
日本語・英語連続単語列ペアは対応度が等しくなり，多対多の対応関係を有する対訳表現
となる．

上記に述べた多対多の関係を有する対訳表現の抽出を避けるために，Dice係数及び
Log-likelihoodの対応度 $sim(w_{J},w_{E})$に対して，原言語と目的言語の連続単語列が
多対多の関係で対応付けられる場合にはその対応度の値が小さくなるような重み付けを与える．
以降，この対応度を $dsim(w_{J},w_{E})$と表記し，{\bf 多対多の対応数を考慮した対応度}
と呼ぶ．$dsim(w_{J},w_{E})$を以下のように定義する．

\[ dsim(w_{J},w_{E})= \frac{sim(w_{J},w_{E})}{\log_{2}(fw_{J \rightarrow E}+fw_{E \rightarrow J})} \]

$sim(w_{J},w_{E})$ は従来のDice係数やLog-likelihoodによる対応度の値である．
$fw_{J \rightarrow E}$は現段階のステップ(4)で生成された全ての連続単語列において，
日本語単語列$w_{J}$を有する連続単語列ペアの数であり，$fw_{E \rightarrow J}$は，
英語単語列 $w_{E}$を有する連続単語列ペアの数である．

上記の式は，日本語単語列$w_{J}$と英語単語列$w_{E}$からなる連続単語列ペアにおいて，
$w_{J}$が対応する英語単語列の数と，$w_{E}$が対応する日本語単語列の数の和が大きいほど
その値が小さくなるように設定されている．また，$w_{E}$と$w_{J}$が1対1で対応する場合の
値はDice係数やLog-likelihoodから計算される$sim(w_{J},w_{E})$の値と等しくなるように
設定されている．

\subsection{対訳文書の分割による漸進的な抽出}

ステップ(5)の処理における英語と日本語の連続単語列の組み合わせ数は，対訳文書の文数が
多くなるにしたがい増大する．この連続単語列候補の生成を抑えるために，文書分割による
漸進的な抽出手法を提案する．

まず，ステップ(1)-3で対象となる対訳文書をあらかじめ定められた
文数の単位\footnote{何文単位で分割するのが適切かは、\ref{文書分割実験}節の
文書分割による影響の項で議論する．}で分割し，
まず1単位で，100\%の精度が保証される出現回数まで抽出を繰り返す(ステップ(7)-1)．
その単位での処理が終了すれば，さらに1単位を追加して抽出を繰り返す(ステップ(7)-2)．
追加しながら処理を繰り返し行い，抽出対象が対訳文書全体に及べば処理を終了する．

対象とする文を徐々に拡大することで，対象とする文数が少ない初期の段階で抽出された
対訳表現に関する英語・日本語連続単語列を候補から除外することができる．これにより，
抽出対象が拡大された時の連続単語列候補の生成を削減することができる．

\end{document}

