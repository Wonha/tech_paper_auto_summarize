検索エンジンを用いた関連性の測定
  
本章では，Web上の情報を用いて語の関連度を測る手法を提案する．

\subsection{検索エンジンのヒット件数の利用と従来手法の問題点}

検索エンジンのヒット件数を用いて2語の関連度を計算する手法について説明す
る．
ここでは，従来研究で用いられている相互情報量を計算指標として関連度を算
出する．そして，その関連度を検証し，従来手法の問題点について述べる．


具体的な例を使って説明しよう．ここで用いられている手法は，
\cite{Baroni04}のものと同一である\footnote{ただし，Baroniらは検索エンジンとしてAltavistaを用いているが，
Altavistaは日本語に正式に対応していないため，検索エンジンはGoogleを用いた．}．
関連度を測りたい語を，例えば
「インク」「インターレーザー」「プリンタ」「印刷」「液晶」「Aquos」「テレビ」「Sharp」の8語とする．
これらの語群は，Epsonのプリンタであるインターレーザーに関する語と，Sharpの液晶TVであるAquosに関する語であり，
各語の関連度を得ることで，2つのグループを適切に分けたいと仮定する．

表\ref{singlehit}に示しているのは，語群の各語に対して，
検索エンジンによって得られたヒット件数である．
表\ref{cooccur-list}には，語群中の2語を検索エンジンのクエリーとしたときのヒット件数を行列形式にしたものを示す．
例えば，「インク」と「プリンタ」であれば，
\begin{center}
``インク''  \ \ \ ``プリンタ''
\end{center}
をクエリーとして検索エンジンに入力し，そのヒット件数を調べる\footnote{ダブルクオーテーションで囲んでいるのは，
2単語以上からなるフレーズに対しても適切に処理するためである．}．8語に対してこの行列を得るには，$_8 C_2 = 28$回のクエリーが必要となる．

\begin{table}[b]
\begin{center}
\caption{語単独でのヒット件数}
\label{singlehit}
\begin{tabular}{c|c|c|c|c|c|c|c}
プリンタ & 印刷 &  \hspace{-0.5zw} {\footnotesize インターレーザー} \hspace{-0.5zw} & インク & 液晶 & テレビ & Aquos & Sharp \\ \hline
17000000 & 103000000 & 215 & 18900000 & 69100000 & 192000000 & 2510000 & 186000000 \\ 
	\end{tabular}
	\end{center}
    \vspace{\baselineskip}
\caption{2語でのヒット件数の行列}
\label{cooccur-list}
 \setlength{\tabcolsep}{2.5pt}
\begin{tabular}{c|cccccccc|c}
語/語  & プリンタ  & 印刷  
	&  \hspace{-1.3zw} \scalebox{0.6}[1]{インターレーザー} \hspace{-1.3zw}  
	& インク  & 液晶  & テレビ  & Aquos  & Sharp  & 合計 \\ \hline
プリンタ  & 0  & 4780000  & 273  & 4720000  & 4820000  & 5090000  & 201000  & 990000  & 20601273 
\\ 
印刷  & 4780000  & 0  & 183  & 4800000  & 6520000  & 11200000  & 86400  & 1390000  & 28776583 
\\ 
    \hspace{-1.3zw} \scalebox{0.6}[1]{インターレーザー} \hspace{-0.3zw}  
& 273  & 183  & 0  & 116  & 176  & 91  & 0  & 0  & 839 
\\ 
インク  & 4720000  & 4800000  & 116  & 0  & 3230000  & 4950000  & 144000  & 656000  & 18500116 
\\ 
液晶  & 4820000  & 6520000  & 176  & 3230000  & 0  & 18400000  & 903000  & 4880000  & 38753176 
\\ 
テレビ  & 5090000  & 11200000  & 91  & 4950000  & 18400000  & 0  & 840000  & 2830000  & 43310091 
\\ 
Aquos  & 201000  & 86400  & 0  & 144000  & 903000  & 840000  & 0  & 1790000  & 3964400 
\\ 
Sharp  & 990000  & 1390000  & 0  & 656000  & 4880000  & 2830000  & 1790000  & 0  & 12536000 
\\ \hline
合計  & 20601273  & 28776583  & 839  & 18500116  & 38753176  & 43310091  & 3964400  & 12536000 & 166442478 \\ 
\end{tabular}
\end{table}

Baroniらは，この2つの情報を使って求めた相互情報量の値が，語の関連度を示すよい指標になると述べている．
相互情報量は，語$w_a$の出現確率を$p(w_a)$，語$w_b$の出現確率を$p(w_b)$，語$w_a$と語$w_b$の同時出現確率を$p(w_a \cap w_b)$とすると，
\pagebreak
\begin{align}
     \label{MI}
 MI(w_a,w_b) & = \log \frac{ p(w_a \cap w_b) }{p(w_a) p(w_b)}\\ \nonumber
      & = \log \frac{ N n(w_a,w_b)}{n(w_a) n(w_b)}
\end{align}
と表される．
ここで$n(w_a)$は語$w_a$をクエリーとしたときのヒット数，
$n(w_a ,w_b)$は「語$w_a$ 語$w_b$」をクエリーとしたときのヒット数であり，
また，$N$は検索エンジンのクロールした全ページ数である．Baroniら
は$N$を3億5千万ページとしているが，2006年末現在では，Googleは約150億ページ，
AltaVistaは約120億のページである．ここでは$N=100 \times 10^8$とした．


表\ref{mutual}に相互情報量を示す．
「液晶」の行に注目すると，
「液晶」と関連が強いとあらかじめ想定している語は「テレビ」「Aquos」「Sharp」であるが，「プリンタ」や「インターレーザー」との相互情報量が大きく，
「テレビ」や「Sharp」との値は小さくなっており，適切な関連度が算出されていない．

\begin{table}[b]
	\begin{center}
	\caption{相互情報量行列}
	\label{mutual}
	\begin{tabular}{c|cccccccc}
	
語/語 & プリンタ & 印刷 
	& \scalebox{0.8}[1]{インターレーザー} 
	& インク & 液晶 & テレビ & Aquos & Sharp \\ \hline
プリンタ  & 0  & 4.195  & 7.504  & 5.878  & 4.602  & 3.635  & 4.740  & 2.029 \\ 
印刷  & 4.195  & 0  & 5.302  & 4.093  & 3.103  & 2.622  & 2.094  & 0.567 \\ 
    \scalebox{0.8}[1]{インターレーザー}  
	& 7.504  & 5.302  & 0  & 6.542  & 5.663  & 3.981  & 0.000  & 0.000 \\ 
インク  & 5.878  & 4.093  & 6.542  & 0  & 4.096  & 3.501  & 4.301  & 1.512 \\ 
液晶  & 4.602  & 3.103  & 5.663  & 4.096  & 0  & 3.518  & 4.840  & 2.222 \\ 
テレビ  & 3.635  & 2.622  & 3.981  & 3.501  & 3.518  & 0  & 3.746  & 0.655 \\ 
Aquos  & 4.740  & 2.094  & 0.000  & 4.301  & 4.840  & 3.746  & 0  & 4.534 \\ 
Sharp  & 2.029  & 0.567  & 0.000  & 1.512  & 2.222  & 0.655  & 4.534  & 0 \\ \hline

	\end{tabular}
	\end{center}
\end{table}

この原因は，相互情報量が「出現確率の影響を受ける」という特徴を持つためである．
この特徴は式(\ref{MI})を次式のように書き換えるとわかりやすい．
\begin{equation}
MI(w_a,w_b)=\log p(w_a|w_b) - \log p(w_a)
\end{equation}
$p(w_a|w_b)$は語$w_b$が出現するときに語$w_a$と語$w_b$が共起する条件付き確率を表す．
$p(w_a|w_b)$が等しい場合は，$p(w_a)$の出現確率が小さいほど
相互情報量は大きい値になる．この特徴自体は「共起する確率が同じなら，
出現確率の低い語と共起する方が関連性が強い」と考えられるので，問題がない．
しかし，検索エンジンにおいては語によって出現頻度に大きなばらつきがあり，
また全事象を表す$N$が非常に大きいために出現確率の違いによる影響が大きくなり過ぎてしまう．
例えば，「テレビ」のように出現確率の極端に大きい語と他の語の相互情報量が小さくなる．
表\ref{mutual}の「テレビ」の列に注目すると，いずれの語においても「テレビ」との相互情報量が
小さくなっていることが分かる．
実際に表\ref{singlehit}の語のヒット件数と表\ref{mutual}の各行との
相関係数（式\ref{correlation}）は$-0.35$となり，
相互情報量と語の出現確率にやや強い負の相関があることが分かる．
それに対し，表\ref{cooccur-list}の共起ヒット件数と
表\ref{mutual}の相互情報量のとの相関係数は$0.06$となり，
ほとんど相関がないことが分かる．
\begin{equation}
\label{correlation}
r=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}
{\sqrt{\sum_{i=1}^n{(x_i-\bar{x})^2}}{\sqrt{\sum_{i=1}^n{(x_i-\bar{x})^2}}}}
\left(\bar{x} :x_iの平均値\right)
\end{equation}


このように，従来用いられてきた相互情報量は語の出現確率に影響を受けるため，
関連度を測る際に各語の出現確率に数千倍，数万倍といった開きがある場合，
値の信頼性は低くなるという問題がある．
これは，Jaccard係数やdice係数など他の類似度の指標についても当てはまる．







\subsection{$\chi^2$値を用いた関連度の指標}

本論文では，$\chi^2$値を使った関連度の指標を用いる．
$\chi^2$値は，あるデータ集合内での統計的な偏りを表す指標であり，機械翻訳
やコロケーション処理など，
多くの手法で用いられている．
語の関連度としてはCurranらが用いている\cite{Curran02-2}．


$\chi^2$値を関連度に用いるのは，語の出現頻度のばらつきによる
影響を排除するためである．
相互情報量やJaccard係数を関連度に用いる場合の問題点は，語の出現確率に大きな影響を受ける点である．
この問題の解決策として，出現確率を適切に正規化するというアプローチが考えられる．
$\chi^2$値では，語群を構成する語の出現頻度を正規化要素とし，
値の正規化を行ったうえで，共起の偏りを算出するので，出現確率のばらつきによる影響を抑えることができる\cite{Yang97}．
このため，値のばらつきが大きい検索エンジンのヒット件数を用いて関連度を算出する場合，
$\chi^2$値を計算指標として用いることが適切であると考えられる．


対象とする語群の中で，共起の偏りを統計的に調べるために，
1つ1つの語について，語群内の他の語との共起頻度を標本値とし，
「$w_i,w_j\in G$が共起する確率は，語$w_i$と語群$G$内の語が共起する確率と等
しい」という帰無仮説をおいて検定を行う．
語$w_i$と語$w_j$の実際の共起頻度を$n(w_i,w_j)$，
語$w_i$と語群Gの語との共起頻度の和を$\displaystyle S_{w_i}=\sum_{k} n(w_i,w_k)$，
全ての共起頻度の和を$\displaystyle S_G=\sum_{w_i \in G} S_{w_i}$とするとき，
語$w_i$と語$w_j$に関する$\chi^2$値は次式で表される．
\begin{align}
 \chi^2(w_i,w_j) & = \frac{n(w_i,w_j) - E(w_i,w_j)}{E(w_i,w_j)}  \nonumber \\
 E(w_i,w_j) & = S_{w_i} \times \frac{S_{w_j}}{S_G} 
\label{chi}
\end{align}
$E(w_i,w_j)$は語$w_i,w_j$の
共起頻度の期待値を表している．
例えば，語$w_i$を「プリンタ」，語$w_j$を「インターレーザー」とすると，
$n(w_i,w_j)$ は $273$，$S_{w_i}=20601273$，${S_{w_j}}/{S_G}=839/166552478$となる．
表\ref{chilist}は，表\ref{cooccur-list}から計算された$\chi^2$値行列である．
表\ref{chilist}では，「プリンタ」は「印刷」や「インク」と偏って共起している．
また，「インターレーザー」は「プリンタ」との共起が，「Aquos」は「Sharp」との共起が強いなど，良好な結果となっている．


また，表\ref{chilist2}のような，「プリンタ」「液晶」との関連が低いと考えられる4語と「プリンタ」「液晶」の2語
で構成される計6語の語群を与えた場合を考える．
この語群では，表\ref{singlehit}の語群と違い，「プリンタ」と「液晶」の関連性が強いと考えられる．
「プリンタ」の行に注目すると，確かに「プリンタ」と「液晶」の$\chi^2$値が大きくなっており，
語群に基づいた適切な結果が得られている．


\begin{table}[tb]
		\caption{$\chi^2$ 行列}
	\label{chilist}
	\begin{tabular}{c|cccccccc}
	
 語/語 & プリンタ  & 印刷  
	& \hspace{-1.3zw} \scalebox{0.8}[1]{インターレーザー} \hspace{-1.3zw} 
	& インク  & 液晶  & テレビ  & Aquos  & Sharp \\ \hline
プリンタ  & 0.000  & 416649  & 275.5  & 2579092  & 113.8  & 0.000  & 0.000  & 0.000 \\ 
印刷  & 416649  & 0.000  & 9.925  & 801848  & 0.000  & 1840173  & 0.000  & 0.000 \\ 
    \hspace{-1.3zw} \scalebox{0.8}[1]{インターレーザー} \hspace{-0.3zw} 
	& 275.5  & 9.925  & 0.000  & 5.548  & 0.000  & 0.000  & 0.000  & 0.000 \\ 
インク  & 2579092  & 801848  & 5.548  & 0.000  & 0.000  & 3846  & 0.000  & 0.000 \\ 
液晶  & 113.8  & 0.000  & 0.000  & 0.000  & 0.000  & 6858012  & 0.000  & 1317796 \\ 
テレビ  & 0.000  & 1840173  & 0.000  & 3846  & 6858012  & 0.000  & 0.000  & 0.000 \\ 
Aquos  & 0.000  & 0.000  & 0.000  & 0.000  & 0.000  & 0.000  & 0.000  & 7449430 \\ 
Sharp  & 0.000  & 0.000  & 0.000  & 0.000  & 1317796  & 0.000  & 7449430  & 0.000 \\ \hline 

	\end{tabular}
\end{table}

\begin{table}[tb]
	\begin{center}
		\caption{$\chi^2$ 行列-2}
	\label{chilist2}
	\begin{tabular}{c|cccccc}
	 語/語 & プリンタ & 小説 & 液晶 & 紅茶 & バイオリン & 化粧品 \\ \hline
	 プリンタ & 0.000 & 0.000 & 2402760 & 0.000 & 0.000 & 0.000 \\ 
	 小説 & 0.000 & 0.000 & 0.000 & 277513 & 712208 & 19024 \\ 
	 液晶 & 2402760 & 0.000 & 0.000 & 0.000 & 0.000 & 116983 \\ 
	 紅茶 & 0.000 & 277513 & 0.000 & 0.000 & 11149 & 597032 \\ 
	 バイオリン & 0.000 & 712208 & 0.000 & 11149 & 0.000 & 0.000  \\ 
	 化粧品 & 0.000 & 19024 & 116983 & 597032 & 0.000 & 0.000 \\ \hline
	\end{tabular}
	\end{center}
\end{table}


関連度を用いたネットワークに基づくクラスタリング


従来は，確率分布の類似度に基づいた分布クラスタリングの方法を用いて，
関連語をクラスタに分けることが多かった．
本研究では，語の関連度からネットワークを構築し，ネットワークに基づく新
しいクラスタリングの方法を適用する．
関連語ネットワーク上でNewman法によりクラスタリングを行い，
その結果，同じクラスタに分類されたもの同士を関連語として取り出す．
このクラスタリング法は，語の数が大規模になったときにでも適用でき，
対象によってはよいクラスタを生成するので近年注目を集めている．

\subsection{関連語ネットワークの構築}

まず，語の関連性を用いて，語のネットワークを構築する．
ノードが語，エッジが強い関連を表す．
本論文では，これを関連語ネットワークと呼ぶ．

関連語ネットワークは次のように構成される．

\begin{figure}[b]
\begin{center}
  \includegraphics[width=120mm,clip]{network.eps}
 \caption{関連語ネットワーク}
 \label{related-network}
\end{center}
\end{figure}


\begin{enumerate}
\item 語群$G$を与える．
\item 次式により2語$w_i， w_j \in G$の関連度$\chi^2_{w_i, w_j}$を計算する．
\begin{align}
\label{chi2}
 \chi^2_{w_i,w_j} & = \frac{n(w_i,w_j) - E(w_i,w_j)}{E(w_i,w_j)}   \nonumber \\
 E(w_i,w_j) & = S_{w_i} \times \frac{S_{w_j}}{S_G} \\
 S_{w_i} & = \sum_{k} n(w_i,w_k) \nonumber \\
 S_G & = \sum_{w_i \in G} S_{w_i} \nonumber 
\end{align}
\item 各語$w_i \in G$をノードとして配置する．
\item $\chi^2_{w_i,w_j} > 0$のとき，ノード$w_i$，$w_j$間にエッジを張る．
\end{enumerate}


例を図\ref{related-network}に示す．
これは，Webから獲得したコーパス中に高頻度に出現する
計90語をこのネットワークの構成語として用い，
ヒット件数を得る検索エンジンとしてGoogleを用いた関連語ネットワークである．
この関連語ネットワーク上では，関連の強い語同士が近く配置されている．
例えば，図\ref{related-network}の左下には「疾患」「患者」などの医学関連の語が密集している．
また，上部では「アプリケーション」「ファイル」などのコンピュータ関連の語が密集している．
このように関連語ネットワーク上では，関連の強い語同士が密集して存在している．


\subsection{ネットワークに基づくクラスタリング}


従来のシソーラス構築における語のクラスタリングには
確率分布を用いた分布クラスタリング手法が一般的に用いられている．
\cite{Pereira93,Dhillon02}．
また情報検索の分野では，語を属性とする高次元のベクトルを用いた語のクラスタリング手法も多く，
LSAやrandom projectionといった次元を圧縮する手法も有効である\cite{Deerwester90,papadim98}．

一方で，近年ではデータをネットワークとして表した上で，それを分析する
手法が提案され，着目を集めており，語の関係性の分析にも用いられている\cite{Widdows02,motter02,Palla05}．
SigmanはWordNetがネットワーク構造としての性質を持っていることを示し，
WordNetにネットワーク分析の手法を適用できることを示している\cite{sigman02}．

\begin{table}[b]
	\begin{center}
		\caption{階層的クラスタリングで用いられる距離関数$D(c_i,c_j)$}
	\label{Hierarchical}
	\begin{tabular}{|c|c|c|c|}
	\hline
	手法 & 最大距離法 & 最小距離法 & 群平均法  \\ \hline
	$D(c_i,c_j)$ & $\displaystyle \max_{w_k \in c_i, w_l \in c_j} Sim(w_k,w_l)$ & $\displaystyle \min_{w_k \in c_i, w_l \in c_j} Sim(w_k,w_l)$ & 
	$\frac{1}{n_i n_j} \displaystyle \sum_{w_k \in c_i} \sum_{w_l \in c_j} Sim(w_k,w_l)$ \\ \hline
	\end{tabular}
	\end{center}
\end{table}

ネットワークのクラスタリングには，従来，
表\ref{Hierarchical}のように距離関数$D(c_i,c_j)$を定義し（$n_i$はクラスタ$c_i$に含まれる語の数，$Sim(w_k,w_l)$は$語w_k,w_l$の類似度を表す），
距離の近い順に各クラスタをマージしていく階層的クラスタリング手法や，
EMアルゴリズム，NaiveBayesといった機械学習の手法を用いたクラスタリング手法が一般的に用いられてきた．
しかし，ここ数年で新たなクラスタリング手法がいくつも提案されている．
代表例としては，betweennessクラスタリングがあげられる．betweennessクラスタリングは，グラフ\footnote{ネットワークは，エッジに重みや長さなどの数値が付加されているのに対し，
グラフはエッジに数値の付加されていない，接続関係だけを表すものである．}のbetweennessというエッジの媒介性を表
す指標（あるエッジが他のエッジの最短パスにどの程度の割合で含まれているか）
に注目し，できるだけ部分グラフをつなぐようなbetweennessの高いエッジを削除し
ていくことにより，密度の濃いサブグラフを同定する手法である\cite{Newman02}．


これらの手法は高次元のベクトルに対しても有効であり，以前の手法と比べて
高い精度で現実のクラスタ構造を再現することができる．
その反面，時間計算量が大きく，大規模なネットワークに適用することは難しい．
例えば，ネットワークのノード数を$n$，エッジ数を$m$とするとき，
betweennessクラスタリングの時間計算量は$O(n^3)$または$O(m^2n)$であり，ノード数が多いネットワーク上で
betweennessクラスタリングを行うことは困難である．
そこで，本研究では大規模なネットワークにも適用可能な
クラスタリング手法であるNewman法を用いる．


Newman法は，階層的クラスタリング手法の一つであるが，クラスタリングを評価関数$Q$の最大値導出問題に置き換えた手法である\cite{Newman04}．
評価関数$Q$とは，各クラスタの結合度を表す関数であり，$Q$が大きいほど
各クラスタ内の結合が強いことを表している．
Newman法では，$Q$の高い状態がより適切にクラスタリングされた状態であると定義している．
そして，$Q$の最大値を求めることで，
そのネットワークに最適なクラスタリング結果を得ることを目標としている．


評価関数$Q$は次式で表される．
\begin{equation}
\label{newman}
Q=\frac{1}{2m}\left[ \left( \sum_{v,w} A_{vw}\delta(c_v,c_w)\right) - 
\left(\sum_{v,w} \frac{k_v k_w}{2m}\delta(c_v,c_w)\right) \right] 
\end{equation}

$k_v$は頂点vが持っているエッジの本数，$m$は全エッジ本数の合計，
$c_v$は頂点$v$が属しているクラスタを表
している．$\delta(c_v,c_w)$はクロネッカーの$\delta$である．
式(\ref{newman})の第1項において，$A_{vw}$は頂点$v,w$間のエッジの有無を表しており，
また頂点$v,w$が同じクラスタのときのみ，$\delta(c_v,c_w)=1$となる．
つまり，第1項は各クラスタ内に含まれるエッジの本数の合計を表している．
同様に第2項においては，$\frac{k_v k_w}{2m}$は頂点$v,w$間にエッジが引かれる確率を表しているため，
第2項は，各クラスタ内に含まれるエッジの本数の合計の期待値を表している．

すなわち，評価関数$Q$とは，クラスター内に存在するエッジの本数の合計が期待値からどの程度ずれているかを相対的に表した値である．
クラスター内のエッジ本数の和が期待値と同じなら$Q=0$，
それより強いクラスターなら$Q>0$であり，弱いクラスターなら$Q<0$となる．
$Q$が最大であるとき，各クラスター内での結合度が最大であるので，
ネットワーク全体として最も良くクラスタリングされた状態であると考えられる．


しかし$Q$の最大値を求める場合，エッジ数$m$，ノード数$n$のとき，計算量が
$O(n^3)$もしくは$O(m^2n)$となり，大きくなってしまう．
そこでNewman法ではGreedyアルゴリズムを用いて
$Q$の値が極大値をとるようにクラスタリングを行う．
Greedyアルゴリズムなので，「$Q$の変化量$\Delta Q$が最大になるようにクラ
スタ，もしくはノードをマージする」という手順を繰り返していく．
そして「$\Delta Qの最大値<0$」となった時点でクラスタリングを終了とする．
このようにして$Q$の極大値を求めている．
この際，常に「$\Delta Q$が最大になるような2つのクラスタを選んでマージ」
するため，クラスタがマージされていく順序は一意であり，
初期条件によってクラスタリングの結果は変化しない．
また，クラスタ数を任意に制御したい場合は，終了条件を
$\Delta Q < 0$ではなくクラスタ数にすることも可能である．

Newman法とbetweennessクラスタリングを比較すると，NewmanらによりNewman法は
betweennessクラスタリングとほぼ同じ精度のクラスタリング結果が得られることが示されている．
また，Newman法の時間計算量は$O((m+n)n)$もしくは$O(n^2)$であり，
時間計算量が$O(m^2n)$あるいは$O(n^3)$であるbetweennessクラスタリングと比べ，
計算量が少なく，高速な手法となっている．
そのため，Newman法はノード数やエッジ数が大きい大規模ネットワークに適用可能である．

\subsection{Newman法による関連語の獲得}

語群$G$を用いてシソーラスを構築する場合，Newman法を用いて関連語を同定する
手順は次のようになる．

\begin{enumerate}
\item 検索エンジンのヒット件数と$\chi^2$値を用いて語群$G$の語の関連度を算出する．
\item 関連度をもとに語群$G$を構成語とする関連語ネットワークを構築する．
\item 1つの語を1つのクラスタとする．
\item ある2つのクラスタが1つのクラスタになったと仮定して，$Q$の変化量
      $\Delta Q$（式\ref{deltaq}）を計算する．

\item (4)を全てのクラスタの組み合わせについて行う．
\item $\Delta Q$が最大となるような2つのクラスタをマージし，1つのクラスタとする．ただし，
      最大の$\Delta Q<0$なら(8)へ．
\item マージしたクラスタの$e_{ij},a_i$を再計算し，(4)に戻る．
\item 同じクラスタに属している語を関連語とみなす．
\end{enumerate}
\begin{align}
\label{deltaq}
 \Delta Q_{ij} & = 2(e_{ij}-a_i a_j) \nonumber \\
 e_{ij} & = クラスタi，j間のエッジの本数（割合）  \\
 a_i & = \sum_{i} e_{ii} \nonumber 
\end{align}


議論
語の関連は，相対的なものである．候補となる語群によって，あるときは関連した語同士でも，他の場合には関連していないこともあり得る．
ある語群において全ての語同士の関連度が分かっている
とき，どの語とどの語を関連語と見なすかは，関連度によって規定される語の関
係性によると考えられる．語の関連性を図\ref{ex-clustering}のようなネット
ワーク図（ノード間の距離を（1/語の関連度）とおく）で可視化すると，図
\ref{ex-clustering}-aのような時
は部分集合A,B,Cそれぞれが，関連語の集まった関連語群であると言える．
同様に図\ref{ex-clustering}-bであれば，部分集合A,B,C,Dそれぞれが関連語群
であると言える．このように語のネットワー
ク上で周囲と比べて密度が高くなっている部分を抽出することで，各語の関連語
を同定することができる．

\begin{figure}[b]
 \begin{center}
  \includegraphics[width=8cm,clip]{ex-clustering.eps}
 \end{center}
 \caption{クラスタリングによる関連語の同定}
  \label{ex-clustering}
\end{figure}


Webは非常に多様性に富んだテキストから構成されている．
したがって，目的に合わせた語の関連性を得るには，
Webから適切な文書集合を切り出した上で，
その文書集合内での関連度を求めるという方法が考えられる．これには，
検索クエリーに特定の検索語(keyword spice)を加える方法が有効であろう\cite{Oyama04}．


本論文では，関連語ネットワーク上のエッジには重みを与えていないが，語の関連性が多値的であることを考えると，
重みを考慮する必要がある．ただし，既存のNewman法は重みのあるネットワークに対応していない．
そこで，重みを扱えるようにNewman法を改良することで，重みつきのネットワーク上でクラスタリングを行うことが考えられる．
語の関連性を「関連がある，ない」の2値ではなく，重みという多値で扱うこと
で，クラスタ数の自動取得も含めて，より適切なクラスタリング結果が
得られることが予想される．


加えて，Newman法では1語が1つのクラスタリングにしか所属できないハードクラスタリングであるため，
語の持つ多義性を解消することができない，という問題点がある．
しかし，Newman法をもとにしたソフトクラスタリングの手法も提案されており\cite{Reichard04}，
この手法を関連語ネットワークに適用することで語の多義性を解消できると考えられる．


また本研究では，同義・類義，上位語・下位語，連想語をすべて関連語としたが，
こういった語を関係性を分類していくことも重要であろう．
こういった研究には，前置詞を手がかりとして語の関係性を同定する\cite{Litkowski02}の手法があるが，
これを検索エンジンを利用していかに効率的に行うかは今後の検討課題のひとつである．



