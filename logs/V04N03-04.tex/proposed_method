従来の研究

統計的手法に基づき，
言語の比較を計量的に行う研究は，従来から広く行われてきている．
Kroeber および Chr\'{e}tien は，1930 年代に，
音韻や語形等の言語的特徴から言語間の相関係数を求め，
これに基づきインド・ヨーロッパ諸言語 9 ヶ国語およびヒッタイト語
の間の類似性を求める研究を行っている\cite{Kroeber37,Kroeber39}．
また，クラスター分析に基づき，自動的に言語や方言を分類する研究
に関しても，いくつかの先行研究がある．
文献\cite{Yasumoto95Book}には，これらの研究の古典的な諸方法が概説されている．
また，数学的手法に基づく言語のクラスタリングに関する最近の研究として，
Batagelj らの研究\cite{Batagelj92}があり，
そこでは 65 ヵ国語の言語に対するクラスタリング結果が示されている．

ここで，従来研究では，言語間の距離(あるいは類似度)を導入するために，
どのような方法が用いられてきたかを若干紹介する．
文献\cite{Yasumoto95Book}の４章で述べられている方法では，
インド・ヨーロッパ諸言語の一致度を調べるために，
２つの言語の対応する数詞の最初の子音が一致しているか否かを
調べている．
たとえば，ドイツ語とスペイン語の数詞 ``1'' は
それぞれ ``eins'' および ``uno'' であるが，これらの２単語において，
最初に出現する子音は共に ``n'' であるので，
数詞 ``1'' に関しては，ドイツ語とスペイン語は一致していると考える．
10 個の数詞のうち，何個の数詞について最初の子音が一致しているかを
調べ，これをもとに言語間の距離を導入している．

また，Batagelj らの研究では，２つの言語の対応する単語の
文字列間距離に基づき，言語間の距離を定義している．
いま，２つの文字列 $u$ および $v$ が与えられたとき，
文字列 $u$ 中に文字を追加したり，あるいは $u$ 中の文字を削除したりして，
文字列 $u$ を文字列 $v$ に変換することを考える．
このとき，文字列 $u$ を文字列 $v$ に変換するために必要な
最小の追加および削除文字数で，２つの文字列間の距離を定義する．
たとえば，$u = \mbox{``belly''}$, $v = \mbox{``bauch''}$ に対しては，
まず $u$ から ``elly'' を削除し，次に ``auch'' を追加することにより，
$u$ を $v$ に変換することができるので，
この場合の文字列間距離は８(削除４文字＋追加４文字)である．
以上の文字列間距離を各言語から抽出した 16 個の単語について求め，
これらの距離の和により，言語間距離を定義している
\footnote
{
Batagelj らは，文字の追加・削除に加え置換を考慮した文字列間距離など，
他の文字列間距離についても議論しているが，本稿では省略する．
}．

以上のように，従来の研究では，
あらかじめ人間が言語を分類する上で有用であると思われる
音韻や語形等の言語的特徴を抽出したり，
あるいは比較のための基礎語彙を選定するなどの作業が必要であった．
また，言語間距離の定義にも恣意的な部分が残されていたと
いうことができる．

確率モデルに基づく言語のクラスタリング

本稿で提案する方法の概略を図 \ref{Fig:OurApproach} に示す．
この方法では，まず各言語の言語データから
確率的言語モデルを自動的に学習し，
次に確率モデル間に距離を導入することにより，言語間の距離を定義する．
このように，本稿の方法は，自己組織的(self-organizing)であり，
あらかじめ人間が各言語の言語的特徴を抽出したり，
基礎語彙を選定する必要はない．
また，本稿の方法の利点として，
各言語のデータを独立に選ぶことができるという点をあげることができる．
たとえば，言語によって違うジャンルのテキストであったり，
あるいはデータのサイズが異なっていても，
これらのデータの揺れを確率モデルの中に吸収することができる．

確率モデルとしては，様々なものが考えられるが，
４節で述べる評価実験では文字の trigram モデルを用いた．
trigram モデルは，$N$-gram モデルの特別な場合($N=3$ の場合)であり，
以下では $N$-gram モデルについて簡単に説明する．
$N$-gram モデルに関する詳細な説明は，
たとえば文献\cite{Jelinek90,Kita96Book}などを参照せよ．

\begin{figure}
\begin{center}
  \atari(80,98)
\end{center}
\caption{確率モデルに基づく言語のクラスタリング}
\label{Fig:OurApproach}
\end{figure}

\subsection{$N$-gram モデル}

たとえば，英語では文字 q には文字 u が後続するとか，
ドイツ語においては文字 c に後続するのは h や k であるなど，
文字の連鎖には確率・統計的な性質が存在する．
$N$-gram モデルは，このような文字の連鎖をモデル化するために適した
確率モデルである．

文字の $N$-gram モデルは，文字の生起を $N-1$ 重マルコフ過程により
近似したモデルであり，
文字の生起は直前に出現した $N-1$ 文字にのみ依存すると考える．
すなわち，
$n$ 文字から成る文字列 $c_{1}, \cdots ,c_{n}$ に対し，
\begin{equation}
        P(c_n|c_1, \cdots, c_{n-1}) \approx P(c_n|c_{n-N+1}, \cdots, c_{n-1})
        \label{Eq:NgramDef}
\end{equation}
となる．

$N$-gram モデルを用いた場合，文字列 $c_{1}, \cdots ,c_{n}$ の生成確率は，
次のようにして計算することができる．
\begin{eqnarray}
        P(c_1, \cdots, c_n) & = & \prod_{i=1}^{n} P(c_i|c_1, \cdots ,c_{i-1}) \nonumber\\
                & \approx & \prod_{i=1}^{n} P(c_i|c_{i-N+1}, \cdots, c_{i-1})
\end{eqnarray}
上式において，最初の等式は，確率論の基本定理から導かれる．
また，２番目の近似式は，式(\ref{Eq:NgramDef})による．

いま，文字列 $c_{1}, \cdots ,c_{n}$ が言語データ中に出現する回数を
$F(c_{1} \cdots c_{n})$ で表すことにする．
$N$-gram の確率は，言語データ中に出現する文字の $N$ 個組と $(N-1)$ 個組の
出現回数から，次のように推定することができる．
\begin{equation}
        P(c_n|c_{n-N+1}, \cdots, c_{n-1})
        = \frac{F(c_{n-N+1}, \cdots, c_n)}{F(c_{n-N+1}, \cdots, c_{n-1})}
        \label{Eq:NgramTraining}
\end{equation}
$N$ の値が大きい場合には，統計的に信頼性のある確率値を
コーパスから推定することが難しくなるため，
通常は $N=3$ あるいは $N=2$ のモデルが用いられることが多い．
なお，$N=3$ の場合を trigram モデル，
$N=2$ の場合を bigram モデル，
$N=1$ の場合を unigram モデルと呼ぶ．

\subsection{$N$-gram モデルのスムージング}

$N$-gram の確率値は，式(\ref{Eq:NgramTraining})に示すように，
言語データ中の文字列の頻度から推定することができる．
しかし，与えられた言語データが少ない場合には，
精度のよい確率値を推定することが難しくなる．
この問題に対処するために，我々の実験では，線形補間法と呼ばれる方法を用いて，
$N$-gram モデルのスムージング(平滑化)を行った．

線形補間法では，
$N$-gram の確率値を低次の $M$-gram $(M < N)$ の確率値と線形に補間する．
trigram の場合には，次のようになる．
\begin{equation}
  P(c_{n}|c_{n-2} c_{n-1})
 = \lambda_{1} P(c_{n}|c_{n-2} c_{n-1})
   + \lambda_{2} P(c_{n}|c_{n-1}) + \lambda_{3} P(c_{n})
\label{Eq:NgramLinearInterpolation}
\end{equation}
ここで，$\lambda_{1},\lambda_{2},\lambda_{3}$ は，
それぞれ trigram, bigram, unigram に対する重み係数であり，
$\displaystyle \sum_{i} \lambda_{i} = 1$ となるように設定される．
式(\ref{Eq:NgramLinearInterpolation})の補間では，
学習データ中に三つ組 $c_{n-2}, c_{n-1}, c_{n}$ が出現しない場合には，
bigram と unigram から $P(c_{n}|c_{n-2}, c_{n-1})$ の値を推定している．
二つ組 $c_{n-1}, c_{n}$ も出現しない場合には，
unigram の値によって近似している．
なお，$\lambda_{i}$ の値は，削除補間法と呼ばれる方法によって推定した．
削除補間法については，
たとえば文献\cite{Jelinek80,Kita96Book}などを参照されたい．


\subsection{言語モデル間の距離}

次に，言語モデル間に距離を導入する．
我々の用いた距離は，文献\cite{Juang85,Rabiner93}において提案されて
いるものと同一である．
上記文献においては，隠れマルコフ・モデル(Hidden Markov Model; HMM)間の距離として
定義されているが，一般の言語モデルに対しても同様に用いることができる．

いま，言語 $L_1$ および言語 $L_2$ の言語データとして，
それぞれ $D_1$，$D_2$ が与えられているとする．
$D_i$ $(i=1,2)$ は，文字列データであり，
その長さ(文字数)を $|D_i|$ と表記する．
また，言語データ $D_i$ から作成された言語モデルを $M_i$ で表す．

まず，言語モデル $M_1$ および $M_2$ に対し，
距離尺度 $d_0(M_1, M_2)$ を次のように定義する．
\begin{equation}
        d_0(M_1, M_2) = \frac{1}{|D_2|}
                \left [
                \log P(D_2|M_2) - \log P(D_2|M_1)
                \right ]
        \label{Eq:DistanceMeasure0}
\end{equation}
式(\ref{Eq:DistanceMeasure0})では，
言語 $L_1$ と $L_2$ の間の距離を，
言語 $L_1$ のモデル $M_1$ からデータ $D_2$ が生成される確率と，
言語 $L_2$ のモデル $M_2$ から同一のデータ $D_2$ が生成される確率の差に
基づいて決めている．
もし，言語 $L_1$ と $L_2$ が類似していれば，
モデルからのデータの生成確率も似た値になるので距離は小さくなるし，
類似していなければ，データの生成確率が大きく違うので距離は大きくなる．

式(\ref{Eq:DistanceMeasure0})は，言語モデル $M_1$ および $M_2$ に対し，
非対称である(すなわち $d_0(M_1, M_2) \neq d_0(M_2, M_1)$)．
対称形にするために，$d_0(M_1, M_2)$ と $d_0(M_2, M_1)$ の平均を取る．
従って，言語モデル $M_1$ と $M_2$ の間の距離 $d(M_1, M_2)$ は，
最終的に次のように定義される．
\begin{equation}
        d(M_1, M_2) = \frac{d_0(M_1, M_2) + d_0(M_2, M_1)}{2}
        \label{Eq:DistanceMeasure}
\end{equation}

