名詞句の指示性とは名詞句の対象への指示の仕方である．
まず名詞句を，その名詞句の類の成員すべてか類自体を指示対象とする総称名詞句と，類の成員の一部を指示対象とする非総称名詞句に分ける．
次に，非総称名詞句を指示対象が確定しているか否かで，定名詞句と不定名詞句に分ける(図[REF_fig:sijisei_bunrui])．
[t]
\fbox{
[c]{220pt}
{ \tiny
}
}
総称名詞句は，その名詞句が意味する類に属する任意の成員（単数でも，複数でも，不可算のものでもよい）のすべて，もしくはその名詞句が意味する類それ自身を指示する．
例えば，次の文([REF_eqn:doguse])の「犬」は総称名詞句である．
ここでの「犬」は「犬」という類に属する成員のすべてを指示対象としている．
定名詞句は，その名詞句が意味する類に属する文脈上唯一の成員（単数でも複数でも不可算のものでもよい）を指示する．
例えば，次の文([REF_eqn:thedoguse])の「その犬」は定名詞句である．
ここでの「その犬」は，「犬」という類に属する文脈上唯一の成員を指示対象としている．
このことは，指示詞「その」によって表わされており，聞き手は「その犬」なるものを確定できる．
不定名詞句は，その名詞句が意味する類に属するある不特定の成員（単数でも複数でも不可算のものでもよい）を指示する．
不特定の成員を指示するというのは，現時点での聞き手の情報ではその名詞句が成員のどれを指し示すのか確定していないという意味である．
また，現時点での聞き手の情報では，その名詞句が成員のどれを指し示しているとしても，その文の解釈として間違っていないということでもある．
不定名詞句は総称名詞句とは異なり，その名詞句の意味する類の成員のすべてを指示するのではなくて，その名詞句の意味する類の成員の一部を指示する．
次の文の「犬」は不定名詞句である．
ここでの「犬」は犬という類に属する任意の三匹の成員を指示対象として持ちえる．
これはどんな犬でも三匹いればこの文が使えるということである．
先行研究[CITE]では，「可能性」と「得点」という二つの評価値を用い，人手で作成した規則により，各指示性に「可能性」と「得点」を与えこの評価値により指示性を推定していた．
各規則によって与えられる「可能性」と「得点」は，「可能性」については指示性ごとにANDをとり，「得点」については指示性ごとに足し算を行なう．
その結果，「可能性」が存在し「得点」の合計が最も大きい指示性を解であると推定していた．
[t]
\fbox{
[c]{220pt} \protect(規則の適用条件)
\protect{不定(可能性得点)
\protect定(可能性得点)
\protect総称(可能性得点)}
}
規則は図[REF_fig:rule_kouzou_sijisei]の構造をしており，図の「規則の適用条件」には，その規則が適用されるかどうかの条件として，文中の手がかりとなる表現を記述する．
各分類には「可能性」と「得点」を一つずつ与えている．
「可能性」は1か0のみであり，「得点」は0から10の間の整数である．
「可能性」が1の分類がただ一つ求まった場合は，その分類を推定の結果とする．
「可能性」が1の分類が複数ある場合は，その中で「得点」の合計が最も大きい分類を推定の結果としていた．
この推定方法では，「得点」だけでなく「可能性」という評価値も用いている．
これは，人手での調整を軽減するために，確実に決まりそうなところは「可能性」によって確実に決め「得点」の調整を不要にするためであった．
規則は86個作成していた．
全規則については文献[CITE]を参照のこと．
主要なものをいくつか以下に示す．
指示詞(「この」や「その」など)によって修飾される時，
{不定名詞句(0 0) 定名詞句(1 2) 総称名詞句(0 0)}
(例文)この本はおもしろい.
(訳文) This book is interesting.
名詞句につく助詞が「は」で述語が過去形の時，
{不定名詞句(1 0) 定名詞句(1 3) 総称名詞句(1 1)}
(例文)犬は向こうに行きました．
(訳文) The dog went away.
名詞句につく助詞が「は」で述語が現在形の時，
{不定名詞句(1 0) 定名詞句(1 2) 総称名詞句(1 3)}
(例文)犬は役に立つ動物です．
(訳文) Dogs  are useful animals.
他にも，(i)「地球」「宇宙」のような名詞句自身から定名詞句と推定する規則，(ii)名詞句に数詞がかかることから総称名詞句以外と推定する規則，(iii)同一名詞の既出により定名詞句と推定する規則，(iv)「いつも」「昔は」「〜では」のような副詞が動詞にかかることから総称名詞句と推定する規則，(v)「〜が好き」「〜を楽しむ」のような動詞から総称名詞句と推定する規則，(vi)「用」「向き」のような接尾辞から総称名詞句と推定する規則などがある．
手がかりとなる語がない時は不定名詞句と推定するようにしている
例として，次の文の中に現れる名詞句「我々が昨日摘みとった果物」に注目し，これにどのような規則が適用され得点がどのようになるか，具体的に説明する．
我々が昨日摘みとった果物は味がいいです．
The fruit that we picked yesterday tastes delicious.
以下のように七つの規則が適用され，この「果物」は定名詞句と推定された．
名詞句につく助詞が「は」で述語が現在形の時，
(果物は味がいいです．
)
{不定名詞句(1 0) 定名詞句(1 2) 総称名詞句(1 3)}
述部が過去形の節が係る時，
(摘みとった)
{不定名詞句(1 0) 定名詞句(1 1) 総称名詞句(1 0)}
「は」か「が」がついた定名詞句を含む節が係る時，
(我々が)
{不定名詞句(1 0) 定名詞句(1 1) 総称名詞句(1 0)}
助詞がついた定名詞句を含む節が係る時，
(我々が)
{不定名詞句(1 0) 定名詞句(1 1) 総称名詞句(1 0)}
代名詞を含む節が係る時，
(我々が)
{不定名詞句(1 0) 定名詞句(1 1) 総称名詞句(1 0)}
名詞句につく助詞が「は」で述語が形容詞の時，
(果物は味がいいです．
)
{不定名詞句(1 0) 定名詞句(1 3) 総称名詞句(1 4)}
主要部の名詞が普通名詞の時，
(果物)
{不定名詞句(1 1) 定名詞句(1 0) 総称名詞句(1 0)}
これらすべての規則の適用の結果として「果物」の最終の「可能性」と「得点」は，
{不定名詞句(1 1) 定名詞句(1 9) 総称名詞句(1 7)}
となり，定名詞句と推定される．
このような解析を，対象とする文章の各名詞句について最初のものから順番に決定的に行なっていく．
既に推定された指示性は後続の名詞句の解析において手がかりとして用いられる場合がある(例：上記の規則(c),(d))．
先行研究での推定方法はおおよそ以上のとおりである．
この方法では，「可能性」と「得点」という二つの評価値をうまく解析できるように人手で付与する必要があり，人手の介入が大きいものとなっている．
規則の条件部分にある，解析に効果のある手がかり表現については人手で収集するのも有効かもしれないが，「可能性」と「得点」の二つの評価値についてはなんらかの機械学習手法で解決できるのではないかと考えた．
そこで，本稿では次節で述べるような手法を利用することで，「可能性」と「得点」の二つの評価値を人手でふる必要性をないものとした．
本稿での指示性の推定は教師あり機械学習手法に基づいて行なう．
機械学習手法としては，正解の名詞句の指示性を付与した大規模なコーパスを作成するのはコストが大きく困難であるので，データスパースネスに強い最大エントロピー法を利用することにした．
最大エントロピー法とは，分類先の推定において，素性(解析に用いられる情報の細かい単位のこと)を定義しておくと，学習データから素性の各出現パターンに対して各分類になる確率を求めるもので，この確率を求める際に，エントロピーを最大にする操作を行なうため，この方法は最大エントロピー法と呼ばれている．
このエントロピーを最大にする操作は，確率モデルを一様にする効果を示し，このことが最大エントロピー法がデータスパースネスに強い理由とされている．
最大エントロピー法の詳細な説明は付録[REF_sec:me]最大エントロピー法(文献\protect[CITE]より)で行なっている．
本研究の最大エントロピー法の利用では，文献[CITE]のシステムを用いた．
解析は，そのシステムの出力から総称名詞句，定名詞句，不定名詞句の三つの確率を計算し，その確率の大きいものが解であると推定することによって行なう．
最大エントロピー法の利用においては学習に用いる素性が必要となる．
学習に用いる素性としては，先行研究で用いていた86個の人手で作成した規則の条件部を用いた．
このため，学習に用いる素性の個数は86個となる．
例えば，先にあげた[REF_sec:decide_pre]節の三つの規則1〜3だと，条件部分だけを取り出して以下のような三つの素性が得られる．
指示詞(「この」や「その」など)によって修飾されるか．
名詞句につく助詞が「は」で述語が過去形か．
名詞句につく助詞が「は」で述語が現在形か．
最大エントロピー法によってどのように指示性が解析されるかを，前節であげた以下の同じ例文で具体的に説明する．
我々が昨日摘みとった果物は味がいいです．
The fruit that we picked yesterday tastes delicious.
前節と同じように「我々が昨日摘みとった果物」に注目する．
規則としては前節と同じように以下の七つの規則が適用される．
規則の指示性の各分類につけてある数は，各規則だけが適用される場合のその分類になる条件確率のことで，学習コーパスから最大エントロピー法によって計算される値である．
(ここで付与している値は実際に[REF_sec:jikken]節の機械学習2において得られたものである．
)
名詞句につく助詞が「は」で述語が現在形の時，
(果物は味がいいです．
)
{不定名詞句  0.31, 定名詞句  0.29, 総称名詞句  0.40}
述部が過去形の節が係る時，
(摘みとった)
{不定名詞句  0.31, 定名詞句  0.49, 総称名詞句  0.19}
「は」か「が」がついた定名詞句を含む節が係る時，
(我々が)
{不定名詞句  0.19, 定名詞句  0.61, 総称名詞句  0.19}
助詞がついた定名詞句を含む節が係る時，
(我々が)
{不定名詞句  0.01, 定名詞句  0.80, 総称名詞句  0.18}
代名詞を含む節が係る時，
(我々が)
{不定名詞句  0.20, 定名詞句  0.44, 総称名詞句  0.37}
名詞句につく助詞が「は」で述語が形容詞の時，
(果物は味がいいです．
)
{不定名詞句  0.13, 定名詞句  0.80, 総称名詞句  0.07}
主要部の名詞が普通名詞の時，
(果物)
{不定名詞句  0.72, 定名詞句  0.15, 総称名詞句  0.14}
最大エントロピー法を用いた方法では，上記の規則についている値を分類ごとに掛け合わせ，それらを正規化した結果が最も大きい分類を求める分類先とする(ここでのかけ算と正規化の演算は最大エントロピー法では付録[REF_sec:me]の式([REF_eq:p])の演算を行なっていることに相当する．
)．
この場合で，すべての規則にふられた数値を掛け合わせて正規化(各分類の数値を足すと1になるように)すると，
{不定名詞句0.001, 定名詞句0.996, 総称名詞句0.002}
となり，定名詞句の値が最も大きく定名詞句と正しく推定される．
文章全体での解析の流れは，先行研究と全く同じで，対象とする文章の各名詞について最初のものから順番に決定的に指示性の推定を行なっていく．
本付録では読者の便を考え最大エントロピー法について説明している．
本付録の最大エントロピー法の説明は文献[CITE]での説明を一部改変のうえそのまま引用している．
一般に確率モデルでは，文脈(観測される情報のこと)とそのときに得られる出力値との関係は既知のデータから推定される確率分布によって表される．
いろいろな状況に対してできるだけ正確に出力値を予測するためには文脈を細かくする必要があるが，細かくしすぎると既知のデータにおいてそれぞれの文脈に対応する事例の数が少なくなりデータスパースネスの問題が生じる．
最大エントロピー法では，文脈は素性と呼ばれる個々の要素によって表され，確率分布は素性を引数とした関数として表される．
そして，各々の素性はトレーニングデータにおける確率分布のエントロピーが最大になるように重み付けされる．
このエントロピーを最大にするという操作によって，既知データに観測されなかったような素性あるいはまれにしか観測されなかった素性については，それぞれの出力値に対して確率値が等確率になるようにあるいは近付くように重み付けされる．
このため最大エントロピー法はデータスパースネスに強いとされている．
このモデルは例えば言語現象などのように既知データにすべての現象が現れ得ないような現象を扱うのに適したモデルであると言える．
以上のような性質を持つ最大エントロピー法では，確率分布の式は以下のように求められる．
文脈[MATH]で出力値[MATH]となる事象[MATH]の確率分布[MATH]を最大エントロピー法により推定することを考える．
文脈[MATH]は[MATH]個の素性[MATH]の集合で表す．
そして，文脈[MATH]において，素性[MATH]が観測されかつ出力値が[MATH]となるときに1を返す以下のような関数を定義する．
{
}これを素性関数と呼ぶ．
ここで，[MATH]は，文脈[MATH]において素性[MATH]が観測されるか否かによって1あるいは0の値を返す関数とする．
次に，それぞれの素性が既知のデータ中に現れた割合は未知のデータも含む全データ中においても変わらないとする制約を加える．
つまり，推定するべき確率分布[MATH]による素性[MATH]の期待値と，既知データにおける確率分布[MATH]による素性[MATH]の期待値が等しいと仮定する．
これは以下の制約式で表せる．
{
}ここで，[MATH]，[MATH]は，[MATH]，[MATH]をそれぞれ既知データにおける事象[MATH]，[MATH]の出現頻度として以下のように推定する．
{
}
次に，式([REF_eq:constraint])の制約を満たす確率分布[MATH]のうち，エントロピー{
}を最大にする確率分布を推定するべき確率分布とする．
これは，最も一様な分布となる．
このような確率分布は唯一存在し，以下の確率分布[MATH]として記述される．
{
}ただし，{
}であり，[MATH]は素性関数[MATH]のパラメータである．
このパラメータは文脈[MATH]のもとで出力値[MATH]となることを予測するのに素性[MATH]がどれだけ重要な役割を果たすかを表している．
訓練集合が与えられたとき，パラメータの推定にはImproved Iterative Scaling(IIS)アルゴリズム[CITE]などが用いられる．
学習コーパスから実際に式([REF_eq:p])の確率分布を求めるために，われわれはRistadのツール[CITE]を使っている．
名詞句の指示性とは名詞句の対象への指示の仕方である．
まず名詞句を，その名詞句の類の成員すべてか類自体を指示対象とする総称名詞句と，類の成員の一部を指示対象とする非総称名詞句に分ける．
次に，非総称名詞句を指示対象が確定しているか否かで，定名詞句と不定名詞句に分ける(図[REF_fig:sijisei_bunrui])．
[t]
\fbox{
[c]{220pt}
{ \tiny
}
}
総称名詞句は，その名詞句が意味する類に属する任意の成員（単数でも，複数でも，不可算のものでもよい）のすべて，もしくはその名詞句が意味する類それ自身を指示する．
例えば，次の文([REF_eqn:doguse])の「犬」は総称名詞句である．
ここでの「犬」は「犬」という類に属する成員のすべてを指示対象としている．
定名詞句は，その名詞句が意味する類に属する文脈上唯一の成員（単数でも複数でも不可算のものでもよい）を指示する．
例えば，次の文([REF_eqn:thedoguse])の「その犬」は定名詞句である．
ここでの「その犬」は，「犬」という類に属する文脈上唯一の成員を指示対象としている．
このことは，指示詞「その」によって表わされており，聞き手は「その犬」なるものを確定できる．
不定名詞句は，その名詞句が意味する類に属するある不特定の成員（単数でも複数でも不可算のものでもよい）を指示する．
不特定の成員を指示するというのは，現時点での聞き手の情報ではその名詞句が成員のどれを指し示すのか確定していないという意味である．
また，現時点での聞き手の情報では，その名詞句が成員のどれを指し示しているとしても，その文の解釈として間違っていないということでもある．
不定名詞句は総称名詞句とは異なり，その名詞句の意味する類の成員のすべてを指示するのではなくて，その名詞句の意味する類の成員の一部を指示する．
次の文の「犬」は不定名詞句である．
ここでの「犬」は犬という類に属する任意の三匹の成員を指示対象として持ちえる．
これはどんな犬でも三匹いればこの文が使えるということである．
先行研究[CITE]では，「可能性」と「得点」という二つの評価値を用い，人手で作成した規則により，各指示性に「可能性」と「得点」を与えこの評価値により指示性を推定していた．
各規則によって与えられる「可能性」と「得点」は，「可能性」については指示性ごとにANDをとり，「得点」については指示性ごとに足し算を行なう．
その結果，「可能性」が存在し「得点」の合計が最も大きい指示性を解であると推定していた．
[t]
\fbox{
[c]{220pt} \protect(規則の適用条件)
\protect{不定(可能性得点)
\protect定(可能性得点)
\protect総称(可能性得点)}
}
規則は図[REF_fig:rule_kouzou_sijisei]の構造をしており，図の「規則の適用条件」には，その規則が適用されるかどうかの条件として，文中の手がかりとなる表現を記述する．
各分類には「可能性」と「得点」を一つずつ与えている．
「可能性」は1か0のみであり，「得点」は0から10の間の整数である．
「可能性」が1の分類がただ一つ求まった場合は，その分類を推定の結果とする．
「可能性」が1の分類が複数ある場合は，その中で「得点」の合計が最も大きい分類を推定の結果としていた．
この推定方法では，「得点」だけでなく「可能性」という評価値も用いている．
これは，人手での調整を軽減するために，確実に決まりそうなところは「可能性」によって確実に決め「得点」の調整を不要にするためであった．
規則は86個作成していた．
全規則については文献[CITE]を参照のこと．
主要なものをいくつか以下に示す．
指示詞(「この」や「その」など)によって修飾される時，
{不定名詞句(0 0) 定名詞句(1 2) 総称名詞句(0 0)}
(例文)この本はおもしろい.
(訳文) This book is interesting.
名詞句につく助詞が「は」で述語が過去形の時，
{不定名詞句(1 0) 定名詞句(1 3) 総称名詞句(1 1)}
(例文)犬は向こうに行きました．
(訳文) The dog went away.
名詞句につく助詞が「は」で述語が現在形の時，
{不定名詞句(1 0) 定名詞句(1 2) 総称名詞句(1 3)}
(例文)犬は役に立つ動物です．
(訳文) Dogs  are useful animals.
他にも，(i)「地球」「宇宙」のような名詞句自身から定名詞句と推定する規則，(ii)名詞句に数詞がかかることから総称名詞句以外と推定する規則，(iii)同一名詞の既出により定名詞句と推定する規則，(iv)「いつも」「昔は」「〜では」のような副詞が動詞にかかることから総称名詞句と推定する規則，(v)「〜が好き」「〜を楽しむ」のような動詞から総称名詞句と推定する規則，(vi)「用」「向き」のような接尾辞から総称名詞句と推定する規則などがある．
手がかりとなる語がない時は不定名詞句と推定するようにしている
例として，次の文の中に現れる名詞句「我々が昨日摘みとった果物」に注目し，これにどのような規則が適用され得点がどのようになるか，具体的に説明する．
我々が昨日摘みとった果物は味がいいです．
The fruit that we picked yesterday tastes delicious.
以下のように七つの規則が適用され，この「果物」は定名詞句と推定された．
名詞句につく助詞が「は」で述語が現在形の時，
(果物は味がいいです．
)
{不定名詞句(1 0) 定名詞句(1 2) 総称名詞句(1 3)}
述部が過去形の節が係る時，
(摘みとった)
{不定名詞句(1 0) 定名詞句(1 1) 総称名詞句(1 0)}
「は」か「が」がついた定名詞句を含む節が係る時，
(我々が)
{不定名詞句(1 0) 定名詞句(1 1) 総称名詞句(1 0)}
助詞がついた定名詞句を含む節が係る時，
(我々が)
{不定名詞句(1 0) 定名詞句(1 1) 総称名詞句(1 0)}
代名詞を含む節が係る時，
(我々が)
{不定名詞句(1 0) 定名詞句(1 1) 総称名詞句(1 0)}
名詞句につく助詞が「は」で述語が形容詞の時，
(果物は味がいいです．
)
{不定名詞句(1 0) 定名詞句(1 3) 総称名詞句(1 4)}
主要部の名詞が普通名詞の時，
(果物)
{不定名詞句(1 1) 定名詞句(1 0) 総称名詞句(1 0)}
これらすべての規則の適用の結果として「果物」の最終の「可能性」と「得点」は，
{不定名詞句(1 1) 定名詞句(1 9) 総称名詞句(1 7)}
となり，定名詞句と推定される．
このような解析を，対象とする文章の各名詞句について最初のものから順番に決定的に行なっていく．
既に推定された指示性は後続の名詞句の解析において手がかりとして用いられる場合がある(例：上記の規則(c),(d))．
先行研究での推定方法はおおよそ以上のとおりである．
この方法では，「可能性」と「得点」という二つの評価値をうまく解析できるように人手で付与する必要があり，人手の介入が大きいものとなっている．
規則の条件部分にある，解析に効果のある手がかり表現については人手で収集するのも有効かもしれないが，「可能性」と「得点」の二つの評価値についてはなんらかの機械学習手法で解決できるのではないかと考えた．
そこで，本稿では次節で述べるような手法を利用することで，「可能性」と「得点」の二つの評価値を人手でふる必要性をないものとした．
本稿での指示性の推定は教師あり機械学習手法に基づいて行なう．
機械学習手法としては，正解の名詞句の指示性を付与した大規模なコーパスを作成するのはコストが大きく困難であるので，データスパースネスに強い最大エントロピー法を利用することにした．
最大エントロピー法とは，分類先の推定において，素性(解析に用いられる情報の細かい単位のこと)を定義しておくと，学習データから素性の各出現パターンに対して各分類になる確率を求めるもので，この確率を求める際に，エントロピーを最大にする操作を行なうため，この方法は最大エントロピー法と呼ばれている．
このエントロピーを最大にする操作は，確率モデルを一様にする効果を示し，このことが最大エントロピー法がデータスパースネスに強い理由とされている．
最大エントロピー法の詳細な説明は付録[REF_sec:me]最大エントロピー法(文献\protect[CITE]より)で行なっている．
本研究の最大エントロピー法の利用では，文献[CITE]のシステムを用いた．
解析は，そのシステムの出力から総称名詞句，定名詞句，不定名詞句の三つの確率を計算し，その確率の大きいものが解であると推定することによって行なう．
最大エントロピー法の利用においては学習に用いる素性が必要となる．
学習に用いる素性としては，先行研究で用いていた86個の人手で作成した規則の条件部を用いた．
このため，学習に用いる素性の個数は86個となる．
例えば，先にあげた[REF_sec:decide_pre]節の三つの規則1〜3だと，条件部分だけを取り出して以下のような三つの素性が得られる．
指示詞(「この」や「その」など)によって修飾されるか．
名詞句につく助詞が「は」で述語が過去形か．
名詞句につく助詞が「は」で述語が現在形か．
最大エントロピー法によってどのように指示性が解析されるかを，前節であげた以下の同じ例文で具体的に説明する．
我々が昨日摘みとった果物は味がいいです．
The fruit that we picked yesterday tastes delicious.
前節と同じように「我々が昨日摘みとった果物」に注目する．
規則としては前節と同じように以下の七つの規則が適用される．
規則の指示性の各分類につけてある数は，各規則だけが適用される場合のその分類になる条件確率のことで，学習コーパスから最大エントロピー法によって計算される値である．
(ここで付与している値は実際に[REF_sec:jikken]節の機械学習2において得られたものである．
)
名詞句につく助詞が「は」で述語が現在形の時，
(果物は味がいいです．
)
{不定名詞句  0.31, 定名詞句  0.29, 総称名詞句  0.40}
述部が過去形の節が係る時，
(摘みとった)
{不定名詞句  0.31, 定名詞句  0.49, 総称名詞句  0.19}
「は」か「が」がついた定名詞句を含む節が係る時，
(我々が)
{不定名詞句  0.19, 定名詞句  0.61, 総称名詞句  0.19}
助詞がついた定名詞句を含む節が係る時，
(我々が)
{不定名詞句  0.01, 定名詞句  0.80, 総称名詞句  0.18}
代名詞を含む節が係る時，
(我々が)
{不定名詞句  0.20, 定名詞句  0.44, 総称名詞句  0.37}
名詞句につく助詞が「は」で述語が形容詞の時，
(果物は味がいいです．
)
{不定名詞句  0.13, 定名詞句  0.80, 総称名詞句  0.07}
主要部の名詞が普通名詞の時，
(果物)
{不定名詞句  0.72, 定名詞句  0.15, 総称名詞句  0.14}
最大エントロピー法を用いた方法では，上記の規則についている値を分類ごとに掛け合わせ，それらを正規化した結果が最も大きい分類を求める分類先とする(ここでのかけ算と正規化の演算は最大エントロピー法では付録[REF_sec:me]の式([REF_eq:p])の演算を行なっていることに相当する．
)．
この場合で，すべての規則にふられた数値を掛け合わせて正規化(各分類の数値を足すと1になるように)すると，
{不定名詞句0.001, 定名詞句0.996, 総称名詞句0.002}
となり，定名詞句の値が最も大きく定名詞句と正しく推定される．
文章全体での解析の流れは，先行研究と全く同じで，対象とする文章の各名詞について最初のものから順番に決定的に指示性の推定を行なっていく．
本付録では読者の便を考え最大エントロピー法について説明している．
本付録の最大エントロピー法の説明は文献[CITE]での説明を一部改変のうえそのまま引用している．
一般に確率モデルでは，文脈(観測される情報のこと)とそのときに得られる出力値との関係は既知のデータから推定される確率分布によって表される．
いろいろな状況に対してできるだけ正確に出力値を予測するためには文脈を細かくする必要があるが，細かくしすぎると既知のデータにおいてそれぞれの文脈に対応する事例の数が少なくなりデータスパースネスの問題が生じる．
最大エントロピー法では，文脈は素性と呼ばれる個々の要素によって表され，確率分布は素性を引数とした関数として表される．
そして，各々の素性はトレーニングデータにおける確率分布のエントロピーが最大になるように重み付けされる．
このエントロピーを最大にするという操作によって，既知データに観測されなかったような素性あるいはまれにしか観測されなかった素性については，それぞれの出力値に対して確率値が等確率になるようにあるいは近付くように重み付けされる．
このため最大エントロピー法はデータスパースネスに強いとされている．
このモデルは例えば言語現象などのように既知データにすべての現象が現れ得ないような現象を扱うのに適したモデルであると言える．
以上のような性質を持つ最大エントロピー法では，確率分布の式は以下のように求められる．
文脈[MATH]で出力値[MATH]となる事象[MATH]の確率分布[MATH]を最大エントロピー法により推定することを考える．
文脈[MATH]は[MATH]個の素性[MATH]の集合で表す．
そして，文脈[MATH]において，素性[MATH]が観測されかつ出力値が[MATH]となるときに1を返す以下のような関数を定義する．
{
}これを素性関数と呼ぶ．
ここで，[MATH]は，文脈[MATH]において素性[MATH]が観測されるか否かによって1あるいは0の値を返す関数とする．
次に，それぞれの素性が既知のデータ中に現れた割合は未知のデータも含む全データ中においても変わらないとする制約を加える．
つまり，推定するべき確率分布[MATH]による素性[MATH]の期待値と，既知データにおける確率分布[MATH]による素性[MATH]の期待値が等しいと仮定する．
これは以下の制約式で表せる．
{
}ここで，[MATH]，[MATH]は，[MATH]，[MATH]をそれぞれ既知データにおける事象[MATH]，[MATH]の出現頻度として以下のように推定する．
{
}
次に，式([REF_eq:constraint])の制約を満たす確率分布[MATH]のうち，エントロピー{
}を最大にする確率分布を推定するべき確率分布とする．
これは，最も一様な分布となる．
このような確率分布は唯一存在し，以下の確率分布[MATH]として記述される．
{
}ただし，{
}であり，[MATH]は素性関数[MATH]のパラメータである．
このパラメータは文脈[MATH]のもとで出力値[MATH]となることを予測するのに素性[MATH]がどれだけ重要な役割を果たすかを表している．
訓練集合が与えられたとき，パラメータの推定にはImproved Iterative Scaling(IIS)アルゴリズム[CITE]などが用いられる．
学習コーパスから実際に式([REF_eq:p])の確率分布を求めるために，われわれはRistadのツール[CITE]を使っている．
