関連した研究

近年, 大量のコーパスが利用可能になったことを背景に, コーパスから得られ
た情報を用いて語義の曖昧性を解消する研究が多数行なわれている 
\cite[など]{Brown1991,Schutze1992,Zernik1991,Yarowsky1992,Niwa1994}.

Yarowsky らは, Roget のシソーラスカテゴリを利用し, 統計手法を用いるこ
とでテキスト中に現れる多義語の曖昧性を解消する手法を提案した.  彼らの
手法は, 統計情報を用いてシソーラスカテゴリに出現する単語に重み付けを行
なった後, その結果を利用して多義語の周辺語の重みの和から多義語がどのシ
ソーラスカテゴリに属するかを決定するというものである.  この手法を12 の
多義語名詞に適用し実験を行なった結果, 平均解消率92\% という高い正解率
が得られることが報告されている \cite{Yarowsky1992}.  しかし, Yarowsky 
らのシソーラスを用いる問題として, データスパースネスの問題が指摘されて
いる.  すなわち, シソーラスカテゴリに示されている語が抽象的な語で定義
されているため, 文書の種類によっては, その語が文書に出現しない場合があ
る\cite{Niwa1995}. また, Yarowskyらは彼らの手法が動詞の多義解消につい
ては名詞と同様の正解率が得られないことを指摘している.

丹羽らは, 文脈を構成する単語をベクトルで表現し, 文脈をそれらベクト
ルの和で表した.  任意の文脈 A における単語の意味は, 多義の各意味を表す
文脈例を各意味に応じてあらかじめ用意しておき, 各々の例と文脈 A におけ
る単語の意味との類似度(内積)を計算し, その値が最も大きい文脈が示す意味
であるとした.  この手法を名詞の多義判定に適用した結果, 平均80\%の正
解率が得られている \cite{Niwa1994}.

Brown らは対訳テキストを用い, 一方の言語の語義の曖昧性を他方の語の情報
を利用することで解消する手法を提案している \cite{Brown1991}.  彼らは実
際に英仏機械翻訳システムにこの手法を適用し, 検証を行なっている.  しか
し彼らは問題点として, (1) 多義語の持つ意味を予め高々2つに限定している. 
(2) 語が, ターゲット言語の2つの異なる訳に翻訳できないとき, 語義の解消
ができない. (3) 膨大な対訳テキストを必要とする, を挙げている.

Zernik や Sch\"{u}tze らは, 動詞の多義を判定するための情報として名詞と
動詞の共起関係を利用している.  任意の動詞がどの意味を持つかは, 動詞と
共起する名詞の集合に応じて決定される.  しかし, 名詞の集合を意味に応じ
て分割する処理は人手で行なっているため, 語の分類は人間の言語的な直観に
頼ることになってしまう.

本稿では, Yarowsky らがシソーラスカテゴリを利用しているのに対し, 単一
言語コーパスから抽出した動詞の語義情報を利用し, 文中に含まれる多義語の
曖昧性を解消する手法について述べる.  我々の手法は名詞の集合を意味に応
じて人手により分割する Zernik や Sch\"{u}tzeらの手法, と異なり, 多義解
消に必要な情報は, 与えられた多義語を含む動詞グループに対し, クラスタリ
ングアルゴリズムを適用することで自動的に得られるため, 人間の介在を必要
としない.  また, Brown らが多義語の持つ意味を予め高々2つに限定している
のに対し, 本手法では, 多義語を含む動詞グループに対し, クラスタリングア
ルゴリズムを適用するため, 2つ以上の意味を持つ語に対しても曖昧性の解消
が可能である.


多義解消に必要な情報の抽出

一般に, 意味的に近い2つの動詞は同じ名詞と共起して現れる.  

\vspace*{2mm}
\begin{tabular}{ll}
(s1) &\parbox[t]{12cm}{In the past, however, coke has typically \underline{taken} a
minority \underline{stake} in such ventures.} \\
(s1') &\parbox[t]{12cm}{Guber and Peters tried to \underline{buy} a
\underline{stake} in Mgm in 1988.} \\
\end{tabular}

\begin{tabular}{ll}
(s2) &\parbox[t]{12cm}{That process of sorting out specifies is likely
to \underline{take} \underline{time}.}  \\
(s2') &\parbox[t]{12cm}{We \underline{spent} a lot of \underline{time}
and money in building our group of stations.} \\
\end{tabular}
\vspace*{2mm}

\noindent
例えば, {\it Wall Street Journal} から抽出した例文(s1)$\sim$(s2')において, (s1), (s1')に現れる\underline{take} と
\underline{buy} は共に
\underline{stake} と共起して現れ, ほぼ同じ意味を持つ  \cite{Liberman1991}.  同様に(s2),
(s2')に現れる$\!$\underline{take}$\!$と$\!$\underline{spend}$\!$は共に$\!$\underline{time}$\!$と共起して現れ, 両者は同じ意味を持つ.  従って
多義語$\!$\underline{take}$\!$がもつ複数の意味は, 各意味に対応した動詞
\underline{buy}, \underline{spend}と共起して現れる名詞
\underline{stake}, \underline{time}と特徴づけて考えることができる.  す
なわち, 多義語を含む文において, もし多義語と共起する名詞のうち少なくと
も一つが多義語の意味を特徴づける名詞と同じ(あるいは名詞の集合に属する)
ならば, 文中の多義語の意味はその名詞と共起する動詞の意味に同定すること
ができる.  我々は文中に現れる多義語の曖昧性を, その語と共起する名詞を用いるこ
とで解消した.  

以下, 3.1節では仮想動詞について述べる.  3.2節では語の意味的な偏差を計
算する手法について述べ, 3.3節では3.2節で述べた偏差の値を用いてクラスタ
リングを行なうためのアルゴリズムについて説明する.  多義語を含む動詞グルー
プに対し, クラスタリングアルゴリズムを適用することで多義語の各意味を示
す動詞(仮想動詞)と共起する名詞の集合が, 動詞の個数分得られる.  3.4節で
は仮想動詞, 及びそれと共起する名詞との相互情報量を求める手法
について述べる.  仮想動詞と名詞の相互情報量は, 文中に現れる名詞が複数
の(名詞の)集合に含まれる場合にどの集合に含まれるかを一意に決定するため
に用いられる.



\subsection{仮想動詞}

本手法では, 多義を判定しながら意味的なクラスタリングを行なうことで多義
語の曖昧性解消に必要な情報, すなわち, 多義語の意味を特徴づける名詞の集合
を抽出する.  そこで, 表層上は一つの要素である多義語を, 多義が持つ各意
味がまとまった複数要素であると捉え, これを一つ一つの意味に対応させた要
素 (本稿ではこの要素を{\bf 仮想動詞}ベクトルと呼ぶ)に分解した上でクラスタを作成する
という手法を用いた.

我々は, 動詞をベクトルと捉え, 動詞と共起する$n$個の名詞を軸とする$n$次
元名詞空間上でこれを表した. 軸$i$(1 $\leq$ $i$ $\leq$ $n$)における動詞
ベクトルの長さは, $i$軸で示される名詞と動詞の相互情報量\hspace{-0.15mm}\cite{Church1990}\hspace{-0.1mm}の値を用いた. 仮に2つの動詞に多義性がなく, かつこの2
つの動詞が意味的に近いとすると, これらの動詞はこの空間上で互いに距離が
近いため, 同一のクラスタに含まれることになる. 一方, \hspace{-0.05mm}(s1)\hspace{-0.05mm}と\hspace{-0.05mm}(s2)\hspace{-0.05mm}に現れ
る\underline{take}は多義であるため, 各意味を表す動詞
ベクトル \underline{buy}, \underline{spend}のいずれともクラス
タを構成しなければならない.  そこで, ベクトル \underline{take}を各軸
に従って (この場合, \underline{stake}と\underline{time}の2軸) 分割する
ことを考える.  ベクトル \underline{take}を
\underline{stake}と\underline{time}の軸に従って分割した結果を図\ref{cluster1}に示す.

{
\begin{figure}[htbp]
\centerline{
\epsfile{file=cluster1.eps,height=45mm}}
\caption{ベクトル take の分割} \label{cluster1}
\begin{center}
\vspace*{-5mm}
Figure 1 The decomposition of the verb \underline{take} 
\end{center}
\end{figure}
}

\noindent
図\ref{cluster1}において, ベクトル \hspace{-0.2mm}\underline{take}は, \underline{stake}\hspace{-0.2mm}と\hspace{-0.2mm}\underline{time}の軸上でベクトル \hspace{-0.2mm}{\sf take1}\hspace{-0.3mm}と\hspace{-0.2mm}{\sf take2}に分割されている.  {\sf take1}と{\sf take2}を{\bf 仮想動詞}
ベクトルと呼ぶ.  図\ref{cluster1}は
仮想動詞ベクトルを導入することで, 各々意味的に近い要素を持つ2つのクラスタ 
\{{\sf take1}, buy\},
\{{\sf take2}, spend\}が得られることを示す.


\subsection{動詞グループの偏差}

クラスタリングアルゴリズムは動詞グループの意味的な偏差を比較し, 偏差の
少ない順にクラスタを生成する.  今$m$個から成る動詞グループをVG = \{$v_{1}$,
$\cdots$, $v_{m}$\}とすると, VG の偏差$Dev(\mbox{VG})$は式(\ref{22})で示される.  ただ
し, $n$ は動詞と共起する名詞の個数とする.

\begin{eqnarray}
Dev(\mbox{VG}) & = &
\frac{1}{\mid\bar{g}\mid(\beta \ast m +
\gamma)}\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}(v_{ij}-\bar{g_{j}})^{2}} \label{22}
\end{eqnarray}

\noindent
(\ref{22})の$\bar{g_{j}}$ \ \hspace{-0.3mm}( \hspace{-0.3mm}= \ \hspace{-0.3mm}$\frac{1}{m}\sum_{i=1}^{m} v_{ij}$ )は, $j$軸での重心の値を示す. また, $\mid$ $\bar{g}$ $\mid$ \ \hspace{-0.3mm}( \hspace{-0.5mm}= \ \hspace{-0.5mm}$\frac{1}{m}\sqrt{\sum_{j=1}^{n}(\sum_{i}^{m}v_{ij})^{2}}$ ) は重心ベクトルの長さを示す.  (\ref{22})の$v_{ij}$は,

\begin{equation}
v_{ij}  = \left\{ \begin{array}{ll}
	 Mu(v_{i},n_{j}) & \mbox{if $Mu$($v_{i}$,$n_{j}$) $\geq$ $\alpha$} \\
		 0 &\mbox{otherwise} 
	\end{array} \right. \label{v}
\end{equation}

\noindent
とする.  ここで, $Mu(v_{i},n_{j})$は動詞$v_{i}$(1 $\leq$ $i$ $\leq$
$m$)と名詞$n_{j}$(1 $\leq$ $j$ $\leq$ $n$)の相互情報量の値を表し, 式
(\ref{church_mu})で示される.

\begin{eqnarray}
Mu(x,y) & = & log_{2}\frac{P(x,y)}{P(x)P(y)} \label{church_mu}
\end{eqnarray}

\noindent
$P(x)$, $P(y)$ は, $x$, $y$の頻度数$f(x)$, $f(y)$をそれぞれコーパスに
出現する語の総数$N$で正規化したものであり, \hspace{0.1mm}$P(x,y)$は$x$\hspace{-0.1mm}と\hspace{-0.1mm}$y$の共起頻
度数$f(x,y)$を\hspace{-0.1mm}$N$\hspace{-0.1mm}で正規化したものである. また, 式(\ref{v})における$\alpha$は閾値とする.  式(\ref{22})の\hspace{-0.1mm}$\beta$ $\ast$ $m$ + $\gamma$ は, 
動詞の偏差を示す値が動詞の個数に比例して増加することを防ぐために最小2
乗法を用いて行なった正規化である\footnote{{\it Wall Street Journal} を
用いた実験では, $\alpha$ を 3に設定し, $\beta$, $\gamma$ それぞれ 
0.964, -0.495 を得た.}.  式(\ref{22})はその値が小さいほどより偏差が少
ないことを示す.


\subsection{クラスタリング手法}


クラスタリングアルゴリズムは, non-overlapping と overlapping アルゴリ
ズムに大別できる.  本手法はoverlapping クラスタリングアルゴリズムに含
まれる.  Overlapping アルゴリズムの代表的なものとして 
$B_{k}$ ($k$ = 1,2,$\cdots$) 手法がある\cite{Jardine1968}. 

本手法と$B_{k}$手法との違いは, $B_{k}$手法では要素が複数のクラスタに属
すか否かは$k$の個数に依存して決まるのに対し, 我々の手法は, 複数のクラ
スタに属すか否かを判定する条件をアルゴリズムの中に導入している点が異な
る.  我々の手法では, 動詞ベクトルを分割して仮想動詞ベクトルを作成し, 
\ その仮想動詞ベクトルを含
むクラスタの偏差を比較することで, \ 要素が 複数のクラスタに属すか否か, す
なわち多義であるかどうかの判定を行なっている.  例えば,
\underline{take} が\underline{buy}\hspace{-0.1mm}と\underline{spend}\hspace{-0.1mm}の意味
を持つかどうかを判定するために, ベクトル\hspace{-0.1mm}\underline{take}\hspace{-0.1mm}を\underline{stake}\hspace{-0.1mm}と
\underline{time}\hspace{-0.1mm}の軸に従い分割し, 仮想動詞ベクトル {\sf take1}と{\sf take2}を
作成する.  \underline{take} が多義であるか否かは, \{{\sf take1},
buy\}, \{{\sf take2}, spend\} 及び, \{take,buy,spend\} のクラスタの偏差
を比較することにより決定される.

\subsubsection{Splitting と Lumping}

今 $v$と$w_{p}$を動詞とし, $w_{1}$, $\cdots$, $w_{n}$ を動詞, または仮想動詞とする.
また, $Dev(v, w_{i})$ $\leq$ $Dev(v, w_{j})$ (1 $\leq$ $i$ $\leq$ $j$
$\leq$ $n$) かつ, $Dev(v, w_{1})$ $\leq$ $Dev(v, w_{p})$とする.  本手
法では$v$が$w_{1}$と$w_{p}$ で示される2つの意味を持つか否かを判定する
ために, (\ref{split})と(\ref{lump})で示されるクラスタを作成し, それぞ
れの偏差を比較する.

\begin{eqnarray}
\{v_{x}, w_{p}\}, \{v_{y}, w_{1}, \cdots, w_{n}\} \label{split} \\
\{v, w_{1}, \cdots, w_{p}, \cdots, w_{n}\} \label{lump}
\end{eqnarray}

\noindent 
ただし, (\ref{lump})の $w_{1}$, $\cdots$, $w_{p}$, $\cdots$, $w_{n}$ 
は $Dev(v, w_{i})$ $\leq$ $Dev(v, w_{j})$ (1 $\leq$ $i$ $\leq$ $j$
$\leq$ $n$)を満たすとする.  (\ref{split})の $v_{x}$ と $v_{y}$ は$v$の
仮想動詞を示す.  以下では, (4)で示されるクラスタを作成するために,  \hspace{-0.1mm}$v$,  \hspace{-0.1mm}$w_{1}$, \hspace{-0.1mm}$w_{p}$を入力とし, 仮想動詞$v_{x}$\hspace{-0.1mm}と\hspace{-0.1mm}$v_{y}$を出力する関数 
$split$, 及び, \hspace{-0.1mm}(5)で示されるクラスタを作成する過程で仮想動詞$v_{x}$,
$v_{y}$が現れた場合にそれらをマージする関数 $lump$ を定義する.

\begin{enumerate}

\item 関数 $split$ は入力 $v$,
$w_{1}$, $w_{p}$に対し, $v_{x}$ と $v_{y}$ を出力する.  ただしベクトル 
$v$ は, ($v_{1}$, $\cdots$, $v_{n}$)で示されるとする.


\begin{eqnarray}
split(v,w_{p},w_{1}) & = & (v_{x},v_{y}) \label{sp} \\
\mbox{where} \ \ \ Dev(v, w_{1}) & \leq & Dev(v, w_{p}) 
\end{eqnarray}

\vspace*{-2mm}
\[ 
	v_{x}  = \left[ \begin{array}{l}
		v_{x1} \\
		v_{x2} \\
		\vdots \\
		v_{xn} 
		\end{array} \right] \  \hbox{s.t.} \ \
v_{xj} \  = \left\{ \begin{array}{lll}
	v_{j}  &\mbox{if $w_{pj}$ $\neq$ 0} &(8)   \nonumber \\
	0 & \mbox{otherwise} \hspace*{2.2cm} &(8') \nonumber
	\end{array}
           \right. 
\]
\[
 v_{y}  = \left[ \begin{array}{l}
		v_{y1} \\
		v_{y2} \\
		\vdots \\
		v_{yn} 
		\end{array} \right] \  \hbox{s.t.} \ \
v_{yj} \  = \left\{ \begin{array}{lll}
	v_{j}  &\mbox{if ($w_{1j}$ $\neq$ 0 or $w_{pj}$ = $w_{1j}$ =
0)} &(9)  \nonumber \\
	0 & \mbox{otherwise} &(9')   \nonumber
	\end{array}
           \right. 
\]
\addtocounter{equation}{2}

式(8), (9) において $v$と共起する$n_{j}$が, $w_{p}$と$w_{1}$の両方と共
起する場合には, $v_{xj}$ と $v_{yj}$ は共に $v_{j}$ \ = \
$Mu(v,n_{j})$ とした. また式(9) において $v$ と共起する$n_{j}$ が,
$w_{1}$ と $w_{p}$のいずれとも共起しない場合には, $v_{yj}$ の値は 
$v_{j}$ の値とした.  これは, $v_{j}$ が $v_{x}$ と$v_{y}$の両方に含ま
れない場合, \{$v_{y}$, $w_{1}$\} の偏差は常に, \{$v_{x}$, $w_{p}$\}よりも小さくな
る.  よって, $v_{x}$ と $v_{y}$ の偏差をできるだけ均等にするため,
$v_{yj}$の値は, $v_{j}$の値とした.

\item 関数 $lump$ は仮想動詞 $v_{x}$ と $v_{y}$ を入力とし $w$を出
力する.


\vspace*{-5mm} 
\begin{eqnarray}
lump(v_{x}, v_{y}) & = & w \label{lu}
\end{eqnarray}

\vspace*{-7mm} 
\begin{equation}
\hspace*{-3mm}  w = \left[ \begin{array}{l}
		w_{1} \\
		w_{2} \\
		\vdots \\
		w_{n} 
		\end{array} \right] \ \hbox{s.t.} \ w_{j}
\ = \left\{ \begin{array}{ll}
	v_{xj} + v_{yj}   &\mbox{if $v_{xj}$$\neq$$v_{yj}$} \\
	v_{xj}  & \mbox{if $v_{xj}$ = $v_{yj}$}
	\end{array}
           \right. 
\end{equation}

\end{enumerate}

\noindent
実験では, (\ref{split})で示される二つのクラスタの偏差の値が共に
(\ref{lump})で示されるクラスタの偏差の値よりも小さい場合に動詞$v$は多義
とみなした.


\subsubsection{クラスタリングアルゴリズム}


クラスタリングアルゴリズムの流れを図\ref{flow_algo} に示す.  図\ref{flow_algo} の`('
はその上で示される関数の処理を示す.

{\begin{figure}[htbp]
\begin{center}
\fbox{
\parbox{130mm}{
{\bf begin} \\ 
\hspace*{5mm}ICS := {\sf Make-Initial-Cluster-Set}(VG) \\
\[  \left( \begin{array}{l}
\mbox{VG} = \{v_{i} \mid i = 1,\cdots,m\} \\ 
\mbox{ICS} = \{Set_{1}, \cdots, Set_{\frac{m(m-1)}{2}}\} \\
ただし \ Set_{p} = \{v_{i},v_{j}\} \ と \ Set_{q} = \{v_{k},v_{l}\} \in
\mbox{ICS} \ (1 \leq p < q \leq m) は \hspace*{2cm}\\
Dev(v_{i},v_{j}) \leq Dev(v_{k},v_{l}) を満たす.
\end{array} \right. \]
\hspace*{5mm}{\bf for} \ $i$ := 1 {\bf to} $\frac{m(m-1)}{2}$ {\bf do} \\
\hspace*{10mm}
\parbox{125mm}{
{\bf if} \ CCS = $\phi$ \\
\hspace*{10mm}{\bf then} \ $Set_{\gamma}$ := $Set_{i}$ \\
\hspace*{10mm} \hspace*{10mm} i.e. $Set_{i}$ は新たに得られるクラスタ
として CCS に蓄積される. \\
{\bf else if} \ $Set_{\alpha}$ $\in$ CCS exists {\it
such that} $Set_{i}$ $\subset$ $Set_{\alpha}$  \\
\hspace*{10mm}{\bf then} \ $Set_{i}$ が ICS から削除され,
$Set_{\gamma}$ := $\phi$ となる. \\
{\bf else if} \\ 
\hspace*{10mm}{\bf for all} $Set_{\alpha}$ $\in$ CCS {\bf do} \\
\hspace*{15mm}{\bf if} \ $Set_{i}$ $\cap$ $Set_{\alpha}$ = $\phi$ \\
\hspace*{20mm}{\bf then} \ $Set_{\gamma}$ := $Set_{i}$ \\
\hspace*{15mm} \hspace*{15mm} i.e. $Set_{i}$ は新たに得られるクラスタ
として CCS に蓄積 \\
\hspace*{38mm}される. \\
\hspace*{15mm}{\bf end\_if} \\
{\bf else} $Set_{\beta}$ := {\sf
Make-Temporary-Cluster-Set}($Set_{i}$,CCS) \\
\hspace*{0.8cm}{\bf (}$Set_{\beta}$ := $Set_{\alpha}$ $\in$ \mbox{CCS} \ {\it such that} \
$Set_{i}$ $\cap$ $Set_{\alpha}$ $\neq$ $\phi$ \hspace*{4cm} \\
\hspace*{10mm}$Set_{\gamma}$ := {\sf
Recognition-of-Polysemy}($Set_{i}$,$Set_{\beta}$)  \\
{\bf end\_if} \\ 
{\bf end\_if} \\ 
{\bf end\_if}} \\ \\
\hspace*{10mm}{\bf if} \ $Set_{\gamma}$ = VG  \\
\hspace*{15mm}{\bf then} \ {\it for\_loop} を抜ける. \\
\hspace*{10mm}{\bf end\_if} \\
\hspace*{5mm}{\bf end\_for} \\
{\bf end}}}
\caption{クラスタリングアルゴリズムの流れ} \label{flow_algo}
Figure 2 The flow of the clustering algorithm
\end{center}
\end{figure}
}

\noindent
図\ref{flow_algo} において, 関数 {\sf Make-Initial-Cluster-Set} は, 動
詞グループ VG を入力とし, VG の任意の動詞対の組合せに対し, 意味的な偏
差の値を計算し, 任意の動詞対と偏差の値をその値が昇順になるように出力
する.  この結果をICS(Initial Cluster Set)と呼ぶ.

CCS(Created Cluster Set) は作成されたクラスタの集合を示す.  関数{\sf
Make-Temporary-Cluster-Set} は$Set_{i}$ のどちらか一方の動詞を含むクラ
スタを CCS から抽出する.  その結果である$Set_{\beta}$が関数 {\sf
Recognition-of-Polysemy}に渡される.  関数{\sf Recognition-of-Polysemy}
は動詞が多義か否かを判定する関数である.

今 $Set_{i}$ と $Set_{\beta}$の両方に属する動詞を$v$とする.  $v$が多
義であり$w_{p}$(ただし$w_{p}$は$Set_{i}$の要素とする) と$w_{1}$
(ただし $w_{1}$ は $Set_{\beta}$ の要素とする)の意味を持つか否かを判定
するために, (\ref{split})と(\ref{lump})で示されるクラスタが作成される. 
具体的には関数 (\ref{sp}) が $v$, $w_{1}$, と $w_{p}$ に適用され 
$v_{x}$ と$v_{y}$が作成される.  もし $v_{x}$ と $v_{y}$ が
(\ref{lump})で示されるクラスタを作成する過程で存在する場合, 関数 
(\ref{lu}) が $v_{x}$ と $v_{y}$に適用され, $w$ が作成される.

この処理は新しく得られるクラスタ$Set_{\gamma}$がVG と等しくなるか, あ
るいはICS 
の要素がなくなるまで適用される.


\subsection{仮想動詞と名詞の相互情報量}

多義語を含む動詞グループに対し, 前節で述べたアルゴリズムを適用すること
で, 多義語の各意味を示す動詞と共起する名詞の集合が動詞の個数分得られる. 

\vspace*{-0.5cm}
{\footnotesize
\begin{table}[htbp]
\begin{center}
\caption{\{take, obtain, spend, buy\} のクラスタリング結果}
\label{app_1}
Table 1 The clustering results of \{take, obtain, spend, buy\} \\
\begin{tabular}{l||l|r|r|c|r|c}  \hline \hline
\multicolumn{2}{}{} &\multicolumn{3}{|l|}{クラスタリング結果得られる値}
&\multicolumn{2}{l}{(\ref{re_cal}), (\ref{church_mu})より得られる値} \\ \hline 
$v_{i}$ &$n_{ij}$ &$f(n_{ij})$ &$f(v,n_{ij})$ &$Mu(v,n_{ij})$ &$f(v_{i})$ &$Mu(v_{i},n_{ij})$ \\ \hline
{\sf take1} &columbia  &418 &\hspace*{2mm}5  &3.543 &214 &7.330 \\ 
(buy) &equity  &510 &\hspace*{2mm}6   &3.519 &214 &7.306 \\ 
 &lot  &610 &\hspace*{2mm}8  &3.676 &214 &7.463 \\ 
 &note  &936 &14 &7.653  &214 &3.866 \\ 
 &option &640 &\hspace*{2mm}7  &3.414 &214 &7.201 \\ 
 &order  &1004 &\hspace*{2mm}9  &3.127 &214 &6.914 \\ 
 &part  &1664 &27 &7.770   &214 &3.983 \\ 
 &property  &505 &\hspace*{2mm}7  &3.756 &214 &7.543 \\ 
 &stake  &1081 &28  &4.658 &214 &8.445 \\ 
 &thrift  &494 &\hspace*{2mm}9  &4.150  &214 &7.937 \\ \hline  
{\sf take2} &hour &443 &\hspace*{2mm}9 &4.307 &270 &7.759 \\ 
(spend) &lot &610 &\hspace*{2mm}8  &3.676 &270 &7.127 \\ 
 &minute &197 &\hspace*{2mm}5  &4.628 &270 &8.080 \\ 
 &money &1569 &19  &3.561 &270 &7.012 \\ 
 &month &3546 &39  &3.422 &270 &6.874 \\ 
 &time &2866 &45  &3.936 &270 &7.387 \\ 
 &week &2647 &26  &3.259  &270 &6.710 \\ \hline 
{\sf take3} &drug &1164 &11  &3.203 
&\hspace*{2mm}41 &9.374 \\ 
(obtain) &loan &1369 &12  &3.095
&\hspace*{2mm}41 &9.265 \\ \hline  
\multicolumn{7}{c}{} \\ 
\multicolumn{7}{c}{} \\  \hline 
\multicolumn{2}{}{} &\multicolumn{3}{|l|}{クラスタリング結果得られる値}
&\multicolumn{2}{l}{(\ref{re_cal_oh}), (\ref{church_mu})より得られる値} \\ \hline 
\footnotesize{$v_{r}$} &\footnotesize{$n_{rj}$} &\footnotesize{$f(n_{rj})$} &\footnotesize{$f(v,n_{rj})$}
&\footnotesize{$Mu(v,n_{rj})$} &\footnotesize{$f(v_{r})$}
&\footnotesize{$Mu(v_{r},n_{rj})$} \\ \hline
residue &account &375 &33   &6.422
&\hspace*{-2mm}2429 &6.704 \\ 
 &action &560 &52  &6.500
&\hspace*{-2mm}2429 &6.782 \\ 
 &\multicolumn{6}{l}{etc. total 102} \\ 
\end{tabular} 
\end{center} 
\end{table}
} 

\noindent
表 \ref{app_1} は, 多義語 take を含む動詞グループ\{take, obtain,
spend, buy\} に対し, クラスタリングアルゴリズムを適用した結果を示す. 

クラスタリングの結果得られるこのテーブルを{\it pvn} (polysemous verb
noun)テーブルと呼ぶ.  $v_{i}$ は仮想動詞{\sf take1}, {\sf take2}, {\sf
take3}を示し, それぞれ, `buy', `spend', `obtain' を示す.  $v_{r}$ 
は $v_{i}$ 以外の意味を示す仮想動詞`residue'を示す.  $n_{ij}$ は, 仮想
動詞$v_{i}$と共起する名詞を示し, $n_{rj}$は仮想動詞$v_{r}$
と共起する名詞を示す.  $f(n_{ij})$ と$f(n_{rj})$はそれぞれ
$n_{ij}$, $n_{rj}$の頻度を示し, $f(v,n_{ij})$ と$f(v,n_{r
j})$はそれぞれ`take'と$n_{ij}$, `take' と$n_{rj}$の共起頻度数を
示す.

文中に現れる動詞の多義解消は基本的に名詞$n_{ij}$ 及び$n_{rj}$を用いて行なわれる. 
すなわち, 文中に現れる動詞と共起する名詞が表\ref{app_1}に示されているとき, 文中の動
詞は, その名詞
と共起する仮想動詞の意味となる.  例えば, (s3) において,
\underline{stake} は表\ref{app_1}に示されている.  従って(s3)の
\underline{taken}の意味は, {\sf take1}が示す意味である`buy' と判定される.

\vspace*{2mm}
\begin{tabular}{ll}
(s3) &\parbox[t]{12cm}{In the past, however, Coke has typically
\underline{taken} a minority \underline{stake} in such ventures.} \\
\end{tabular}
\vspace*{2mm}

\noindent 
名詞の中には, 例えば表\ref{app_1}の `lot' のように複数の集合に属する名
詞が存在する. この場合は, 各仮想動詞と `lot' との相互情報量の中で
大きい値を持つ仮想動詞の意味とした.  ただし, 表 \ref{app_1} の $Mu(v,
n_{ij})$ 及び $Mu(v,n_{rj})$は, `take' と各名詞との相互情報量を示す.  そこで, 仮想動
詞$v_{i}$及び$v_{r}$と各名詞との相互情報量$Mu(v_{i},n_{ij})$及び
$Mu(v_{r},n_{rj})$を以下のよう
にして求めた.

\begin{enumerate}
\item $v_{i}$ \ (1 $\leq$ $i$ $\leq$ $k$) を仮想動詞とし, $v_{r}$ を 
$v$ における各仮想動詞以外の意味を示す仮想動詞とする.  $num(i)$ $(1
\leq i \leq k)$ を$v_{i}$と共起する名詞の個数とし, $n_{ij}$ $(1 \leq i
\leq k, \ 1 \leq j
\leq num(i))$を$v_{i}$\hspace{-0.1mm}と共起する$j$軸の名詞とする.
\hspace{-0.1mm}$v_{i}$の頻度$f(v_{i})$と$v_{r}$の頻度$f(v_{r})$ は以下の式で示される.

\begin{eqnarray}
f(v_{i}) & = & f(v) \times
\frac{\sum_{j=1}^{num(i)}f(v,n_{ij})}{\sum_{p=1}^{k}(\sum_{q=1}^{num(p)}f(v,n_{pq}))}
\label{re_cal} \\
f(v_{r}) & = & f(v) - \sum_{i=1}^{k}f(v_{i}) \label{re_cal_oh}
\end{eqnarray}

\item 式(\ref{re_cal})と(\ref{church_mu}), 及び(\ref{re_cal_oh})と(\ref{church_mu})を用い
て, $Mu(v_{i},n_{ij})$ と$Mu(v_{r},n_{rj})$を求める.

\end{enumerate}

\noindent
表\ref{app_1}の$Mu(v_{i}, n_{ij})$ と$Mu(v_{r},n_{rj})$は 
それぞれ仮想動詞$v_{i}$と名詞$n_{ij}$, 仮想動詞$v_{r}$と名詞
$n_{rj}$との相互情報量を示す.


多義語の解消

文中の多義語 $v$の意味は, $v$の {\it pvn}テーブルを用いて以下のように決定され
る.

\begin{enumerate}
\item[\bf 1.] $v$の後方5語以内に出現する名詞を$x$とすると, $x$が{\it 
pvn} テーブルに存在する場合:

\begin{enumerate}
\item[\bf 1-1.] $x$が一つのみ存在する場合, $v$の意味は, $x$と共起する仮想動詞の意味
とする.

\item[\bf 1-2.] $x$が二つ以上存在する場合, $v$の意味は, $x$と共起する仮想動
詞のうち, $x$との相互情報量の値が最も高い仮想動詞の意味とする.

\end{enumerate}

\item[\bf 2.] $x$ が {\it pvn} テーブルに存在しない場合,
$rel(v_{i},x)$の値が最大になるような仮想動詞$v_{i}$を求める.  $v$の意
味は, $v_{i}$の意味とする.
\end{enumerate}

\noindent
$rel(v_{i},x)$は, $v_{i}$と$x$の意味的な関係を示す式であり, 以下の
ように定義した.

\begin{eqnarray}
rel(v_{i},x) & = & \max_{y \in N_{i}}(\frac{Mu(v_{i},y)}{Dis(x,y)}) \ \ \ (1 \leq
i \leq k) \label{co}
\end{eqnarray}

\noindent
式(\ref{co})において, $N_{i}$は$v_{i}$と共起する名詞の集合を示す.  $Dis(x,y)$ は, $x$と{\it pvn}テーブルに登録されている名詞$y$\hspace{-0.05mm}との偏差を示
す. すなわち, \hspace{-0.1mm}式(\ref{22})\hspace{-0.1mm}において$m$を2とし, $v_{1j}$と$v_{2j}$をそれぞれ,
$x$\hspace{-0.05mm}, $y$\hspace{-0.05mm}とする. さらに式(\ref{22})\hspace{-0.1mm}中の動詞と共起する名詞の個数を名詞$x$\hspace{-0.05mm}及
び$y$\hspace{-0.05mm}と共起する動詞の個数に置き換えることにより$Dis(x,y)$が得られる.  

\vspace*{-2mm}
