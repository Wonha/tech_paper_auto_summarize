\section{はじめに}
  音声認識技術はその発達にともなって，その適用分野を広げ，
日本語においても新聞など一般の文章を認識対象とした研究が
行なわれるようになった\cite{MATSUOKA,NISIMURA4}．
この要因として，音素環境依存型HMMによる音響モデルの高精度化に加え，
多量の言語コーパスが入手可能になった結果，文の出現確率
を単語{\it N}個組の生起確率から推定する{\it N}-gramモデルが
実現できるようになったことが挙げられる．
  日本語をはじ\break
めとして単語の概念が明確ではない言語における音声認識を
実現する場合，どのような単位を認識単位として採用する
かが大きな問題の1つとなる．この問題はユーザーの発声単位
に制約を課す離散発声の認識システムの場合に限らない．連続
音声の認識においても，ユーザーが適\break
時ポーズを置くことを許容
しなければならないため，やはり発声単位を考慮して認識単位を
決\break
める必要がある．従来日本語を対象とした自然言語処理では
形態素単位に分割することが一般\break
的であり，またその解析ツールが比較的
\mbox{よく整備されていたことから{\it N}-gramモデル作成におい}ても「形態素」を
単位として採用したものがほとんどである\cite{MATSUOKA,ITOHK}．
しかしながら，音声認識という立場からあらためてその処理単位に要請される
条件を考えなおしてみると，以下のことが考えられる．
\begin{itemize}
  \item 認識単位は発声単位と同じか，より細かい単位でなければならない．
形態素はその本来の定義から言えば必ずこの条件を満たしているが，実際の
形態素解析システムにおいては，複合名詞も１つの単位として登録することが
普通であるし，解析上の都合から連続した付属語列
のような長い単位も採用している場合が
あるためこの要請が満たされているとは限らない．
  \item 長い認識単位を採用する方が，音響上の
識別能力という観点からは望ましい．つまり連続して発声される
可能性が高い部分については，それ自身を認識単位としてもっておく
方がよい．
  \item 言語モデルを構築するためには，多量のテキストを認識単位に分割する
必要があり，処理の多くが自動化できなければ実用的ではない．
\end{itemize}
  これらは，言い換えれば人間が発声のさいに分割する(可能性がある)
単位のMinimum Cover Set
を求めることに帰着する．
人が感覚的にある単位だと判断する
\mbox{日本語トークンについて考}察した研究は
過去にも存在する．原田\cite{HARADA}は人が文節という単位について一貫した概念を
持っているかについて調査し，区切られた箇所の平均一致率が76\%であり
付属語については多くの揺れがあったと報告している．また
横田，藤崎\cite{YOKOTA}は人が
短時間に認識できる文字数とその時間との関係から人の認知単位を求め，その
単位を解析にも用いることを提案している．しかしながら，これらの研究はいずれも
目的が異なり，音声認識を考慮したものではない．
そこで，われわれは，
人が潜在意識としてもつ単語単位を形態素レベルのパラメータで
モデル化するとともに，そのモデルに基づいて文を分割，{\it N}-gramモデルを作成する
手法を提案し，認識率の観点からみて有効であることを示した\cite{NISIMURA3}．
本論文では主として言語処理上の観点からこの単語単位{\it N}-gramモデルを考察し，
必要な語彙数，コーパスの量とパープレキシティの関係を明らかにする．
とくに新聞よりも「話し言葉」に近いと考えられるパソコン通信の電子会議室から収集した
文章を対象に加え，新聞との違いについて実験結果を述べる．
