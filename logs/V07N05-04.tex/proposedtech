




\documentstyle[epsf,jnlpbbl,eepic,epic]{jnlp_j}

\setcounter{page}{71}
\setcounter{巻数}{7}
\setcounter{号数}{5}
\setcounter{年}{2000}
\setcounter{月}{11}
\受付{2000}{1}{6}
\再受付{2000}{3}{28}
\採録{2000}{4}{13}

\setcounter{secnumdepth}{2}


\newcommand{\undercirc}{}
\newcommand{\undercross}{}

\title{３つ以下の候補から係り先を選択する係り受け解析モデル}
\author{金山 博\affiref{u-tokyo} \and 鳥澤 健太郎\affiref{u-tokyo}\affiref{PRESTO}
	\and 光石 豊\affiref{u-tokyo} \and 辻井 潤一\affiref{u-tokyo}\affiref{UMIST}}


\headauthor{金山, 鳥澤, 光石, 辻井}
\headtitle{３つ以下の候補から係り先を選択する係り受け解析モデル}

\affilabel{u-tokyo}{東京大学大学院理学系研究科 情報科学専攻}
	{Department of Information Science, Graduate School of Science, University of Tokyo}
\affilabel{PRESTO}{科学技術事業団 さきがけ研究21}
	{PRESTO, Japan Science and Technology Corporation}
\affilabel{UMIST}{マンチェスター工科大 計算言語学センター}
	{University of Manchester Institute of Science and Technology}

\jabstract{
本稿では、日本語係り受け解析のための統計的手法について述べる。
この手法は、統計値の計算方法が従来の手法と異なる。
従来の手法では、２つの文節間が依存関係にある確率をそれぞれの文節の組に対して計算するが、
本研究で提案する「３つ組／４つ組モデル」は、
係り元の文節と係り先の文節の候補となる全ての文節に関する情報を確率の条件部として、
ある文節が係り先として選択される確率を求める。
なお、係り先の候補は、HPSGに基づいた文法及びヒューリスティクスによって高々３つに絞られる。
確率の推定には最大エントロピー法を用いており、
我々の構文解析器はEDRコーパスに対して文節正解率88.6$\%$という
高い解析精度を達成した。
}

\jkeywords{係り受け解析, 統計的手法, 文法, ＭＥ法}

\etitle{A Statistical Japanese Dependency Analysis Model \\
with Choice Restricted to at Most Three \\ Modification Candidates}
\eauthor{KANAYAMA Hiroshi\affiref{u-tokyo} 
	\and TORISAWA Kentaro\affiref{u-tokyo}\affiref{PRESTO}
	\and MITSUISHI Yutaka\affiref{u-tokyo}
	\and TSUJII Jun'ichi\affiref{u-tokyo}\affiref{UMIST}
}

\eabstract{
This paper describes a statistical method for Japanese dependency analysis.
The method differs from conventional statistical models in the way of 
calculating statistical values.
The conventional models calculate the probability of a correct dependency 
between two {\it bunsetsu}s (phrasal units of Japanese) for each
pair of {\it bunsetsu}s.
On the other hand, we propose the {\sl triplet/quadruplet model},
in which the conditional part of the probability 
consists of information on a modifier {\it bunsetsu} and
all its modification candidates,
and the probability that a candidate is chosen as the modifiee is calculated. 
The number of candidates is restricted to three or less by an HPSG-based grammar and heuristics.
With a maximum entropy estimation, our parser achieves high accuracy:
88.6$\%$ for the analysis of the EDR annotated corpus.
}

\ekeywords{Dependency analysis, Statistical method, Grammar, Maximum entropy method}

\begin{document}
\maketitle

\newpage

\newcounter{enums}


\section{本研究の手法}\label{sec:ourmodel}

本節では、「３つ組／４つ組モデル」を用いて係り受け解析をする手順を解説する。
係り受け解析の全体の流れは図\ref{fig:flow}~のようになっている。

３つ組／４つ組モデルの準備として、
\ref{subsec:restrict}~節で述べる手法により、
各文節の係り先候補を３つ以下に制限する。
まず、文法を用いて、
各文節の係り先として文法的に正しいものを列挙する。
その中で係り元から一番近い文節・二番目に近い文節・
最も遠い文節を選び出し、他を無視する。
そして、係り先の候補の集合の中で、ある要素が係り先として選択される確率を、
係り元文節と全ての係り先の候補の属性を同時に考慮するモデル（３つ組／４つ組モデル）で推定する。
\ref{subsec:tripquad}~節では、モデルの特徴及び利点について述べる。
最後に、上記のモデルを用いて文全体の最適な係り受けを選択する方法を、
\ref{subsec:sentence}~節で解説する。


\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.35mm}
	\begin{picture}(360,180)
	
	\put(0,160){\framebox(100,10){文}}
	\put(50,160){\vector(0,-1){20}}
	\put(0,130){\framebox(100,10){可能な全ての構文木}}
	\put(100,135){\vector(1,0){40}}
	\put(140,130){\framebox(100,10){各文節の係り先候補}}
	\put(160,130){\vector(0,-1){50}}
	\put(140,70){\framebox(100,10){各係り受けの確率}}
	\put(160,70){\vector(-4,-1){80}}
	\put(50,130){\vector(0,-1){80}}
	\put(0,40){\framebox(100,10){統計値付き構文木}}
	\put(50,40){\vector(0,-1){20}}
	\put(0,10){\framebox(100,10){最適な構文木}}
	\put(100,15){\vector(1,0){40}}
	\put(140,10){\framebox(100,10){最適な文節係り受け}}

	\put(50,145){\makebox(30,10){\shortstack{文法}}}
	\put(160,115){\makebox(70,10){\shortstack{最大３つの候補}}}
	\put(160,85){\makebox(60,20){\shortstack{\bf ３つ組・\\ \bf ４つ組モデル}}}

	\put(260,120){\framebox(100,10){学習コーパス}}
	\put(310,100){\line(0,1){20}}
	\put(220,100){\line(1,0){90}}
	\put(220,100){\vector(0,-1){20}}
	\put(220,85){\makebox(80,10){\shortstack{ＭＥ法にて推定}}}

	\put(100,125){\makebox(40,10){\shortstack{変換}}}
	\put(100,5){\makebox(40,10){\shortstack{変換}}}

	\end{picture}
	\caption{係り受け解析の流れ}
	\label{fig:flow}
	\end{center}
\end{figure}


\subsection{準備：係り受け候補の制限}\label{subsec:restrict}

\subsubsection{文法の利用}

本システムでは、文を入力とし、JUMAN\cite{JUMAN}で形態素解析をした後、
文法SLUNG\cite{Mitsuishi98}で構文解析する。
SLUNGは、JUMANの形態素を解析の単位として、
文法的に正しい全ての構文木を出力する。
これを係り先候補の制限に使うために、
それぞれの構文木中の部分木を、図~\ref{fig:transform}のようにして、
文節単位の係り受け構造に帰着させる。
部分木{\sf M}の左部分木{\sf L}、右部分木{\sf R}の最も右側にある語を
それぞれ$l$, $r$とし、それらが属する文節を$b(l)$, $b(r)$とするとき、
$b(l)$は$b(r)$に係ることになる。

一つの構文木は一つの係り受け構造に対応するが、可能な構文木が複数あるため、
一つの係り元文節に対して、係り先候補となる文節が複数求まる。
以下では、その候補の中から正しいものを選び出すことを考える。

\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.25mm}
	\begin{picture}(180,55)
	\put(0,10){\line(2,1){80}}
	\put(30,25){\line(2,-1){30}}
	\put(80,10){\line(2,1){40}}
	\put(80,50){\line(2,-1){80}}
	\put(0,10){\line(1,0){60}}
	\put(80,10){\line(1,0){80}}
	\put(45,-5){\makebox(10,10){\small\shortstack{$l$}}}
	\put(145,-5){\makebox(10,10){\small\shortstack{$r$}}}
	\put(70,55){\makebox(20,10){\shortstack{\sf M}}}
	\put(15,28){\makebox(10,10){\shortstack{\sf L}}}
	\put(125,33){\makebox(10,10){\shortstack{\sf R}}}
	\end{picture}
	\begin{picture}(40,55)
	\put(0,20){\line(1,0){5}}
	\put(0,20){\line(0,1){5}}
	\put(5,15){\line(0,1){5}}
	\put(5,15){\line(2,3){5}}
	\put(0,25){\line(1,0){5}}
	\put(5,25){\line(0,1){5}}
	\put(5,30){\line(2,-3){5}}
	\end{picture}
	\begin{picture}(120,55)
	\put(0,10){\framebox(40,20){$b(l)$}}
	\put(80,10){\framebox(40,20){$b(r)$}}
	\put(20,30){\line(0,1){20}}
	\put(20,50){\line(1,0){80}}
	\put(100,50){\vector(0,-1){20}}	
	\end{picture}
	\caption
	{部分木から文節係り受けへの変換}
	\label{fig:transform}
	\end{center}
\end{figure}


人手で記述する文法を用いることには、
\ref{sec:introduction}~節で述べたような我々の最終目標に達するための要件である他に、
決してありえない構造を排除することができるという利点がある。
文法の制約が過剰でないことは、\ref{sec:related}~節で述べたように
SLUNGの被覆率が高いことが保証している。

\subsubsection{係り先候補の３つ以下への制限}

日本語の文節の係り先の傾向として、
(1)近くから遠くになるに従って割合が減少すること、
(2)最も遠い文節に係る場合だけは比較的多いことが知られている。
この傾向は例えば\cite{Maruyama92}で分析されている。

SLUNGにより係り先候補を絞った場合にもこの傾向はやはり顕著である。
EDRコーパスの文をSLUNGで解析した際の、
係り先候補の数、及び正しい係り先の位置の関係の分布を表\ref{tab:position}~に示す。
表中の「第一」「第二」…は、
文法で制限された係り先候補のうち、
係り元文節から近い順に何番目が正しい係り先であるかを意味する。
「最遠」は係り元から最も遠い候補である。
このデータより、
係り元文節から(1)最も近い文節・(2)二番目に近い文節・(3)最も遠い文節のいずれかに係る場合だけで
98.6$\%$を占める\footnote{これは、文法で候補を制限しているためである。
文法を用いずに３つの文節に絞っても、92.6$\%$程度しかカバーできない。}ことがわかる。

この性質を利用して、係り先の候補が４つ以上存在する場合にも上記の３文節だけを考え、
その他の文節を無視することにする。
この制限によって、係り受け精度の上限は98.6$\%$となるが、
わずか1.4$\%$の犠牲により
問題を大幅に単純化することができ、
次節で述べる３つ組／４つ組モデルの構成が可能になる。

\begin{table}[tb]
\begin{center}
\small
\begin{tabular}{|c|c||c|c|c|c|c|c||c|}
\hline 候補の数  & 比率 & {\bf 第一} & {\bf 第二} & 
第三 & 第四 & $..$ & {\bf 最遠} & 第一,第二,最遠 \\
\hline \hline
1     & 32.7 & 100  & $-$    & $-$    & $-$    & $-$      & \footnotesize (100) & 100  \\ \hline
2     & 28.1 & 74.3 & 26.7 & $-$    & $-$    & $-$      & \footnotesize (26.7) & 100  \\ \hline
3     & 17.5 & 70.6 & 12.6 &\footnotesize(16.8) &  $-$  & $-$ &  16.8 & 100  \\ \hline
4     & 9.9  & 70.4 & 11.1 &  4.7 &\footnotesize(13.8) & $-$ &  13.8   & 95.3 \\ \hline
5     & 5.4  & 70.1 & 11.6 &  4.2 & 2.5  &$..$& 11.5   & 93.2 \\ \hline
6以上 & 6.4  & 70.3 & 10.8 &  3.9 & 2.4  & $..$&  9.6   & 90.7 \\ \hline 
\hline
合計  & 100  & $-$    & $-$    &  $-$   & $-$ & $..$& $-$    & {\bf 98.6} \\ \hline
\end{tabular}
\caption{\small 係り先の候補の数に対する、正しい係り先の分布（単位は$\%$）\\
{\footnotesize 「比率」は、候補の数の分布を示す。括弧付きの値は他の項との重複を表す。}}
\label{tab:position}
\end{center}
\end{table}

\subsection{３つ組／４つ組モデル}\label{subsec:tripquad}

３つ組／４つ組モデルは、文節$i$が文節$t_n$に係る確率$P(i \rightarrow t_n)$を
式(\ref{equ:triplet2}),~式(\ref{equ:quadruplet2})で計算する。
但し、$t_n$は文節$i$の係り先の（３つ以下に限定された）候補、
$\Phi_i$は文節$i$の属性、$\Psi_{i,t_n}$は$t_n$及び二文節$i, t_n$間の属性を表す。
\begin{eqnarray}
P(i \rightarrow t_n) = & P(n \mid \Phi_i, \Psi_{i,t_1}, \Psi_{i,t_2}) & （係り先の候補が２つのとき : n=1,2）
\label{equ:triplet2}
 \refstepcounter{enums}
\\
P(i \rightarrow t_n) = & P(n \mid \Phi_i, \Psi_{i,t_1}, \Psi_{i,t_2}, \Psi_{i,t_3}) & （係り先の候補が３つのとき : n=1,2,3）
\label{equ:quadruplet2}
\end{eqnarray} \refstepcounter{enums}

このモデルの特徴は、上記の式から推測される通り、
「{\bf 係り元文節と、係り先の候補となる全ての文節の属性を同時に考慮}すること」、そして
「それぞれの係り先の候補の係りやすさを求めるのではなく、
{\bf 各候補が選ばれる確率}を求める」ことである。
これらの意義は次の３点にある。

\vskip 2mm
\begin{description}
\item[意義1]
文節間の距離でなく、係り先の{\bf 候補の中での相対的位置}を用いて係り先を選べること
\item[意義2]
着目している候補だけでなく、{\bf 文脈}、すなわち他の候補の属性を考慮できること
\item[意義3]
ある係り元に対する全ての候補への係りやすさを、{\bf 同じ条件の下で計算}できること
\end{description}
\vskip 2mm
以下で、これらの意義について順に述べる。

\subsubsection{意義1~:~候補の中での相対的位置}

文節間の距離は、係り受け解析における重要な要素として考えられているが、
係り先の候補の中の位置の方が重要な場合がある。
例として、(\ref{sent:karega})の各文における「彼が」の係り先を推定する時を考える。
両者とも、「走るのを」が正しい係り先と考えられる。

\vskip 2mm
\begin{tabular}{lll}
\refstepcounter{enums}\label{sent:karega}
 (\theenums) & a. & {\bf 彼が} \underline{走るのを} 見た ことが ありますか。\\
 & b. & {\bf 彼が} ゆっくり \underline{走るのを} 見た ことが ありますか。
\end{tabular}
\refstepcounter{equation}
\vskip 2mm

文法を用いずに文節数を距離とするモデルでは、
「彼が」と「走るのを」の文節間距離は
aでは1、bでは2と異なっている反面、
aでの「彼が → 見た」
\footnote{「→」は、二文節間の係り受けを表す。}
とbでの「彼が → 走るのを」が、
係り元からの距離が2である動詞であるという点で、
似た事象であると見なされる。

一方、文法で係り先を絞った場合、
a,~bとも「彼が」の係り先の候補は「走るのを」と「見た」の２つとなる。
このように、係り先の候補のみに着目すれば、両者を同じ事象として扱えるので、
より効率のよい学習が行えるようになる。


\subsubsection{意義2~:~文脈の考慮}

(\ref{sent:watashino})において、「私の」の係り先を考える。
正解は、それぞれ「娘に」「友人の」である。

\vskip 2mm
\begin{tabular}{lll}
\refstepcounter{enums} 
\label{sent:watashino}
(\theenums) & a. & {\bf 私の} かわいい \underline{娘に} 道で ばったり 会った。\\
            & b. & {\bf 私の} \underline{友人の} 娘に 道で ばったり 会った。
\end{tabular}
\refstepcounter{equation}
\vskip 2mm

係り元文節と係り先文節、及び文節間距離を考えるモデルでは、
a, bにおける「私の → 娘に」は区別されることなく、全く同じ係り受け確率が付与される。
しかしながら、この確率は非常に低くなる。なぜなら、
実際にEDRコーパスの一部を観察したところ、aの「${\sf N_1}$の ${\sf A}$ ${\sf N_2}$」
\footnote{{\sf N}と{\sf A}はそれぞれ名詞、形容詞を表す。}
という構文に対し、
bのような「${\sf N_1}$の ${\sf N_2}$ ${\sf N_3}$」の構文の頻度が４倍程度あり、
後者の構文では、${\sf N_1}$は近くの${\sf N_2}$を修飾する場合が約75％と、
圧倒的に多いからである。
従って、aにおいて、「私の → 娘に」に比べて
「私の → かわいい」の確率のほうが高くなり、解析誤りを引き起こす
\footnote{なお、「顔のかわいい女の子」のような構文では、
「顔の」は「かわいい」に係るのが正しいが、この構文が現れる頻度は低い。
このような場合は、語彙情報を加えることによって解決できよう。}。

係り元と係り先の３つの候補全てを同時に考慮すると、この誤りを防ぐことができる。
aにおいて「私の」と、その係り先候補である「かわいい」「娘に」「会った。」を同時に考えて、
三者のそれぞれが選ばれる確率を計算した場合、
第二候補であっても、第一候補の形容詞連体形よりも高い確率が割り当てられ、
正しく係り先を求めることができる。
このような現象は、
第一候補である形容詞や副詞を飛び越えて第二候補に係るケースなどで
一般的に数多く見受けられる。


\subsubsection{意義3~:~同じ条件下での係りやすさの計算}

これは意義2~とも関連するが、
ある一つの係り元に対する係り受けの確率を、
共通の前件を持った条件付き確率で計算できるという利点である。

(\ref{sent:watashino}a)の「私の」の係り先を考える際には、
従来の手法は式(\ref{equ:pairmodel})、我々の手法は式(\ref{equ:tqmodel})を求めることになる
\footnote{$\Psi$は文節間の属性も含むため、係り元文節にも依存するが、以下の式・図では省略している。}。
(\ref{equ:pairmodel})ではそれぞれの条件付き確率の前件が異なるため、５つの値の和は1にならないのに対し、
式(\ref{equ:tqmodel})では３つの和が1になる。
従って、３つ組／４つ組モデルにおいて
推定する条件付き確率は、
係り元とその係り先候補がある文脈において、
それぞれの係り先候補が選ばれる確率に一致することになる。
なお、考慮する条件を図示すると、それぞれ図\ref{fig:oldmodel}~、図\ref{fig:abcd}~のようになる。

\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.35mm}
	\begin{picture}(340,65)
	\put(0,15){\thicklines\framebox(40,10){\small 私の}}
	\put(60,15){\thicklines\framebox(40,10){\small かわいい}}
	\put(120,15){\thicklines\framebox(40,10){\small 娘に}}
	\put(180,15){\thicklines\framebox(40,10){\small 道で}}
	\put(240,15){\thicklines\framebox(40,10){\small ばったり}}
	\put(292,15){\thicklines\framebox(56,10){\small 会った。}}
	\put(26,25){\line(0,1){14}}
	\put(23,25){\line(0,1){17}}
	\put(20,25){\line(0,1){20}}
	\put(17,25){\line(0,1){23}}
	\put(14,25){\line(0,1){26}}
	\put(26,39){\line(1,0){54}}
	\put(23,42){\line(1,0){117}}
	\put(20,45){\line(1,0){180}}
	\put(17,48){\line(1,0){243}}
	\put(14,51){\line(1,0){306}}
	\put(80,39){\vector(0,-1){14}}
	\put(140,42){\vector(0,-1){17}}
	\put(200,45){\vector(0,-1){20}}
	\put(260,48){\vector(0,-1){23}}
	\put(320,51){\vector(0,-1){26}}
	\put(0, 0){\makebox(40,10){\shortstack{$\Phi_{私の}$}}}
	\put(60, 0){\makebox(40,10){\shortstack{$\Psi_{かわいい}$}}}
	\put(120, 0){\makebox(40,10){\shortstack{$\Psi_{娘に}$}}}
	\put(180, 0){\makebox(40,10){\shortstack{$\Psi_{道で}$}}}
	\put(240, 0){\makebox(40,10){\shortstack{$\Psi_{ばったり}$}}}
	\put(300, 0){\makebox(40,10){\shortstack{$\Psi_{会った。}$}}}
	\put(60, 39){\line(1,1){23}}
	\put(120,42){\line(1,1){20}}
	\put(180,45){\line(1,1){17}}
	\put(240,48){\line(1,1){14}}
	\put(300,51){\line(1,1){11}}
	\put(60,39){\circle*{2}}
	\put(120,42){\circle*{2}}
	\put(180,45){\circle*{2}}
	\put(240,48){\circle*{2}}
	\put(300,51){\circle*{2}}
	\put(80,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow かわいい)$}}}
	\put(140,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 娘に)$}}}
	\put(200,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 道で)$}}}
	\put(260,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow ばったり)$}}}
	\put(320,59){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 会った。)$}}}
	\end{picture}
	\caption{従来のモデルで考慮する条件 \\
	{\footnotesize 各係り受けの確率が、それぞれの二文節の属性を用いて
	(\ref{equ:pairmodel})式で計算される。}}
	\label{fig:oldmodel}
	\end{center}
\end{figure}

\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.35mm}
	\begin{picture}(340,45)
	\put(0,15){\thicklines\framebox(40,10){\small 私の}}
	\put(60,15){\thicklines\framebox(40,10){\small かわいい}}
	\put(120,15){\thicklines\framebox(40,10){\small 娘に}}
	\put(180,15){\makebox(40,10){\shortstack{道で}}}
	\put(240,15){\makebox(40,10){\shortstack{ばったり}}}
	\put(292,15){\thicklines\framebox(56,10){\small 会った。}}
	\put(20,25){\line(0,1){10}}
	\put(30,35){\oval(20,20)[tl]}
	\put(30,45){\line(1,0){280}}
	\put(70,35){\oval(20,20)[tr]}
	\put(130,35){\oval(20,20)[tr]}
	\put(310,35){\oval(20,20)[tr]}
	\put(80,35){\vector(0,-1){10}}
	\put(140,35){\vector(0,-1){10}}
	\put(320,35){\vector(0,-1){10}}
	\put(80, 35){\line(1,1){15}}
	\put(140,35){\line(1,1){15}}
	\put(320,35){\line(1,1){15}}
	\put(80,35){\circle*{2}}
	\put(140,35){\circle*{2}}
	\put(320,35){\circle*{2}}
	\put(84,48){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow かわいい)$}}}
	\put(144,48){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 娘に)$}}}
	\put(324,48){\makebox(40,10){\shortstack{\tiny $P(私の \rightarrow 会った。)$}}}
	\put(0, 0){\makebox(40,10){\shortstack{$\Phi_{私の}$}}}
	\put(60, 0){\makebox(40,10){\shortstack{$\Psi_{かわいい}$}}}
	\put(120, 0){\makebox(40,10){\shortstack{$\Psi_{娘に}$}}}
	\put(300, 0){\makebox(40,10){\shortstack{$\Psi_{会った。}$}}}
	\end{picture}
	\caption{３つ組／４つ組モデルで考慮する条件 \\
		{\footnotesize 係り元と３つに絞られた係り先候補の属性を用いて、
		それぞれの候補に係る確率を(\ref{equ:tqmodel})式で求める。}}
	\label{fig:abcd}
	\end{center}
\end{figure}

\begin{equation}
\left. \begin{array}{ll}
P(私の \rightarrow かわいい)  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{かわいい}) \\
P(私の \rightarrow 娘に)  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{娘に}) \\
P(私の \rightarrow 道で)  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{道で}) \\
P(私の \rightarrow ばったり)  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{ばったり}) \\
P(私の \rightarrow 会った。) \qquad  & = P(\mbox{T} \mid \Phi_{私の}, \Psi_{会った。})
\end{array}
\right\}
\nonumber \\
\qquad \qquad \qquad \quad \quad 
\label{equ:pairmodel}
\end{equation}\refstepcounter{enums}
\begin{equation}
\left. \begin{array}{ll}
P(私の \rightarrow かわいい) & = P(1 \mid \Phi_{私の}, \Psi_{かわいい}, \Psi_{娘に}, \Psi_{会った。}) \\
P(私の \rightarrow 娘に) & = P(2 \mid \Phi_{私の}, \Psi_{かわいい}, \Psi_{娘に}, \Psi_{会った。}) \\
P(私の \rightarrow 会った。)\qquad  & = P(3 \mid \Phi_{私の}, \Psi_{かわいい}, \Psi_{娘に}, \Psi_{会った。})
\end{array}
\right\}
\nonumber \\
\qquad
\label{equ:tqmodel}
\end{equation}\refstepcounter{enums}


\subsection{最適な係り受けの選択}\label{subsec:sentence}

各文節間の係りやすさ$P(i \rightarrow j)$を求めるにあたって、
係り元文節に対する係り先文節の候補の数に依って、次のようなモデルを用いることにする。

\begin{itemize}
\item
係り先候補が１つの場合：その係り先に確定するため、$P(i \rightarrow j) = 1.0$となる。
\item
係り先候補が２つの場合：係り元と２つの係り先の文節の情報を考慮する「３つ組モデル」を用いる。
\item
係り先候補が３つ以上の場合：係り先の候補のうち、係り元に最も近い文節、二番目に近い文節、
最も遠い文節の３つだけを考え、係り元とその３つの文節の情報を考慮する「４つ組モデル」を用いる。
\end{itemize}

こうして求まった値を用いて、SLUNGの出力した全ての部分木M
に対して、統計値$Q(\mbox{M})$を以下のようなアルゴリズムで割り振る。
なお、SLUNGの出力する構文木の終端記号は、
文節単位でなく、単語（JUMANの出力する形態素）を単位としている。

\begin{itemize}
\item
部分木Mがただ一つの単語からなる場合、$Q(\mbox{M})$ = 1.0
\item
そうでない場合、図\ref{fig:partialtree}~の部分木において、
左部分木Lの最も右側の単語を$l$、右部分木Rの最も右側の単語を$r$として、
$l$、$r$の属する文節をそれぞれ$b(l)$、$b(r)$とする。
このとき、
\begin{equation}
Q(\mbox{M}) = Q(\mbox{L}) \times Q(\mbox{R}) \times P(b(l) \rightarrow b(r))
\end{equation}\refstepcounter{enums}
\end{itemize}

文全体に対応する構文木で、この統計値が最大になるようなものを探索し、
その構文木を再び文節の係り受け関係に変換して出力する。
こうして得られた文の係り受けは、必ず文法的に正しい構文木に対応しており、
係り受け同士が交差することはない。


\begin{figure}[t]
	\begin{center}
	\small
	\setlength{\unitlength}{.35mm}
	\begin{picture}(140,55)
	\put(0,10){\line(2,1){70}}
	\put(30,25){\line(2,-1){30}}
	\put(80,10){\line(2,1){30}}
	\put(70,45){\line(2,-1){70}}
	\put(0,10){\line(1,0){60}}
	\put(80,10){\line(1,0){60}}
	\put(50,0){\makebox(10,10){\shortstack{$l$}}}
	\put(130,0){\makebox(10,10){\shortstack{$r$}}}
	\put(60,45){\makebox(20,10){\shortstack{M}}}
	\put(20,25){\makebox(10,10){\shortstack{L}}}
	\put(110,25){\makebox(10,10){\shortstack{R}}}
	\end{picture}
	\caption{SLUNGの出力する部分木M}
	\label{fig:partialtree}
	\end{center}
\end{figure}

\section{考察}\label{sec:observations}

ここでは、本稿で提案する手法がどのように精度向上に寄与しているかの観察、
及び他研究との比較を行う。

\subsection{「３つ組／４つ組モデル」の効用}

表\ref{tab:control_exp}~にある対照実験の結果は、
以下の理由から３つ組／４つ組モデルの有効性を示しているといえる。

\begin{itemize}
\item
「３つ組／４つ組モデル」の精度は「２つ組モデル」の精度よりも
約0.9$\%$上回っている。
両者とも、文法とヒューリスティクスにより係り先候補を３つ以下に限定しているが、
それらの係り先候補を同時に考慮するモデルを用いた方が精度が上がることが確認された。
\item
「２つ組モデル」は、「文法なしモデル」より1.0$\%$、「候補限定なしモデル」よりも
0.3$\%$高い精度を出している。
従って、文法を用いることや係り先候補を３つに限定することは
妥当な措置であり、「２つ組モデル」は「３つ組／４つ組モデル」の比較対象として適当である。
\end{itemize}

次に、両者のモデルで実際に解析を行う時の、
具体的なMEのパラメータを観察してみる。
例として、文(\ref{sent:kodomo})
の「子供たちの」の各候補への係りやすさを計算する。
「子供たちの」の係り先候補は、「甲高い」「声で」「騒然となる。」の３文節で、
正解は「声で」である。

\vskip 2mm
\refstepcounter{enums}\label{sent:kodomo}
(\theenums)  そんなとき、 {\bf 子供たちの} 甲高い \underline{声で} 騒然となる。
\refstepcounter{equation}
\vskip 2mm

各候補への係りやすさを２つ組モデル・４つ組モデル
\footnote{候補数が３なので、４つ組モデルが使われる。}で推定する際の
ME法のパラメータ$\alpha_k$のうち主な（$| \log\alpha_k |$が大きい）ものを、
それぞれ表\ref{tab:pair_me}~,表\ref{tab:quad_me}~に示す。
パラメータ$\alpha_k$のうち、履歴$a$、出力値$b$に対応する素性のものを掛け合わせるので、
$\alpha_k$の値が1.0より大きいものは出力値を$b$にすることを助長するパラメータ、
1.0より小さいものは$b$にすることを抑制するパラメータである。
「$\alpha_k$の積」の項は、
表に載せていないものも含め、
対応する出力値に関する全てのパラメータの積である。

\subsubsection{２つ組モデルの場合}

このモデルでは、係り先ごとに別々の条件で係りやすさを計算する。
各係り先への係りやすさ$P(i \rightarrow j)$は、
出力値Ｔに対する$\alpha_k$の積を、
出力値Ｔ,~Ｆに対する$\alpha_k$の積の和で割ったものである。
例えば、$P(子供たちの \rightarrow 甲高い)$は、
$0.93/(0.93+0.81)=0.53$となる。

「声で」に係る場合のパラメータに注目すると、
係り元助詞「の」は隣の文節に係る傾向が強いことから、
文節間距離が「２から５」に対するパラメータが小さくなっている。
そのため、「甲高い」に係る確率の方が高くなってしまう。

\subsubsection{４つ組モデルの場合}

全ての係り先への係りやすさを共通の確率分布を用いて計算する。
出力値$b$は$1,2,3$の３値をとり、
第一候補への係りやすさ$P(i \rightarrow t_1)$は
出力値１に対する$\alpha_k$の積を、
３つの出力値に対する$\alpha_k$の積の和で割ったものであり、
表\ref{tab:quad_me}~の例では
$0.682/(0.682+2.39+0.106)=0.215$となる。

出力値が2となる場合のパラメータに着目する。
係り元が「の」で、第一候補が「形容詞」であること、
第二候補が「名詞」であること、第三候補が「形容詞」であることの全てが
第二候補に係るパラメータを高めており、第二候補に係る確率が
第一候補に係る確率を上回っている。
特に、出力値$b$と異なる候補（この場合、第一・第三候補）に関係する素性
も強い影響を及ぼしていることが興味深い。

\begin{table}[t]
	\scriptsize
	\begin{center}
	\begin{tabular}{|p{.8cm}||c|p{5cm}|p{.7cm}|p{1.15cm}|r|r|}
	\hline
	係り先$j$ & 
	\smash{\lower1.6ex\hbox{素性番号}} & 
	\smash{\lower1.6ex\hbox{履歴$a$}} & 
	出力値$b$ & パラメータ$\alpha_k$ & 
	\smash{\lower1.6ex\hbox{$\alpha_k$の積}} & 
	\smash{\lower1.6ex\hbox{$P(i \rightarrow j)$}} \\
	\hline \hline
	甲高い & 26 & 係り元語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「１」 
	& T & 0.83 & 0.93 & 0.53 \\
	\cline{2-6}
	& 6 & 係り先活用形「基本形」・距離「１」 & F & 0.69 & 0.81 & \\
	\cline{2-5}
	& 26 & 係り元語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「１」 & F & 1.19 & & \\
	\cline{2-5}
	& 27 & 係り元主辞「普通名詞」・語形「接続助詞」・係り先主辞「形容詞」「基本形」・距離「１」 & F & 0.81 & & \\
	\hline \hline
	声で & 3 & 係り元助詞「の」・距離「２〜５」 & T & 0.78 & 0.57 & 0.31 \\
	\cline{2-5}
	& 10 & 係り先主辞「声」・距離「２〜５」 & T & 0.79 & & \\
	\cline{2-5}
	& 23 & 係り元助詞「の」・係り先助詞「で」・距離「２〜５」 & T & 1.82 & & \\
	\cline{2-5}
	& 26 & 係り元語形「接続助詞」・係り先品詞「名詞」・距離「２〜５」 & T & 0.84 & & \\
	\cline{2-5}
	& 27 & 係り元主辞「普通名詞」・係り元語形「接続助詞」「の」・係り先主辞「普通名詞」・距離「２〜５」 & T & 0.81 & & \\
	\cline{2-6}
	& 27 & 係り元主辞「普通名詞」・係り元語形「接続助詞」「の」・係り先主辞「普通名詞」・距離「２〜５」 & F & 1.06 & 1.26 & \\
	\hline \hline
	騒然となる & 3 & 係り元助詞「の」・距離「２〜５」 & T & 0.78 & 0.11 & 0.10 \\
	\cline{2-5}
	& 8 & 係り先主辞「形容詞」・距離「２〜５」 & T & 0.86 & & \\
	\cline{2-5}
	& 26 & 係り先語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「２〜５」 & T & 0.48 & & \\
	\cline{2-6}
	& 26 & 係り元語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「２〜５」 & F & 1.08 & 1.03 & \\
	\hline
	\end{tabular}
	\caption{(\ref{sent:kodomo})の「子供たちの」の係り先推定の際の、２つ組モデルにおけるＭＥのパラメータ \\
	{\footnotesize それぞれの係り先に対して、別個に「係る（Ｔ）」「係らない（Ｆ）」を出力値とする
		パラメータが計算される。}}
	\label{tab:pair_me}
	\end{center}
\end{table}

\begin{table}[t]
	\scriptsize
	\begin{center}
	\begin{tabular}{|c|p{6cm}|p{.7cm}|p{1.15cm}|r|r|}
	\hline
	\smash{\lower1.6ex\hbox{素性番号}} & 
	\smash{\lower1.6ex\hbox{履歴$a$}} & 
	出力値$b$ & 
	パラメータ$\alpha_k$ & 
	\smash{\lower1.6ex\hbox{$\alpha_k$ の積}} & 
	\smash{\lower1.6ex\hbox{$P(i \rightarrow t_b)$}}\\
	\hline \hline
	10-2 & 第二候補主辞語彙「声」 & 1 & 0.83 & 0.682 & 0.215 \\
	\cline{1-4}
	21-1 & 係り元助詞「の」・第一候補主辞語彙「その他」 & 1 & 0.78 & & \\
	\cline{1-4}
	26-1 & 係り元語形「接続助詞」「の」・読点「無」・第一候補主辞「形容詞」 & 1 & 0.84 & & \\
	\hline
	11-2 & 第二候補助詞「で」 & 2 & 1.29 & 2.39 & 0.752 \\
	\cline{1-4}
	13-1 & 第一候補活用形「基本形」 & 2 & 0.77 &  &  \\
	\cline{1-4}
	26-1 & 係り元語形「接続助詞」「の」・読点「無」・第一候補主辞「形容詞」 & 2 & 1.23 & & \\
	\cline{1-4}
	26-2 & 係り元語形「接続助詞」「の」・読点「無」・第二候補主辞「名詞」 & 2 & 1.25 & & \\
	\cline{1-4}
	26-3 & 係り元語形「接続助詞」「の」・読点「無」・第三候補主辞「形容詞」 & 2 & 1.24 & & \\
	\cline{1-4}
	27-1 & 係り元主辞「普通名詞」・語形「接続助詞」・第一候補主辞「形容詞」「基本形」 & 2 & 0.84 & & \\
        \hline
	3 & 係り元助詞「の」 & 3 & 0.59 & 0.106 & 0.034 \\
	\cline{1-4}
	7 & 係り元読点「無」 & 3 & 0.84 & & \\
	\cline{1-4}
	10-2 & 第二候補主辞語彙「声」 & 3 & 2.15 & & \\
	\cline{1-4}
	11-2 & 第二候補助詞「で」 & 3 & 0.46 & & \\
	\cline{1-4}
	17-3 & 係り元-第三候補文節間読点「無」 & 3 & 1.40 & & \\
	\cline{1-4}
	20-3 & 係り元語形品詞「形容詞」・第三候補読点「無」 & 3 & 0.80 & & \\
	\cline{1-4}
	27-2 & 係り元主辞「普通名詞」・語形「接続助詞」・第二候補主辞「名詞」 & 3 & 0.79 & & \\
	\cline{1-4}
	27-3 & 係り元主辞「普通名詞」・語形「接続助詞」・第三候補主辞「形容詞」「基本形」 & 3 & 0.70 & & \\
	\hline
	\end{tabular}
	\caption{(\ref{sent:kodomo})の「子供たちの」の係り先推定の際の、４つ組モデルにおけるＭＥのパラメータ \\
	{\footnotesize 各候補が選ばれる（出力値が1,2,3）場合のパラメータを一つの確率分布で求めている。}}
	\label{tab:quad_me}
	\end{center}
\end{table}


\subsection{他研究との比較}
\subsubsection{EDRコーパスでの精度の比較}

係り受けの精度判定にEDRコーパスを用いている他研究と比較してみる。
決定木を用いた手法\cite{Haruno98}での精度は84〜85$\%$、
語の共起確率を用いた手法\cite{Fujio99}では、86.8$\%$となっている。
我々の手法はこれらを上回っており、
EDRコーパスに対してテストした中では
最も高い水準といえよう。

また、\cite{Kanayama99}では、３つ組／４つ組モデルを
単純な相対頻度を用いて構成している。
そこでの精度は86.7$\%$であり、ME法の利用によって
約1.9$\%$精度が向上したことになる。
精度向上の要因は、
ME法によってデータスパースネスの問題が軽減でき、
従来は入れられなかった語彙や活用に関する素性を追加できたことであると思われる。


\subsubsection{京大コーパスでの精度の比較}


いくつかの研究では、京大コーパス\cite{kc}を用いて精度を測っている。
構文的・語彙的情報を統合して構文木の生起確率を求めている手法\cite{Shirai98}
での精度は85〜86$\%$である。

本研究と同様に、ME法を用いた研究~\cite{Sekine99},~\cite{Uchimoto99b}
では、京大コーパスの1月9日分の1,246文を用いている。
比較のために、同じコーパスでテストした結果は、
表\ref{tab:accuracy_kc}~のようになった。

\begin{table}
	\begin{center}
	\begin{tabular}{|l|l|rc|}
	\hline
	\smash{\lower2.0ex\hbox{解析成功文}} & 
		文節正解率 & 87.08$\%$ & (8299/9530) \\
			\cline{2-4}

&		文正解率   & 44.70$\%$ & (493/1103) \\
	\hline
	\end{tabular}
	\caption{EDRコーパスで学習し、京大コーパスでテストした際の解析結果}
	\label{tab:accuracy_kc}
	\end{center}
\end{table}

文末から決定的に係り先を決定するモデル\cite{Sekine99}の精度は87.14$\%$で我々と同程度、
後方文脈を考慮するモデル\cite{Uchimoto99b}は87.93$\%$で我々の精度よりも高くなっている。
その原因として、以下のことが考えられる。

\begin{itemize}
\item
我々は、学習データとしてEDRコーパスを用いている。
\cite{Uchimoto99b}などと比べて約24倍の学習データがあるとはいえ、
括弧付けの方針の違いなどから、
京大コーパスでの解析の誤りを引き起こすことが多い。
\item
関根ら、内元らは京大コーパス中にある形態素解析・文節区切りの結果を用いているのに対し、
我々はJUMANで解析したものを用いているため、形態素解析の誤りを含み、
解析誤りの原因となっている。
\item
文法SLUNGがEDRコーパスの括弧付けの方針に従って作られており、
京大コーパスにあるような係り方を許さない場合がある。
\end{itemize}

現在のところ、京大コーパスの解析には被覆率・精度ともに充分でないが、
文法やシステムの改変により対処した上で
本論文で提案する手法を有効に適用できるようにすれば、
より高い精度が得られると考えている。

\subsubsection{学習量の比較}

図\ref{fig:graph}~より、最高値に近い精度を得るためには、
10〜15万文の学習コーパスを要している。
この学習量は、EDRコーパスを用いている研究\cite{Fujio99}と同程度であり、
\cite{Uchimoto99}などの京大コーパスを用いた場合より、
20倍程度の学習量になっている。

一般に、ME法を用いることにより学習量を減らすことができると考えられているが、
３つ組／４つ組モデルでは、複数の係り先に関する属性を同時に捉える
条件付き確率を用いているため、
区別される事象の数が大きくなり、多くの学習量が必要になっている。

我々のモデルは、EDRコーパスのような多くの学習データを有効に
利用できるモデルであるといえる反面、京大コーパスのように
学習データ量が限られている時には、
より効率のよい素性選択などが要求されるであろう。



\subsubsection{解析速度の比較}

本研究での係り受け解析は、あくまで詳細な構文構造を得るという目標の
前段階であるため、速度に焦点を当ててはいないが、参考のために比較しておく。

文末から決定的に係り先を決定するモデル\cite{Sekine99}では、
一文当たり平均0.03秒（Sun Ultra10, 300MHz）で解析できるのに対し、
一方、我々のシステムではEDRコーパスの文に対して平均約0.5秒
（Pentium III, 500MHz : 経験的に、上記の計算機の約３倍の速度）を要する。
両者には大きな差があるものの、我々の速度も非実用的なものではない。

また、そのほとんどはHPSGパーザによる部分木の生成の時間である。
単に係り受け構造を求めるだけなら速度を向上する余地は多分にあるうえ、
HPSGパーザ自体の高速化も研究されており\cite{Nishida99,Torisawa00}、
速度の問題は深刻であるとは考えていない。



\section{まとめ}\label{sec:conclusion}

本稿では、文法を用いて係り受け解析をする際に望ましい統計モデルについて論じた。
係り先の候補を文法が許すものに制限した後、
係り元から最も近い文節・二番目に近い文節・最も遠い文節のみに絞る。
これにより、係り元と全ての係り先候補の属性を同時に考慮する
「３つ組／４つ組モデル」を用いることができるようになり、
88.6$\%$という高い係り受け精度を達成した。
また、このモデルが精度向上に確かに寄与していることを示した。



\bibliographystyle{jnlpbbl}
\bibliography{nlp-j}


\begin{biography}
\biotitle{略歴}
\bioauthor{金山 博}{
1998年東京大学理学部情報科学科卒業。
2000年同大学院理学系研究科修士課程修了。
同年日本アイ・ビー・エム（株）入社、
現在に至る。
構文解析・機械翻訳等に関する研究に従事。
}

\bioauthor{鳥澤 健太郎}{
1992年東京大学理学部情報科学科卒業。
1995年同大学大学院理学系研究科情報科学専攻博士課程退学、
同年より同大学大学院理学系研究科情報科学専攻助手。
1998年より科学技術振興事業団さきがけ研究21研究員兼任。
計算言語学の研究に従事。理学博士。言語処理学会会員。
}

\bioauthor{光石 豊}{
1996年東京大学理学部情報科学科卒業。
1998年同大学院理学系研究科修士課程修了。
現在、同博士課程在学中。
HPSGの枠組による日本語文法に関する研究に従事。
ACM SIGMODJ学生会員。
}

\bioauthor{辻井 潤一}{
1971年京都大学工学部電気工学科卒業。1973年同大学大学院工学研究科修士課程修了。
京都大学工学部電気第2工学科助手、
助教授を経て、1988年英国マンチェスタ大学科学技術研究所(UMIST)教授、
1995年より、東京大学大学院理学系研究科教授（情報科学専攻）、現在に至る。
1981〜82年フランス・CNRS（グルノーブル）、招聘研究員。 
自然言語処理、機械翻訳の研究に従事。工学博士。
国際計算言語学委員会(ICCL)メンバ、情報処理学会、人工知能学会など、会員。
2000年6月より言語処理学会会長。
}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\newpage
\thispagestyle{plain}

\verb+ +

\end{document}

