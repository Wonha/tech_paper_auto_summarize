\section{関連研究}\label{sec:related}

本節では、これまでに提案されてきた日本語構文解析のための統計的アプローチと、
本研究で構文解析に用いる日本語文法SLUNG、及び
確率モデルの推定に用いる最大エントロピー法を紹介する。

\subsection{従来の統計的構文解析手法}\label{subsec:conventional}

日本語の係り受け解析のための統計的手法として、様々なモデルが考案されており、
次の２つに大別される。

\begin{description}
\item[生起確率を計算するモデル]
文$s$が与えられた時に、ある構文木$T$が生起する条件付き確率を求める方法である。
すなわち、次のような構文木$T$を選択する。
\begin{equation}
\mathop{\rm argmax}_T P(T|s)
\end{equation} \refstepcounter{enums}
\item[文中の係り受け確率の積をとるモデル]
文節$i$と文節$j$が係り受け関係にある確率$P(i \rightarrow j)$を考え、
式(\ref{equ:product})に示すような、
文中にある全ての係り受けの積を最大化する係り受け関数 $dep(i)$ を求める方法である。
\begin{equation}
\mathop{\rm argmax}_{dep}
\displaystyle{\prod_i} P(i \rightarrow dep(i))
\label{equ:product}
\end{equation} \refstepcounter{enums}
\end{description}

前者に属するものとして、確率文脈自由文法を用いたもの\cite{Mori98}や、
確率一般化LR法を用いたもの\cite{Shirai98}などがある。
これらは、数学的に妥当な確率を用いることができ、
形態素解析など様々なレベルとの統合が容易であるという利点があるものの、
現状では係り受け解析の精度は最高でも白井らの85〜86$\%$にとどまっている。

一方、後者の手法は、比較的学習が容易なため、
高い解析精度が得られる手法が多数提案されている。
実際、最大87.9$\%$と、生起確率に基づくものよりも高い精度が報告されている\cite{Uchimoto99b}。
本研究の手法もこのアプローチに基づいており、
以下でいくつかの研究を紹介する。
これらの手法及び本稿で提案する手法は、上記の$P(i \rightarrow j)$の求め方に違いがある。


決定木を用いたモデル\cite{Haruno98}、最大エントロピー法を用いたモデル\cite{Uchimoto99}、
距離確率と語彙確率を用いたモデル\cite{Fujio99}では、
係り元文節$i$の品詞や語彙や読点の有無など、係り先文節$j$の品詞や語彙、
そして二文節間の距離・読点や副助詞「は」の数などを属性として、
ある属性を持った二文節が存在する時に
それが係り受け関係にある確率を二文節$i, j$間の係り受けのしやすさとしている。
英語の統計的構文解析において
二語間の距離が係り受けを決定する重要な要素となる\cite{Collins96}のと同様に、
日本語の解析においても二文節間の距離が重要であるとされ、
上記のモデルではいずれも文節間にある文節数を属性として用いている。
これらのモデルでは、文節$i$と$j$以外の文節の情報は、
文節間の距離などの属性を除いては反映されない。


係り元・係り先とそのまわりの文節を考慮するモデル\cite{Uchimoto99b}では、
係り元文節$i$の係り先文節$j$への係りやすさの計算に、
$i$より右側にある全ての文節の情報を用いている。
そのために、二文節間の関係を、「係る」「係らない」の二値ではなく、
「越えて、遠くの文節に係る」「係る」「手前の文節に係る」の三値を出力するものとして学習する。
そして、$i$が$j$に係る確率を、$i$が$i, j$間の文節を「越える」確率と
$i$が$j$より右側の文節より「手前に係る」確率の積で補正する。
これにより、ある種の文脈情報が取り扱えることになり、
解析精度が\cite{Uchimoto99}より約$1\%$向上したことが報告されている。
但し、このモデルでは、係り元文節がそれより右側にあるそれぞれの文節に
「係る」か「越える」か「手前に係る」かを
互いに独立であると仮定しなければならない。


本研究で用いる３つ組／４つ組モデル\cite{Kanayama99}では、
２つまたは３つの係り先候補の属性を
同時に考慮できるため、文脈情報が扱えるうえ、
さまざまな望ましい点がある。
これに関しては本論文の\ref{sec:ourmodel}~節以降で詳しく解説する。


\subsection{日本語文法SLUNG}\label{subsec:slung}

本論文で提案する手法では、人手で書かれた文法で候補を絞ることが必須である。
我々が用いるSLUNG\cite{Mitsuishi98}は、
HPSG\cite{PollardSag94}の枠組みで記述された日本語文法であり、
8つのスキーマと、48個の語彙項目テンプレート\footnote{主に自立語に対して、品詞毎に振る舞いを記述したもの。
具体的な語彙は区別していない。}、105個の語彙項目\footnote{助詞や接尾辞などに対しては、
それぞれの語に対して文法的性質が記述されている。}からなる。
EDR日本語コーパス\cite{EDR}の文に対して98.4$\%$と、
非常に高い被覆率（構文木を一つでも返した文の割合）を示している。

文法自体は曖昧性解消の機構を持っていないため、SLUNGを用いて構文解析した場合、
文法的に許される全ての構文木が出力される。
本研究では、文節係り受けの統計モデルを用いることにより、
出力された構文木から最も優先度の高いものを選び出すことができるようになる。


\subsection{最大エントロピー法}\label{subsec:me}

統計モデルの推定に、最大エントロピー法(ME法)\cite{Berger96}を用いる。
ME法では、
「学習コーパス中の履歴の特定の条件を満たし、かつ特定の出力値を得る場合」
（素性）の頻度を得て、
様々な素性に対するパラメータを、
出力値の確率分布が最も一様分布に近づくように調整して求める。
別の素性に対し、それぞれ満たす集合に重なりがあってもよく、
抽象度の高い素性と低い素性を任意に混ぜることができるため、
統計モデルを構築する際のデータスパースネスの問題を軽減できる。
日本語係り受け解析でもME法は非常に有用で\cite{Uchimoto99}、
品詞の情報だけでなく、頻度の高い単語に対しては語彙的情報も加えるといった
柔軟な素性の追加が容易である。

本稿での実験における精度は、単純な相対頻度で推定した３つ組／４つ組モデル\cite{Kanayama99}
よりも約1.9$\%$向上しているが、
その要因として、ME法を用いることで以前よりも多くの素性を追加できたことが挙げられる。


