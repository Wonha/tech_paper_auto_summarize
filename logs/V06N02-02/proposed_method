superwordモデルの定式化
単語\cite{mori96a}や文字列の$n$-gram\cite{aito96}では与え
られた系列を単語ないし文字列に分割するやり方が一意に決まらないため，こ
れらのモデルは直前の\(n-1\)個の単語や文字列を状態とする，隠れマルコフ
モデルの一種と考えられる．単語や文字列の集合は，語彙知識として人手で与
えられるか，あるいは経験的な規則に基づいて訓練テキストから抽出されるも
のである．ここで定義するsuperwordとはこれら単語や文字列を一般化したも
のであるが，それらと対照的なのは，訓練テキスト中の任意の文字列を含み得
る点である．ただし，言語モデルとして意味を持つために必要最小限のヒュー
リスティクスは導入せねばならない．そこで，次の条件を満たす文字列を
superwordと定義する．
\begin{itemize}
\item 訓練テキスト中に最低2回出現する
\end{itemize}
または
\begin{itemize}
\item 長さ1の文字列である
\end{itemize}
訓練テキストにおける再現性の仮定は，ある文字列が何らかの言語的な
まとまりを成すか否かに対する基準となるものであり，そのような基準として
考え得る制約の中でもっとも緩い条件\break
として与えてある．すなわち，ある文字
列が訓練テキスト中で1回しか出現しない，または1回\break
も出現しないならば，そ
の文字列が何らかのまとまりを成すだろうという証拠は，他に人間が知識とし
て与えない限り得られない．

また，再現性とは独立に，長さ1の文字列は全てsuperwordと定義してい
る．これにより，全ての文は少なくとも1通りのsuperwordの系列として表現で
きることが保証される．superword $n$-gram確率
\(P(w_i|w_{i-(n-1)}\cdots w_{i-1})\)は，直前に
\(n-1\)個のsuperwordの列 \(w_{i-(n-1)}\cdots w_{i-1}\)が\break
生起したと仮定
した時のsuperword \(w_i\)の条件付き生起確率である．

与えられた文\(\futo{C}=C_1C_2\cdots C_k\)がsuperwordの列\(w_1w_2\cdots
w_l\)に分割できるとき，\(w_1w_2\cdots w_l\in\futo{C}\)と書く．
superword $n$-gramモデルは，$\futo{C}$の全ての可能な分割に関して計算\break
し
たsuperword $n$-gram確率の積の総和をもって$\futo{C}$の発生確率を
推定するものである．すなわち，その確率を次式で与える．
\begin{equation}
P(\futo{C})=\!\!\!\!\!\!\!\sum_{w_1\cdots w_l\in\futo{C}}
\prod_{i=1}^lP(w_i|w_{i-(n-1)}\cdots w_{i-1})
\label{forward}
\end{equation}
ここで\(n=1\)の時，すなわちsuperword unigramモデルは，文全体の生起確率
がそれぞれ独立なsuperwordの生起確率の積で表されるとするものであり，
multigram\cite{deligne95}と呼ばれる可変長単語列に基づく言語モ
デルと同一のものである．

superword $n$-gramモデルのクラスは，単語や文字列の$n$-gramモデルのクラ
スを包含する．この性質は，パラメータさえ適切に与えることができれば，
superwordに基づくモデルの性能が単語や文字列の$n$-gramモデルの性能と同
等かそれ以上になることを保証する．
\vspace{-2mm}
superwordモデルの学習法
\vspace{-1mm}
\subsection{superword集合の獲得}
\vspace{-1mm}
モデルの獲得にあたっては，パラメータの学習に先立ち，訓練テキストから
superwordの集合を求める必要がある．長さ1のsuperwordについては自明であ
るから，再現性のある文字列を集める作業が核心である．これには，訓練テキ
ストの全ての位置から始まる半無限文字列をソートして任意長$n$-gram統計を
求め\cite{nagao94}，2回以上出現する文字列を記録\break
する方法が考えられる．
しかし，再現性のある文字列だけに興味がある場合には，短い文字列から長い
文字列へと逐次的に求める簡便な方法で十分である\cite{mori96b}．

実験で用いたテキストコーパスでは，長さ$L$のsuperwordの種類は大きな$L$
では単調に減少\break
することが観察されている．
\subsection{確率分布のForward-Backward学習}
superwordモデルでは，ある状態から別の状態に移る時に，ある確率で一つの
superwordを出力する．状態は，直前\(n-1\)個のsuperwordによって定まるも
のとする．ただし，\(n=1\)の場合はただ1つの状態のみ存在するものとする．
superwordモデルの出力はsuperword列としてで\break
はなく文字の系列として観測さ
れる．そこで，通常の隠れマルコフモデルと同様に扱うことを\break
可能にするため
副状態を導入して，1回の状態遷移で1文字を出力する等価なモデルを考える．\break
副状態は，状態を分割したもので，そこに移る時最後に出力したsuperwordの
各文字に対応す\break
る．superword \(w_i\)の表記を\(C_1\cdots C_j\cdots C_L\)
とし，\(w_i\)の長さ$j$のプレフィックスを\(w_{i,j}\)とする．\break
そして，
superword モデルにおける\(w_i\)の出力を，等価なモデルでは次のように表
す．すなわち，\break
確率\(P(w_i|w_{i-(n-1)}\cdots w_{i-1})\)で副状態
\(w_{i-(n-2)}\cdots w_{i-1}w_{i,1}\)に移る時に\(C_1\)を出力し，以後確
率\break
1で副状態\(w_{i-(n-2)}\cdots w_{i-1}w_{i,j}\)\mbox{に移る時に\(C_j\)を出力
し，最終的に状態\break
\(w_{i-(n-2)}\cdots w_{i-1}w_i\)に}\break
至ると考える．
例として，図\ref{fig4-1}の状態遷移図では「東北大学」というsuperwordを
出力して状態\break
（東北大学）に至る様子を示している．等価なモデルでは，本来
の確率で副状態（東）に移る時に文字「東」を出力，確率1で（東北）に移る
時に「北」を出力，確率1で（東北大）に移る時\break
に「大」を出力，最終的に確
率1で（東北大学）に移る時に「学」を出力する．分割前の状態が異なる副状
態は同一視しないので，例えば（東北大学）の副状態（東）と（東京）の副状
態（東）は異なることに注意すべきである．

\begin{figure}
\begin{center}
\epsfile{file=33.eps}
\caption{「東北大学」というsuperwordの各文字に対応した副状態の系列}
\label{fig4-1}
\end{center}
\end{figure}

\(n=1\)，すなわちsuperword unigram確率の学習のための初期確率としては，
全てのsuperwordが等確率で発生するとして，superwordの数の逆数を与
える．
\(n>1\)については，対応するsuperwordの\((n-1)\)-gram確率で初期化する．

確率の再推定のために，図\ref{fig4-2}のように訓練テキストから全ての
superwordを洗い出す．
\begin{figure}[t]
\vspace{-7mm}
\begin{center}
\epsfile{file=34.eps}
\caption{「東北大学」というテキストの解析．矩形はsuperwordを，実線は可
能なパスを表す．\\``\$''は文の終端}
\label{fig4-2}
\end{center}
\end{figure}
次に，連接可能な$n$個のsuperwordの組に関して，次式によって確率を更新す
る．
\begin{eqnarray}
\lefteqn{\tilde{P}(w_i|w_{i-(n-1)}\cdots w_{i-1})=} \nonumber \\
& &\frac{\displaystyle\sum_t\alpha_{t-1}(w_{i-(n-1)}\cdots w_{i-1})
P(w_i|w_{i-(n-1)}\cdots w_{i-1})\beta_t(w_{i-(n-2)}\cdots w_i)}
{\displaystyle\sum_t\alpha_t(w_{i-(n-2)}\cdots w_i)\beta_t(w_{i-(n-2)}\cdots w_i)}
\end{eqnarray}
ただし，$\alpha$, $\beta$はそれぞれForward確率，Backward確率
で，以下のように再帰的に定義する．
\begin{equation}
\alpha_1(w)=P(w|\#), \qquad \mbox{\#は文頭を表す状態}
\end{equation}
時刻$t$(\(t>1\))でsuperword \(w_i\)の第1字目を出力するとき
\begin{equation}
\alpha_t(w_{i-(n-2)}\cdots w_i)=\sum_{w_{i-(n-1)}}
\alpha_{t-1}(w_{i-(n-1)}\cdots w_{i-1})P(w_i|w_{i-(n-1)}\cdots
w_{i-1})
\end{equation}
時刻$t$(\(t>1\))でsuperword \(w_i=C_1\cdots C_j\cdots C_L\)の第$j$字
目(\(j>1\))を出力するとき
\begin{equation}
\alpha_t(w_{i-(n-2)}\cdots w_{i-1}w_{i,j})
=\alpha_{t-1}(w_{i-(n-2)}\cdots w_{i-1}w_{i,j-1})
\end{equation}
ただし
\begin{equation}
\alpha_t(w_{i-(n-2)}\cdots w_{i-1}w_i)
=\alpha_t(w_{i-(n-2)}\cdots w_{i-1}w_{i,L})
\end{equation}
同様に
\begin{equation}
\beta_T(\$)=1, \qquad \mbox{$T$は文末記号 ``\$'' を出力する時刻}
\end{equation}
時刻$t$(\(t<T\))でsuperword \(w_i\)の第1字目を出力するとき
\begin{equation}
\beta_{t-1}(w_{i-(n-1)}\cdots w_{i-1})
=\sum_{w_i}\beta_t(w_{i-(n-2)}\cdots w_i)
P(w_i|w_{i-(n-1)}\cdots w_{i-1})
\end{equation}
時刻$t$(\(t<T\))でsuperword \(w_i=C_1\cdots C_j\cdots C_L\)の第$j$字
目(\(j>1\))を出力するとき
\begin{equation}
\beta_{t-1}(w_{i-(n-2)}\cdots w_{i-1}w_{i,j-1})
=\beta_t(w_{i-(n-2)}\cdots w_{i-1}w_{i,j})
\end{equation}
ただし
\begin{equation}
\beta_t(w_{i-(n-2)}\cdots w_{i-1}w_i)
=\beta_t(w_{i-(n-2)}\cdots w_{i-1}w_{i,L})
\end{equation}
長さ制限の導入
再現性のある文字列の長さを十分大きく取れば，前節までに述べたモ
デルは与えられた訓練テキストに対して一意に求まる．以下では，これを
一般superword $n$-gramモデルと呼ぶ．
しか\break
し，一般モデルのパラメータ数は大きい．特に，\(n>2\)ではsuperwordの
組み合わせが爆発し，現実的ではない．さらに，あまりに長いsuperwordは訓
練テキストに特化してしまう恐れがあり，汎化能力の低下を招く．

これに対処するため，一般モデルに加えて長さ制限付きのsuperwordモデル
を導入する．これは，逐次的な再現性文字列の獲得を早い段階で打ち切っ
て小さなsuperwordの集合をつくり，その集合に基づいて
Forward-Backward学習を行うことで得ることができる．以下では，長さ$l$に
制限されたsuperword $n$-gram確率を\(P_{|w|\leq
l}(w_i|w_{i-(n-1)}\cdots w_{i-1})\)と表記する．

長さが$l$に制限されたsuperword $n$-gramモデルは，図\ref{ergodic}に示す
ような，状態数が高々字種の\break
$l$乗に制限されたエルゴーディックHMMと
なる．ただし，図は\(l=2\)とした例である．
\begin{figure}
\begin{center}
\epsfile{file=35.eps}
\vspace*{-3mm}
\caption{「東北大学」の部分文字列をsuperwordの集合とする時の長さ制限モ
デル}
\label{ergodic}
\end{center}
\end{figure}
複合モデル
$n$-gram に代表される確率モデルにおいては，モデルのパラメータを精度良
く推定するに足るサンプルが得られないことが多く，パラメータ空間のさまざ
まなスムージング法が提案されている\cite{federico95}．その一つに，いく
つかのモデルの確率の重み付き線形和で表現する方法がある\cite{jelinek80}．
これは本来，詳細なモデルの値が信用できない場合に，パラメータの少ない安
定したモデルの値を代用するものであ\break
るが，性質の異なる複数のモデルを組み
合わせてより良いモデルを得るという積極的な利用も可能である．本節では，
この線形補間に基づくいくつかの複合モデルを考える．

superword bigram(\(n=2\))モデル
に対しては，superword unigram確率によって補間された確率は次式で与
えられる．
\begin{equation}
\hat{P}(w_i|w_{i-1})=\lambda_{\mbox{\scriptsize g}}
P(w_i|w_{i-1})+(1-\lambda_{\mbox{\scriptsize g}})P(w_i)
\end{equation}
重み係数\(\lambda_{\mbox{\scriptsize g}}\)は，訓練テキストとは別のサン
プル(held-outデータ)またはクロスバリデーション\break
によって得られ
る仮想的な未知データの確率を最大にするように再推定する．

前述したように，一般superword bigramはパラメータ量が多くなり過ぎ
るので，実際にはsuperwordの長さを最大$l$に制限したモデルと組み合
わせる．これは次式で与えられる．
\begin{equation}
\hat{P}_{|w|\leq l}(w_i|w_{i-1})=
\lambda_{\mbox{\scriptsize b}}P_{|w|\leq l}(w_i|w_{i-1})
+(1-\lambda_{\mbox{\scriptsize b}})P_{|w|\leq l}(w_i)
\label{limited}
\end{equation}

式(\ref{limited})のような制限されたモデルでは，長い語の表現に難がある
ことも考えられる．そこで，長さ制限付きsuperword bigramモデルと
一般superword unigramモデルの複合モデルを導\break
入する．
複合superword bigram確率は次式で定義される．
\begin{equation}
P_{\mbox{\scriptsize comp}}(w_i|w_{i-1})=
\lambda_{\mbox{\scriptsize c}}\hat{P}_{|w|\leq l}(w_i|w_{i-1})
+(1-\lambda_{\mbox{\scriptsize c}})P(w_i)
\label{composite}
\end{equation}

さらに，複合superword bigramモデルを，文字のtrigramモデルによってスムー
ジングする\break
ことを考える．文字のtrigramモデルはそれ自身で強力な曖昧性削
減能力を持っているが\cite{mori96}，単語$n$-gramモデルと
融合させることにより，認識対象中の未知の文字列の存在による単語解析精度
の低下の影響を低減させ，頑健なモデルとすることができる\cite{mori96a}
．文字によって補間された複合superword bigram確率は次式で定義され
る．
\begin{equation}
\hat{P}_{\mbox{\scriptsize comp}}(w_i|w_{i-1})=
\lambda_{\mbox{\scriptsize w}}P_{\mbox{\scriptsize comp}}(w_i|w_{i-1})
+(1-\lambda_{\mbox{\scriptsize w}})\hat{P}_{\mbox{\scriptsize c}}(w_i|w_{i-1})
\label{charint}
\end{equation}
ただし，\(\hat{P}_{\mbox{\scriptsize c}}(w_i|w_{i-1})\)はsuperword
\(w_i\)が生起する確率を，補間された文字trigram確率の積によって求めたも
のである．すなわち，\(w_i\)の表記を\(C_1\cdots C_{L(w_i)}\)，
\(w_{i-1}\)の最後の2文字を\(C_{-1}C_0\)と書くとき
\begin{equation}
\hat{P}_{\scr{c}}(w_i|w_{i-1})=
\left(\prod_{j=1}^{L(w_i)}\hat{P}_\scr{c}(C_j|C_{j-2}C_{j-1})\right)
\cdot d(L(w_i))
\label{charmodel}
\end{equation}
ただし\(\hat{P}_\scr{c}(C_j|C_{j-2}C_{j-1})\)はbigram, unigram等により
補間された文字trigram確率である．また，\(d(L(w_i))\)は文字モデル
が生成する単語の長さに関する分布関数である．
