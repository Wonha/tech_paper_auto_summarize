単語[CITE]や文字列の[MATH]-gram[CITE]では与えられた系列を単語ないし文字列に分割するやり方が一意に決まらないため，これらのモデルは直前の[MATH]個の単語や文字列を状態とする，隠れマルコフモデルの一種と考えられる．
単語や文字列の集合は，語彙知識として人手で与えられるか，あるいは経験的な規則に基づいて訓練テキストから抽出されるものである．
ここで定義するsuperwordとはこれら単語や文字列を一般化したものであるが，それらと対照的なのは，訓練テキスト中の任意の文字列を含み得る点である．
ただし，言語モデルとして意味を持つために必要最小限のヒューリスティクスは導入せねばならない．
そこで，次の条件を満たす文字列をsuperwordと定義する．
訓練テキスト中に最低2回出現する
または
長さ1の文字列である
訓練テキストにおける再現性の仮定は，ある文字列が何らかの言語的なまとまりを成すか否かに対する基準となるものであり，そのような基準として考え得る制約の中でもっとも緩い条件として与えてある．
すなわち，ある文字列が訓練テキスト中で1回しか出現しない，または1回も出現しないならば，その文字列が何らかのまとまりを成すだろうという証拠は，他に人間が知識として与えない限り得られない．
また，再現性とは独立に，長さ1の文字列は全てsuperwordと定義している．
これにより，全ての文は少なくとも1通りのsuperwordの系列として表現できることが保証される．
superword [MATH]-gram確率[MATH]は，直前に[MATH]個のsuperwordの列[MATH]が生起したと仮定した時のsuperword [MATH]の条件付き生起確率である．
与えられた文[MATH]がsuperwordの列[MATH]に分割できるとき，[MATH]と書く．
superword [MATH]-gramモデルは，[MATH]の全ての可能な分割に関して計算したsuperword [MATH]-gram確率の積の総和をもって[MATH]の発生確率を推定するものである．
すなわち，その確率を次式で与える．
ここで[MATH]の時，すなわちsuperword unigramモデルは，文全体の生起確率がそれぞれ独立なsuperwordの生起確率の積で表されるとするものであり，multigram[CITE]と呼ばれる可変長単語列に基づく言語モデルと同一のものである．
superword [MATH]-gramモデルのクラスは，単語や文字列の[MATH]-gramモデルのクラスを包含する．
この性質は，パラメータさえ適切に与えることができれば，superwordに基づくモデルの性能が単語や文字列の[MATH]-gramモデルの性能と同等かそれ以上になることを保証する．
モデルの獲得にあたっては，パラメータの学習に先立ち，訓練テキストからsuperwordの集合を求める必要がある．
長さ1のsuperwordについては自明であるから，再現性のある文字列を集める作業が核心である．
これには，訓練テキストの全ての位置から始まる半無限文字列をソートして任意長[MATH]-gram統計を求め[CITE]，2回以上出現する文字列を記録する方法が考えられる．
しかし，再現性のある文字列だけに興味がある場合には，短い文字列から長い文字列へと逐次的に求める簡便な方法で十分である[CITE]．
実験で用いたテキストコーパスでは，長さ[MATH]のsuperwordの種類は大きな[MATH]では単調に減少することが観察されている．
superwordモデルでは，ある状態から別の状態に移る時に，ある確率で一つのsuperwordを出力する．
状態は，直前[MATH]個のsuperwordによって定まるものとする．
ただし，[MATH]の場合はただ1つの状態のみ存在するものとする．
superwordモデルの出力はsuperword列としてではなく文字の系列として観測される．
そこで，通常の隠れマルコフモデルと同様に扱うことを可能にするため副状態を導入して，1回の状態遷移で1文字を出力する等価なモデルを考える．
副状態は，状態を分割したもので，そこに移る時最後に出力したsuperwordの各文字に対応する．
superword [MATH]の表記を[MATH]とし，[MATH]の長さ[MATH]のプレフィックスを[MATH]とする．
そして，superwordモデルにおける[MATH]の出力を，等価なモデルでは次のように表す．
すなわち，確率[MATH]で副状態[MATH]に移る時に[MATH]を出力し，以後確率 1で副状態[MATH]に移る時に[MATH]を出力し，最終的に状態[MATH]に至ると考える．
例として，図[REF_fig4-1]の状態遷移図では「東北大学」というsuperwordを出力して状態（東北大学）に至る様子を示している．
等価なモデルでは，本来の確率で副状態（東）に移る時に文字「東」を出力，確率1で（東北）に移る時に「北」を出力，確率1で（東北大）に移る時に「大」を出力，最終的に確率1で（東北大学）に移る時に「学」を出力する．
分割前の状態が異なる副状態は同一視しないので，例えば（東北大学）の副状態（東）と（東京）の副状態（東）は異なることに注意すべきである．
[MATH]，すなわちsuperword unigram確率の学習のための初期確率としては，全てのsuperwordが等確率で発生するとして，superwordの数の逆数を与える．
[MATH]については，対応するsuperwordの[MATH]-gram確率で初期化する．
確率の再推定のために，図[REF_fig4-2]のように訓練テキストから全てのsuperwordを洗い出す．
次に，連接可能な[MATH]個のsuperwordの組に関して，次式によって確率を更新する．
ただし，[MATH], [MATH]はそれぞれForward確率，Backward確率で，以下のように再帰的に定義する．
時刻[MATH]([MATH])でsuperword [MATH]の第1字目を出力するとき
時刻[MATH]([MATH])でsuperword [MATH]の第[MATH]字目([MATH])を出力するとき
ただし
同様に
時刻[MATH]([MATH])でsuperword [MATH]の第1字目を出力するとき
時刻[MATH]([MATH])でsuperword [MATH]の第[MATH]字目([MATH])を出力するとき
ただし
再現性のある文字列の長さを十分大きく取れば，前節までに述べたモデルは与えられた訓練テキストに対して一意に求まる．
以下では，これを一般superword [MATH]-gramモデルと呼ぶ．
しかし，一般モデルのパラメータ数は大きい．
特に，[MATH]ではsuperwordの組み合わせが爆発し，現実的ではない．
さらに，あまりに長いsuperwordは訓練テキストに特化してしまう恐れがあり，汎化能力の低下を招く．
これに対処するため，一般モデルに加えて長さ制限付きのsuperwordモデルを導入する．
これは，逐次的な再現性文字列の獲得を早い段階で打ち切って小さなsuperwordの集合をつくり，その集合に基づいてForward-Backward学習を行うことで得ることができる．
以下では，長さ[MATH]に制限されたsuperword [MATH]-gram確率を[MATH]と表記する．
長さが[MATH]に制限されたsuperword [MATH]-gramモデルは，図[REF_ergodic]に示すような，状態数が高々字種の[MATH]乗に制限されたエルゴーディックHMMとなる．
ただし，図は[MATH]とした例である．
[MATH]-gramに代表される確率モデルにおいては，モデルのパラメータを精度良く推定するに足るサンプルが得られないことが多く，パラメータ空間のさまざまなスムージング法が提案されている[CITE]．
その一つに，いくつかのモデルの確率の重み付き線形和で表現する方法がある[CITE]．
これは本来，詳細なモデルの値が信用できない場合に，パラメータの少ない安定したモデルの値を代用するものであるが，性質の異なる複数のモデルを組み合わせてより良いモデルを得るという積極的な利用も可能である．
本節では，この線形補間に基づくいくつかの複合モデルを考える．
superword bigram([MATH])モデルに対しては，superword unigram確率によって補間された確率は次式で与えられる．
重み係数[MATH]は，訓練テキストとは別のサンプル(held-outデータ)またはクロスバリデーションによって得られる仮想的な未知データの確率を最大にするように再推定する．
前述したように，一般superword bigramはパラメータ量が多くなり過ぎるので，実際にはsuperwordの長さを最大[MATH]に制限したモデルと組み合わせる．
これは次式で与えられる．
式([REF_limited])のような制限されたモデルでは，長い語の表現に難があることも考えられる．
そこで，長さ制限付きsuperword bigramモデルと一般superword unigramモデルの複合モデルを導入する．
複合superword bigram確率は次式で定義される．
さらに，複合superword bigramモデルを，文字のtrigramモデルによってスムージングすることを考える．
文字のtrigramモデルはそれ自身で強力な曖昧性削減能力を持っているが[CITE]，単語[MATH]-gramモデルと融合させることにより，認識対象中の未知の文字列の存在による単語解析精度の低下の影響を低減させ，頑健なモデルとすることができる[CITE]．
文字によって補間された複合superword bigram確率は次式で定義される．
ただし，[MATH]はsuperword [MATH]が生起する確率を，補間された文字trigram確率の積によって求めたものである．
すなわち，[MATH]の表記を[MATH]，[MATH]の最後の2文字を[MATH]と書くとき
ただし[MATH]はbigram, unigram等により補間された文字trigram確率である．
また，[MATH]は文字モデルが生成する単語の長さに関する分布関数である．
単語[CITE]や文字列の[MATH]-gram[CITE]では与えられた系列を単語ないし文字列に分割するやり方が一意に決まらないため，これらのモデルは直前の[MATH]個の単語や文字列を状態とする，隠れマルコフモデルの一種と考えられる．
単語や文字列の集合は，語彙知識として人手で与えられるか，あるいは経験的な規則に基づいて訓練テキストから抽出されるものである．
ここで定義するsuperwordとはこれら単語や文字列を一般化したものであるが，それらと対照的なのは，訓練テキスト中の任意の文字列を含み得る点である．
ただし，言語モデルとして意味を持つために必要最小限のヒューリスティクスは導入せねばならない．
そこで，次の条件を満たす文字列をsuperwordと定義する．
訓練テキスト中に最低2回出現する
または
長さ1の文字列である
訓練テキストにおける再現性の仮定は，ある文字列が何らかの言語的なまとまりを成すか否かに対する基準となるものであり，そのような基準として考え得る制約の中でもっとも緩い条件として与えてある．
すなわち，ある文字列が訓練テキスト中で1回しか出現しない，または1回も出現しないならば，その文字列が何らかのまとまりを成すだろうという証拠は，他に人間が知識として与えない限り得られない．
また，再現性とは独立に，長さ1の文字列は全てsuperwordと定義している．
これにより，全ての文は少なくとも1通りのsuperwordの系列として表現できることが保証される．
superword [MATH]-gram確率[MATH]は，直前に[MATH]個のsuperwordの列[MATH]が生起したと仮定した時のsuperword [MATH]の条件付き生起確率である．
与えられた文[MATH]がsuperwordの列[MATH]に分割できるとき，[MATH]と書く．
superword [MATH]-gramモデルは，[MATH]の全ての可能な分割に関して計算したsuperword [MATH]-gram確率の積の総和をもって[MATH]の発生確率を推定するものである．
すなわち，その確率を次式で与える．
ここで[MATH]の時，すなわちsuperword unigramモデルは，文全体の生起確率がそれぞれ独立なsuperwordの生起確率の積で表されるとするものであり，multigram[CITE]と呼ばれる可変長単語列に基づく言語モデルと同一のものである．
superword [MATH]-gramモデルのクラスは，単語や文字列の[MATH]-gramモデルのクラスを包含する．
この性質は，パラメータさえ適切に与えることができれば，superwordに基づくモデルの性能が単語や文字列の[MATH]-gramモデルの性能と同等かそれ以上になることを保証する．
モデルの獲得にあたっては，パラメータの学習に先立ち，訓練テキストからsuperwordの集合を求める必要がある．
長さ1のsuperwordについては自明であるから，再現性のある文字列を集める作業が核心である．
これには，訓練テキストの全ての位置から始まる半無限文字列をソートして任意長[MATH]-gram統計を求め[CITE]，2回以上出現する文字列を記録する方法が考えられる．
しかし，再現性のある文字列だけに興味がある場合には，短い文字列から長い文字列へと逐次的に求める簡便な方法で十分である[CITE]．
実験で用いたテキストコーパスでは，長さ[MATH]のsuperwordの種類は大きな[MATH]では単調に減少することが観察されている．
superwordモデルでは，ある状態から別の状態に移る時に，ある確率で一つのsuperwordを出力する．
状態は，直前[MATH]個のsuperwordによって定まるものとする．
ただし，[MATH]の場合はただ1つの状態のみ存在するものとする．
superwordモデルの出力はsuperword列としてではなく文字の系列として観測される．
そこで，通常の隠れマルコフモデルと同様に扱うことを可能にするため副状態を導入して，1回の状態遷移で1文字を出力する等価なモデルを考える．
副状態は，状態を分割したもので，そこに移る時最後に出力したsuperwordの各文字に対応する．
superword [MATH]の表記を[MATH]とし，[MATH]の長さ[MATH]のプレフィックスを[MATH]とする．
そして，superwordモデルにおける[MATH]の出力を，等価なモデルでは次のように表す．
すなわち，確率[MATH]で副状態[MATH]に移る時に[MATH]を出力し，以後確率 1で副状態[MATH]に移る時に[MATH]を出力し，最終的に状態[MATH]に至ると考える．
例として，図[REF_fig4-1]の状態遷移図では「東北大学」というsuperwordを出力して状態（東北大学）に至る様子を示している．
等価なモデルでは，本来の確率で副状態（東）に移る時に文字「東」を出力，確率1で（東北）に移る時に「北」を出力，確率1で（東北大）に移る時に「大」を出力，最終的に確率1で（東北大学）に移る時に「学」を出力する．
分割前の状態が異なる副状態は同一視しないので，例えば（東北大学）の副状態（東）と（東京）の副状態（東）は異なることに注意すべきである．
[MATH]，すなわちsuperword unigram確率の学習のための初期確率としては，全てのsuperwordが等確率で発生するとして，superwordの数の逆数を与える．
[MATH]については，対応するsuperwordの[MATH]-gram確率で初期化する．
確率の再推定のために，図[REF_fig4-2]のように訓練テキストから全てのsuperwordを洗い出す．
次に，連接可能な[MATH]個のsuperwordの組に関して，次式によって確率を更新する．
ただし，[MATH], [MATH]はそれぞれForward確率，Backward確率で，以下のように再帰的に定義する．
時刻[MATH]([MATH])でsuperword [MATH]の第1字目を出力するとき
時刻[MATH]([MATH])でsuperword [MATH]の第[MATH]字目([MATH])を出力するとき
ただし
同様に
時刻[MATH]([MATH])でsuperword [MATH]の第1字目を出力するとき
時刻[MATH]([MATH])でsuperword [MATH]の第[MATH]字目([MATH])を出力するとき
ただし
再現性のある文字列の長さを十分大きく取れば，前節までに述べたモデルは与えられた訓練テキストに対して一意に求まる．
以下では，これを一般superword [MATH]-gramモデルと呼ぶ．
しかし，一般モデルのパラメータ数は大きい．
特に，[MATH]ではsuperwordの組み合わせが爆発し，現実的ではない．
さらに，あまりに長いsuperwordは訓練テキストに特化してしまう恐れがあり，汎化能力の低下を招く．
これに対処するため，一般モデルに加えて長さ制限付きのsuperwordモデルを導入する．
これは，逐次的な再現性文字列の獲得を早い段階で打ち切って小さなsuperwordの集合をつくり，その集合に基づいてForward-Backward学習を行うことで得ることができる．
以下では，長さ[MATH]に制限されたsuperword [MATH]-gram確率を[MATH]と表記する．
長さが[MATH]に制限されたsuperword [MATH]-gramモデルは，図[REF_ergodic]に示すような，状態数が高々字種の[MATH]乗に制限されたエルゴーディックHMMとなる．
ただし，図は[MATH]とした例である．
[MATH]-gramに代表される確率モデルにおいては，モデルのパラメータを精度良く推定するに足るサンプルが得られないことが多く，パラメータ空間のさまざまなスムージング法が提案されている[CITE]．
その一つに，いくつかのモデルの確率の重み付き線形和で表現する方法がある[CITE]．
これは本来，詳細なモデルの値が信用できない場合に，パラメータの少ない安定したモデルの値を代用するものであるが，性質の異なる複数のモデルを組み合わせてより良いモデルを得るという積極的な利用も可能である．
本節では，この線形補間に基づくいくつかの複合モデルを考える．
superword bigram([MATH])モデルに対しては，superword unigram確率によって補間された確率は次式で与えられる．
重み係数[MATH]は，訓練テキストとは別のサンプル(held-outデータ)またはクロスバリデーションによって得られる仮想的な未知データの確率を最大にするように再推定する．
前述したように，一般superword bigramはパラメータ量が多くなり過ぎるので，実際にはsuperwordの長さを最大[MATH]に制限したモデルと組み合わせる．
これは次式で与えられる．
式([REF_limited])のような制限されたモデルでは，長い語の表現に難があることも考えられる．
そこで，長さ制限付きsuperword bigramモデルと一般superword unigramモデルの複合モデルを導入する．
複合superword bigram確率は次式で定義される．
さらに，複合superword bigramモデルを，文字のtrigramモデルによってスムージングすることを考える．
文字のtrigramモデルはそれ自身で強力な曖昧性削減能力を持っているが[CITE]，単語[MATH]-gramモデルと融合させることにより，認識対象中の未知の文字列の存在による単語解析精度の低下の影響を低減させ，頑健なモデルとすることができる[CITE]．
文字によって補間された複合superword bigram確率は次式で定義される．
ただし，[MATH]はsuperword [MATH]が生起する確率を，補間された文字trigram確率の積によって求めたものである．
すなわち，[MATH]の表記を[MATH]，[MATH]の最後の2文字を[MATH]と書くとき
ただし[MATH]はbigram, unigram等により補間された文字trigram確率である．
また，[MATH]は文字モデルが生成する単語の長さに関する分布関数である．
