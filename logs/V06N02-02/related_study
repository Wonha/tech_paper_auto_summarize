これらの手法に対して，伊藤ら[CITE]は統計的な基準によって文字列の集合を選定し，その文字列に分割されたテキストを使って[MATH]-gramを学習する方法を提案している．文字列を選定する基準としては，単純な頻度，および語彙の自動獲得のために提案されている正規化頻度[CITE]の高いものから選ぶ方式が有効であったとされる．この方法は，形態素解析を必要としない点で優れている．しかし，抽出すべき文字列の最適な個数を見出す方法については述べられていない．また，用いられている基準と言語モデルの能力との理論的関係は浅く，最良の分割方法である保証はない．さらに，この手法ではテキストが明示的に分割される．このため，接辞を伴った語や複合語などの長い文字列が抽出された場合，その文字列を構成するもっと短い語は出現しなかったのと同様な扱いを受けることになる．有限のテキストから汎化性の高い言語モデルを構築したい場合に，このような明示的な分割が最良の結果を与えるとは限らない．

本論文では，高い曖昧性削減能力を持つ新しい言語モデルを提案する．このモデルは，superwordと呼ぶ文字列の集合の上の[MATH]-gramモデルとして定義される．superwordは訓練テキスト中の文字列の再現性のみに基づいて定義される概念であり，与えられた訓練テキストに対して一意に定まる．具体的な確率分布は，訓練テキストからForward-Backwardアルゴリズムによって求める．訓練テキストを明示的に分割せぬまま学習を行うため，長い文字列中の部分文字列を「再利用」することが可能となり，少量の訓練テキストでも効率の良いモデル化が期待できる．本論文ではまた，いくつかのモデルの融合による汎化性の向上についても検討する．

実時間性が要求される大語彙連続音声認識システムにおいては，緩い言語モデルを用いて可能性をしぼり込んだ後，詳細な言語モデルによって最終出力を導く２パス処理が一般的である．本論文で提案するような字面の適格性を与える言語モデルは，ディクテーションシステムの第２パス，すなわち後処理用の言語モデルとして有用であるものと考えられる．また，文字[MATH]-gramを用いた認識手法[CITE]を本手法に応用することも可能である．単語[CITE]や文字列の[MATH]-gram[CITE]では与えられた系列を単語ないし文字列に分割するやり方が一意に決まらないため，これらのモデルは直前の[MATH]個の単語や文字列を状態とする，隠れマルコフモデルの一種と考えられる．単語や文字列の集合は，語彙知識として人手で与えられるか，あるいは経験的な規則に基づいて訓練テキストから抽出されるものである．ここで定義するsuperwordとはこれら単語や文字列を一般化したものであるが，それらと対照的なのは，訓練テキスト中の任意の文字列を含み得る点である．ただし，言語モデルとして意味を持つために必要最小限のヒューリスティクスは導入せねばならない．そこで，次の条件を満たす文字列をsuperwordと定義する．訓練テキスト中に最低2回出現するまたは長さ1の文字列である訓練テキストにおける再現性の仮定は，ある文字列が何らかの言語的なまとまりを成すか否かに対する基準となるものであり，そのような基準として考え得る制約の中でもっとも緩い条件として与えてある．すなわち，ある文字列が訓練テキスト中で1回しか出現しない，または1回も出現しないならば，その文字列が何らかのまとまりを成すだろうという証拠は，他に人間が知識として与えない限り得られない．

paragraph score: 1.00598796033533
