================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[1542] このモデルはsuperwordと呼ぶ文字列の集合の上の[MATH]-gramとして定義され，従来の単語や文字列の[MATH]-gramモデルを包含するものになっている．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[1637] 文字列を選定する基準としては，単純な頻度，および語彙の自動獲得のために提案されている正規化頻度[CITE]の高いものから選ぶ方式が有効であったとされる．

================================================================
[section type  : proposed_method]
[section title : superwordモデルの定式化]
================================================================
[1713] 単語[CITE]や文字列の[MATH]-gram[CITE]では与えられた系列を単語ないし文字列に分割するやり方が一意に決まらないため，これらのモデルは直前の[MATH]個の単語や文字列を状態とする，隠れマルコフモデルの一種と考えられる．

================================================================
[section type  : proposed_method]
[section title : superwordモデルの学習法]
================================================================
[933] 本論文では，知識に依存しない，高い曖昧性削減能力を持つ新しい言語モデルを提案する．
-----------------------------------------------------
  [subsection title : superword集合の獲得]
-----------------------------------------------------
  [1427] これには，訓練テキストの全ての位置から始まる半無限文字列をソートして任意長[MATH]-gram統計を求め[CITE]，2回以上出現する文字列を記録する方法が考えられる．
-----------------------------------------------------
  [subsection title : 確率分布のForward-Backward学習]
-----------------------------------------------------
  [1307] 副状態は，状態を分割したもので，そこに移る時最後に出力したsuperwordの各文字に対応する．

================================================================
[section type  : proposed_method]
[section title : 長さ制限の導入]
================================================================
[1396] これは，逐次的な再現性文字列の獲得を早い段階で打ち切って小さなsuperwordの集合をつくり，その集合に基づいてForward-Backward学習を行うことで得ることができる．

================================================================
[section type  : proposed_method]
[section title : 複合モデル]
================================================================
[1659] 文字のtrigramモデルはそれ自身で強力な曖昧性削減能力を持っているが[CITE]，単語[MATH]-gramモデルと融合させることにより，認識対象中の未知の文字列の存在による単語解析精度の低下の影響を低減させ，頑健なモデルとすることができる[CITE]．

================================================================
[section type  : experiment_result]
[section title : 評価実験]
================================================================
[1697] superwordに基づいたモデル単独では訓練テキストに対して過学習する傾向があり，未知テキストに対して脆弱な側面があるが，未知テキストに対して頑健な文字trigramモデルとの融合によりそれが克服できることを意味する．

================================================================
[section type  : conclusion]
[section title : あとがき]
================================================================
[1742] これはsuperwordを可能な限り一般的に定義したためであり，特に大規模なコーパスを用いてモデルを学習する場合には，再現性の仮定を見直す必要があることが考えられる．

