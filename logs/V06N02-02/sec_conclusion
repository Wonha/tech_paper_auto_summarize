本論文では，superwordの概念に基づいた新しい言語モデルを提案した．
このモデルは従来の[MATH]-gramの枠組を包含したより一般的なものであり，コーパス以外の知識に全く依存しない．
また，本論文で導入した長さ制限モデルとスムージング手法により，現実的なコーパスの量の範囲でモデルの学習が可能となった．
評価実験の結果，長さ制限を施したsuperword bigramモデルを文字trigramモデルと組み合わせて頑健性を向上させたモデルの性能が高く，形態素解析に基づく手法，および高頻度文字列抽出による方法を超える能力が得られた．
superwordに基づく言語モデルは，可搬性に優れた強力なものであるが，欠点として訓練テキストに比べモデルの規模が非常に大きいことが挙げられる．
superword unigramモデルのパラメータ数はsuperword集合の大きさにほぼ比例する．
通常の[MATH]-gramではモデルのサイズの上界がコーパスの量に対して線形のオーダーで与えられるのに対し，superwordの場合にはそれよりも大きくなる可能性がある．
これはsuperwordを可能な限り一般的に定義したためであり，特に大規模なコーパスを用いてモデルを学習する場合には，再現性の仮定を見直す必要があることが考えられる．
また，superword bigramモデルは長さ制限を加えた場合でも非常に大きくなる．
今回構築した長さ3のsuperword bigram確率テーブルは約170Mbyteの大きさのファイルとなり，一般superword unigram確率テーブルの約10倍である．
これは，与えられたテキストのsuperwordによる解析結果が極めて曖昧性が大きいものであることが原因である．
モデルのサイズを小さくし，実際のパターン認識システムで利用できるようにするためには，モデルの最適化が必要である．
すなわち，学習の過程で非常に小さな確率を付与された状態遷移のアークは刈り取る，あるいは外から遷移してくる確率が十分小さな状態は削除する，などである．
しかし，この種の枝刈りは，訓練サンプルに特化する危険がある．
今後はパープレキシティを上げることなくモデルをコンパクトにするための枝刈り手法の開発が課題である．
