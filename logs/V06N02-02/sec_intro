音声認識・文字認識の精度向上のため，より高い性能を持つ言語モデルを求めることは重要である．
近年は，モデル構築やメンテナンスの容易さの点から，コーパスに基づく統計的言語モデルの研究が盛んである．
大語彙ないしタスク非依存のシステムのための統計的言語モデルとして今日もっとも有望視されているものに，[MATH]-gramが挙げられる．
[MATH]-gramは大量のテキストコーパスからの単純な数え上げによって得られる統計量であり，強力かつ頑健性に優れている．
英語などのヨーロッパ系言語においては，[MATH]-gramの単位として単語を用いることが多い．
大語彙のシステムでは単語はカテゴリ数が非常に大きくなるため，単語の代わりに品詞を用いる[CITE]，または単語クラスタリングによって得られる単語クラスを用いることが多い．
これらの言語においては単語は分かち書きされるため機械的に取り出すことができ，数え上げも容易に行える．
これに対し，日本語や中国語には分かち書きの習慣がない．
朝鮮語は文節ごとに分かち書きをするが，その分かち方は一定しないうえ，[MATH]-gramの単位としては大き過ぎて汎化性に難がある．
よって，これらの言語を[MATH]-gramによってモデル化する際には，テキストコーパスに何らかの前処理が必要である．
これには次の可能性が考えられる．
人手によって分割されたタグ付きコーパスを使う
自動形態素解析システムによって単語に分割する
経験的な統計基準によって文字列に分割する
このうちタグ付きコーパスを使う方法には，コーパス自体の入手が質的・量的な困難を伴うという欠点がある．
形態素解析に基づく方法は有効であるが，モデルを学習するためにはまず形態素解析システムを用意せねばならないうえ，特定タスクに対して高い性能を得るためには予め辞書をチューニングする必要があると考えられ，メンテナンスのコストがかかる．
また，形態素解析システムの文法規則によっては機能語が短めに分割される傾向があり，[MATH]-gramの性能を必ずしも最大にするものではない．
これらの手法に対して，伊藤ら[CITE]は統計的な基準によって文字列の集合を選定し，その文字列に分割されたテキストを使って[MATH]-gramを学習する方法を提案している．
文字列を選定する基準としては，単純な頻度，および語彙の自動獲得のために提案されている正規化頻度[CITE]の高いものから選ぶ方式が有効であったとされる．
この方法は，形態素解析を必要としない点で優れている．
しかし，抽出すべき文字列の最適な個数を見出す方法については述べられていない．
また，用いられている基準と言語モデルの能力との理論的関係は浅く，最良の分割方法である保証はない．
さらに，この手法ではテキストが明示的に分割される．
このため，接辞を伴った語や複合語などの長い文字列が抽出された場合，その文字列を構成するもっと短い語は出現しなかったのと同様な扱いを受けることになる．
有限のテキストから汎化性の高い言語モデルを構築したい場合に，このような明示的な分割が最良の結果を与えるとは限らない．
本論文では，高い曖昧性削減能力を持つ新しい言語モデルを提案する．
このモデルは，superwordと呼ぶ文字列の集合の上の[MATH]-gramモデルとして定義される．
superwordは訓練テキスト中の文字列の再現性のみに基づいて定義される概念であり，与えられた訓練テキストに対して一意に定まる．
具体的な確率分布は，訓練テキストからForward-Backwardアルゴリズムによって求める．
訓練テキストを明示的に分割せぬまま学習を行うため，長い文字列中の部分文字列を「再利用」することが可能となり，少量の訓練テキストでも効率の良いモデル化が期待できる．
本論文ではまた，いくつかのモデルの融合による汎化性の向上についても検討する．
実時間性が要求される大語彙連続音声認識システムにおいては，緩い言語モデルを用いて可能性をしぼり込んだ後，詳細な言語モデルによって最終出力を導く２パス処理が一般的である．
本論文で提案するような字面の適格性を与える言語モデルは，ディクテーションシステムの第２パス，すなわち後処理用の言語モデルとして有用であるものと考えられる．
また，文字[MATH]-gramを用いた認識手法[CITE]を本手法に応用することも可能である．
音声認識・文字認識の精度向上のため，より高い性能を持つ言語モデルを求めることは重要である．
近年は，モデル構築やメンテナンスの容易さの点から，コーパスに基づく統計的言語モデルの研究が盛んである．
大語彙ないしタスク非依存のシステムのための統計的言語モデルとして今日もっとも有望視されているものに，[MATH]-gramが挙げられる．
[MATH]-gramは大量のテキストコーパスからの単純な数え上げによって得られる統計量であり，強力かつ頑健性に優れている．
英語などのヨーロッパ系言語においては，[MATH]-gramの単位として単語を用いることが多い．
大語彙のシステムでは単語はカテゴリ数が非常に大きくなるため，単語の代わりに品詞を用いる[CITE]，または単語クラスタリングによって得られる単語クラスを用いることが多い．
これらの言語においては単語は分かち書きされるため機械的に取り出すことができ，数え上げも容易に行える．
これに対し，日本語や中国語には分かち書きの習慣がない．
朝鮮語は文節ごとに分かち書きをするが，その分かち方は一定しないうえ，[MATH]-gramの単位としては大き過ぎて汎化性に難がある．
よって，これらの言語を[MATH]-gramによってモデル化する際には，テキストコーパスに何らかの前処理が必要である．
これには次の可能性が考えられる．
人手によって分割されたタグ付きコーパスを使う
自動形態素解析システムによって単語に分割する
経験的な統計基準によって文字列に分割する
このうちタグ付きコーパスを使う方法には，コーパス自体の入手が質的・量的な困難を伴うという欠点がある．
形態素解析に基づく方法は有効であるが，モデルを学習するためにはまず形態素解析システムを用意せねばならないうえ，特定タスクに対して高い性能を得るためには予め辞書をチューニングする必要があると考えられ，メンテナンスのコストがかかる．
また，形態素解析システムの文法規則によっては機能語が短めに分割される傾向があり，[MATH]-gramの性能を必ずしも最大にするものではない．
これらの手法に対して，伊藤ら[CITE]は統計的な基準によって文字列の集合を選定し，その文字列に分割されたテキストを使って[MATH]-gramを学習する方法を提案している．
文字列を選定する基準としては，単純な頻度，および語彙の自動獲得のために提案されている正規化頻度[CITE]の高いものから選ぶ方式が有効であったとされる．
この方法は，形態素解析を必要としない点で優れている．
しかし，抽出すべき文字列の最適な個数を見出す方法については述べられていない．
また，用いられている基準と言語モデルの能力との理論的関係は浅く，最良の分割方法である保証はない．
さらに，この手法ではテキストが明示的に分割される．
このため，接辞を伴った語や複合語などの長い文字列が抽出された場合，その文字列を構成するもっと短い語は出現しなかったのと同様な扱いを受けることになる．
有限のテキストから汎化性の高い言語モデルを構築したい場合に，このような明示的な分割が最良の結果を与えるとは限らない．
本論文では，高い曖昧性削減能力を持つ新しい言語モデルを提案する．
このモデルは，superwordと呼ぶ文字列の集合の上の[MATH]-gramモデルとして定義される．
superwordは訓練テキスト中の文字列の再現性のみに基づいて定義される概念であり，与えられた訓練テキストに対して一意に定まる．
具体的な確率分布は，訓練テキストからForward-Backwardアルゴリズムによって求める．
訓練テキストを明示的に分割せぬまま学習を行うため，長い文字列中の部分文字列を「再利用」することが可能となり，少量の訓練テキストでも効率の良いモデル化が期待できる．
本論文ではまた，いくつかのモデルの融合による汎化性の向上についても検討する．
実時間性が要求される大語彙連続音声認識システムにおいては，緩い言語モデルを用いて可能性をしぼり込んだ後，詳細な言語モデルによって最終出力を導く２パス処理が一般的である．
本論文で提案するような字面の適格性を与える言語モデルは，ディクテーションシステムの第２パス，すなわち後処理用の言語モデルとして有用であるものと考えられる．
また，文字[MATH]-gramを用いた認識手法[CITE]を本手法に応用することも可能である．
