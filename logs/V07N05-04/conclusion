考察\label{sec:observations}

ここでは、本稿で提案する手法がどのように精度向上に寄与しているかの観察、
及び他研究との比較を行う。

\subsection{「３つ組／４つ組モデル」の効用}

表\ref{tab:control_exp}~にある対照実験の結果は、
以下の理由から３つ組／４つ組モデルの有効性を示しているといえる。

\begin{itemize}
\item
「３つ組／４つ組モデル」の精度は「２つ組モデル」の精度よりも
約0.9$\%$上回っている。
両者とも、文法とヒューリスティクスにより係り先候補を３つ以下に限定しているが、
それらの係り先候補を同時に考慮するモデルを用いた方が精度が上がることが確認された。
\item
「２つ組モデル」は、「文法なしモデル」より1.0$\%$、「候補限定なしモデル」よりも
0.3$\%$高い精度を出している。
従って、文法を用いることや係り先候補を３つに限定することは
妥当な措置であり、「２つ組モデル」は「３つ組／４つ組モデル」の比較対象として適当である。
\end{itemize}

次に、両者のモデルで実際に解析を行う時の、
具体的なMEのパラメータを観察してみる。
例として、文(\ref{sent:kodomo})
の「子供たちの」の各候補への係りやすさを計算する。
「子供たちの」の係り先候補は、「甲高い」「声で」「騒然となる。」の３文節で、
正解は「声で」である。

\vskip 2mm
\refstepcounter{enums}\label{sent:kodomo}
(\theenums)  そんなとき、 {\bf 子供たちの} 甲高い \underline{声で} 騒然となる。
\refstepcounter{equation}
\vskip 2mm

各候補への係りやすさを２つ組モデル・４つ組モデル
\footnote{候補数が３なので、４つ組モデルが使われる。}で推定する際の
ME法のパラメータ$\alpha_k$のうち主な（$| \log\alpha_k |$が大きい）ものを、
それぞれ表\ref{tab:pair_me}~,表\ref{tab:quad_me}~に示す。
パラメータ$\alpha_k$のうち、履歴$a$、出力値$b$に対応する素性のものを掛け合わせるので、
$\alpha_k$の値が1.0より大きいものは出力値を$b$にすることを助長するパラメータ、
1.0より小さいものは$b$にすることを抑制するパラメータである。
「$\alpha_k$の積」の項は、
表に載せていないものも含め、
対応する出力値に関する全てのパラメータの積である。

\subsubsection{２つ組モデルの場合}

このモデルでは、係り先ごとに別々の条件で係りやすさを計算する。
各係り先への係りやすさ$P(i \rightarrow j)$は、
出力値Ｔに対する$\alpha_k$の積を、
出力値Ｔ,~Ｆに対する$\alpha_k$の積の和で割ったものである。
例えば、$P(子供たちの \rightarrow 甲高い)$は、
$0.93/(0.93+0.81)=0.53$となる。

「声で」に係る場合のパラメータに注目すると、
係り元助詞「の」は隣の文節に係る傾向が強いことから、
文節間距離が「２から５」に対するパラメータが小さくなっている。
そのため、「甲高い」に係る確率の方が高くなってしまう。

\subsubsection{４つ組モデルの場合}

全ての係り先への係りやすさを共通の確率分布を用いて計算する。
出力値$b$は$1,2,3$の３値をとり、
第一候補への係りやすさ$P(i \rightarrow t_1)$は
出力値１に対する$\alpha_k$の積を、
３つの出力値に対する$\alpha_k$の積の和で割ったものであり、
表\ref{tab:quad_me}~の例では
$0.682/(0.682+2.39+0.106)=0.215$となる。

出力値が2となる場合のパラメータに着目する。
係り元が「の」で、第一候補が「形容詞」であること、
第二候補が「名詞」であること、第三候補が「形容詞」であることの全てが
第二候補に係るパラメータを高めており、第二候補に係る確率が
第一候補に係る確率を上回っている。
特に、出力値$b$と異なる候補（この場合、第一・第三候補）に関係する素性
も強い影響を及ぼしていることが興味深い。

\begin{table}[t]
	\scriptsize
	\begin{center}
	\begin{tabular}{|p{.8cm}||c|p{5cm}|p{.7cm}|p{1.15cm}|r|r|}
	\hline
	係り先$j$ & 
	\smash{\lower1.6ex\hbox{素性番号}} & 
	\smash{\lower1.6ex\hbox{履歴$a$}} & 
	出力値$b$ & パラメータ$\alpha_k$ & 
	\smash{\lower1.6ex\hbox{$\alpha_k$の積}} & 
	\smash{\lower1.6ex\hbox{$P(i \rightarrow j)$}} \\
	\hline \hline
	甲高い & 26 & 係り元語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「１」 
	& T & 0.83 & 0.93 & 0.53 \\
	\cline{2-6}
	& 6 & 係り先活用形「基本形」・距離「１」 & F & 0.69 & 0.81 & \\
	\cline{2-5}
	& 26 & 係り元語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「１」 & F & 1.19 & & \\
	\cline{2-5}
	& 27 & 係り元主辞「普通名詞」・語形「接続助詞」・係り先主辞「形容詞」「基本形」・距離「１」 & F & 0.81 & & \\
	\hline \hline
	声で & 3 & 係り元助詞「の」・距離「２〜５」 & T & 0.78 & 0.57 & 0.31 \\
	\cline{2-5}
	& 10 & 係り先主辞「声」・距離「２〜５」 & T & 0.79 & & \\
	\cline{2-5}
	& 23 & 係り元助詞「の」・係り先助詞「で」・距離「２〜５」 & T & 1.82 & & \\
	\cline{2-5}
	& 26 & 係り元語形「接続助詞」・係り先品詞「名詞」・距離「２〜５」 & T & 0.84 & & \\
	\cline{2-5}
	& 27 & 係り元主辞「普通名詞」・係り元語形「接続助詞」「の」・係り先主辞「普通名詞」・距離「２〜５」 & T & 0.81 & & \\
	\cline{2-6}
	& 27 & 係り元主辞「普通名詞」・係り元語形「接続助詞」「の」・係り先主辞「普通名詞」・距離「２〜５」 & F & 1.06 & 1.26 & \\
	\hline \hline
	騒然となる & 3 & 係り元助詞「の」・距離「２〜５」 & T & 0.78 & 0.11 & 0.10 \\
	\cline{2-5}
	& 8 & 係り先主辞「形容詞」・距離「２〜５」 & T & 0.86 & & \\
	\cline{2-5}
	& 26 & 係り先語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「２〜５」 & T & 0.48 & & \\
	\cline{2-6}
	& 26 & 係り元語形「接続助詞」「の」・読点「無」・係り先主辞「形容詞」・距離「２〜５」 & F & 1.08 & 1.03 & \\
	\hline
	\end{tabular}
	\caption{(\ref{sent:kodomo})の「子供たちの」の係り先推定の際の、２つ組モデルにおけるＭＥのパラメータ \\
	{\footnotesize それぞれの係り先に対して、別個に「係る（Ｔ）」「係らない（Ｆ）」を出力値とする
		パラメータが計算される。}}
	\label{tab:pair_me}
	\end{center}
\end{table}

\begin{table}[t]
	\scriptsize
	\begin{center}
	\begin{tabular}{|c|p{6cm}|p{.7cm}|p{1.15cm}|r|r|}
	\hline
	\smash{\lower1.6ex\hbox{素性番号}} & 
	\smash{\lower1.6ex\hbox{履歴$a$}} & 
	出力値$b$ & 
	パラメータ$\alpha_k$ & 
	\smash{\lower1.6ex\hbox{$\alpha_k$ の積}} & 
	\smash{\lower1.6ex\hbox{$P(i \rightarrow t_b)$}}\\
	\hline \hline
	10-2 & 第二候補主辞語彙「声」 & 1 & 0.83 & 0.682 & 0.215 \\
	\cline{1-4}
	21-1 & 係り元助詞「の」・第一候補主辞語彙「その他」 & 1 & 0.78 & & \\
	\cline{1-4}
	26-1 & 係り元語形「接続助詞」「の」・読点「無」・第一候補主辞「形容詞」 & 1 & 0.84 & & \\
	\hline
	11-2 & 第二候補助詞「で」 & 2 & 1.29 & 2.39 & 0.752 \\
	\cline{1-4}
	13-1 & 第一候補活用形「基本形」 & 2 & 0.77 &  &  \\
	\cline{1-4}
	26-1 & 係り元語形「接続助詞」「の」・読点「無」・第一候補主辞「形容詞」 & 2 & 1.23 & & \\
	\cline{1-4}
	26-2 & 係り元語形「接続助詞」「の」・読点「無」・第二候補主辞「名詞」 & 2 & 1.25 & & \\
	\cline{1-4}
	26-3 & 係り元語形「接続助詞」「の」・読点「無」・第三候補主辞「形容詞」 & 2 & 1.24 & & \\
	\cline{1-4}
	27-1 & 係り元主辞「普通名詞」・語形「接続助詞」・第一候補主辞「形容詞」「基本形」 & 2 & 0.84 & & \\
        \hline
	3 & 係り元助詞「の」 & 3 & 0.59 & 0.106 & 0.034 \\
	\cline{1-4}
	7 & 係り元読点「無」 & 3 & 0.84 & & \\
	\cline{1-4}
	10-2 & 第二候補主辞語彙「声」 & 3 & 2.15 & & \\
	\cline{1-4}
	11-2 & 第二候補助詞「で」 & 3 & 0.46 & & \\
	\cline{1-4}
	17-3 & 係り元-第三候補文節間読点「無」 & 3 & 1.40 & & \\
	\cline{1-4}
	20-3 & 係り元語形品詞「形容詞」・第三候補読点「無」 & 3 & 0.80 & & \\
	\cline{1-4}
	27-2 & 係り元主辞「普通名詞」・語形「接続助詞」・第二候補主辞「名詞」 & 3 & 0.79 & & \\
	\cline{1-4}
	27-3 & 係り元主辞「普通名詞」・語形「接続助詞」・第三候補主辞「形容詞」「基本形」 & 3 & 0.70 & & \\
	\hline
	\end{tabular}
	\caption{(\ref{sent:kodomo})の「子供たちの」の係り先推定の際の、４つ組モデルにおけるＭＥのパラメータ \\
	{\footnotesize 各候補が選ばれる（出力値が1,2,3）場合のパラメータを一つの確率分布で求めている。}}
	\label{tab:quad_me}
	\end{center}
\end{table}


\subsection{他研究との比較}
\subsubsection{EDRコーパスでの精度の比較}

係り受けの精度判定にEDRコーパスを用いている他研究と比較してみる。
決定木を用いた手法\cite{Haruno98}での精度は84〜85$\%$、
語の共起確率を用いた手法\cite{Fujio99}では、86.8$\%$となっている。
我々の手法はこれらを上回っており、
EDRコーパスに対してテストした中では
最も高い水準といえよう。

また、\cite{Kanayama99}では、３つ組／４つ組モデルを
単純な相対頻度を用いて構成している。
そこでの精度は86.7$\%$であり、ME法の利用によって
約1.9$\%$精度が向上したことになる。
精度向上の要因は、
ME法によってデータスパースネスの問題が軽減でき、
従来は入れられなかった語彙や活用に関する素性を追加できたことであると思われる。


\subsubsection{京大コーパスでの精度の比較}


いくつかの研究では、京大コーパス\cite{kc}を用いて精度を測っている。
構文的・語彙的情報を統合して構文木の生起確率を求めている手法\cite{Shirai98}
での精度は85〜86$\%$である。

本研究と同様に、ME法を用いた研究~\cite{Sekine99},~\cite{Uchimoto99b}
では、京大コーパスの1月9日分の1,246文を用いている。
比較のために、同じコーパスでテストした結果は、
表\ref{tab:accuracy_kc}~のようになった。

\begin{table}
	\begin{center}
	\begin{tabular}{|l|l|rc|}
	\hline
	\smash{\lower2.0ex\hbox{解析成功文}} & 
		文節正解率 & 87.08$\%$ & (8299/9530) \\
			\cline{2-4}

&		文正解率   & 44.70$\%$ & (493/1103) \\
	\hline
	\end{tabular}
	\caption{EDRコーパスで学習し、京大コーパスでテストした際の解析結果}
	\label{tab:accuracy_kc}
	\end{center}
\end{table}

文末から決定的に係り先を決定するモデル\cite{Sekine99}の精度は87.14$\%$で我々と同程度、
後方文脈を考慮するモデル\cite{Uchimoto99b}は87.93$\%$で我々の精度よりも高くなっている。
その原因として、以下のことが考えられる。

\begin{itemize}
\item
我々は、学習データとしてEDRコーパスを用いている。
\cite{Uchimoto99b}などと比べて約24倍の学習データがあるとはいえ、
括弧付けの方針の違いなどから、
京大コーパスでの解析の誤りを引き起こすことが多い。
\item
関根ら、内元らは京大コーパス中にある形態素解析・文節区切りの結果を用いているのに対し、
我々はJUMANで解析したものを用いているため、形態素解析の誤りを含み、
解析誤りの原因となっている。
\item
文法SLUNGがEDRコーパスの括弧付けの方針に従って作られており、
京大コーパスにあるような係り方を許さない場合がある。
\end{itemize}

現在のところ、京大コーパスの解析には被覆率・精度ともに充分でないが、
文法やシステムの改変により対処した上で
本論文で提案する手法を有効に適用できるようにすれば、
より高い精度が得られると考えている。

\subsubsection{学習量の比較}

図\ref{fig:graph}~より、最高値に近い精度を得るためには、
10〜15万文の学習コーパスを要している。
この学習量は、EDRコーパスを用いている研究\cite{Fujio99}と同程度であり、
\cite{Uchimoto99}などの京大コーパスを用いた場合より、
20倍程度の学習量になっている。

一般に、ME法を用いることにより学習量を減らすことができると考えられているが、
３つ組／４つ組モデルでは、複数の係り先に関する属性を同時に捉える
条件付き確率を用いているため、
区別される事象の数が大きくなり、多くの学習量が必要になっている。

我々のモデルは、EDRコーパスのような多くの学習データを有効に
利用できるモデルであるといえる反面、京大コーパスのように
学習データ量が限られている時には、
より効率のよい素性選択などが要求されるであろう。



\subsubsection{解析速度の比較}

本研究での係り受け解析は、あくまで詳細な構文構造を得るという目標の
前段階であるため、速度に焦点を当ててはいないが、参考のために比較しておく。

文末から決定的に係り先を決定するモデル\cite{Sekine99}では、
一文当たり平均0.03秒（Sun Ultra10, 300MHz）で解析できるのに対し、
一方、我々のシステムではEDRコーパスの文に対して平均約0.5秒
（Pentium III, 500MHz : 経験的に、上記の計算機の約３倍の速度）を要する。
両者には大きな差があるものの、我々の速度も非実用的なものではない。

また、そのほとんどはHPSGパーザによる部分木の生成の時間である。
単に係り受け構造を求めるだけなら速度を向上する余地は多分にあるうえ、
HPSGパーザ自体の高速化も研究されており\cite{Nishida99,Torisawa00}、
速度の問題は深刻であるとは考えていない。



まとめ\label{sec:conclusion}

本稿では、文法を用いて係り受け解析をする際に望ましい統計モデルについて論じた。
係り先の候補を文法が許すものに制限した後、
係り元から最も近い文節・二番目に近い文節・最も遠い文節のみに絞る。
これにより、係り元と全ての係り先候補の属性を同時に考慮する
「３つ組／４つ組モデル」を用いることができるようになり、
88.6$\%$という高い係り受け精度を達成した。
また、このモデルが精度向上に確かに寄与していることを示した。



\bibliographystyle{jnlpbbl}
\bibliography{nlp-j}


\begin{biography}
\biotitle{略歴}
\bioauthor{金山 博}{
1998年東京大学理学部情報科学科卒業。
2000年同大学院理学系研究科修士課程修了。
同年日本アイ・ビー・エム（株）入社、
現在に至る。
構文解析・機械翻訳等に関する研究に従事。
}

\bioauthor{鳥澤 健太郎}{
1992年東京大学理学部情報科学科卒業。
1995年同大学大学院理学系研究科情報科学専攻博士課程退学、
同年より同大学大学院理学系研究科情報科学専攻助手。
1998年より科学技術振興事業団さきがけ研究21研究員兼任。
計算言語学の研究に従事。理学博士。言語処理学会会員。
}

\bioauthor{光石 豊}{
1996年東京大学理学部情報科学科卒業。
1998年同大学院理学系研究科修士課程修了。
現在、同博士課程在学中。
HPSGの枠組による日本語文法に関する研究に従事。
ACM SIGMODJ学生会員。
}

\bioauthor{辻井 潤一}{
1971年京都大学工学部電気工学科卒業。1973年同大学大学院工学研究科修士課程修了。
京都大学工学部電気第2工学科助手、
助教授を経て、1988年英国マンチェスタ大学科学技術研究所(UMIST)教授、
1995年より、東京大学大学院理学系研究科教授（情報科学専攻）、現在に至る。
1981〜82年フランス・CNRS（グルノーブル）、招聘研究員。 
自然言語処理、機械翻訳の研究に従事。工学博士。
国際計算言語学委員会(ICCL)メンバ、情報処理学会、人工知能学会など、会員。
2000年6月より言語処理学会会長。
}

\bioreceived{受付}
\biorevised{再受付}
\bioaccepted{採録}

\end{biography}

\newpage
\thispagestyle{plain}

\verb+ +

