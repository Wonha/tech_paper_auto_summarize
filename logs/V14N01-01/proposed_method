本節では，日本語の構文的特徴と典型的な日本語文の解析の手順について整理する．
日本語は基本的にはSOV言語である．
語順は比較的自由である．
英語では，文中の語の構文的機能は語順で表される．
一方，日本語では，後置詞(postpositions)によって表される．
この点では，名詞の後に置かれる日本語の格助詞はドイツ語名詞の格変化と類似の役割を持っている．
ドイツ語名詞は格変化することによって，文法的な格を表している．
文節の概念は上記の日本語の性質と親和性があり，日本語文を構文的に分析するときに使われてきた．
文節は，1個以上の内容語(content words)とそれに続く0個以上の機能語(function words)から構成される．
文節をこのように定義することによって，ドイツ語のような屈折言語において文中の語の文法的役割を分析するときと似た手法を日本語文を分析するときにも使うことができる．
それゆえ，厳密なことを言えば，日本語の場合，語順が自由なのではなく，文節の順序が自由である．
ただし，文の主動詞を含む文節は文の末尾に置かれなければならない．
例えば，以下の2文は同じ意味を持つ: (1)健が彼女に本をあげた．
(2)健が本を彼女にあげた．
この2例文で，最も右の文節「あげた」(動詞の語幹と，過去や完了を表すマーカで構成されている)は文の末尾に置かれていることに注意されたい．
ここで，上に述べたものも含めて，通常の書き言葉の日本語で仮定される係り受けの制約条件をまとめておく．
最も右の文節を除いて，全ての文節は必ず一つの\TermHead{}を持つ．
\TermHead{}となる文節は，必ず係り元の文節の右側に位置する．
係り関係は交差しない．
これらの特徴は，基本的には韓国語やモンゴル語でも共通である．
日本語には前節のような特徴があるので，日本語文の解析では次のような手順が非常に一般的である:
文を形態素に分割する(つまり形態素解析する)
それらを文節にまとめ上げる
文節間の係り関係を解析する
それぞれの係り関係にagent，object，locationなどの意味的役割のラベルを付ける
我々は(3)における係り受け解析に焦点を置く．
本節では，提案アルゴリズムを解析時に使うものと，学習時に使うものとに分けて記す．
解析時のアルゴリズム，その時間計算量，学習時のアルゴリズムを順に述べ，最後に提案アルゴリズムの特徴のまとめと関連研究との理論的な比較を述べる．
我々の提案する係り受け解析のアルゴリズムの擬似コードを図 [REF_code:analysis]に示す．
このアルゴリズムは，ある文節が別の文節に係るかどうかを決定する推定器(estimator)とともに用いる．
推定器の典型的なものとして，SVMや決定木などの訓練できる分類器が考えられる．
ここでは，文中の二つの文節の係り関係を推定できる，つまり係るか否かを決定できる何らかの分類器があり，その分類器の時間計算量は文の長さに影響されないと仮定しておく．
係り関係の推定器を別にすれば，このアルゴリズムで使うデータ構造はわずか二つである．
一つは入力に関するもので，もう一つは出力に関するものである．
前者は，チェックすべき係り元の文節のIDを保持するためのスタックである．
後者は，既に解析された係り先文節のIDを保持する整数の配列である．
以下では，例を使いながら先に示したアルゴリズムの動作を説明する．
図[REF_code:analysis]の擬似コードに沿って，図[REF_sample-parsing]にある例文を解析してみよう．
説明のため，図[REF_code:analysis]のestimate_dependency()として完璧な分類器があるとする．
この分類器は図 [REF_sample-parsing]の例文に対して必ず正しい結果を返すとする．
まず始めに0 (健が)をスタックに積む．
0は文の先頭の文節のIDである．
この初期化の後，forループの各繰り返し(iteration)の中で解析がどのように進むかを見る．
最初のiterationでは，0番目の文節と1番目の文節(彼女に)の係り関係をチェックする．
0番目の文節は1番目の文節に係らないから，0をスタックに積み，次に1を積む．
ここで，スタックの底は0であって1ではないことに注意されたい．
より小さいIDが必ずスタックの底のほうに保持される．
この性質のおかげで，非交差の制約(第[REF_sec:prop]のC3)を破らずに解析を進めることができる．
2回目のiterationでは，1をスタックから降ろし，1番目の文節と2番目の文節(あの)の係り関係をチェックする．
1番目の文節は2番目には係らないので，再び1と2をスタックに積む．
3回目のiterationでは，2をスタックから降ろし，2番目の文節と3番目の文節(本を)の係り関係をチェックする．
2番目の文節は3番目に係るので，その関係をoutdep[ ]に格納する．
outdep[[MATH]]の値は，第[MATH]番目の文節の係り先を表す．
例えばoutdep[[MATH]は2番目の文節の係り先は3番目の文節であることを意味している．
次に，1をスタックから降ろし，1番目の文節と3番目の文節の係り関係をチェックする．
1番目の文節は3番目に係らないので1を再びスタックに積む．
その後，3をスタックに積む．
この段階で，スタックには頭から底に向けて3，1，0が格納されている．
3回目のiterationでは，3をスタックから降ろす．
3番目の文節と4番目の係り関係はチェックする必要がない．
4番目の文節は文中の最も末尾の文節であり，3番目の文節は必ず4番目にかかるからである．
そこでoutdep[[MATH]] [MATH]とする．
次に1をスタックから降ろす．
この場合も，1番目の文節と4番目との係り関係のチェックはする必要がない．
同様に，0番目の文節も4番目に係る．
結果としてoutdep[[MATH]] [MATH]とoutdep[[MATH]] [MATH]となる．
この時点でスタックは空となり，この解析の関数analyze( )は終了する．
解析結果である係り受け関係は，配列outdep[ ]に得られている．
一見したところ，提案したアルゴリズムの時間計算量の上限は，2重ループを含むため[MATH]と思える．
しかしそうではない．
時間計算量の上限が[MATH]であることを，図[REF_code:analysis]におけるwhileループの条件部が何回実行されるかを考えることによって示す．
条件部が失敗する回数と成功する回数とに分けて考える．
whileループの条件部は[MATH]回失敗する．
何故なら外側のforループが1から[MATH]へ，つまり[MATH]回実行されるからである．
もちろん，whileループが無限ループになることはない．
whileループの内部でstackに値を新たに積むことなく降ろしているので，いつか[MATH]になる．
つまり[MATH]を満たさず，whileループを抜けることになる．
一方，この条件部は[MATH]回成功する．
何故ならoutdep[[MATH]] [MATH]が[MATH]回実行されるからである．
文節[MATH]それぞれについて，outdep[[MATH]] [MATH]は必ず一度だけ実行される．
j = stack.pop()を実行すると，[MATH]に格納されている値は失われ，その値は二度と再びスタックに積まれることはないからである．
つまり，whileループは，高々[MATH]回実行される．
これは末尾の文節を除く文節数に等しい．
結局，whileループの条件部の実行回数は，失敗回数[MATH]と成功回数[MATH]を合計し[MATH]となる．
これは時間計算量の上限が[MATH]となることを意味している．
前節のアルゴリズムで用いる分類器のための訓練事例を作り出すには，図[REF_code:generate]に示すアルゴリズムを使う．
図[REF_code:analysis]にある解析用のアルゴリズムと殆ど同じである．
違いは，indep[ ]を使ってestimate_dependency()が正しい係り関係の判定を返す点と，outdep[ ]に係り先のIDを格納する必要がない点である．
我々の提案アルゴリズムは次のような特徴を持つ:
特定の機械学習の方法に依存しない．
訓練できる分類器ならどれでも使える．
左から右へ文を一度だけスキャンする．
時間計算量の上限は[MATH]である．
アルゴリズム中，最も時間を消費する部分である分類器の呼び出しの回数は，高々[MATH]回である．
アルゴリズムの流れとデータ構造は非常に簡単である．
そのため，実装も易しい．
我々のアルゴリズムと最も関連が深いモデルの一つは，[CITE]の Cascaded Chunking Modelである．
彼らのモデルと我々のアルゴリズムはF1を始め多くの共通点がある．
彼らのモデルと我々のアルゴリズムの大きな違いは，入力文を何回スキャンするかにある(F2)．
彼らのモデルでは，入力文を何回かスキャンする必要があり，これは計算上の非効率につながっている．
最悪の場合では[MATH]の計算が必要になる．
我々の解析アルゴリズムは，左から右に一度だけしかスキャンせず，実時間の音声認識などのような実用的なアプリケーションに対しても，より好適であろう．
それに加えて，アルゴリズムの流れと利用するデータ構造は，Cascaded Chunking Modelで使われるものよりもずっと簡単である(F4)．
彼らのモデルでは，チャンクタグを保持する配列が必要となり，入力文を何度もスキャンする間，この配列は正しく更新されなければならない．
NivreによるProjective Dependency Parsingの手法[CITE]も，我々のアルゴリズムと深い関係がある．
彼のアルゴリズムも，スタックを用いており，時間計算量の上限も[MATH]である．
ただし，我々のアルゴリズムが日本語を対象とし，係り先が必ず右にあることを前提にしているのに対し，Nivreのアルゴリズムは依存関係の向きはどちらでもよい．
その点では，彼のアルゴリズムは我々の手法をより一般的にしたものと考えることができる．
一方，[CITE]では，単語間の依存関係を決めるルールを用意しておき，ある一定の優先度で選ぶとしている．
我々は，依存関係が一方向である日本語に対して，機械学習を用いる方法を提示し，実際に検証している．
我々の解析アルゴリズムは，shift-reduce法の最も簡単な形の一つと考えられる．
典型的なshift-reduce法との違いは，アクションの型を複数持つ必要がなく，スタックの先頭のみ調べればよいという点である．
これらの簡潔さの理由は，日本語が制約C2 (第[REF_sec:prop]節参照)を仮定できること，文脈自由文法の解析ではなく，係り受け関係のみの解析であることの二つによる．
2文節間の係り関係を推定するために，2文節に関係する形態的，文法的情報を素性のベクタとして表現し，それを入力として分類器に係るか否かを判断させる．
その分類器として，サポートベクタマシン(SVMs) [CITE]を用いた．
SVMsは優れた特徴を持っている．
その一つは，多項式カーネルを用いると，ある事例の持つ素性の組合せが自動的に考慮される点である．
現在まで多数の分類タスクに対して，非常に優れた性能が報告されている．
SVMsの形式的な記述については，文献[CITE]を参照されたい．
素性として，第[REF_subsec:stfe]節以降で述べるものを用いた．
実際には2文節間の係り関係の推定の処理は，図[REF_code:analysis]のestimate_dependency()の中で行なう．
推定しようとする2文節の形態的，文法的情報を素性のベクタとして表現し，SVMsに係るか否かを判定させることになる．
以下では，まず基本となる標準素性を述べ，次にそれに追加して用いる付加的な素性について述べる．
ここで「標準素性」といっているものは，[CITE]でほぼ共通に使われている素性セットを指す．
それぞれの文節について以下の素性を使った:
主辞品詞，主辞品詞細分類，主辞活用型，主辞活用形，主辞表層形
語形品詞，語形品詞細分類，語形活用型，語形活用形，語形表層形
句読点
開き括弧，閉じ括弧
位置—文の先頭か文の末尾か
ここで主辞とは，概ね文節内の最も右の内容語に相当する．
品詞が特殊，助詞，接尾辞を除き，最も文末に近い形態素を指す．
語形とは，概ね文節内の最も右の機能語に相当する．
品詞が特殊となるものを除き，最も文末に近い形態素を指す．
これらに加えて，2文節間のギャップに関する素性も用いた．
距離(1，2--5，6以上)と，助詞，括弧，句読点である．
注目している係り元文節，係り先文節の前後の文節も有用である．
それらが固定的な表現や格フレーム，その他の連語を表すことがあるからである．
第[MATH]番目の文節が係り元文節で，第[MATH]番目の文節が係り先文節の候補だとする．
[MATH]番目の文節と[MATH]番目の文節の前後にある文節のうち，次の三つを素性として考慮する: [MATH]番目の文節([MATH]に係るときのみ)と，[MATH]番目の文節，[MATH]番目の文節の三つである．
我々のアルゴリズムでは，[MATH]を満たし[MATH]番目の文節が[MATH]番目の文節に係るかチェックしているとき，[MATH]番目の文節は必ず[MATH]番目の文節に係っていることに注意されたい．
提案手法におけるデータ構造を簡単にしておくために，[MATH]番目，[MATH]番目の文節からさらに遠い文節については考慮しなかった．
なお，[MATH]番目の文節が[MATH]番目の文節に係るかどうかはoutdep[ ]を見れば簡単にチェックできる．
注目している文節の前後を使うのは，[CITE]における動的素性と似ている．
「標準素性」では，文節内に二つ以上の機能語を含むとき格助詞の情報を見落とすことがある．
ある文節が格助詞と提題助詞を持つとする．
このとき格助詞の後ろに提題助詞が来る．
それゆえ，格助詞の情報を見落としてしまう．
「標準素性」では文節内の最も右の機能語しか素性として扱われないからである．
こういった情報を見落とさないように，文節内の全ての格助詞を素性として扱う．
「標準素性」で見落とされる重要な情報は他にもある．
それは，係り先候補の文節の最も左の語の情報である．
この語は係り元の文節の最も右の語と慣用表現のような強い相関関係を持つことも多い．
これに加えて，係り先候補文節の直後の文節の表層形も素性として使う．
これは，第[REF_subsec:loc]節の素性とともに用いる．
並列構造を正しく認識することは，長い文を正しく解析する際に最も難しいことの一つである．
KurohashiとNagaoは，二つの文節列の類似度を計算することによって並列句を認識する手法を提案している[CITE]．
現在までのところ，機械学習を使うシステムの中で並列構造を認識するための素性はあまり研究されていない．
我々は最初のステップとして，並列構造を認識するための基本的な二つの素性を試した．
注目している文節がキー文節(distinctive key bunsetsu) [CITE]であるとき，この二つの素性は使われる．
一つ目の素性は，係り元文節がキー文節であるときアクティブになるものである．
もう一つの素性は，係り元文節がキー文節で，その係り元文節と係り先候補の文節の主辞表層形が一致していればアクティブになるものである．
単純さを保つため，対象とする主辞品詞は名詞のみとした．
