日本語文の解析
本節では，日本語の構文的特徴と典型的な日本語文の解析の手順について整理
する．
\subsection{日本語の構文的特徴}\label{sec:prop}
日本語は基本的には SOV 言語である．語順は比較的自由である．
英語では，文中の語の構文的機能は語順で表される．一方，日本語では，後置
詞 (postpositions) によって表される．
この点では，
名詞の後に置かれる
日本語の格助詞は
ドイツ語名詞の格変化と類似の役割を持っている．
ドイツ語名詞は格変化することによって，文法的な格を表している．

文節の概念\footnote{
韓国語の {\em eojeol} も日本語の文節と似た概念である \cite{Yoon1999}．
文献 \cite{Abney1991} で定義される英語のチャンク (chunk) も文節に近いと
いえる．}
は上記の日本語の性質と親和性があり，日本語文を構文的に分析するときに使われてきた．
{\em 文節}は，1~個以上の内容語 (content words) とそれに続く 0~個以上
の機能語 (function words) から構成される．
文節をこのように定義することによって，ドイツ語のような屈折言語において
文中の語の文法的役割を分析するときと似た手法を日本語文を分析するときに
も使うことができる．

それゆえ，厳密なことを言えば，日本語の場合，語順が自由なのではなく，文
節の順序が自由である．
ただし，文の主動詞を含む文節は文の末尾に置かれなければならない．
例えば，以下の2文は同じ意味を持つ:
(1) 健が 彼女に 本を あげた． (2) 健が 本を 彼女に あげた．
この2例文で，最も右の文節「あげた」 (動詞の語幹と，過去や完了を表すマーカで
構成されている) は文の末尾に置かれていることに注意されたい．

ここで，上に述べたものも含めて，通常の書き言葉の日本語で仮定される係り
受けの制約条件をまとめておく．
\begin{description} 
\item[C1.] 最も右の文節を除いて，全ての文節は必ず一つの\TermHead{}を持つ．
\item[C2.] \TermHead{}となる文節は，必ず係り元の文節の右側に位置する．
\item[C3.] 係り関係は交差しない\footnote{
「これが僕は正しいと思う」
という例文のようにこの制約条件が成立しないこともあるが，
書き言葉では殆どの場合
成り立つ\cite[187ページ]{Nagao1996}．}．
\end{description}
これらの特徴は，基本的には韓国語やモンゴル語でも共通である．

\subsection{日本語文解析の典型的な手順}
日本語には前節のような特徴があるので，日本語文の解析で
は次のような手順が非常に一般的である:
\begin{enumerate}
\item 文を形態素に分割する (つまり形態素解析する)
\item それらを文節にまとめ上げる
\item 文節間の係り関係を解析する
\item それぞれの係り関係に agent，object，location などの意味的役割のラベル
を付ける
\end{enumerate}
我々は (3) における係り受け解析に焦点を置く．

アルゴリズム\label{sec:algo}
本節では，提案アルゴリズムを解析時に使うものと，学習時に使うものとに分
けて記す．
解析時のアルゴリズム，その時間計算量，学習時のアルゴリズムを
順に述べ，
最後に提案アルゴリズムの特徴のまとめと関連研究との理論的な比較を述べる．

\subsection{文を解析するアルゴリズム}

\begin{figure}[t]
// 入力: N: 一文中の文節の数 \\
// \hspace*{1em}  w[ ]: 処理対象の文の文節列を保持する配列 \\
// 出力: outdep[ ]: 解析結果，つまり文節間の係り関係を格納する整数の配列． \\
// \hspace*{1em} 例えば，j 番目の文節の係り先 ID は {\rm outdep[j]} 
で表現される． \\
// \\
// stack: 係り元文節の ID を保持する．もし空なら，このスタックに対する \\
// \hspace*{1em} pop メソッドは，{\rm EMPTY} ($-1$) を返す． \\
// function estimate\_dependency(j, i, w[ ]): \\
// \hspace*{1em} {\rm j} 番目の文節が {\rm i} 番目の文節に係ると
判断するとき，非ゼロを返す． \\
// \hspace*{1em} それ以外のとき，ゼロを返す． \\
function analyze(w[ ], N, outdep[ ]) \\
stack.push(0); \hspace*{1em} // ID 0 をスタックに積む． \\
for (int i = 1; i $<$ N; i++) \{ \hspace*{1em} // 変数 i は係り先文節，
変数 j は係り元文節を指すのに使う．\\
\hspace*{1em}  int j = stack.pop(); \hspace*{1em} // スタックから値を
降ろし，変数 j にセットする． \\
\hspace*{1em} while (j != EMPTY \&\& (i == N $-$ 1 $||$ estimate\_dependency(j, i, w))) \{ \\
\hspace*{2em} outdep[j] = i; \hspace*{1em} // j 番目の文節が i 番目の
文節に係る． \\
\hspace*{2em} j = stack.pop(); \hspace*{1em} // 次にチェックすべき係り
元文節の ID をスタックから降ろす． \\
\hspace*{1em}  \} \\
\hspace*{1em}  if (j != EMPTY) \\
\hspace*{2em}    stack.push(j); \\
\hspace*{1em}  stack.push(i); \\
 \}
\par\vspace{8pt}
\caption{提案手法の擬似コード (解析時)．ここで ``i == N - 1''
は i 番目の文節が文末の文節であることを\hspace*{27pt}意味している．}
\label{code:analysis}
\end{figure}

\begin{figure}[t]
// indep[ ]:  訓練事例から与えられる正しい係り受け関係を保持する整数の配列． \\ 
// \\
// function estimate\_dependency(j, i, w[ ], indep[ ]): \\
// \hspace*{1em} indep[j] == i が満たされるとき，非ゼロを返す．それ以外のとき，ゼロを返す． \\
// \hspace*{1em} 同時に，j 番目の文節が i 番目の文節に係るか係らないかに
応じて， 1 あるいは -1 の \\
// \hspace*{1em} ラベル付きで，素性ベクタ (エンコードされた事例) を出力する． \\
function generate\_examples(w[ ], N, indep[ ]) \\
stack.push(0); \\
for (int i = 1; i $<$ N; i++) \{ \\
\hspace*{1em}  int j = stack.pop(); \\
\hspace*{1em} while (j != EMPTY \&\& (i == N $-$ 1 $||$ estimate\_dependency(j, i, w, indep))) \{ \\
\hspace*{2em}    j = stack.pop(); \\
\hspace*{1em}  \} \\
\hspace*{1em}  if (j != EMPTY) \\
\hspace*{2em}    stack.push(j); \\
\hspace*{1em}  stack.push(i); \\
\}
\par\vspace{8pt}
\caption{訓練事例を作るための擬似コード．変数 w[ ],
N と stack は図~\ref{code:analysis} のものと同じである．}
\label{code:generate}
\end{figure}

我々の提案する係り受け解析のアルゴリズムの擬似コードを図
~\ref{code:analysis} に示す．
このアルゴリズムは，ある文節が別の文節に係るかどうかを決定する推定器 
(estimator) とともに用いる．
推定器の典型的なものとして，SVM や決定木などの訓練できる分類器が考えら
れる．
ここでは，文中の二つの文節の係り関係を推定できる，つまり係るか否かを決
定できる何らかの分類器があり，その分類
器の時間計算量は文の長さに影響されないと仮定しておく．


係り関係の推定器を別にすれば，
このアルゴリズムで使うデータ構造はわずか二つである．
一つは入力に関するもので，もう一つは出力に関するものである．
前者は，チェックすべき係り元の文節のID を保持するためのスタックである．
後者は，既に解析された係り先文節のID を保持する整数の配列である．


\begin{figure}[t]
\begin{center}
\begin{tabular}{llllll}
       & 健が & 彼女に & あの & 本を & あげた． \\
文節 ID     & 0    &  1     & 2    & 3    & 4 \\
係り先 & 4  &4  & 3    & 4    & -
\end{tabular}
\end{center}
\vspace{8pt}
\caption{例文}
\label{sample-parsing}
\end{figure}

以下では，例を使いながら先に示したアルゴリズムの動作を説明する．
図~\ref{code:analysis} の擬似コードに沿って，図~\ref{sample-parsing} にある例
文を解析してみよう．
説明のため，図~\ref{code:analysis} の {\it estimate\_dependency}() と
して完璧な分類器があるとする．この分類器は図
~\ref{sample-parsing} の例文に対して必ず正しい
結果を返すとする．

まず始めに 0 (健が) をスタックに積む．0 は文の先頭の文節のIDである．
この初期化の後，{\rm for} ループの各繰り返し (iteration) の中で解析が
どのように進むかを見る．
最初の iteration では，0 番目の文節と 1 番目の文節 (彼女に) の係り関係
をチェックする．
0 番目の文節は 1 番目の文節に係らないから，
0 をスタックに積み，次に 1 を積む．
ここで，スタックの底は 0 であって 1 ではないことに注意されたい．
より小さいIDが必ずスタックの底のほうに保持される．
この性質のおかげで，非交差の制約 (第~\ref{sec:prop} の C3) を破らずに
解析を進めることができる．

2 回目の iteration では，1 をスタックから降ろし，1 番目の文節と 2 番目
の文節 (あの) の係り関係をチェックする．
1 番目の文節は 2 番目には係らないので，再び 1 と 2 をスタックに積む．

3 回目の iteration では，2 をスタックから降ろし，2 番目の文節と 3 番目
の文節 (本を) の係り関係をチェックする．
2 番目の文節は 3 番目に係るので，その関係を {\it outdep}[ ] に格納する．
{\it outdep}[$j$] の値は，第~$j$ 番目の文節の係り先を表す．
例えば {\it outdep}[$2$]$ = 3$ は 2 番目の文節の係り先は 3 番目の文節であ
ることを意味している．

次に，1 をスタックから降ろし，1 番目の文節と 3 番目の文節の係り関係を
チェックする．
1 番目の文節は 3 番目に係らないので 1 を再びスタックに積む．
その後，3 をスタックに積む．
この段階で，スタックには頭から底に向けて 3，1，0 が格納されている．

3 回目の iteration では，3 をスタックから降ろす．
3 番目の文節と 4 番目の係り関係はチェックする必要がない．
4 番目の文節は文中の最も末尾の文節であり，3 番目の文節は必ず 4 番目に
かかるからである．
そこで {\it outdep}[$3$] $= 4$ とする．
次に 1 をスタックから降ろす．
この場合も，1 番目の文節と 4 番目との係り関係のチェックはする必要がな
い．
同様に，0 番目の文節も 4 番目に係る．
結果として {\it outdep}[$1$] $= 4$ と {\it outdep}[$0$] $= 4$ となる．
この時点でスタックは空となり，この解析の関数 {\it analyze}( ) は終了する．
解析結果である係り受け関係は，配列 \linebreak
{\it outdep}[ ] に得られている．

\subsection{時間計算量}\label{subsec:time}
一見したところ，提案したアルゴリズムの時間計算量の上限は，2 重ループを
含むため $O(n^{2})$ と思える．
しかしそうではない．
時間計算量の上限が $O(n)$ であることを，図~\ref{code:analysis} に
おける {\rm while} ループの条件部が何回実行されるかを考えることによって
示す．
条件部が失敗する回数と成功する回数とに分けて考える．

{\rm while} ループの条件部は $N - 2$ 回失敗する．何故なら
外側の {\rm for} ループが 1 から $N-1$ へ，つまり $N - 2$ 回実行されるからである．
もちろん，{\rm while} ループが無限ループになることはない．
{\rm while} ループの内部で stack に値を新たに積むことなく降ろしている
ので，いつか $j == {\rm EMPTY}$ になる．
つまり $j\  != {\rm EMPTY}$ を満たさず，
{\rm while} ループを抜けることになる．


一方，この条件部は $N - 1$ 回成功する．
何故なら {\it outdep}[$j$] $= i$ が $N - 1$ 回実行されるからである．
文節 $j$ それぞれについて，{\it outdep}[$j$] $= i$ は必ず一度だけ実行される．
{\it j = stack.pop}() を実行すると，$j$ に格納されている値は失われ，
その値は二度と再びスタックに積まれることはないからである．
つまり，{\rm while} ループは，高々 $N - 1$ 回実行される．これは
末尾の文節を除く文節数に等しい．

結局，{\rm while} ループの条件部の実行回数は，失敗回数 $N - 2$ と 
成功回数 $N - 1$ を合計し $2N - 3$ となる．
これは時間計算量の上限が $O(n)$ となることを意味している．


\subsection{訓練事例を作り出すアルゴリズム}
前節のアルゴリズムで用いる分類器のための訓練事例を作り出すには，
図~\ref{code:generate} に示すアルゴリズムを使う．
図~\ref{code:analysis} にある解析用のアルゴリズムと殆ど同じである．
違いは，{\it indep}[ ] を使って {\it estimate\_dependency}() が正しい係り
関係の判定を返す点と，{\it outdep}[ ] に係り先の ID を格納する必要がない点
である．

\subsection{特徴のまとめと関連研究との理論的な比較}\label{comp:theory}
我々の提案アルゴリズムは次のような特徴を持つ:
    \begin{itemize} 
\item[F1.] 特定の機械学習の方法に依存しない．
訓練できる分類器ならどれでも使える．
\item[F2.] 左から右へ文を一度だけスキャンする．
\item[F3.] 時間計算量の上限は $O(n)$ である．
アルゴリズム中，最も時間を消費する部分である分類器の呼び出しの回数は，
高々 $2N - 3$ 回である．
\item[F4.] アルゴリズムの流れとデータ構造は非常に簡単である．そのため，
実装も易しい．
    \end{itemize}

我々のアルゴリズムと
最も関連が深いモデルの一つは，\cite{Kudo2002} の \linebreak
Cascaded Chunking
 Model である．
彼らのモデルと我々のアルゴリズムは F1 を始め多くの共通点がある
\footnote{
文献 \cite{Uchimoto1999} に代表される 2 文節間の係り受け確率を考える
確率モデルと，Cascaded Chunking Model との比較は
文献 \cite{Kudo2002} で詳細になされている．
ほぼ全ての議論が確率モデルと我々の提案手法との比較
にも当てはまる．
}．
彼らのモデルと我々のアルゴリズムの大きな違いは，入力文を何回スキャンするかに
ある (F2)．
彼らのモデルでは，入力文を何回かスキャンする必要があり，
これは計算上の非効率につながっている．最悪の場合では $O(n^{2})$ の
計算が必要になる．
我々の解析アルゴリズムは，左から右に一度だけしかスキャンせず，
実時間の音声認識などのような実用的なアプリケーションに対しても，
より好適であろう．
それに加えて，アルゴリズムの流れと利用するデータ構造は，Cascaded Chunking
Model で使われるものよりもずっと簡単である (F4)．
彼らのモデルでは，チャンクタグを保持する配列が必要となり，
入力文を何度もスキャンする間，この配列は正しく更新されなければならない．

Nivre による Projective Dependency Parsing の手法 \cite{Nivre2003} も，
我々のアルゴリズムと深い関係がある\footnote{
Nivre のいう ``projective'' とは，依存関係 (係り関係) が交差しないこと
と同じである．
}．
彼のアルゴリズムも，
スタックを用いており，時間計算量の上限も $O(n)$ である．
ただし，
我々のアルゴリズムが日本語を対象とし，係り先が必ず右にあることを前提に
しているのに対し，Nivre のアルゴリズムは依存関係の向きはどちらでもよい．
その点では，彼のアルゴリズムは我々の手法をより一般的にしたものと考え
ることができる．
一方，\cite{Nivre2003} では，単語間の依存関係を決めるルールを用意して
おき，ある一定の優先度で選ぶとしている\footnote{
スウェーデン語を対象に126のルールを人手で記述している．ルールを，
「右向きに係る」「左向きに係る」「対象語をスタックから降ろす」「対象語
をスタックに積む」の四つのタイプに分け (詳細はここでは略す)，タイプ間
の優先度は，事前に人手で決める場合のみ実験で検証されている．ここでいう
対象語とは，アルゴリズム中で係り関係をチェックする対象となっている語を指す．
}．
我々は，依存関係が一方向である日本語に対して，機械学習を用いる方法を提
示し，実際に検証している．

我々の解析アルゴリズムは，shift-reduce 法の最も簡単な形の一つと考えられる．
典型的な shift-reduce 法との違いは，アクションの型を複数持つ必要がなく，
スタックの先頭のみ調べればよいという点である．
これらの簡潔さの理由は，
日本語が制約 C2 (第~\ref{sec:prop} 節参照) を仮定できること，
文脈自由文法の解析ではなく，係り受け関係のみの解析であることの二つによる．


係り関係を推定するためのモデル\label{sec:models}
2 文節間の係り関係を推定するために，2 文節に関係する形態的，文法的情
報を素性のベクタとして表現し，それを入力として分類器に係るか否かを判
断させる．

その分類器として，サポートベクタマシン (SVMs) \cite{Vapnik1995} を
用いた．
SVMs は優れた特徴を持っている．その一つは，
多項式カーネルを用いると，ある事例の持つ素性の組合せが自動的に
考慮される点である．
現在まで多数の分類タスクに対して，非常に優れた性能が報告されている．
SVMs の形式的な記述については，文献 \cite{Vapnik1995} を参照されたい．

素性として，第~\ref{subsec:stfe} 節以降で述べるものを用いた．
実際には 2 文節間の係り関係の推定の処理は，
図~\ref{code:analysis} の {\it estimate\_dependency}() の中で行なう．
推定しようとする 2 文節の形態的，文法的情報を素性のベクタとして表現
し，SVMs に係るか否かを判定させることになる．
以下では，まず基本となる標準素性を述べ，次にそれに追加して用いる付加的な素性に
ついて述べる．

\subsection{標準素性}\label{subsec:stfe}
ここで「標準素性」といっているものは，
\cite{Uchimoto1999,Sekine2000Backward,Kudo2000Japanese,Kudo2002} 
でほぼ共通に使われている素性セットを指す．
それぞれの文節について以下の素性を使った:
\begin{enumerate}
\item 主辞品詞，主辞品詞細分類，主辞活用型，主辞活用形，主辞表層形
\item 語形品詞，語形品詞細分類，語形活用型，語形活用形，語形表層形
\item 句読点
\item 開き括弧，閉じ括弧
\item 位置—文の先頭か文の末尾か
\end{enumerate}
ここで主辞とは，概ね文節内の最も右の内容語に相当する．品詞が
特殊，助詞，接尾辞を除き，最も文末に近い形態素を指す．
語形とは，概ね文節内の最も右の機能語に相当する．品詞が
特殊となるものを除き，最も文末に近い形態素を指す．

これらに加えて，2 文節間のギャップに関する素性も用いた．
距離 (1，2--5，6以上) と，助詞，括弧，句読点である．

\subsection{注目文節の前後の文節}\label{subsec:loc}
注目している係り元文節，係り先文節の前後の文節も有用である．
それらが固定的な表現や格フレーム，その他の連語を表すことがあるからである．
第~$j$ 番目の文節が係り元文節で，第~$i$ 番目の文節が係り先文節の
候補だとする．
$j$ 番目の文節と $i$ 番目の文節の前後にある文節のうち，次の三つを素性
として考慮する:
$j - 1$ 番目の文節 ($j$ に係るときのみ) と，
$i - 1$ 番目の文節，
$i + 1$ 番目の文節の三つである．
我々のアルゴリズムでは，$j < i - 1$ を満たし $j$ 番目の文節が $i$ 番目の文節に
係るかチェックしているとき，
$i - 1$ 番目の文節は必ず $i$ 番目の文節に
係っていることに注意されたい．
提案手法におけるデータ構造を簡単にしておくために，$j$ 番目，
$i$ 番目の文節からさらに遠い文節については考慮しなかった．
なお，$j - 1$ 番目の文節が $j$ 番目の文節に
係るかどうかは {\it outdep}[ ] を見れば簡単にチェックできる．
注目している文節の前後を使うのは，\cite{Kudo2002} における動的素性\footnote{
Kudo らのモデルは，以下の三つから動的素性を作る:
$j$ 番目の文節に係るもの (タイプ B)，$i$ 番目の文節に係るもの (タイプ A)，
$i$ 番目の文節の係り先 (タイプ C)．
我々の提案手法では解析が左から右に進むので，
タイプ C の素性を取り入れるためには，
スタッキング \cite{Wolpert1992} やその他の手法を用いる必要がある．
}と似ている．


\subsection{文節内の追加素性} 
「標準素性」では，文節内に二つ以上の機能語を含むとき格助詞の情報を見落とすことがある．
ある文節が格助詞と提題助詞\footnote{
提題助詞とは，主題を提示する助詞である \cite[page 50]{Masuoka1992}．
代表的な提題助詞は「は」である．
}を持つとする．
このとき格助詞の後ろに提題助詞が来る．
それゆえ，格助詞の情報を見落としてしまう．
「標準素性」では文節内の最も右の機能語しか素性として扱われないからであ
る\footnote{
例えば，係り元文節が「本-に-は」の場合，第~\ref{subsec:stfe} の語形の
素性として「は」に関する素性が採用される．「に」に関する素性は使われな
い．}．
こういった情報を見落とさないように，文節内の全ての格助詞を素性として扱う．


「標準素性」で見落とされる重要な情報は他にもある．
それは，係り先候補の文節の最も左の語の情報である．この語は係り元の文節の
最も右の語と慣用表現のような強い相関関係を持つことも多い．

これに加えて，係り先候補文節の直後の文節の表層形も素性として使う．
これは，第~\ref{subsec:loc} 節の素性とともに用いる．


\subsection{並列句のための素性}
並列構造を正しく認識することは，長い文を正しく解析する際に最も難しい
ことの一つである．
Kurohashi と Nagao は，二つの文節列の類似度を
計算することによって並列句を認識する手法を提案している \cite{Kurohashi1994}．

現在までのところ，機械学習を使うシステムの中で並列構造を認識するための素性はあまり研究されていない．
我々は最初のステップとして，並列構造を認識するための基本的な二つの素性を
試した．
注目している文節が{\em キー文節} ({\it distinctive key bunsetsu}) 
\cite[page 510]{Kurohashi1994} であるとき，
この二つの素性は使われる．
一つ目の素性は，係り元文節がキー文節であるときアクティブに
なるものである．
もう一つの素性は，係り元文節がキー文節で，その係り元文節と
係り先候補の文節の
主辞表層形が
一致していればアクティブになるものである．
単純さを保つため，対象とする
主辞品詞は
名詞のみとした．


