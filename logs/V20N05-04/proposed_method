WSDの対象単語[MATH]の語義の集合を[MATH]，[MATH]を含む文（入力データ）を[MATH]とする．
WSDの問題は最大事後確率推定を利用すると，以下の式の値を求める問題として表現できる．
つまり訓練データを利用して語義の分布[MATH]と各語義上での入力データの分布[MATH]を推定することでWSDの問題は解決できる．
今，ソース領域を[MATH]，ターゲット領域を[MATH]とした場合，WSDの領域適応の問題は[MATH]と[MATH]から生じている．
[MATH]が成立していることは明らかだが，[MATH]に対しては一考を要する．
一般の領域適応の問題では[MATH]であるが，WSDに限れば[MATH]と考えることもできる．
実際Chanらは[MATH]と[MATH]の違いの影響は非常に小さいと考え，[MATH]を仮定し，[MATH]をEMアルゴリズムで推定することでWSDの領域適応を行っている[CITE]．
古宮らは2つのソース領域の訓練データを用意し，そこからランダムに訓練データを取り出してWSDの分類器を学習している[CITE]．
論文中では指摘していないが，これも[MATH]を[MATH]に近づける工夫である．
ソース領域が1つだとランダムに訓練データを取り出しても[MATH]は変化しないが，ソース領域を複数用意することで[MATH]が変化する．
ただし[MATH]が成立していたとしても，WSDの領域適応の問題が[MATH]の推定に帰着できるわけでない．
仮に[MATH]であったとしても，領域[MATH]の訓練データだけから[MATH]を推定することは困難だからである．
これは共変量シフトの問題[CITE]と関連が深い．
共変量シフトの問題とは入力[MATH]と出力[MATH]に対して，推定する分布[MATH]が領域[MATH]と[MATH]で共通しているが，[MATH]における入力の分布[MATH]と[MATH]における入力の分布[MATH]が異なる問題である．
[MATH]の仮定の下では，入力[MATH]と出力[MATH]が逆になっているので，共変量シフトの問題とは異なる．
ただしWSDの場合，全く同じ文[MATH]が別領域に出現したとしても，[MATH]内の多義語[MATH]の語義が異なるケースは非常に稀であるため[MATH]が仮定できる．
[MATH]は語義識別そのものなので，WSDの領域適応の問題は共変量シフトの問題として扱えることができる．
共変量シフト下では訓練事例[MATH]に対して密度比[MATH]を推定し，密度比を重みとして尤度を最大にするようにモデルのパラメータを学習する．
Jiangらは密度比を手動で調整し，モデルにはロジステック回帰を用いている[CITE]．
齋木らは[MATH]をunigramでモデル化することで密度比を推定し，モデルには最大エントロピーモデルを用いている[CITE]．
ただしどちらの研究もタスクはWSDではない．
WSDでは[MATH]が単純な言語モデルではなく，「[MATH]は対象単語[MATH]を含む」という条件が付いているので，密度比[MATH]の推定が困難となっている．
また教師なしの枠組みで共変量シフトの問題が扱えるのかは不明である．
本論文では[MATH]を仮定したアプローチは取らず，[MATH]を仮定する．
この仮定があったとしても，領域[MATH]の訓練データだけから[MATH]を推定するのは困難である．
ここではこれをスパース性の問題と考える．
つまり領域[MATH]の訓練データ[MATH]は領域[MATH]においてスパースになっていると考える．
スパース性の問題だと考えれば，半教師あり学習や能動学習を領域適応に応用するのは自然である (Rai, Saha, Daum{e}, and Venkatasubramanian 2010)[CITE]．
また半教師あり学習や能動学習のアプローチを取った場合，[MATH]の訓練データが増えるので語義の分布の違い自体も同時に解消されていく[CITE]．
ここで指摘したいのは[MATH]が成立しており[MATH]の推定を困難にしているのがスパース性の問題だとすれば，領域[MATH]の訓練データ[MATH]は多いほどよい推定が行えるはずで，[MATH]が大きくなったとしても推定が悪化するはずがない点である．
しかし現実には[MATH]を大きくするとWSD自体の精度が悪くなる場合もあることが報告されている（例えば[CITE]）．
これは一般に負の転移現象[CITE]と呼ばれている．
WSDの場合[MATH]を推定しようとして，逆に語義の分布[MATH]の推定が悪化することから生じる．
つまり領域[MATH]におけるWSDの解決には[MATH]におけるデータスパースネスの問題に対処しながら，同時に[MATH]の推定が悪化することを避けることが必要となる．
また領域適応ではアンサンブル学習も有効な手法である．
アンサンブル学習自体はかなり広い概念であり，実際，バギング，ブースティングまた混合分布もアンサンブル学習の一種である．
Daum{e}らは領域適応のための混合モデルを提案している(Daum{e} and Marcu 2006)[CITE]．
そこでは，ソース領域のモデル，ターゲット領域のモデル，そしてソース領域とターゲット領域を共有したモデルの3つを混合モデルの構成要素としている．
Daiらは代表的なブースティングアルゴリズムのAdaBoostを領域適応の問題に拡張したTrAdaBoostを提案している[CITE]．
またKamishimaらはバギングを領域適応の学習用に拡張したTrBaggを提案している[CITE]．
WSDの領域適応については古宮の一連の研究[CITE]があるが，そこではターゲット領域のラベルデータの使い方に応じて学習させた複数の分類器を用意しておき，単語や事例毎に最適な分類器を使い分けることで，WSDの領域適応を行っている．
これらの研究もアンサンブル学習の一種と見なせる．

領域[MATH]におけるデータスパースネスの問題に対処する際に，[MATH]の推定が悪化することを避けるために，本論文では識別の際に[MATH]の情報をできるだけ利用しないという方針をとる．
そのためにk近傍法を利用する．
どのような学習手法を取ったとしても，何らかの汎化を行う以上，[MATH]の影響を受けるが，k近傍法はその影響が少ない．
k近傍法はデータ[MATH]のクラスを識別するのに，訓練データの中から[MATH]と近いデータ[MATH]個を取ってきて，それら[MATH]個のデータのクラスの多数決により[MATH]のクラスを識別する．
k近傍法が[MATH]の影響が少ないのは[MATH]の場合（最近傍法）を考えればわかりやすい．
例えば，クラスが[MATH]であり，[MATH]，[MATH]であった場合，通常の学習手法であれば，ほぼ全てのデータを[MATH]と識別するが，最近傍法では，入力データ[MATH]と最も近いデータ1つだけがクラス[MATH]であれば，[MATH]のクラスを[MATH]と判断する（図[REF_zu1]参照）．
つまりk近傍法ではデータ全体の分布を考慮せずに[MATH]個の局所的な近傍データのみでクラスを識別するために，その識別には[MATH]の影響が少ない．
ただしk近傍法は近年の学習器と比べるとその精度が低い．
そのためここではk近傍法を補助的に利用する．
具体的には通常の識別はSVMで行い，SVMでの識別の信頼度が閾値[MATH]以下の場合のみ，k近傍法の識別結果を利用することにする．
ここで[MATH]の値が問題だが，語義の数が[MATH]個である場合，識別の信頼度（その語義である確率）は少なくとも[MATH]以上の値となる．
そのためここではこの値の1割をプラスし[MATH]とした．
なおこの値は予備実験等から得た最適な値ではないことを注記しておく．
領域[MATH]におけるデータスパースネスの問題に対処するために，ここではトピックモデルを利用する．
WSDの素性としてシソーラスの情報を利用するのもデータスパースネスへの1つの対策である．
シソーラスとしては，分類語彙表などの手作業で構築されたものとコーパスから自動構築されたものがある．
前者は質が高いが分野依存の問題がある．
後者は質はそれほど高くないが，分野毎に構築できるという利点がある．
ここでは領域適応の問題を扱うので，後者を利用する．
つまり領域[MATH]からシソーラスを自動構築し，そのシソーラス情報を領域[MATH]の訓練事例と領域[MATH]のテスト事例に含めることで，WSDの識別精度の向上を目指す．
注意として，WSDでは単語間の類似度を求めるためにシソーラスを利用する．
そのため実際にはシソーラスを構築するのではなく，単語間の類似度が測れる仕組みを作っておけば良い．
この仕組みが単語のクラスタリング結果に対応する．
つまりWSDでの利用という観点では，シソーラスと単語クラスタリングの結果は同等である．
そのため本論文においてシソーラスと述べている部分は，単語のクラスタリング結果を指している．
この単語のクラスタリング結果を得るためにトピックモデルを利用する．
トピックモデルとは文書[MATH]の生起に[MATH]個の潜在的なトピック[MATH]を導入した確率モデルである．
トピックモデルの1つであるLatent Dirichlet Allocation (LDA) [CITE]を用いた場合，単語[MATH]に対して[MATH]が得られる．
つまりトピック[MATH]をひとつのクラスタと見なすことで，LDAを利用して単語のソフトクラスタリングが可能となる．
領域[MATH]のコーパスとLDAを利用して，[MATH]に適した[MATH]が得られる．
[MATH]の情報をWSDに利用するいくつかの研究[CITE]があるが，ここではハードタグ[CITE]を利用する．
ハードタグとは[MATH]に対して最も関連度の高いトピック[MATH]を付与する方法である．
まずトピック数を[MATH]としたとき，[MATH]次元のベクトル[MATH]を用意し，入力事例[MATH]中に[MATH]種類の単語[MATH]が存在したとき，各[MATH]([MATH])に対して最も関連度の高いトピック[MATH]を求め，[MATH]の[MATH]次元の値を1にする．
これを[MATH]から[MATH]まで行い[MATH]を完成させる．
作成できた[MATH]をここではトピック素性と呼ぶ．
トピック素性を通常の素性ベクトル（ここでは基本素性と呼ぶ）に結合することで，新たな素性ベクトルを作成し，その素性ベクトルを対象に学習と識別を行う．
なお，本論文で利用した基本素性は，対象単語の前後の単語と品詞及び対象単語の前後3単語までの自立語である．
