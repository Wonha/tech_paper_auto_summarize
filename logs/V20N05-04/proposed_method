WSD の領域適応の問題


WSD の対象単語\( w\)の語義の集合を\( C = \{c_1,c_2,\cdots,c_k \}\)，
\( w\)を含む文（入力データ）を\( x \)とする．
WSD の問題は最大事後確率推定を利用すると，以下の式の値を求める問題として表現できる．
\[
\arg \max_{c \in C } P(c) P(x|c)
\]
つまり訓練データを利用して語義の分布\( P(c) \) と各語義上での入力データの分布
\( P(x|c) \)を推定することで WSD の問題は解決できる．
今，ソース領域を\( S\)，ターゲット領域を\( T\)とした場合，
WSD の領域適応の問題は\( P_S (c) \ne P_T (c)\) と
\( P_S (x|c) \ne P_T (x|c)\) から生じている．

\( P_S (c) \ne P_T (c)\)が成立していることは明らかだが，
\( P_S (x|c) \ne P_T (x|c)\) に対しては一考を要する．
一般の領域適応の問題では\( P_S (x|c) \ne P_T (x|c)\)であるが，
WSD に限れば\( P_S (x|c) = P_T (x|c)\)と考えることもできる．
実際 Chan らは\( P_S (x|c) \) と \( P_T (x|c)\)の違いの影響は
非常に小さいと考え，\( P_S (x|c) = P_T (x|c)\)を仮定し，
\( P_T (c)\)を EM アルゴリズムで推定することで WSD の領域適応を
行っている\cite{chan2005word,chan2006estimating}．
古宮らは2つのソース領域の訓練データを用意し，そこからランダムに訓練データを取り出して
WSD の分類器を学習している\cite{komiya-nenji2013}．論文中では指摘していないが，これも
\( P_S (c)\)を\( P_T (c)\)に近づける工夫である．ソース領域が1つだとランダムに
訓練データを取り出しても\( P_S (c)\)は変化しないが，
ソース領域を複数用意することで\( P_S (c)\)が変化する．

ただし\( P_S (x|c) = P_T (x|c)\)が成立していたとしても，
WSD の領域適応の問題が\( P_T (c)\)の推定に帰着できるわけでない．
仮に\( P_S (x|c) = P_T (x|c)\)であったとしても，領域\( S \)の訓練データだけから
\( P_T (x|c)\)を推定することは困難だからである．
これは共変量シフトの問題\cite{shimodaira2000improving,sugiyama-2006-09-05}と関連が深い．
共変量シフトの問題とは入力\( x \)と出力\( y \)に対して，推定する分布\( P(y|x)\) が
領域\( S \)と\( T \)で共通しているが，\( S \)における入力の分布\( P_S(x) \)と
\( T \)における入力の分布\( P_T(x) \)が異なる問題である．
\( P_S (x|c) = P_T (x|c)\)の仮定の下では，入力\( x \)と出力\( c \)が逆になっているので，
共変量シフトの問題とは異なる．
ただし WSD の場合，全く同じ文\( x \)が別領域に出現したとしても，\( x \)内の多義語\( w \)
の語義が異なるケースは非常に稀であるため\( P_S (c|x) = P_T (c|x)\)が仮定できる．
\( P_T (c|x)\)は語義識別そのものなので，WSD の領域適応の問題は
共変量シフトの問題として扱えることができる．
共変量シフト下では訓練事例\( x_i \)に対して密度比\( P_T(x_i)/P_S(x_i) \)を推定し，
密度比を重みとして尤度を最大にするようにモデルのパラメータを学習する．
Jiang らは密度比を手動で調整し，
モデルにはロジステック回帰を用いている\cite{jiang2007instance}．
齋木らは\( P(x) \)を unigram でモデル化することで密度比を推定し，
モデルには最大エントロピーモデルを用いている\cite{saiki-2008-03-27}．
ただしどちらの研究もタスクは WSD ではない．
WSD では\( P(x) \)が単純な言語モデルではなく，「\( x \)は対象単語\( w \)を含む」
という条件が付いているので，密度比\( P_T(x)/P_S(x) \)の推定が困難となっている．
また教師なしの枠組みで共変量シフトの問題が扱えるのかは不明である．

本論文では\( P_S (c|x) = P_T (c|x)\)を仮定したアプローチは取らず，
\( P_S (x|c) = P_T (x|c)\)を仮定する．この仮定があったとしても，
領域\( S \)の訓練データだけから\( P_T (x|c)\)を推定するのは困難である．
ここではこれをスパース性の問題と考える．
つまり領域\( S \)の訓練データ\( D \)は領域\( T \)においてスパースになっていると考える．
スパース性の問題だと考えれば，半教師あり学習や能動学習を領域適応に応用するのは自然である
\footnote{ただし\( D \)は領域\( T \)内のサンプルではなく不均衡な訓練データという点には注意すべきであり，この点を考慮した半教師あり学習や能動学習が必要である．}
    (Rai, Saha, Daum{\'e}, and Venkatasubramanian 2010)\nocite{rai2010domain}．
また半教師あり学習や能動学習のアプローチを取った場合，
\( T \)の訓練データが増えるので語義の分布の違い自体も同時に解消されていく\cite{chan2007domain}．

ここで指摘したいのは\( P_S (x|c) = P_T (x|c)\)が成立しており\( P_T (x|c)\)の推定を
困難にしているのがスパース性の問題だとすれば，領域\( S \)の訓練データ\( D \)は
多いほどよい推定が行えるはずで，\( D \)が大きくなったとしても推定が悪化するはずがない点である．
しかし現実には\( D \)を大きくすると WSD 自体の精度が悪くなる場合もあることが報告されている
（例えば\cite{komiya-nenji2013}）．
これは一般に負の転移現象\cite{rosenstein2005transfer}と呼ばれている．
WSD の場合\( P_T (x|c)\)を推定しようとして，逆に語義の分布\( P_T (c)\)の推定が悪化することから
生じる．
つまり領域\( T \)における WSD の解決には\( T \)におけるデータスパースネスの問題に対処しながら，
同時に\( P_T (c)\)の推定が悪化することを避けることが必要となる．

また領域適応ではアンサンブル学習も有効な手法である．
アンサンブル学習自体はかなり広い概念であり，
実際，バギング，ブースティングまた混合分布もアンサンブル学習の一種である．
    Daum{\'e}らは領域適応のための混合モデルを提案している(Daum{\'e} and Marcu 2006)\nocite{daume2006domain}．
そこでは，ソース領域のモデル，ターゲット領域のモデル，そして
ソース領域とターゲット領域を共有したモデルの3つを混合モデルの構成要素としている．
Dai らは代表的なブースティングアルゴリズムの AdaBoost を領域適応の
問題に拡張した TrAdaBoost を提案している\cite{Dai2007}．
また Kamishima らはバギングを領域適応の学習用に拡張した TrBagg を提案している\cite{kamishima2009trbagg}．
WSD の領域適応については古宮の一連の研究\cite{komiya2,komiya3,komiya-nlp2012}があるが，
そこではターゲット領域のラベルデータの使い方に応じて学習させた複数の分類器を用意しておき，
単語や事例毎に最適な分類器を使い分けることで，WSD の領域適応を行っている．
これらの研究もアンサンブル学習の一種と見なせる．



提案手法

\subsection{k~近傍法の利用}

領域\( T \)におけるデータスパースネスの問題に対処する際に，\( P_T (c)\)の推定が
悪化することを避けるために，
本論文では識別の際に\( P_T (c)\)の情報をできるだけ利用しないという方針をとる．
そのために k~近傍法を利用する．
どのような学習手法を取ったとしても，何らかの汎化を行う以上，\( P_T (c)\)の影響を受けるが，
k~近傍法はその影響が少ない．
k~近傍法はデータ\( x \)のクラスを識別するのに，訓練データの中から\( x \)と近いデータ
\( k \)個を取ってきて，それら\( k \)個のデータのクラスの多数決により\( x \)のクラスを識別する．
\mbox{k~近傍法}が\( P_T (c)\)の影響が少ないのは\( k = 1\)の場合（最近傍法）を考えればわかりやすい．
例えば，クラスが\( \{ c_1, c_2 \} \) であり，\( P(c_1) = 0.99 \)，\( P(c_2) = 0.01 \)であった場合，
通常の学習手法であれば，ほぼ全てのデータを\( c_1 \)と識別するが，
最近傍法では，入力データ\( x \)と最も近いデータ 1 つだけがクラス\( c_2 \)であれば，
\( x \)のクラスを\( c_2 \)と判断する（\mbox{図\ref{zu1}}参照）．
つまりk~近傍法ではデータ全体の分布を考慮せずに\( k \)個の局所的な近傍データのみで
クラスを識別するために，その識別には\( P_T (c)\)の影響が少ない．

\begin{figure}[t]
\begin{center}
\includegraphics{20-5ia4f1.eps}
\end{center}
\caption{分布の影響が少ない k-NN}
\label{zu1}
\vspace{-1\Cvs}
\end{figure} 

ただし k~近傍法は近年の学習器と比べるとその精度が低い．
そのためここでは k~近傍法を補助的に利用する．
具体的には通常の識別は SVM で行い，SVM での識別の信頼度が
閾値\( \theta \)以下の場合のみ，k~近傍法の識別結果を利用することにする．

ここで\( \theta \)の値が問題だが，語義の数が\( K \)個である場合，
識別の信頼度（その語義である確率）は少なくとも\( 1/K \)以上の値となる．
そのためここではこの値の1割をプラスし\( \theta = 1.1/K \) とした．
なおこの値は予備実験等から得た最適な値ではないことを注記しておく．



\subsection{トピックモデルの利用}

領域\( T \)におけるデータスパースネスの問題に対処するために，ここでは
トピックモデルを利用する．

WSD の素性としてシソーラスの情報を利用するのもデータスパースネスへの 1 つの
対策である．シソーラスとしては，分類語彙表などの手作業で構築されたものと
コーパスから自動構築されたものがある．前者は質が高いが分野依存の問題がある．
後者は質はそれほど高くないが，分野毎に構築できるという利点がある．
ここでは領域適応の問題を扱うので，後者を利用する．
つまり領域\( T \)からシソーラスを自動構築し，そのシソーラス情報を
領域\( S \)の訓練事例と領域\( T \)のテスト事例に含めることで，
WSD の識別精度の向上を目指す．
注意として，WSD では単語間の類似度を求めるためにシソーラスを利用する．
そのため実際にはシソーラスを構築するのではなく，単語間の類似度が測れる仕組みを
作っておけば良い．この仕組みが単語のクラスタリング結果に対応する．
つまり WSD での利用という観点では，シソーラスと単語クラスタリングの結果は
同等である．そのため本論文においてシソーラスと述べている部分は，
単語のクラスタリング結果を指している．

この単語のクラスタリング結果を得るためにトピックモデルを利用する．
トピックモデルとは文書\( d \)の生起に\( K \)個の潜在的なトピック\( z_i \)を
導入した確率モデルである．
\[
p(d) = \sum_{i = 1}^{K} p(z_i) p(d |z_i)
\]
トピックモデルの 1つである Latent Dirichlet Allocation (LDA) \cite{blei}を用いた場合，
単語\( w \)に対して\( p(w|z_i) \)が得られる．
つまりトピック\( z_i \)をひとつのクラスタと見なすことで，
LDA を利用して単語のソフトクラスタリングが可能となる．

領域\( T \)のコーパスと LDA を利用して，\( T \)に適した\( p(w|z_i) \)が得られる．
\( p(w|z_i) \)の情報を WSD に利用するいくつかの研究\cite{li,boyd1,boyd2}があるが，
ここではハードタグ\cite{cai}を利用する．
ハードタグとは\( w \)に対して最も関連度の高いトピック\( z_{\hat{i}} \)を付与する方法である．
\[
\hat{i} = \arg \max_{i} p(w|z_i) 
\]
まずトピック数を\( K \)としたとき，\( K \)次元のベクトル\( t \)を用意し，
入力事例\( x \)中に\( n \)種類の単語\( w_1, w_2, \cdots, w_n \)が存在したとき，
各\( w_j \)(\(j = 1 \sim n\))に対して最も
関連度の高いトピック\( z_{\hat{i}} \)を求め，\( t \)の\( \hat{i} \)次元の
値を 1 にする．これを\( w_1 \)から\( w_n \)まで行い\( t \)を完成させる．
作成できた\( t \)をここでは{\bf トピック素性}と呼ぶ．
トピック素性を通常の素性ベクトル（ここでは{\bf 基本素性}と呼ぶ）に結合することで，
新たな素性ベクトルを作成し，その素性ベクトルを対象に学習と識別を行う．

なお，本論文で利用した基本素性は，対象単語の前後の単語と品詞及び
対象単語の前後3単語までの自立語である．


