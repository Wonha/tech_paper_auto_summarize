本節では，我々の要約方式について，二通りの視点から評価を行なう．
まずは，検索タスクの精度・効率の良さと言う観点から，評価型情報検索ワークショップであるNTCIR2[CITE]におけるTSC(Text Summarization Challenge)における「課題B IRタスク用要約」(以下，TSC Task Bと呼ぶ)に基づいて評価を行なう[CITE]．
つぎに，幾つかの例により，我々の方式が各語に与える重要度を，検索質問を考慮しない語の重みづけ手法(TF値，TF・IDF値)と比較することにより，我々の重要度計算手法の特徴を定性的に評価する．
なお，我々の評価実験においては，要約生成の手順に以下の条件を加えた．
記事の表題(見出し文．
Headline)と本文を区別せずに要約の入力とする．
名詞をキーワードとする．
文書からの名詞抽出には形態素解析器JUMAN3.61[CITE]を用いた．
IDF値等の計算においては，当初TSC実行委員会から発表された使用新聞記事データである毎日新聞1994年，1995年，1998年に加えて，手元にあった1997年を全文書集合とした．
最大距離アルゴリズムにおけるパラメタ[MATH]は0.5とする．
要約を一覧形式で提示することを想定すると，要約文書の長さが統一されているほうが，見やすい．
そのため，要約文書の長さは要約率ではなく，絶対的な長さにより決定する．
具体的には，150形態素をしきい値とする．
文書の総形態素数が150より短い場合には要約をせずに原文書を提示する．
要約生成に当たって，原文書の文が省略されている箇所には「…」を加え，原文書の段落箇所には改行を加える．
図[REF_Fig:SummaryInIR]に情報検索タスクにおける要約品質の評価実験の概要を示す．
TSC Task Bにおいては，TSC実行委員会より配布されたデータセットに，12のトピックがあり，それぞれ，検索要求1，検索文書50文書から構成されている．
検索文書は1998年の毎日新聞の記事集合から検索されたものである．
TSCへの参加者は各自のシステムを用いて，これらの文書を要約し，実行委員会に提出する．
提出された要約文書に対して，TSC実行委員会による被験者を用いた評価が行なわれた．
被験者は学生36名で，各検索要求につき，3人が割り当てられている．
被験者は各要約を読むことによって，それが検索要求に適合しているか否かの判断を行う．
その判断と，あらかじめ原文書に対して付与されている関連性評価(以下，関連度ともいう)を比較することにより，要約の品質が評価される．
すなわち，両者が一致する度合が高いシステムほど有効な要約を生成すると考えることができる．
原文書に付与されている関連性評価はA,B,Cの三段階である．
ここで，Aはその文書が検索要求に適合すること，Bは関係のある文書であること，Cは関係のない文書であることを表す．
これに対して，被験者らには関連性の有無(YES/NO)という二段階で提示してもらう．
よって，両者の一致の判定においては，A判定の文書だけを関連文書とする場合(Answer Level A)と，A判定に加えてB判定の文書も関連文書とする場合(Answer Level B)が考えられる．
表[REF_Table:EvaluationResultAll]に個別の評価尺度についての結果を他の参加システム(8システム)ならびにTSC実行委員会が準備したベースラインシステム(3システム)と比較して示す．
ベースラインシステムは，「全文提示(Fulltext)」(要約率100%)，「質問バイアス付きTFに基づく方式(TF with QB)」(要約率20%)，「文書の先頭を採るリード方式(Lead)」(要約率20%)である．
質問バイアス付きTF法は，TFを語の重みとして重要文抽出を行なうものであるが，この時に，検索質問に現れる語について2倍の重みを与えている．
これらのシステムの概略については，付録[REF_Appendix:NTCIR2 TSC Task Bに参加した他システムの概要]を参照されたい．
評価尺度には以下のものを用いた．
いずれの値も，全てのトピックを通じて集計したものである．
被験者が1検索要求に関するタスク(50文書)に要した時間(TIME)
タスクをどの程度適切に行なえたかを示す指標．
すなわち，再現率(Recall)，適合率(Precision)，F値(F-measure)．
要約文書の長さ(1文書あたりの平均文字数,LENGTH)
[htbp]
\comment{
}
情報検索の結果の文書に対する要約においては，利用者が行なう適合性判断のための時間の短さと，適合性判断の正確さが共に達成されることが必要である．
一方で，両者はトレードオフの関係にある．
例えば，長い要約文書を提示すれば，タスク遂行の時間が長くなるが，一方で，精度は概ね向上すると考えられる．
よって，両者を同時に評価する尺度が必要とされるが，未だ良いものが提案されていない．
そこで，我々のシステムを含む各システムの再現率，適合率，F値について，適合性判断のための所要時間との関係をプロットした．
Answer Level A,Bの場合を図[REF_Fig:Time-RPF]にぞれぞれ示す．
[htbp]
{ccc}
&
&
\multicolumn{3}{c}{(A) Answer Level A}
&
&
\multicolumn{3}{c}{(B) Answer Level B }
\multicolumn{3}{c}{}
\multicolumn{3}{c}{Proposed:我々の手法，Fulltext:原文書，TF with QB:質問バイアス付き}
\multicolumn{3}{c}{TFによる重要文抽出手法，Lead:先頭から20%の文を抽出する手法，}
\multicolumn{3}{c}{無印:他の参加システム}
また，判定時間と要約文書の平均文字数の間の関係を図[REF_Fig:Time-Length]に示す．
[htbp]
これまで示した結果では，全てのトピックに亙る平均値を用いてタスクの遂行精度を議論してきたが，当然，トピックによって各システムの精度が異なるはずである．
そこで，我々の手法と各ベースライン手法による要約において，トピック毎のタスク遂行精度をプロットした．
Answer Level A,Bの場合をそれぞれ図[REF_Fig:Topic-RPF]に示す．
なお，図中`Ave.'は全トピックに亙る平均値である．
[htbp]
{ccc}
&
&
\multicolumn{3}{c}{(A) Answer Level A}
&
&
\multicolumn{3}{c}{(B) Answer Level B }
要約文書の質を今少し詳細に検討するために，各々の文書に対して，関連性有り(YES)と答えた被験者の人数を調べる．
この人数はトピックに対する生成された要約文書の関連度の高さを表す尺度と考えられる．
そこで，原文書を関連性判定(A,B,C)によって分類し，その要約に対してYESと判定した人数毎に文書頻度を集計し，プロットした．
結果を図[REF_Fig:Rel-Judge]に示す．
語の重みづけについては正解というものがないので，定量的にその評価をすることが難しい．
ここでは，最も基本的な重みづけである，TF値，TF・IDF値による重みと，本手法の重みづけを実例により比較し，定性的に我々の語の重要度決定手法の特徴を検討する．
ここでは，典型的な語の重みがどの様になっているかを調べることが主眼であるので，我々のシステムで最もF値の高かったトピック1027に注目した．
このトピックの内容を表[REF_Table:Topic1027]に示す．
このトピックについて，原文書の関連度がそれぞれ，A，B，Cであるものを一つずつ選択し，各種の語の重みけを行なった結果について，上位10位までを求めた．
表[REF_Table:weight1027A],[REF_Table:weight1027B],[REF_Table:weight1027C]にその結果を示す．
表においてIGRsumは式([REF_Eq:IGRsum])に示される情報利得比の和であり，TFIDFIGRsumはTF・IDF値にIGRsumを乗じた値である．
