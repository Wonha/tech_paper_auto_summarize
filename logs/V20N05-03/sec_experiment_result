評価実験を通して，提案手法の有効性を検証する．
具体的には以下の3項目を検証する．
提案手法の比較：前節までで述べた4つのクレーム文検出モデルの中で，どのモデルが最良であるかを検証する．
他手法との比較：提案手法と他手法との比較実験を行い，その結果から提案手法の有効性を検証する．
学習データ量とクレーム文検出精度の関係について：提案したデータ生成手法は学習データを自動生成できるため，人手による生成に比べて遥かに多くの教師データを準備できる．
この利点を実験を通して検証する．
実験には，楽天データ公開において公開された楽天トラベルの施設データを利用した．
このデータは約35万件（平均4.5文／件）の宿泊施設に関するレビューから構成されており，ここから，無作為に選んだ1,000レビューに含まれる文を評価用データとして用い，残りを教師データ生成用に利用した．
評価用データには4,308文が含まれており，その内の24%にあたる1,030文がクレーム文であった．
つまり，4,308文からクレームを述べている1,030文を過不足なく検出することがここでの実験課題である．
評価用データの作成では，まず，レビュー文書中の各文が1行1文となるようにデータを整形し，それを作業者に提示した．
そして作業者は，与えられたデータの1文（1行）ごとにクレーム文か否かを判定していった．
なお，ある文の判定時には，同一レビュー内の他の全ての文が参照できる状態になっている．
2名の作業者によって上記の作業を独立に並行に行ったが，このうち1名の作業結果を評価用データとして採用した．
2名の作業者間の一致度を[MATH]係数の値によって評価したところ，[MATH]であった．
この結果は，作業者間の判断が十分に一致していたことを示している．
作業者間で判断が一致しなかった事例としては，文が長く，ひとつの文で複数の事柄が述べられている場合や，「長身で据え置きのものでは短くて…」のように，クレームの原因が宿泊施設側にあるとは必ずしも言えない場合が多かった．
教師データ生成時に必要となる評価表現辞書には，高村ら[CITE]の辞書作成手法に基いて作成された辞書を使用した．
ただし，高村らのオリジナルの辞書は自動構築されたもので，そのままでは誤りが含まれているため，以下の手続きによって誤り修正を施し，本実験で使用する辞書として採用した．
オリジナルの辞書には各登録語に対して肯定／否定の強さを示すと解釈できる[MATH]の範囲のスコアが付与されている．
このスコアは，値が大きいほど肯定，また，小さいほど否定をあらわし，0付近はどちらでもないことを示していると解釈できる．
そこでまず，このスコアの絶対値の大きいものから0.9付近までの単語を自動的に選択した．
そして，選択された各単語に対して人手による誤り修正を施し，結果として肯定表現760件，否定表現862件からなる辞書を作成し，本実験に用いた．
また，要求表現辞書として，「欲しい」，「ほしい」，「べし」からなる辞書を作成して実験に使用した．
周辺文脈の窓枠長[MATH]の指定は，データ作成時，モデル学習時，評価用データの分類時のすべての過程で同期させている．
また，各データの単語分割はMeCabによって行った．
また，計算の都合上，[MATH]の場合は[MATH]とした．
今回のように，分類すべきクラスがクレーム／非クレームという2クラスの分類問題の場合，\eq{eq0}による意思決定は，以下の\eq{deci}の符号が正の場合にクレームと判定することになる．
しかし，本研究では，\eq{deci}に意思決定の閾値[MATH]を加えた次の条件式を新たに導入し，この条件式が成立する場合にクレームと判定し，成立しない場合は非クレームと判定することとした．
\eq{deci2}の左辺は，クレームと判定する際の確信度を示していると考えることができ，閾値[MATH]はこの確信度に応じて出力を制御する役割りを持つ．
閾値を[MATH]と設定すると，これは\eq{deci}を用いた通常の意思決定と同じ動作となる．
閾値を[MATH]から大きくすると，より確信度が高い場合のみクレームと判定することになる．
実験では，閾値[MATH]を増減させ，以下の式で計算される適合率および再現率，あるいはその要約である11点平均適合率[CITE]を求め，検出精度を評価した．
11点平均適合率とは再現率が[MATH]となる11点における適合率の平均値である．
適合率&=\frac{正しくクレーム文として検出できた数}{クレーム文として出力された数}
[1ex]再現率&=\frac{正しくクレーム文として検出できた数}{クレーム文の数}
データにおけるクレーム文と非クレーム文の割合等に応じて，検出性能に対して最適な[MATH]を自動推定することも考えられるが，これについては今後の課題である．
実験結果を\fig{model_length}に示す．
このグラフは，4つの各検出モデルについて，考慮する周辺文脈の窓枠長を変化させながら性能変化をプロットしたものである．
文脈長[MATH]の場合は，どのモデルも同じになるため，グラフ上では1点に集まっている．
NBモデルの結果（``◇''）を基準に考えると，核文と近接文を区別しないNB+ctx(all)では文脈長を[MATH]から[MATH]のどの文脈の長さに変更しても性能が向上しない一方で，核文と近接文の区別を考慮するNB+ctx(divide)とNB+BFctx(divide)は文脈長を[MATH]にすることで，一貫して性能が向上することがわかる．
このことから，文脈情報を適切にモデルに反映させるためには，単にモデルを拡張するだけでは効果がなく，データとモデルを上手く組合せて，核文と近接文を区別することが重要であることが確認できる．
性能の向上が見られたNB+ctx(divide)とNB+BFctx(divide)を比較すると，どちらも[MATH]の場合は文脈長の変化に対しては鈍感な傾向を示しているが，近接文の相対位置を考慮するNB+BFctx(divide)の方が総じて良い結果を示しており，本論文で述べた4つのクレーム文検出モデルの中では，NB+BFctx(divide)モデルが最良であることがわかる．
次に，教師データとして自動生成されたクレーム近接文に含まれる単語を確認したところ，\tab{context_word}のような単語がクレーム核文には現れず，クレーム近接文にのみ現れていた．
このような単語の情報は，周辺文脈の情報を取り込むことで初めて考慮できるようになった情報であり，定性的にもクレーム検出における周辺文脈情報の利用の有効性が確認できる．
また文脈情報を取り込むことで正しく分類ができるようになった事例を以下に示す．
下線の引かれた文が分類対象であり，左端の数字は分類対象文からの相対位置を示す．
【正しくクレーム文であると判定できた例】
【正しく非クレーム文であると判定できた例】
次に，誤りの傾向を分析したところ，以下のような事例について，判定誤りが多く見られた．
【誤ってクレーム文と判定する例】
不満の表明ではあるが，その対象・原因が宿泊者にある場合
【例】仕事で到着が遅くなり，ゆっくりできなかったのが残念でした．
クレームの対象となりやすい事物が文中に多く記述されている場合
【例】部屋はデスク，姿見，椅子，コンセントがあり，…従業員の対応もまずまずでした．
【誤って非クレーム文と判定する例】
記述の省略を伴う場合
【例】バイキングにステーキがあればなぁ．
外部的な知識を要する場合
【例】全体的な評価としてはEランクでした．
誤ってクレーム文と判定する事例のうち，A.のような事例に対応するには，意見の対象や原因を特定する等の詳細な自動解析の実現が望まれる．
手元のデータによると，B.に該当する上述の例のうち，下線部がクレーム対象となりやすい事物であった．
このような事例については，文中では名詞が多く現れることから，単語の品詞情報を考慮する等，単語や単語クラス毎にモデル内での扱いを変更することが考えられる．
また，誤って非クレーム文と判定する事例については，「Eランク」を否定極性の単語として扱うなど，ヒューリスティック規則によるチューニングは可能であるが，総体的には現在の技術では改善が困難な事例が多い印象である．
次に，提案手法と他手法との比較実験を行い，その結果から提案手法の有効性を検証する．
他手法としては，以下に示す3手法を検討した．
始めの2つは，従来から考えられるラベル付け方法に基づく手法であり，残りの1つは，教師あり学習を適用しない，辞書の情報に基づいたルールベースの手法である．
人手によって教師データを作成する手法（以下，人手ラベル）
教師データ用のレビュー集合から2,000件のレビューを無作為に抽出し，そこに含まれる全ての文に対して人手でクレーム／非クレームのラベル付けを行ったものを教師データとしてモデル学習に用いる．
この手法で得られるデータでは核文と近接文の区別がないため，学習には通常のナイーブベイズ・モデルを用いる．
提案手法と比べると，この手法では量は少量だが質の高い学習データが利用できる．
このデータ作成作業は，評価実験の正解データ作成と同一の作業となる．
ただし，このデータ作成には正解データの作成に従事した作業者のうちの1名によって執り行なった．
作業時間は約30時間であった．
文書ラベルを教師データ作成に用いる手法（以下，文書ラベル）
本実験で使用しているレビューデータには，本研究でいうクレームとほぼ同等の概念を示している「苦情」というラベルがレビュー単位に付与されている．
そこで，ここでは，文よりも粗い文書に対する教師情報を利用して，文単位の教師データを自動生成することを考える[CITE]．
具体的には，「苦情」ラベルが付与されたレビューに含まれている全ての文をクレーム文とみなし，逆に，「苦情」ラベルが付与されていないレビューに含まれている全ての文を非クレーム文とみなすことで教師データを自動生成し，モデル学習に用いる．
モデルは先と同様の理由で通常のナイーブベイズ・モデルを用いる．
提案手法と比べると，この手法では相対的に質は低いが，大量の学習データが利用できる．
辞書による手法
この手法は教師あり学習は行わず，辞書のエントリをルールとみなしたルールベース手法である．
評価用データに対して\sec{data_core}で述べた核文ラベル付け，および\sec{data_context}で述べた近接文ラベル付けの手続きを直接適用してクレーム文を検出する．
ただし，ここでの焦点はデータ生成時とは違って，クレーム文を検出できるか否かであるため，ラベル付けの結果，クレームとラベル付けされた文以外は全て非クレームであるとみなして評価した．
なお，辞書は\sec{setting}で述べた辞書を用いる．
実験結果を\fig{baseline}に示す．
また，\tab{data}に提案手法のラベル付けと人手ラベル，文書ラベルの各手法によるラベル付けの特徴をまとめる．
\fig{baseline}において，辞書による手法は，ナイーブベイズモデルを用いた分類時に導入した閾値のパラメータが存在しないため，11点平均適合率を計算できない．
そのため，ここでは再現率と適合率によって分類性能を評価している．
なお，図中の``提案ラベル''が提案手法の結果であり，さきほどの評価実験で最良であった拡張モデルNB+ctxBF(divide)で文脈長[MATH]の実験結果を掲載している．
また，``辞書（核）''は，辞書による手法のうち，核文ラベル付けのみを考慮した場合の結果であり，``辞書（核+近接）''が，核文ラベル付けと近接文ラベル付けの両方を考慮した場合の結果である．
\fig{baseline}から，比較したどの手法よりも提案手法が良い性能を示していることがわかる．
辞書による方法は，辞書に登録されている単語が含まれていない文に対しては適用できないため，再現率が低い．
近接文を考慮することである程度の再現率を確保することは可能であるが，当然ながらその代償として適合率が下がる結果となっている．
ここで，固有表現抽出課題がそうであるように，一般に，辞書に基づいた手法では再現率が低くなるがその一方で適合率が高くなる傾向がある．
近接文の情報を用いない``辞書（核）''の結果は特にその傾向を示している．
ただし，今回の実験結果では，再現率を固定させて適合率を見ると，ナイーブベイズ・モデルを用いた手法の方が適合率がより高い結果となっていた．
これは，本研究課題では，辞書に登録されている一部の単語の情報だけでは文全体のクラス（クレーム／非クレーム）が正しく決定できない場合があり，このような場合には，辞書による方法よりも文内の単語情報を総合的に考慮できるナイーブベイズ・モデルが適していたためと考えられる．
次に，文書ラベルを利用する方法は，文書内のすべての文を教師データとして利用できる．
そのため，提案手法と同程度かそれ以上の教師データが利用できるという特徴がある．
しかし，文書内には一般的にクレームと非クレームが混在することから，文書ラベルと整合していない信頼性の低いデータを多く含む結果となり，そのことが性能の低下に繋がっていると考えられる．
最後に，人手作成による方法は，もっとも質の高い教師データを準備することができるが，作成負荷の高さから，量を確保することが難しい．
今回は人手で2,000件（8,639文）のレビューから教師データを作成したが，提案手法を上回ることはなかった．
先でも述べたように，一般に，人手作成された教師データは質が高い反面，多くの量を準備することが困難である．
一方，提案手法のように自動生成された教師データは人手作成されたデータよりも質が落ちるが，ラベルのない生データを準備するだけで手軽に増量できる．
ここでは，人手によって教師データを作成する場合と第\sec{gen}の提案手法によって教師データを自動生成する場合のそれぞれについて，教師データの量と分類性能の関係を調査する．
なお，両者で教師データ以外の実験条件を合わせるために，この実験では，モデルには通常のナイーブベイズ・モデルを用いた．
実験結果を\fig{datasize}に示す．
横軸が学習データ量（対数スケール）であり，縦軸が11点平均適合率である．
どちらの実験結果についても，まず今回の実験において最大で利用可能なデータ量（人手ラベルの場合：レビュー2,000件，提案ラベルの場合：レビュー約347,000件）から性能測定を開始し，そこから一部の学習データを無作為に削除することで使用できる学習データ量がより少ない環境を設定して，これを繰り返しながらグラフをプロットした．
\fig{datasize}から，まず，どちらの手法においてもデータ量を増やすことで性能が向上することが確認できる．
データ量が同じ場合は，当然のことながら，人手による方法の方が良い性能となる．
しかし，提案手法によってデータ量を増加させることで，今回の場合は10,000件までデータ量を増やした時点で両者の性能が同等となり，さらにデータ量を増やすことで提案手法が人手による手法を上回ることができた．
この実験結果は，あくまでひとつのケース・スタディであり，具体的な数値自体に意味を求めることは困難であると考えられる．
しかし，この結果は，人手によって十分な教師データが作成できない状況においては，自動生成手法を適用することで得られる教師データの量的利点という恩恵を受けられることを示唆していると言える．
評価実験を通して，提案手法の有効性を検証する．
具体的には以下の3項目を検証する．
提案手法の比較：前節までで述べた4つのクレーム文検出モデルの中で，どのモデルが最良であるかを検証する．
他手法との比較：提案手法と他手法との比較実験を行い，その結果から提案手法の有効性を検証する．
学習データ量とクレーム文検出精度の関係について：提案したデータ生成手法は学習データを自動生成できるため，人手による生成に比べて遥かに多くの教師データを準備できる．
この利点を実験を通して検証する．
実験には，楽天データ公開において公開された楽天トラベルの施設データを利用した．
このデータは約35万件（平均4.5文／件）の宿泊施設に関するレビューから構成されており，ここから，無作為に選んだ1,000レビューに含まれる文を評価用データとして用い，残りを教師データ生成用に利用した．
評価用データには4,308文が含まれており，その内の24%にあたる1,030文がクレーム文であった．
つまり，4,308文からクレームを述べている1,030文を過不足なく検出することがここでの実験課題である．
評価用データの作成では，まず，レビュー文書中の各文が1行1文となるようにデータを整形し，それを作業者に提示した．
そして作業者は，与えられたデータの1文（1行）ごとにクレーム文か否かを判定していった．
なお，ある文の判定時には，同一レビュー内の他の全ての文が参照できる状態になっている．
2名の作業者によって上記の作業を独立に並行に行ったが，このうち1名の作業結果を評価用データとして採用した．
2名の作業者間の一致度を[MATH]係数の値によって評価したところ，[MATH]であった．
この結果は，作業者間の判断が十分に一致していたことを示している．
作業者間で判断が一致しなかった事例としては，文が長く，ひとつの文で複数の事柄が述べられている場合や，「長身で据え置きのものでは短くて…」のように，クレームの原因が宿泊施設側にあるとは必ずしも言えない場合が多かった．
教師データ生成時に必要となる評価表現辞書には，高村ら[CITE]の辞書作成手法に基いて作成された辞書を使用した．
ただし，高村らのオリジナルの辞書は自動構築されたもので，そのままでは誤りが含まれているため，以下の手続きによって誤り修正を施し，本実験で使用する辞書として採用した．
オリジナルの辞書には各登録語に対して肯定／否定の強さを示すと解釈できる[MATH]の範囲のスコアが付与されている．
このスコアは，値が大きいほど肯定，また，小さいほど否定をあらわし，0付近はどちらでもないことを示していると解釈できる．
そこでまず，このスコアの絶対値の大きいものから0.9付近までの単語を自動的に選択した．
そして，選択された各単語に対して人手による誤り修正を施し，結果として肯定表現760件，否定表現862件からなる辞書を作成し，本実験に用いた．
また，要求表現辞書として，「欲しい」，「ほしい」，「べし」からなる辞書を作成して実験に使用した．
周辺文脈の窓枠長[MATH]の指定は，データ作成時，モデル学習時，評価用データの分類時のすべての過程で同期させている．
また，各データの単語分割はMeCabによって行った．
また，計算の都合上，[MATH]の場合は[MATH]とした．
今回のように，分類すべきクラスがクレーム／非クレームという2クラスの分類問題の場合，\eq{eq0}による意思決定は，以下の\eq{deci}の符号が正の場合にクレームと判定することになる．
しかし，本研究では，\eq{deci}に意思決定の閾値[MATH]を加えた次の条件式を新たに導入し，この条件式が成立する場合にクレームと判定し，成立しない場合は非クレームと判定することとした．
\eq{deci2}の左辺は，クレームと判定する際の確信度を示していると考えることができ，閾値[MATH]はこの確信度に応じて出力を制御する役割りを持つ．
閾値を[MATH]と設定すると，これは\eq{deci}を用いた通常の意思決定と同じ動作となる．
閾値を[MATH]から大きくすると，より確信度が高い場合のみクレームと判定することになる．
実験では，閾値[MATH]を増減させ，以下の式で計算される適合率および再現率，あるいはその要約である11点平均適合率[CITE]を求め，検出精度を評価した．
11点平均適合率とは再現率が[MATH]となる11点における適合率の平均値である．
適合率&=\frac{正しくクレーム文として検出できた数}{クレーム文として出力された数}
[1ex]再現率&=\frac{正しくクレーム文として検出できた数}{クレーム文の数}
データにおけるクレーム文と非クレーム文の割合等に応じて，検出性能に対して最適な[MATH]を自動推定することも考えられるが，これについては今後の課題である．
実験結果を\fig{model_length}に示す．
このグラフは，4つの各検出モデルについて，考慮する周辺文脈の窓枠長を変化させながら性能変化をプロットしたものである．
文脈長[MATH]の場合は，どのモデルも同じになるため，グラフ上では1点に集まっている．
NBモデルの結果（``◇''）を基準に考えると，核文と近接文を区別しないNB+ctx(all)では文脈長を[MATH]から[MATH]のどの文脈の長さに変更しても性能が向上しない一方で，核文と近接文の区別を考慮するNB+ctx(divide)とNB+BFctx(divide)は文脈長を[MATH]にすることで，一貫して性能が向上することがわかる．
このことから，文脈情報を適切にモデルに反映させるためには，単にモデルを拡張するだけでは効果がなく，データとモデルを上手く組合せて，核文と近接文を区別することが重要であることが確認できる．
性能の向上が見られたNB+ctx(divide)とNB+BFctx(divide)を比較すると，どちらも[MATH]の場合は文脈長の変化に対しては鈍感な傾向を示しているが，近接文の相対位置を考慮するNB+BFctx(divide)の方が総じて良い結果を示しており，本論文で述べた4つのクレーム文検出モデルの中では，NB+BFctx(divide)モデルが最良であることがわかる．
次に，教師データとして自動生成されたクレーム近接文に含まれる単語を確認したところ，\tab{context_word}のような単語がクレーム核文には現れず，クレーム近接文にのみ現れていた．
このような単語の情報は，周辺文脈の情報を取り込むことで初めて考慮できるようになった情報であり，定性的にもクレーム検出における周辺文脈情報の利用の有効性が確認できる．
また文脈情報を取り込むことで正しく分類ができるようになった事例を以下に示す．
下線の引かれた文が分類対象であり，左端の数字は分類対象文からの相対位置を示す．
【正しくクレーム文であると判定できた例】
【正しく非クレーム文であると判定できた例】
次に，誤りの傾向を分析したところ，以下のような事例について，判定誤りが多く見られた．
【誤ってクレーム文と判定する例】
不満の表明ではあるが，その対象・原因が宿泊者にある場合
【例】仕事で到着が遅くなり，ゆっくりできなかったのが残念でした．
クレームの対象となりやすい事物が文中に多く記述されている場合
【例】部屋はデスク，姿見，椅子，コンセントがあり，…従業員の対応もまずまずでした．
【誤って非クレーム文と判定する例】
記述の省略を伴う場合
【例】バイキングにステーキがあればなぁ．
外部的な知識を要する場合
【例】全体的な評価としてはEランクでした．
誤ってクレーム文と判定する事例のうち，A.のような事例に対応するには，意見の対象や原因を特定する等の詳細な自動解析の実現が望まれる．
手元のデータによると，B.に該当する上述の例のうち，下線部がクレーム対象となりやすい事物であった．
このような事例については，文中では名詞が多く現れることから，単語の品詞情報を考慮する等，単語や単語クラス毎にモデル内での扱いを変更することが考えられる．
また，誤って非クレーム文と判定する事例については，「Eランク」を否定極性の単語として扱うなど，ヒューリスティック規則によるチューニングは可能であるが，総体的には現在の技術では改善が困難な事例が多い印象である．
次に，提案手法と他手法との比較実験を行い，その結果から提案手法の有効性を検証する．
他手法としては，以下に示す3手法を検討した．
始めの2つは，従来から考えられるラベル付け方法に基づく手法であり，残りの1つは，教師あり学習を適用しない，辞書の情報に基づいたルールベースの手法である．
人手によって教師データを作成する手法（以下，人手ラベル）
教師データ用のレビュー集合から2,000件のレビューを無作為に抽出し，そこに含まれる全ての文に対して人手でクレーム／非クレームのラベル付けを行ったものを教師データとしてモデル学習に用いる．
この手法で得られるデータでは核文と近接文の区別がないため，学習には通常のナイーブベイズ・モデルを用いる．
提案手法と比べると，この手法では量は少量だが質の高い学習データが利用できる．
このデータ作成作業は，評価実験の正解データ作成と同一の作業となる．
ただし，このデータ作成には正解データの作成に従事した作業者のうちの1名によって執り行なった．
作業時間は約30時間であった．
文書ラベルを教師データ作成に用いる手法（以下，文書ラベル）
本実験で使用しているレビューデータには，本研究でいうクレームとほぼ同等の概念を示している「苦情」というラベルがレビュー単位に付与されている．
そこで，ここでは，文よりも粗い文書に対する教師情報を利用して，文単位の教師データを自動生成することを考える[CITE]．
具体的には，「苦情」ラベルが付与されたレビューに含まれている全ての文をクレーム文とみなし，逆に，「苦情」ラベルが付与されていないレビューに含まれている全ての文を非クレーム文とみなすことで教師データを自動生成し，モデル学習に用いる．
モデルは先と同様の理由で通常のナイーブベイズ・モデルを用いる．
提案手法と比べると，この手法では相対的に質は低いが，大量の学習データが利用できる．
辞書による手法
この手法は教師あり学習は行わず，辞書のエントリをルールとみなしたルールベース手法である．
評価用データに対して\sec{data_core}で述べた核文ラベル付け，および\sec{data_context}で述べた近接文ラベル付けの手続きを直接適用してクレーム文を検出する．
ただし，ここでの焦点はデータ生成時とは違って，クレーム文を検出できるか否かであるため，ラベル付けの結果，クレームとラベル付けされた文以外は全て非クレームであるとみなして評価した．
なお，辞書は\sec{setting}で述べた辞書を用いる．
実験結果を\fig{baseline}に示す．
また，\tab{data}に提案手法のラベル付けと人手ラベル，文書ラベルの各手法によるラベル付けの特徴をまとめる．
\fig{baseline}において，辞書による手法は，ナイーブベイズモデルを用いた分類時に導入した閾値のパラメータが存在しないため，11点平均適合率を計算できない．
そのため，ここでは再現率と適合率によって分類性能を評価している．
なお，図中の``提案ラベル''が提案手法の結果であり，さきほどの評価実験で最良であった拡張モデルNB+ctxBF(divide)で文脈長[MATH]の実験結果を掲載している．
また，``辞書（核）''は，辞書による手法のうち，核文ラベル付けのみを考慮した場合の結果であり，``辞書（核+近接）''が，核文ラベル付けと近接文ラベル付けの両方を考慮した場合の結果である．
\fig{baseline}から，比較したどの手法よりも提案手法が良い性能を示していることがわかる．
辞書による方法は，辞書に登録されている単語が含まれていない文に対しては適用できないため，再現率が低い．
近接文を考慮することである程度の再現率を確保することは可能であるが，当然ながらその代償として適合率が下がる結果となっている．
ここで，固有表現抽出課題がそうであるように，一般に，辞書に基づいた手法では再現率が低くなるがその一方で適合率が高くなる傾向がある．
近接文の情報を用いない``辞書（核）''の結果は特にその傾向を示している．
ただし，今回の実験結果では，再現率を固定させて適合率を見ると，ナイーブベイズ・モデルを用いた手法の方が適合率がより高い結果となっていた．
これは，本研究課題では，辞書に登録されている一部の単語の情報だけでは文全体のクラス（クレーム／非クレーム）が正しく決定できない場合があり，このような場合には，辞書による方法よりも文内の単語情報を総合的に考慮できるナイーブベイズ・モデルが適していたためと考えられる．
次に，文書ラベルを利用する方法は，文書内のすべての文を教師データとして利用できる．
そのため，提案手法と同程度かそれ以上の教師データが利用できるという特徴がある．
しかし，文書内には一般的にクレームと非クレームが混在することから，文書ラベルと整合していない信頼性の低いデータを多く含む結果となり，そのことが性能の低下に繋がっていると考えられる．
最後に，人手作成による方法は，もっとも質の高い教師データを準備することができるが，作成負荷の高さから，量を確保することが難しい．
今回は人手で2,000件（8,639文）のレビューから教師データを作成したが，提案手法を上回ることはなかった．
先でも述べたように，一般に，人手作成された教師データは質が高い反面，多くの量を準備することが困難である．
一方，提案手法のように自動生成された教師データは人手作成されたデータよりも質が落ちるが，ラベルのない生データを準備するだけで手軽に増量できる．
ここでは，人手によって教師データを作成する場合と第\sec{gen}の提案手法によって教師データを自動生成する場合のそれぞれについて，教師データの量と分類性能の関係を調査する．
なお，両者で教師データ以外の実験条件を合わせるために，この実験では，モデルには通常のナイーブベイズ・モデルを用いた．
実験結果を\fig{datasize}に示す．
横軸が学習データ量（対数スケール）であり，縦軸が11点平均適合率である．
どちらの実験結果についても，まず今回の実験において最大で利用可能なデータ量（人手ラベルの場合：レビュー2,000件，提案ラベルの場合：レビュー約347,000件）から性能測定を開始し，そこから一部の学習データを無作為に削除することで使用できる学習データ量がより少ない環境を設定して，これを繰り返しながらグラフをプロットした．
\fig{datasize}から，まず，どちらの手法においてもデータ量を増やすことで性能が向上することが確認できる．
データ量が同じ場合は，当然のことながら，人手による方法の方が良い性能となる．
しかし，提案手法によってデータ量を増加させることで，今回の場合は10,000件までデータ量を増やした時点で両者の性能が同等となり，さらにデータ量を増やすことで提案手法が人手による手法を上回ることができた．
この実験結果は，あくまでひとつのケース・スタディであり，具体的な数値自体に意味を求めることは困難であると考えられる．
しかし，この結果は，人手によって十分な教師データが作成できない状況においては，自動生成手法を適用することで得られる教師データの量的利点という恩恵を受けられることを示唆していると言える．
