================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:0, score:530] 本論文では，レビュー文書からクレームが記述された文を自動検出する課題に対して，従来から問題となっていた人手負荷を極力軽減することを指向した次の手続きおよび拡張手法を提案する：(1)評価表現と文脈一貫性に基づく教師データ自動生成の手続き．
[i:2, score:504] 提案手法では，大量のレビュー生文書の集合と評価表現辞書が準備できれば，クレーム検出規則の作成・維持・管理，あるいは，検出規則を自動学習するために必要となる教師データの作成にかかる人手負荷は全くかからない利点をもつ．
[i:3, score:453] 評価実験を通して，提案手法によって検出対象文の文脈情報を適切に捉えることで，クレーム文の検出精度を向上させることができること，および，人手によって十分な教師データが作成できない状況においては，提案手法によって大量の教師データを自動生成することで，人手を介在させる場合と同等あるいはそれ以上のクレーム検出精度が達成できることを示した．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[i:11, score:394] 見落とし例2：特に，レビューサイトを通したコミュニケーションでは，ユーザは様々な意見をひとつのレビュー文書中に書き込むことが多く，その中に部分的にクレームが埋め込まれることがある（\fig{review}および\fig{review2}に例を示す．
[i:14, score:385] 本論文では，上記のうち，2つ目の見落とし問題に対処すべく，レビューからクレームを自動検出する手法について述べる．
[i:23, score:443] より具体的には，レビュー文書からクレーム文を自動検出する際の基本的な設定として，テキスト分類において標準的に利用されるナイーブベイズ・モデルを適用することを考え，この設定に対して，極力人手の負荷を軽減させるために，次の手続きおよび拡張手法を提案する．

================================================================
[section type  : proposed_method]
[section title : 教師データ自動生成]
================================================================
[i:35, score:0] 
-----------------------------------------------------
  [subsection title : 教師データ]
-----------------------------------------------------
  [i:lead, 68] まず，生成したい教師データについて整理する．
.....
  [i:37, score:318] 本研究では，クレーム文を検知するためにナイーブベイズ分類器[CITE]を構築し，文がクレームを表しているか，あるいはクレームを表していないかのどちらかに分類したい．
  [i:38, score:381] このような分類器の構築に必要となる教師データは，言うまでもなく，クレームを表している文（以下，クレーム文と呼ぶ）の集合と，クレームを表していない文（以下，非クレーム文）の集合となる．
  [i:39, score:411] 以下では，説明の便宜上，このデータ集合を得る手続きをラベル付けと呼び，【クレーム】および【非クレーム】というラベルによって，どちらの集合の要素となるかを区別することとする．
-----------------------------------------------------
  [subsection title : 核文ラベル付け]
-----------------------------------------------------
  [i:lead, 139] 核文ラベル付けは，評価表現の情報に基いて行う．
.....
  [i:51, score:459] 核文ラベル付けステップでは，評価表現辞書に否定極性として登録されている評価表現に着目し，このような評価表現を含む文はクレームを表しやすいと仮定する．
  [i:56, score:461] もし，ある文が肯定極性をもつ評価表現を含み，かつ「ない」や「にくい」などの否定辞が評価表現の3単語以内に後続していた場合もクレーム核文としてラベル付けする．
  [i:59, score:478] また，評価表現の否定極性と肯定極性を読み替えて上記と同様の手続きを行った場合に得られる文を非クレーム核文と呼び，クレーム核文と同じようにラベル付けしておく．
-----------------------------------------------------
  [subsection title : 近接文ラベル付け]
-----------------------------------------------------
  [i:lead, 66] 那須川ら[CITE]は，彼らの論文の中で，評価表現の（文をまたいだ）周辺文脈には以下のような傾向があると述べており，これを評価表現の文脈一貫性と呼んだ．
.....
  [i:70, score:480] 先の核文ラベル付けの際に考慮した評価表現（あるいは要求表現）を含む文の周辺文脈について，「評価表現（要求表現）の存在に基づいて（非）クレーム文として選ばれた文の前後文脈に位置する文は，やはり（非）クレーム文である」という仮定をおき，この仮定に従って，核文の周辺文脈に対してラベル付けを行う．
  [i:79, score:537] 図の例では，対象となるレビューは8つの文から構成されており，核文ラベル付けによって文[MATH]が非クレーム核文，文[MATH]がクレーム核文とラベル付けされた状態であり，この状態から近接文ラベル付けが開始される．
  [i:91, score:488] 以降，クレーム近接文と非クレーム近接文をあわせた文の集合を[MATH]であらわす，また，必要に応じて，核文の前方文脈から得られた近接文[MATH]と，後方文脈から得られた近接文[MATH]を区別する([MATH])．

================================================================
[section type  : proposed_method]
[section title : ナイーブベイズ・モデルの拡張]
================================================================
[i:92, score:0] 
-----------------------------------------------------
  [subsection title : ナイーブベイズ・モデル (Na\"{i]
-----------------------------------------------------
  [i:lead, 129] 前節で述べた手法によって自動生成された教師データは，人手によって作成された教師データと比べて質が劣化せざるを得ず，標準的な分類モデルをそのまま適用するだけでは期待した精度は得られない．
.....
  [i:93, score:129] 前節で述べた手法によって自動生成された教師データは，人手によって作成された教師データと比べて質が劣化せざるを得ず，標準的な分類モデルをそのまま適用するだけでは期待した精度は得られない．
  [i:105, score:152] q_{w,c}&= \fracn_{w,c}(\mathcal{D})+1\sum_{w}^{}n_{w,c}(\mathcal{D})+|\mathcal{V}|
  [i:110, score:218] また，教師データの利用にあたっても，当然のことながら，核文であるか近接文であるかといった区別はなく，両タイプの文が同等にモデルの構築に利用される．
-----------------------------------------------------
  [subsection title : モデル拡張]
-----------------------------------------------------
  [i:lead, 209] 前節で述べた教師データ生成過程から得られるデータには，核文および近接文という2種類の文が存在する．
.....
  [i:127, score:212] 基本的には，前節で得られる教師データのうち，核文データを[MATH]に割り当て，近接文データを[MATH]に割り当てるが，正確な記述は後述の\sec{wariate}で与える．
  [i:137, score:210] q^{Bctx}_{w,c} & = \fracn_{w,c}(\mathcal{D}^{B}_{ctx})+1\sum_{w}n_{w,c}(\mathcal{D}^{B}_{ctx})+|\mathcal{V}_{Bctx}|
  [i:138, score:211] q^{Fctx}_{w,c} & = \fracn_{w,c}(\mathcal{D}^{F}_{ctx})+1\sum_{w}n_{w,c}(\mathcal{D}^{F}_{ctx})+|\mathcal{V}_{Fctx}|
-----------------------------------------------------
  [subsection title : データ割当規則]
-----------------------------------------------------
  [i:lead, 70] ここでは，さきほどの説明で保留していた，パラメータ推定の際に必要となる教師データの与え方について述べる．
.....
  [i:142, score:389] [MATH]：クレーム核文と非クレーム核文をあわせた文の集合
  [i:143, score:378] [MATH]：クレーム近接文と非クレーム近接文をあわせた文の集合
  [i:175, score:425] ここまでの議論を整理すると，前節の手法で自動生成された教師データを利用するという前提のもとで，通常のナイーブベイズ・モデルも含めて，4つのクレーム文検出モデルが与えられたことになる．

================================================================
[section type  : experiment_result]
[section title : 評価実験]
================================================================
[i:177, score:0] 
-----------------------------------------------------
  [subsection title : 検証項目]
-----------------------------------------------------
  [i:lead, 20] 評価実験を通して，提案手法の有効性を検証する．
.....
  [i:178, score:20] 評価実験を通して，提案手法の有効性を検証する．
  [i:180, score:348] 提案手法の比較：前節までで述べた4つのクレーム文検出モデルの中で，どのモデルが最良であるかを検証する．
  [i:182, score:422] 学習データ量とクレーム文検出精度の関係について：提案したデータ生成手法は学習データを自動生成できるため，人手による生成に比べて遥かに多くの教師データを準備できる．
-----------------------------------------------------
  [subsection title : 実験の設定]
-----------------------------------------------------
  [i:lead, 26] 実験には，楽天データ公開において公開された楽天トラベルの施設データを利用した．
.....
  [i:205, score:346] 今回のように，分類すべきクラスがクレーム／非クレームという2クラスの分類問題の場合，\eq{eq0}による意思決定は，以下の\eq{deci}の符号が正の場合にクレームと判定することになる．
  [i:206, score:349] しかし，本研究では，\eq{deci}に意思決定の閾値[MATH]を加えた次の条件式を新たに導入し，この条件式が成立する場合にクレームと判定し，成立しない場合は非クレームと判定することとした．
  [i:214, score:365] データにおけるクレーム文と非クレーム文の割合等に応じて，検出性能に対して最適な[MATH]を自動推定することも考えられるが，これについては今後の課題である．
-----------------------------------------------------
  [subsection title : 提案手法の比較]
-----------------------------------------------------
  [i:lead, 8] 実験結果を\fig{model_length}に示す．
.....
  [i:220, score:559] 性能の向上が見られたNB+ctx(divide)とNB+BFctx(divide)を比較すると，どちらも[MATH]の場合は文脈長の変化に対しては鈍感な傾向を示しているが，近接文の相対位置を考慮するNB+BFctx(divide)の方が総じて良い結果を示しており，本論文で述べた4つのクレーム文検出モデルの中では，NB+BFctx(divide)モデルが最良であることがわかる．
  [i:221, score:515] 次に，教師データとして自動生成されたクレーム近接文に含まれる単語を確認したところ，\tab{context_word}のような単語がクレーム核文には現れず，クレーム近接文にのみ現れていた．
  [i:241, score:355] また，誤って非クレーム文と判定する事例については，「Eランク」を否定極性の単語として扱うなど，ヒューリスティック規則によるチューニングは可能であるが，総体的には現在の技術では改善が困難な事例が多い印象である．
-----------------------------------------------------
  [subsection title : 他手法との比較]
-----------------------------------------------------
  [i:lead, 15] 次に，提案手法と他手法との比較実験を行い，その結果から提案手法の有効性を検証する．
.....
  [i:245, score:528] 人手によって教師データを作成する手法（以下，人手ラベル）教師データ用のレビュー集合から2,000件のレビューを無作為に抽出し，そこに含まれる全ての文に対して人手でクレーム／非クレームのラベル付けを行ったものを教師データとしてモデル学習に用いる．
  [i:253, score:471] 具体的には，「苦情」ラベルが付与されたレビューに含まれている全ての文をクレーム文とみなし，逆に，「苦情」ラベルが付与されていないレビューに含まれている全ての文を非クレーム文とみなすことで教師データを自動生成し，モデル学習に用いる．
  [i:257, score:546] 評価用データに対して\sec{data_core}で述べた核文ラベル付け，および\sec{data_context}で述べた近接文ラベル付けの手続きを直接適用してクレーム文を検出する．
-----------------------------------------------------
  [subsection title : 学習データとクレーム文検出精度の関係について]
-----------------------------------------------------
  [i:lead, 84] 先でも述べたように，一般に，人手作成された教師データは質が高い反面，多くの量を準備することが困難である．
.....
  [i:279, score:128] 一方，提案手法のように自動生成された教師データは人手作成されたデータよりも質が落ちるが，ラベルのない生データを準備するだけで手軽に増量できる．
  [i:280, score:116] ここでは，人手によって教師データを作成する場合と第\sec{gen}の提案手法によって教師データを自動生成する場合のそれぞれについて，教師データの量と分類性能の関係を調査する．
  [i:284, score:126] どちらの実験結果についても，まず今回の実験において最大で利用可能なデータ量（人手ラベルの場合：レビュー2,000件，提案ラベルの場合：レビュー約347,000件）から性能測定を開始し，そこから一部の学習データを無作為に削除することで使用できる学習データ量がより少ない環境を設定して，これを繰り返しながらグラフをプロットした．

================================================================
[section type  : related_study]
[section title : 関連研究]
================================================================
[i:291, score:371] しかし，本論文では，応用面を重視した際，主に製品やサービスを提供する企業にとっては意見の好不評という側面だけでは十分でないことから，クレームという好不評とは異なる観点を導入し，意見を含むテキストからクレームという特定の意見を検出する手法について述べた．
[i:295, score:358] 彼らの抽出対象である評判，要望の中に本研究におけるクレームも含まれていると考えられるが，彼らの手法は，機械翻訳機構が内部的に備える各種の言語知識のもとに成立しており，運用には人手による多大な管理負荷を要すると考えられる．
[i:300, score:378] 本研究では，レビュー中の各文をクレーム／非クレームに分類する課題に対して，ナイーブベイズ・モデルを採用し，データ特性に合わせて，その拡張を行った．

================================================================
[section type  : conclusion]
[section title : おわりに]
================================================================
[i:306, score:420] 本論文では，レビュー文書からクレームが記述された文を自動検出する手法として，極力人手の負荷を軽減することを指向した次の2つの手法を提案した．
[i:309, score:366] そして，評価実験を通して，これらの提案を組合せ，検出対象となる文の周辺文脈の情報を適切に捉えることで，クレーム文の検出精度を向上させることができることを示した．
[i:324, score:357] 見逃し状況について：クレームを見逃す状況として，本論文では，レビュー文書内に部分的に現れるクレームの見逃しについて扱った．

