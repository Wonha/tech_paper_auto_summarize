評価

単語境界確率の推定方法の評価として，言語モデルの適応の実験を行なった．まず，適応対象
文野の大きな生コーパスに既存手法と提案手法のそれぞれで単語境界確率を付与した．次に，
その結果得られる確率的単語分割コーパスから単語2-gramモデルを推定し，これを一般分野の
単語分割済みコーパスから推定された単語2-gramモデルと補間した．最後に，適応分野のテス
トコーパスに対して，予測力と仮名漢字変換\cite{無限語彙の仮名漢字変換}の精度の評価を行
なった．後者は，理想的な音響モデルを用いた場合の音声認識と考えることも可能である．こ
の節では，実験の結果を提示し，評価を行なう．



\subsection{実験の条件}

実験に用いたコーパスは，「現代日本語書き言葉均衡コーパス」モニター公開データ（2008年
度版）中の人手による単語分割の修正がなされている文（一般コーパス）と医療文書からなる適
応対象のコーパスである．一般コーパスの各文は正しく単語に分割され，各単語に入力記号列
（読み）が付与されている．これを10個に分割し，この内の9個を学習コーパスとし，残りの1個
をテストコーパスとした（\tabref{table:corpus}参照）．自動単語分割器や単語境界確率の推定
のための最大エントロピーモデルはこの学習コーパスから構築される．一方，適応対象のコー
パスは大量にあるが，単語境界情報を持たない．この内の7,000文に入力記号列（読み）を付与し
テストコーパスとし，残りを確率的単語分割コーパスとして言語モデルの学習に用いた
（\tabref{table:raw-corpus}参照）．テストコーパスの内の1,000文には，単語境界情報も付与
し，言語モデルの予測力の評価に用いた．



\subsection{評価基準}

確率的言語モデルの予測力の評価に用いた基準は，テストコーパスにおける単語あたりのパー
プレキシティである．まず，テストコーパス$C_{t}$に対して未知語の予測も含む文字単位のエ
ントロピー$H$を以下の式で計算する\cite{日本語の情報量の上限の推定}．
\begin{displaymath}
  H = -\frac{1}{|C_{t}|}\log_{2} \prod_{\Bdma{w} \in C_{t}}M_{w,n}(\Bdma{w})
\end{displaymath}
ここで，$M_{w,n}(\Bdma{w})$は単語$n$-gramモデルによる単語列$\Bdma{w}$の生成確率を，
$|C_{t}|$はテストコーパス$C_{t}$の文字数を表す．次に，単語単位のパープレキシティを以
下の式で計算する．
\begin{displaymath}
  PP = 2^{H\times\overline{|\Bdma{w}|}}
\end{displaymath}
ここで$\overline{|\Bdma{w}|}$は平均単語長（文字数）である．これらの計算に際しては，単語
境界情報が付与された1,000文を用いた\footnote{本論文での言語モデルの予測力の評価は，文
字列の予測のみならず，人手で付与された単語境界の予測も含まれている．これは，言語モデ
ルの応用を考慮してのことである．純粋に予測力が高いモデルが必要な場合は，既存の単語単
位を用いず，文字単位でモデル化する方がよいと考えられる
\cite{予測単位の変更によるn-gramモデルの改善,ベイズ階層言語モデルによる教師なし形態素解析}．}．


\begin{table}[t]
\begin{minipage}[t]{0.47\textwidth}
\caption{一般コーパス（単語分割済み）}
\label{table:corpus}
\input{03table01.txt}
\end{minipage}
\hfill
  \begin{minipage}[t]{0.47\textwidth}
  \caption{適応対象コーパス（単語境界情報なし）}
\input{03table02.txt}
  \label{table:raw-corpus}
  \end{minipage}
\end{table}

仮名漢字変換の評価基準は，文字誤り率である．文字誤り率は$\mbox{CER} = 1-N_{LCS}/
N_{COR}$と定義される．ここで，$N_{COR}$は正解に含まれる文字数であり，$N_{LCS}$は各文
を一括変換することで得られる最尤解と正解との最長共通部分列(LCS; Longest Common
Subsequence) \cite{文字列中のパターン照合のためのアルゴリズム}の文字数である．



\subsection{単語境界確率の推定方法の評価}

単語境界確率の推定方法の差異を調べるために，以下の2つの確率的単語分割コーパスを作成し
それらから推定された単語2-gramモデルの能力を調べた．
\begin{itemize}
\item[\bf BL:] \ 従来手法 \\ 各単語境界確率は，単語2-gramモデルに基づく自動単語分割器
  の判断に応じて$\alpha$又は$1-\alpha$とする．ここで，$\alpha = 67372/68039$は一般分
  野のテストコーパスにおける単語境界推定精度である（\subref{subsection:EM}参照）．

\item[\bf ME:] \ 提案手法 \\ 各単語境界確率は，最大エントロピーモデルを用いて文脈に応
じて推定される（\subref{subsection:ME}参照）．
\end{itemize}

適応対象分野のテストコーパスにおける予測力と文字誤り率を\tabref{table:result1}に示す．
この結果から，本論文で提案する最大エントロピー法による単語境界確率の推定方法により約
11\%のパープレキシティの削減が実現されている．この結果から，最大エントロピー法により
推定された単語境界確率を持つ確率的単語分割コーパスを用いることで適応対象分野における
単語2-gram確率がより正確に推定されていることがわかる．応用の仮名漢字変換においても，
文字正解率の比較から，提案手法により，従来手法の文字誤りの約3.1\%が削減さた．検定の結
果，有意水準5\%で有意差があるとの結果であった．この点からも言語モデルが改善されている
ことが確認される．従来手法の文字正解率は97.51\%と高いので，提案手法により実現された誤
りの削減は十分有意義であろう．



\begin{table}[t]
  \caption{単語境界確率の推定方法と言語モデルの能力の関係}
\input{03table03.txt}
  \label{table:result1}
\end{table}
\begin{table}[t]
  \caption{1/1のサイズの疑似確率的単語分割コーパスから推定された言語モデルの能力}
\input{03table04.txt}
  \label{table:result2}
\end{table}



\subsection{疑似確率的単語分割コーパスの評価}

本論文のもう一つの論点は，単語分割済みコーパスによる確率的単語分割コーパスの近似であ
る．この評価として，3種類の大きさ(1/1, 1/2, 1/4)の適応分野の疑似確率的単語分割コーパ
スから推定した言語モデルのテストコーパスに対するパープレキシティと文字正解率を複数の
倍率($N = 1,2,4,\cdots, 256$)に対して計算した．\tabref{table:result2}〜
\tabref{table:result4}はその結果である．まず，自動分割の結果を決定的単語分割コーパス
として用いる場合についてである．これと，確率的単語分割コーパスとして用いる場合との比
較では，文献\cite{確率的単語分割コーパスからの単語N-gram確率の計算}の報告と同じように
確率的単語分割により予測力が向上し，文献\cite{無限語彙の仮名漢字変換}の報告と同じよう
に仮名漢字変換の文字正解率も向上している．さらに，本論文で提案する倍率が1の疑似確率的
単語分割は，決定的単語分割に対して，予測力と文字正解率の双方において優れていることが
分る．倍率が1の疑似確率的単語分割と決定的単語分割の唯一の違いは，自動単語分割の際に単
語境界確率を0.5と比較するか，0から1の乱数と比較するかであり，モデル構築の計算コストは
ほとんど同じである．にもかかわらず，予測力と文字正解率の双方が向上している点は注目に
値するであろう．

次に，確率的単語分割と疑似確率的単語分割の比較について述べる．倍率が1の場合は，予測力
や文字正解率は，確率的単語分割コーパスから推定された言語モデルに対して少し低く，倍率
を上げることによりこれらは確率的単語分割コーパスによる結果に近づいていくことがわかる．
これは，疑似確率的単語分割がモンテカルロ法による数値演算の一種になっていることを考え
れば当然の結果である．このことから，ある程度の倍率の疑似確率的単語分割コーパスは，確
率的単語分割コーパスのよい近似となっているといえる．適応分野のコーパスの大きさに係わ
らず，倍率が256の場合の疑似確率的単語分割による結果は，確率的単語分割の結果とほぼ同じ
といえる．

\begin{table}[t]
  \caption{1/2のサイズの疑似確率的単語分割コーパスから推定された言語モデルの能力}
\input{03table05.txt}
  \label{table:result3}
\end{table}
\begin{table}[t]
  \caption{1/4のサイズの疑似確率的単語分割コーパスから推定された言語モデルの能力}
\input{03table06.txt}
  \label{table:result4}
\end{table}



最後に，確率的単語分割と疑似確率的単語分割の計算コストの比較について述べる．確率的単
語分割の語彙サイズは，適応対象の学習コーパスにおける期待頻度が0より大きい16文字以下の
部分文字列と一般コーパスの語彙の合計9,383,985語であった．この語彙に対する単語2-gram頻
度をハッシュ(Berkeley DB 4.6.21)を用いてファイルに出力すると10.0~GBとなった．これをRAM
ディスク上で計算するのに61147.45秒（約17時間）を要した\footnote{この計算に用いた計算機
の中央演算装置はIntel Core 2 Duo 3.91~GHzであり，主記憶は4~GBである．}．同じ計算機で，
16倍の疑似確率的単語分割コーパスから単語2-gram頻度をRAM ディスク上で計算すると，語彙サイ
ズが46,777語であり，単語2-gram頻度のファイルサイズは9.98~MBであり，計算時間は1009.95秒
（約17分）と約61分の1となった．疑似確率的単語分割コーパスを用いた場合には，倍率が256の
場合でも20.2~MBと，ファイルサイズが大きくないので，現在の多くの計算機で主記憶上で計算
が可能である（主記憶上での計算時間は303.29秒）．これに対して，確率的単語分割コーパスか
らの推定では，一部の計算機においてのみ主記憶上での計算が可能である．さらに，実験で用
いた適応対象の分野のコーパスは44,915文と決して大きくはなく，適応分野によっては1桁か2
桁ほど大きい学習コーパスが利用できることも十分考えられる．この場合には，確率的単語分
割では2次記憶（RAMディスクかハードディスク）上での計算が避けられず，モデル作成にかかる
計算時間の違いは非常に大きくなる．したがって，本論文で提案する疑似確率的単語分割は，
この点から有用であると考えられる．

疑似確率的単語分割において，どの程度の倍率がよいかは要求する精度と利用可能な計算機資
源との兼ね合いである．例えば倍率が16の場合は，単語に分割された718,640文から言語モデル
を推定することになる．モデル構築に要する計算時間は，決定的単語分割の場合の16倍程度で
あり，現在の計算機はこの大きさのコーパスを処理する能力が十分ある．したがって，疑似確
率的単語分割により，単語3-gramモデルや可変長記憶マルコフモデル，あるいは言語モデルの
ための単語クラスタリングなどさらなる言語モデルの改善を容易に試みることが可能となる．



