確率的単語分割コーパスからの言語モデルの推定
\label{section:raw}

確率的言語モデルを新たな分野に適応する一般的な方法は，適応分野のコーパスを用意し，そ
れを自動的に単語分割し，単語の頻度統計を計算することである．この方法では，単語分割誤
りにより適応分野のコーパスにのみ出現する単語が適切に扱えないという問題が起こる．この
解決方法として，適応分野のコーパスを確率的単語分割コーパスとして用いることが提案され
ている\cite{確率的単語分割コーパスからの単語N-gram確率の計算}．この節では，確率的単語
分割コーパスからの確率的言語モデルの推定方法について概説する．



\subsection{確率的単語分割コーパス}

\label{subsection:EM}

確率的単語分割コーパスは，生コーパス$C_{r}$（以下，文字列$\Bdma{x}_1^{n_{r}}$として参
照）とその連続する各2文字$x_{i},x_{i+1}$の間に単語境界が存在する確率$P_{i}$の組として
定義される．最初の文字の前と最後の文字の後には単語境界が存在するとみなせるので，$i =
0,\; i = n_{r}$の時は便宜的に$P_{i} = 1$とされる．確率変数$X_{i}$を
\[
X_{i} = \left\{
  \begin{array}{rl}
    1 & \mbox{$x_{i},x_{i+1}$の間に単語境界が存在する場合} \\
    0 & \mbox{$x_{i},x_{i+1}$が同じ単語に属する場合}
  \end{array} \right.
\]
とし($P(X_{i}=1) = P_{i},\;P(X_{i}=0) = 1-P_{i}$)，各$X_0,X_1,$ $\dots,X_{n_r}$は独立
であることが仮定される．

文献\cite{確率的単語分割コーパスからの単語N-gram確率の計算}の実験で用いられている単語
境界確率の推定方法は次の通りである．まず，単語に分割されたコーパスに対して自動単語分
割システムの境界推定精度$\alpha$を計算しておく．次に，適応分野のコーパスを自動単語分
割し，その出力において単語境界であると判定された点では$P_{i} = \alpha$ とし，単語境界
でないと判定された点では$P_{i} = 1-\alpha$とする．後述する実験の従来手法としてこの方
法を採用した．




\subsection{単語$n$-gram頻度}

\label{subsection:EF}

確率的単語分割コーパスに対して単語$n$-gram頻度が以下のように定義される．
\begin{description}
\item[単語0-gram頻度] 確率的単語分割コーパスの期待単語数として以下のように定義される．
  \begin{equation}
    \label{equation:0-gram}
    f(\cdot) = 1 + \sum_{i=1}^{n_{r}-1} P_{i}
  \end{equation}
\item[単語1-gram頻度] 確率的単語分割コーパスに出現する文字列$\Bdma{x}_{i+1}^{k}$が$l
  = k-i$文字からなる単語$w = \Bdma{x'}_{1}^{l}$である必要十分条件は以下の4つである．
  \begin{enumerate}
  \item 文字列$\Bdma{x}_{i+1}^{k}$が単語$w$に等しい．
  \item 文字$x_{i+1}$の直前に単語境界がある．
  \item 単語境界が文字列中にない．
  \item 文字$x_{k}$の直後に単語境界がある．
  \end{enumerate}
  したがって，確率的単語分割コーパスの単語1-gram頻度$f_{r}$は，単語$w$の表記の全ての
  出現$O_{1} = \{(i,k)\,|$ $\Bdma{x}_{i+1}^{k} = w\}$に対する期待頻度の和として以下の
  ように定義される．
  \begin{equation}
    \label{eqnarray:1-gram}
    f_{r}(w)
    = \sum_{(i,k) \in O_{1}}P_{i}\left[\prod_{j=i+1}^{k-1}(1-P_{j})\right]P_{k}
  \end{equation}
\item[単語$n$-gram頻度($\Bdma{n\geq2}$)] $L$文字からなる単語列$\Bdma{w}_{1}^{n} =
  \Bdma{x'}_{1}^{L}$の確率的単語分割コーパス$\Bdma{x}_{1}^{n_{r}}$における頻度，すな
  わち単語$n$-gram頻度について考える．このような単語列に相当する文字列が確率的単語分
  割コーパスの$(i+1)$文字目から始まり$k = i+L$文字目で終る文字列と等しく
  ($\Bdma{x}_{i+1}^{k} = \Bdma{x'}_{1}^{L}$)，単語列に含まれる各単語$w_{m}$に相当する
  文字列が確率的単語分割コーパスの$b_{m}$文字目から始まり$e_{m}$ 文字目で終る文字列と
  等しい($\Bdma{x} _{b_{m}}^{e_{m}} = w_{m},\; 1 \leq \forall m \leq n$;
  $e_{m}+1=b_{m+1},\; 1 \leq \forall m \leq n-1$; $b_{1} = i+1$; $e_{n} = k$)状況を考
  える（\figref{figure:SSC}参照）．確率的単語分割コーパスに出現する文字列
  $\Bdma{x}_{i+1}^{k}$が単語列$\Bdma{w}_{1}^{n}=\Bdma{x'}_{1}^{L}$ である必要十分条件
  は以下の4つである．
  \begin{enumerate}
  \item 文字列$\Bdma{x}_{i+1}^{k}$が単語列$\Bdma{w}_{1}^{n}$に等しい．
  \item 文字$x_{i+1}$の直前に単語境界がある．
  \item 単語境界が各単語に対応する文字列中にない．
  \item 単語境界が各単語に対応する文字列の後にある．
  \end{enumerate}

\begin{figure}[t]
  \begin{center}
\includegraphics{16-4ia6f1.eps}
  \end{center}
  \caption{確率的単語分割コーパスにおける単語$n$-gram頻度}
  \label{figure:SSC}
\end{figure}


  確率的単語分割コーパスにおける単語$n$-gram頻度は以下のように定義される．
  \begin{equation}
    \label{eqnarray:n-gram}
    f_{r}(\Bdma{w}_{1}^{n})
    = 
    \sum_{(i,e_{1}^{n}) \in O_{n}} P_{i} \left[
    \prod_{m=1}^{n} \left\{
    \prod_{j=b_{m}}^{e_{m}-1} (1-P_{j}) \right\}
    P_{e_{m}} \right]
  \end{equation}
  ここで
  \begin{align*}
    e_{1}^{n}
    & =  (e_{1},e_{2},\cdots,e_{n}) \\
    O_{n}     
    & =  \{(i,e_{1}^{n}) | \Bdma{x}_{b_{m}}^{e_{m}} = w_{m}, 1 \leq m \leq n \}
  \end{align*}
  である．
\end{description}




\subsection{単語$n$-gram確率}

確率的単語分割コーパスにおける単語$n$-gram確率は，単語$n$-gram頻度の相対値として計算
される．

\begin{description}
\item[単語1-gram確率] 以下のように単語1-gram頻度を単語0-gram頻度で除することで計算さ
  れる．
  \begin{equation}
    \label{equation:1-gram}
    P_r(w) = \frac{f_r(w)}{f_r(\cdot)}
  \end{equation}
\item[単語$n$-gram確率($\Bdma{n\geq2}$)] 以下のように単語$n$-gram頻度を単語
  $(n-1)$-gram頻度で除することで計算される．
  \begin{equation}
    \label{equation:n-gram}
    P_r(w_{n}|\Bdma{w}_{1}^{n-1})
    = \frac{f_r(\Bdma{w}_{1}^{n})}{f_r(\Bdma{w}_{1}^{n-1})}
  \end{equation}
\end{description}



\subsection{単語$n$-gram頻度の計算コスト}

ある単語列$\Bdma{w}_{1}^{n} = \Bdma{x'}_{1}^{L}$ ($n\geq1$)のある1箇所の出現位置に対
する期待頻度の計算に必要な演算は，\equref{eqnarray:1-gram}や\equref{eqnarray:n-gram}
から明かなように，$L-n$回の浮動小数点に対する減算と$L+1$回の浮動小数点に対する乗算で
ある．動的に単語$n$-gram確率を計算する方法では，この演算が文字列$\Bdma{x'}_{1}^{L}$の
出現回数だけ繰り返される．通常の決定的単語分割コーパスの場合には，単語列の出現回数が
そのまま頻度となるので，上述の浮動小数点に対する演算が全て付加的な計算コストであり，
言語モデルの応用の実行速度を大きく損ねる．あらかじめ単語$n$-gram確率を計算しておく場
合は，ある文（文字数$h$）に出現する全ての$n$個の連続する部分文字列に対して行う必要があ
る．上述の減算や乗算が重複して行われるのを避けるために，まず文の両端を除く全ての位置
に対して$1-P_{i}$を計算（$h-1$回の減算）し，さらにこれら$h-1$個の$1-P_{i}$のうちの任意
個の連続する位置に対する積$\prod_{j=b}^{e}(1-P_{j})$ ($b<e$)を計算（$\sum_{i=1}^{h-2}
=(h-1)(h-2)/2$回の乗算）しておく．ある単語$n$-gramの出現位置は，文に$n+1$個の単語境界
を置くことで決るので，$h$文字の文には重複を含め${}_{h-1}C_{n+1}$個の単語$n$-gramが含
まれる．このそれぞれの期待頻度は，左端の$P_{i}$に$n$個の$\prod_{j=e_{k-1}+1}^{e_{k}}
(1-P_{j})$と$n$個の$P_{e_{k}}$の積（$2n$回の乗算）として得られる．この場合に必要な計算
コストも，決定的単語分割コーパスの場合の単語数と同じ回数のインクリメントに比べて非常
に大きい\footnote{後述の実験での条件では，自動分割結果（決定的単語分割）からの頻度計算
におけるインクリメントは1,377,062回で，確率的単語分割に対する$n=2$の乗算回数の理論値
は20,181,679,570となる．浮動小数点数に対する乗算とインクリメントでは，計算のコストが
異なるが，回数を単純に比較しても実に14,656倍となる．実際の計算時間には，さらに，入力
文の読み込みや文字列（単語表記）から語彙番号への変換が含まれるので，この比率にはならな
い．}．

このように，確率的単語分割コーパスに対する単語$n$-gram頻度の計算のコストは，従来の決
定的単語分割コーパスに対する計算コストに比べて非常に大きくなる．文の長さの分布を無視
すれば，計算回数はコーパスの文数に対しては比例する．文毎に独立なので複数の計算機によ
る分散計算も可能であるが，ある程度の大きさのコーパスからモデルを作成する場合にはこの
計算コストは問題になる．また，単語クラスタリング
\cite{Class-Based.n-gram.Models.of.Natural.Language}や文脈に応じた参照履歴の伸長
\cite{The.Power.of.Amnesia:.Learning.Probabilistic.Automata.with.Variable.Memory.Length}
などの様々な言語モデルの改良においては，最適化の過程において言語モデルを何度も構築す
る．確率的単語分割コーパスにおける単語$n$-gram頻度の計算のコストによって，これらの改
良を試みることが困難になっている．



最大エントロピー法による単語境界確率の推定

この節では，最大エントロピー法による単語分割器を単語境界確率の推定に用いる方法につい
て述べる．



\subsection{単語境界確率の推定}

\label{subsection:ME}

日本語の単語分割の問題は，入力文の各文字間に単語境界が発生するか否かを予測する問題と
みなせる\cite{教師なし隠れマルコフモデルを利用した最大エントロピータグ付けモデル,Training.Conditional.Random.Fields.Using.Incomplete.Annotations}．つまり，文
$\Bdma{x} = \Conc{x}{m}$に対して，$x_{i}x_{i+1}$の間が単語境界であるか否かを表すタグ
$t_{i}$を付与する問題とみなす．付与するタグは，単語境界であることを表すタグ{\bf E}と，
非単語境界であることを表すタグ{\bf N}の2つのタグからなる．各文字間のタグがこのいずれ
かであるかは，単語境界が明示されたコーパスから学習された点推定の最大エントロピーモデ
ル(ME model; maximum entropy model)により推定する\footnote{文献
\cite{Training.Conditional.Random.Fields.Using.Incomplete.Annotations}のように
CRF (Conditional Random Fields) により推定することもできるが，計算コストと記憶領域が大
きくなる．これらの差は，スパースな部分的アノテーションコーパスからの学習において顕著
となる．つまり，CRFのように系列としてモデル化する方法では，アノテーションのない部分も
考慮する必要があるのに対して，点推定の最大エントロピーモデルでは，アノテーションのあ
る部分のみを考慮すればよい．このような考察から，本論文では計算コストの少ない最大エン
トロピーモデルを用いる．}．その結果，より高い確率を与えられたタグをその文字間のタグと
し，単語境界を決定する．すなわち，以下の式が示すように，最大エントロピーモデルにより，
単語境界と推定される確率が非単語境界と推定される確率より高い文字間を単語境界とする．
\[
  t_{i} = \left\{
    \begin{array}{ll}
      \mbox{\bf E}
      & \mbox{if} \; P_{ME}(t_{i}={\bf E}|\Bdma{x}) > P_{ME}(t_{i}={\bf N}|\Bdma{x})\\
      \mbox{\bf N}
      & \mbox{otherwise}
    \end{array}\right.
\]
これにより，入力文を単語に分割することができる．

本論文では，以下のように，タグ$t_{i}$の出現確率を確率的単語分割コーパスにおける単語境
界確率$P_{i}$として用いることを提案する．
\begin{displaymath}
  P_{i} = P_{ME}(t_{i}={\bf E}|\Bdma{x})
\end{displaymath}
これにより，注目する文字の周辺のさまざまな素性を参照し，単語境界確率を適切に推定する
ことが可能になる．



\subsection{参照する素性}

後述する実験においては，$x_{i}x_{i+1}$の間に注目する際の最大エントロピーモデルの素性
としては，$x_{i-1}^{i+2}$の範囲の文字$n$-gramおよび字種$n$-gram($n=1,2,3$)をすべて用
いた\footnote{字種は，漢字，ひらがな，カタカナ，アルファベット，数字，記号の6つとし
た．}．ただし，以下の点を考慮している．
\begin{itemize}

\item 素性として利用する$n$-gramは，先頭文字の字種がその前の文字の字種と同じか否か，
  および，末尾文字の字種がその次の文字の字種と同じか否かの情報を付加して参照する
  \footnote{パラメータ数の急激な増加を抑えつつ素性の情報量を増加させる．これにより，
  参照範囲を前後1文字拡張して$x_{i-2}^{i+3}$の範囲の$n$-gram($n=3,4,5$)を参照する．}．

\item 素性には注目する文字間の位置情報を付加する．

\end{itemize}
たとえば，文字列「文字列を単語に分割する」の「語」「に」の文字間の素性は，
\{
$-$単$+|$, $+$語$|-$, $-|$に$-$, $|-$分$+$, 
$-$単語$|-$, $+$語$|$に$-$, $-|$に分$+$, 
$-$単語$|$に$-$, $+$語$|$に分$+$, 
$-$K$+|$, $+$K$|-$, $-|$H$-$, $|-$K$+$, 
$-$KK$|-$, $+$K$|$H$-$, $-|$HK$+$, 
$-$KK$|$H$-$, $+$K$|$HK$+$, 
\}となる．「$|$」は注目する文字間を表す補助記号であり，「$+$」と「$-$」は前後の文字が
同じ字種である($+$)か否($-$)かを表す補助記号である．「H」と「K」は字種の平仮名と漢字
を表している．

なお，実験においては，パラメータ数を減らすために，学習データで2回以上出現する素性のみ
を用いた．また，最大エントロピーモデルのパラメータ推定には，GISアルゴリズム
\cite{Generalized.Iterative.Scaling.For.Log-Linear.Models}を使用した．



疑似確率的単語分割コーパス

確率的単語分割コーパスに対する単語$n$-gram頻度は，高いコストの計算を要する．また，確
率的単語分割コーパスは，頻度計算の対象となる単語や単語断片（候補）を多数含む．ある単語
$n$-gramの頻度の計算に際しては，その単語の文字列としてのすべての出現に対して，頻度の
インクリメントではなく，複数回の浮動小数点演算を実行しなければならない．この計算コス
トにより，より長い履歴を参照する単語$n$-gramモデルや単語クラスタリングなどの言語モデ
ルの改良が困難になっている．

上述の困難を回避する方法として，単語分割済みコーパスで確率的単語分割コーパスを近似す
る方法を提案する．具体的には，確率的単語分割コーパスに対して以下の処理を最初の文字か
ら最後の文字まで($1 \leq i \leq n_{r}$)行なう．
\begin{enumerate}

\item 文字$x_{i}$を出力する．

\item 0以上1未満の乱数$r_{i}$を発生させ$P_{i}$と比較する．$r_{i} < P_{i}$の場合には単
  語境界記号を出力し，そうでない場合には何も出力しない．

\end{enumerate}
これにより，確率的単語分割コーパスに近い単語分割済みコーパスを得ることができる．これ
を疑似確率的単語分割コーパスと呼ぶ．

上記の方法では，文字列としての出現頻度が低い単語$n$-gramの頻度が確率的単語分割コーパ
スと疑似確率的単語分割コーパスにおいて大きく異なる可能性がある．そもそも，出現頻度が
低い単語$n$-gramの場合，単語分割が正しいとしても，その統計的振る舞いを適切に捉えるの
は困難であるが，近似によって誤差が増大することは好ましくない．従って，この影響を軽減
するために，上記の手続きを$N$回行ない，その結果得られる$N$倍の単語分割済みコーパスを
単語$n$-gram頻度の計数の対象とすることとする．このときの$N$を本論文では倍率と呼ぶこと
とする．

疑似確率的単語分割コーパスは，一種のモンテカルロ法となっている．モンテカルロ法による
$d$次元の単位立方体上$[0,d]^{d}$上の定積分$I = \int_{[0,1]^{d}}f(x)dx$の数値計算法で
は，単位立方体$[0,d]^{d}$上の一様乱数$\Stri{x}{N}$を発生させて$I_{N} = \sum_{i=1}^{N}
f(x_{i})$とする．このとき，誤差$|I_{N}-I|$は次元$d$によらずに$1/\sqrt{N}$に比例する程
度の速さで減少することが知られている．疑似確率的単語分割コーパスにおける単語$n$-gram
頻度の計算はこの特殊な場合であり，$n$の値や文字数によらずに$1/\sqrt{FN}$に比例する程
度の速さで減少する．ここで$F$は単語$n$-gramの文字列としての頻度である．



