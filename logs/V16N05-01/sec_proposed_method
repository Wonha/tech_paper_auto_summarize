確率的言語モデルを新たな分野に適応する一般的な方法は，適応分野のコーパスを用意し，それを自動的に単語分割し，単語の頻度統計を計算することである．
この方法では，単語分割誤りにより適応分野のコーパスにのみ出現する単語が適切に扱えないという問題が起こる．
この解決方法として，適応分野のコーパスを確率的単語分割コーパスとして用いることが提案されている[CITE]．
この節では，確率的単語分割コーパスからの確率的言語モデルの推定方法について概説する．
確率的単語分割コーパスは，生コーパス[MATH]（以下，文字列[MATH]として参照）とその連続する各2文字[MATH]の間に単語境界が存在する確率[MATH]の組として定義される．
最初の文字の前と最後の文字の後には単語境界が存在するとみなせるので，[MATH]の時は便宜的に[MATH]とされる．
確率変数[MATH]を
とし([MATH])，各[MATH] [MATH]は独立であることが仮定される．
文献[CITE]の実験で用いられている単語境界確率の推定方法は次の通りである．
まず，単語に分割されたコーパスに対して自動単語分割システムの境界推定精度[MATH]を計算しておく．
次に，適応分野のコーパスを自動単語分割し，その出力において単語境界であると判定された点では[MATH]とし，単語境界でないと判定された点では[MATH]とする．
後述する実験の従来手法としてこの方法を採用した．
確率的単語分割コーパスに対して単語[MATH]-gram頻度が以下のように定義される．
確率的単語分割コーパスの期待単語数として以下のように定義される．
確率的単語分割コーパスに出現する文字列[MATH]が[MATH]文字からなる単語[MATH]である必要十分条件は以下の4つである．
文字列[MATH]が単語[MATH]に等しい．
文字[MATH]の直前に単語境界がある．
単語境界が文字列中にない．
文字[MATH]の直後に単語境界がある．
したがって，確率的単語分割コーパスの単語1-gram頻度[MATH]は，単語[MATH]の表記の全ての出現[MATH] [MATH]に対する期待頻度の和として以下のように定義される．
[MATH]文字からなる単語列[MATH]の確率的単語分割コーパス[MATH]における頻度，すなわち単語[MATH]-gram頻度について考える．
このような単語列に相当する文字列が確率的単語分割コーパスの[MATH]文字目から始まり[MATH]文字目で終る文字列と等しく([MATH])，単語列に含まれる各単語[MATH]に相当する文字列が確率的単語分割コーパスの[MATH]文字目から始まり[MATH]文字目で終る文字列と等しい([MATH]; [MATH]; [MATH]; [MATH])状況を考える（\figref{figure:SSC}参照）．
確率的単語分割コーパスに出現する文字列[MATH]が単語列[MATH]である必要十分条件は以下の4つである．
文字列[MATH]が単語列[MATH]に等しい．
文字[MATH]の直前に単語境界がある．
単語境界が各単語に対応する文字列中にない．
単語境界が各単語に対応する文字列の後にある．
確率的単語分割コーパスにおける単語[MATH]-gram頻度は以下のように定義される．
ここで
e_{1}^{n} & = (e_{1},e_{2},\cdots,e_{n})
O_{n} & = {(i,e_{1}^{n}) | \Bdma{x}_{b_{m}}^{e_{m}} = w_{m}, 1 \leq m \leq n }
である．
確率的単語分割コーパスにおける単語[MATH]-gram確率は，単語[MATH]-gram頻度の相対値として計算される．
以下のように単語1-gram頻度を単語0-gram頻度で除することで計算される．
以下のように単語[MATH]-gram頻度を単語[MATH]-gram頻度で除することで計算される．
ある単語列[MATH] ([MATH])のある1箇所の出現位置に対する期待頻度の計算に必要な演算は，\equref{eqnarray:1-gram}や\equref{eqnarray:n-gram}から明かなように，[MATH]回の浮動小数点に対する減算と[MATH]回の浮動小数点に対する乗算である．
動的に単語[MATH]-gram確率を計算する方法では，この演算が文字列[MATH]の出現回数だけ繰り返される．
通常の決定的単語分割コーパスの場合には，単語列の出現回数がそのまま頻度となるので，上述の浮動小数点に対する演算が全て付加的な計算コストであり，言語モデルの応用の実行速度を大きく損ねる．
あらかじめ単語[MATH]-gram確率を計算しておく場合は，ある文（文字数[MATH]）に出現する全ての[MATH]個の連続する部分文字列に対して行う必要がある．
上述の減算や乗算が重複して行われるのを避けるために，まず文の両端を除く全ての位置に対して[MATH]を計算（[MATH]回の減算）し，さらにこれら[MATH]個の[MATH]のうちの任意個の連続する位置に対する積[MATH] ([MATH])を計算（[MATH]回の乗算）しておく．
ある単語[MATH]-gramの出現位置は，文に[MATH]個の単語境界を置くことで決るので，[MATH]文字の文には重複を含め[MATH]個の単語[MATH]-gramが含まれる．
このそれぞれの期待頻度は，左端の[MATH]に[MATH]個の[MATH]と[MATH]個の[MATH]の積（[MATH]回の乗算）として得られる．
この場合に必要な計算コストも，決定的単語分割コーパスの場合の単語数と同じ回数のインクリメントに比べて非常に大きい．
このように，確率的単語分割コーパスに対する単語[MATH]-gram頻度の計算のコストは，従来の決定的単語分割コーパスに対する計算コストに比べて非常に大きくなる．
文の長さの分布を無視すれば，計算回数はコーパスの文数に対しては比例する．
文毎に独立なので複数の計算機による分散計算も可能であるが，ある程度の大きさのコーパスからモデルを作成する場合にはこの計算コストは問題になる．
また，単語クラスタリング[CITE]や文脈に応じた参照履歴の伸長[CITE]などの様々な言語モデルの改良においては，最適化の過程において言語モデルを何度も構築する．
確率的単語分割コーパスにおける単語[MATH]-gram頻度の計算のコストによって，これらの改良を試みることが困難になっている．
この節では，最大エントロピー法による単語分割器を単語境界確率の推定に用いる方法について述べる．
日本語の単語分割の問題は，入力文の各文字間に単語境界が発生するか否かを予測する問題とみなせる[CITE]．
つまり，文[MATH]に対して，[MATH]の間が単語境界であるか否かを表すタグ[MATH]を付与する問題とみなす．
付与するタグは，単語境界であることを表すタグEと，非単語境界であることを表すタグNの2つのタグからなる．
各文字間のタグがこのいずれかであるかは，単語境界が明示されたコーパスから学習された点推定の最大エントロピーモデル(ME model; maximum entropy model)により推定する．
その結果，より高い確率を与えられたタグをその文字間のタグとし，単語境界を決定する．
すなわち，以下の式が示すように，最大エントロピーモデルにより，単語境界と推定される確率が非単語境界と推定される確率より高い文字間を単語境界とする．
これにより，入力文を単語に分割することができる．
本論文では，以下のように，タグ[MATH]の出現確率を確率的単語分割コーパスにおける単語境界確率[MATH]として用いることを提案する．
これにより，注目する文字の周辺のさまざまな素性を参照し，単語境界確率を適切に推定することが可能になる．
後述する実験においては，[MATH]の間に注目する際の最大エントロピーモデルの素性としては，[MATH]の範囲の文字[MATH]-gramおよび字種[MATH]-gram([MATH])をすべて用いた．
ただし，以下の点を考慮している．
素性として利用する[MATH]-gramは，先頭文字の字種がその前の文字の字種と同じか否か，および，末尾文字の字種がその次の文字の字種と同じか否かの情報を付加して参照する．
素性には注目する文字間の位置情報を付加する．
たとえば，文字列「文字列を単語に分割する」の「語」「に」の文字間の素性は，{[MATH]単[MATH], [MATH]語[MATH], [MATH]に[MATH], [MATH]分[MATH], [MATH]単語[MATH], [MATH]語[MATH]に[MATH], [MATH]に分[MATH], [MATH]単語[MATH]に[MATH], [MATH]語[MATH]に分[MATH], [MATH]K[MATH], [MATH]K[MATH], [MATH]H[MATH], [MATH]K[MATH], [MATH]KK[MATH], [MATH]K[MATH]H[MATH], [MATH]HK[MATH], [MATH]KK[MATH]H[MATH], [MATH]K[MATH]HK[MATH], }となる．
「[MATH]」は注目する文字間を表す補助記号であり，「[MATH]」と「[MATH]」は前後の文字が同じ字種である([MATH])か否([MATH])かを表す補助記号である．
「H」と「K」は字種の平仮名と漢字を表している．
なお，実験においては，パラメータ数を減らすために，学習データで2回以上出現する素性のみを用いた．
また，最大エントロピーモデルのパラメータ推定には，GISアルゴリズム[CITE]を使用した．
確率的単語分割コーパスに対する単語[MATH]-gram頻度は，高いコストの計算を要する．
また，確率的単語分割コーパスは，頻度計算の対象となる単語や単語断片（候補）を多数含む．
ある単語[MATH]-gramの頻度の計算に際しては，その単語の文字列としてのすべての出現に対して，頻度のインクリメントではなく，複数回の浮動小数点演算を実行しなければならない．
この計算コストにより，より長い履歴を参照する単語[MATH]-gramモデルや単語クラスタリングなどの言語モデルの改良が困難になっている．
上述の困難を回避する方法として，単語分割済みコーパスで確率的単語分割コーパスを近似する方法を提案する．
具体的には，確率的単語分割コーパスに対して以下の処理を最初の文字から最後の文字まで([MATH])行なう．
文字[MATH]を出力する．
0以上1未満の乱数[MATH]を発生させ[MATH]と比較する．
[MATH]の場合には単語境界記号を出力し，そうでない場合には何も出力しない．
これにより，確率的単語分割コーパスに近い単語分割済みコーパスを得ることができる．
これを疑似確率的単語分割コーパスと呼ぶ．
上記の方法では，文字列としての出現頻度が低い単語[MATH]-gramの頻度が確率的単語分割コーパスと疑似確率的単語分割コーパスにおいて大きく異なる可能性がある．
そもそも，出現頻度が低い単語[MATH]-gramの場合，単語分割が正しいとしても，その統計的振る舞いを適切に捉えるのは困難であるが，近似によって誤差が増大することは好ましくない．
従って，この影響を軽減するために，上記の手続きを[MATH]回行ない，その結果得られる[MATH]倍の単語分割済みコーパスを単語[MATH]-gram頻度の計数の対象とすることとする．
このときの[MATH]を本論文では倍率と呼ぶこととする．
疑似確率的単語分割コーパスは，一種のモンテカルロ法となっている．
モンテカルロ法による[MATH]次元の単位立方体上[MATH]上の定積分[MATH]の数値計算法では，単位立方体[MATH]上の一様乱数[MATH]を発生させて[MATH]とする．
このとき，誤差[MATH]は次元[MATH]によらずに[MATH]に比例する程度の速さで減少することが知られている．
疑似確率的単語分割コーパスにおける単語[MATH]-gram頻度の計算はこの特殊な場合であり，[MATH]の値や文字数によらずに[MATH]に比例する程度の速さで減少する．
ここで[MATH]は単語[MATH]-gramの文字列としての頻度である．
確率的言語モデルを新たな分野に適応する一般的な方法は，適応分野のコーパスを用意し，それを自動的に単語分割し，単語の頻度統計を計算することである．
この方法では，単語分割誤りにより適応分野のコーパスにのみ出現する単語が適切に扱えないという問題が起こる．
この解決方法として，適応分野のコーパスを確率的単語分割コーパスとして用いることが提案されている[CITE]．
この節では，確率的単語分割コーパスからの確率的言語モデルの推定方法について概説する．
確率的単語分割コーパスは，生コーパス[MATH]（以下，文字列[MATH]として参照）とその連続する各2文字[MATH]の間に単語境界が存在する確率[MATH]の組として定義される．
最初の文字の前と最後の文字の後には単語境界が存在するとみなせるので，[MATH]の時は便宜的に[MATH]とされる．
確率変数[MATH]を
とし([MATH])，各[MATH] [MATH]は独立であることが仮定される．
文献[CITE]の実験で用いられている単語境界確率の推定方法は次の通りである．
まず，単語に分割されたコーパスに対して自動単語分割システムの境界推定精度[MATH]を計算しておく．
次に，適応分野のコーパスを自動単語分割し，その出力において単語境界であると判定された点では[MATH]とし，単語境界でないと判定された点では[MATH]とする．
後述する実験の従来手法としてこの方法を採用した．
確率的単語分割コーパスに対して単語[MATH]-gram頻度が以下のように定義される．
確率的単語分割コーパスの期待単語数として以下のように定義される．
確率的単語分割コーパスに出現する文字列[MATH]が[MATH]文字からなる単語[MATH]である必要十分条件は以下の4つである．
文字列[MATH]が単語[MATH]に等しい．
文字[MATH]の直前に単語境界がある．
単語境界が文字列中にない．
文字[MATH]の直後に単語境界がある．
したがって，確率的単語分割コーパスの単語1-gram頻度[MATH]は，単語[MATH]の表記の全ての出現[MATH] [MATH]に対する期待頻度の和として以下のように定義される．
[MATH]文字からなる単語列[MATH]の確率的単語分割コーパス[MATH]における頻度，すなわち単語[MATH]-gram頻度について考える．
このような単語列に相当する文字列が確率的単語分割コーパスの[MATH]文字目から始まり[MATH]文字目で終る文字列と等しく([MATH])，単語列に含まれる各単語[MATH]に相当する文字列が確率的単語分割コーパスの[MATH]文字目から始まり[MATH]文字目で終る文字列と等しい([MATH]; [MATH]; [MATH]; [MATH])状況を考える（\figref{figure:SSC}参照）．
確率的単語分割コーパスに出現する文字列[MATH]が単語列[MATH]である必要十分条件は以下の4つである．
文字列[MATH]が単語列[MATH]に等しい．
文字[MATH]の直前に単語境界がある．
単語境界が各単語に対応する文字列中にない．
単語境界が各単語に対応する文字列の後にある．
確率的単語分割コーパスにおける単語[MATH]-gram頻度は以下のように定義される．
ここで
e_{1}^{n} & = (e_{1},e_{2},\cdots,e_{n})
O_{n} & = {(i,e_{1}^{n}) | \Bdma{x}_{b_{m}}^{e_{m}} = w_{m}, 1 \leq m \leq n }
である．
確率的単語分割コーパスにおける単語[MATH]-gram確率は，単語[MATH]-gram頻度の相対値として計算される．
以下のように単語1-gram頻度を単語0-gram頻度で除することで計算される．
以下のように単語[MATH]-gram頻度を単語[MATH]-gram頻度で除することで計算される．
ある単語列[MATH] ([MATH])のある1箇所の出現位置に対する期待頻度の計算に必要な演算は，\equref{eqnarray:1-gram}や\equref{eqnarray:n-gram}から明かなように，[MATH]回の浮動小数点に対する減算と[MATH]回の浮動小数点に対する乗算である．
動的に単語[MATH]-gram確率を計算する方法では，この演算が文字列[MATH]の出現回数だけ繰り返される．
通常の決定的単語分割コーパスの場合には，単語列の出現回数がそのまま頻度となるので，上述の浮動小数点に対する演算が全て付加的な計算コストであり，言語モデルの応用の実行速度を大きく損ねる．
あらかじめ単語[MATH]-gram確率を計算しておく場合は，ある文（文字数[MATH]）に出現する全ての[MATH]個の連続する部分文字列に対して行う必要がある．
上述の減算や乗算が重複して行われるのを避けるために，まず文の両端を除く全ての位置に対して[MATH]を計算（[MATH]回の減算）し，さらにこれら[MATH]個の[MATH]のうちの任意個の連続する位置に対する積[MATH] ([MATH])を計算（[MATH]回の乗算）しておく．
ある単語[MATH]-gramの出現位置は，文に[MATH]個の単語境界を置くことで決るので，[MATH]文字の文には重複を含め[MATH]個の単語[MATH]-gramが含まれる．
このそれぞれの期待頻度は，左端の[MATH]に[MATH]個の[MATH]と[MATH]個の[MATH]の積（[MATH]回の乗算）として得られる．
この場合に必要な計算コストも，決定的単語分割コーパスの場合の単語数と同じ回数のインクリメントに比べて非常に大きい．
このように，確率的単語分割コーパスに対する単語[MATH]-gram頻度の計算のコストは，従来の決定的単語分割コーパスに対する計算コストに比べて非常に大きくなる．
文の長さの分布を無視すれば，計算回数はコーパスの文数に対しては比例する．
文毎に独立なので複数の計算機による分散計算も可能であるが，ある程度の大きさのコーパスからモデルを作成する場合にはこの計算コストは問題になる．
また，単語クラスタリング[CITE]や文脈に応じた参照履歴の伸長[CITE]などの様々な言語モデルの改良においては，最適化の過程において言語モデルを何度も構築する．
確率的単語分割コーパスにおける単語[MATH]-gram頻度の計算のコストによって，これらの改良を試みることが困難になっている．
この節では，最大エントロピー法による単語分割器を単語境界確率の推定に用いる方法について述べる．
日本語の単語分割の問題は，入力文の各文字間に単語境界が発生するか否かを予測する問題とみなせる[CITE]．
つまり，文[MATH]に対して，[MATH]の間が単語境界であるか否かを表すタグ[MATH]を付与する問題とみなす．
付与するタグは，単語境界であることを表すタグEと，非単語境界であることを表すタグNの2つのタグからなる．
各文字間のタグがこのいずれかであるかは，単語境界が明示されたコーパスから学習された点推定の最大エントロピーモデル(ME model; maximum entropy model)により推定する．
その結果，より高い確率を与えられたタグをその文字間のタグとし，単語境界を決定する．
すなわち，以下の式が示すように，最大エントロピーモデルにより，単語境界と推定される確率が非単語境界と推定される確率より高い文字間を単語境界とする．
これにより，入力文を単語に分割することができる．
本論文では，以下のように，タグ[MATH]の出現確率を確率的単語分割コーパスにおける単語境界確率[MATH]として用いることを提案する．
これにより，注目する文字の周辺のさまざまな素性を参照し，単語境界確率を適切に推定することが可能になる．
後述する実験においては，[MATH]の間に注目する際の最大エントロピーモデルの素性としては，[MATH]の範囲の文字[MATH]-gramおよび字種[MATH]-gram([MATH])をすべて用いた．
ただし，以下の点を考慮している．
素性として利用する[MATH]-gramは，先頭文字の字種がその前の文字の字種と同じか否か，および，末尾文字の字種がその次の文字の字種と同じか否かの情報を付加して参照する．
素性には注目する文字間の位置情報を付加する．
たとえば，文字列「文字列を単語に分割する」の「語」「に」の文字間の素性は，{[MATH]単[MATH], [MATH]語[MATH], [MATH]に[MATH], [MATH]分[MATH], [MATH]単語[MATH], [MATH]語[MATH]に[MATH], [MATH]に分[MATH], [MATH]単語[MATH]に[MATH], [MATH]語[MATH]に分[MATH], [MATH]K[MATH], [MATH]K[MATH], [MATH]H[MATH], [MATH]K[MATH], [MATH]KK[MATH], [MATH]K[MATH]H[MATH], [MATH]HK[MATH], [MATH]KK[MATH]H[MATH], [MATH]K[MATH]HK[MATH], }となる．
「[MATH]」は注目する文字間を表す補助記号であり，「[MATH]」と「[MATH]」は前後の文字が同じ字種である([MATH])か否([MATH])かを表す補助記号である．
「H」と「K」は字種の平仮名と漢字を表している．
なお，実験においては，パラメータ数を減らすために，学習データで2回以上出現する素性のみを用いた．
また，最大エントロピーモデルのパラメータ推定には，GISアルゴリズム[CITE]を使用した．
確率的単語分割コーパスに対する単語[MATH]-gram頻度は，高いコストの計算を要する．
また，確率的単語分割コーパスは，頻度計算の対象となる単語や単語断片（候補）を多数含む．
ある単語[MATH]-gramの頻度の計算に際しては，その単語の文字列としてのすべての出現に対して，頻度のインクリメントではなく，複数回の浮動小数点演算を実行しなければならない．
この計算コストにより，より長い履歴を参照する単語[MATH]-gramモデルや単語クラスタリングなどの言語モデルの改良が困難になっている．
上述の困難を回避する方法として，単語分割済みコーパスで確率的単語分割コーパスを近似する方法を提案する．
具体的には，確率的単語分割コーパスに対して以下の処理を最初の文字から最後の文字まで([MATH])行なう．
文字[MATH]を出力する．
0以上1未満の乱数[MATH]を発生させ[MATH]と比較する．
[MATH]の場合には単語境界記号を出力し，そうでない場合には何も出力しない．
これにより，確率的単語分割コーパスに近い単語分割済みコーパスを得ることができる．
これを疑似確率的単語分割コーパスと呼ぶ．
上記の方法では，文字列としての出現頻度が低い単語[MATH]-gramの頻度が確率的単語分割コーパスと疑似確率的単語分割コーパスにおいて大きく異なる可能性がある．
そもそも，出現頻度が低い単語[MATH]-gramの場合，単語分割が正しいとしても，その統計的振る舞いを適切に捉えるのは困難であるが，近似によって誤差が増大することは好ましくない．
従って，この影響を軽減するために，上記の手続きを[MATH]回行ない，その結果得られる[MATH]倍の単語分割済みコーパスを単語[MATH]-gram頻度の計数の対象とすることとする．
このときの[MATH]を本論文では倍率と呼ぶこととする．
疑似確率的単語分割コーパスは，一種のモンテカルロ法となっている．
モンテカルロ法による[MATH]次元の単位立方体上[MATH]上の定積分[MATH]の数値計算法では，単位立方体[MATH]上の一様乱数[MATH]を発生させて[MATH]とする．
このとき，誤差[MATH]は次元[MATH]によらずに[MATH]に比例する程度の速さで減少することが知られている．
疑似確率的単語分割コーパスにおける単語[MATH]-gram頻度の計算はこの特殊な場合であり，[MATH]の値や文字数によらずに[MATH]に比例する程度の速さで減少する．
ここで[MATH]は単語[MATH]-gramの文字列としての頻度である．
