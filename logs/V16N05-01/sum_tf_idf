================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:0, score:0.26280] 言語モデルの分野適応において，適応対象の分野の単語境界情報のない生コーパスの有効な利用方法として，確率的単語分割コーパスとしての利用が提案されている．
[i:1, score:0.26670] この枠組では，生コーパス中の各文字間に単語境界が存在する確率を付与し，それを用いて単語[MATH]-gram確率などが計算される．
[i:3, score:0.23627] さらに，確率的単語分割コーパスを従来の決定的に単語に分割されたコーパスで模擬する方法を提案し，言語モデルの能力を下げることなく計算コストが大幅に削減できることを示す．

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[i:9, score:0.33261] この枠組では，適応対象の生コーパスは，各文字の間に単語境界が存在する確率が付与された確率的単語分割コーパスとみなされ，単語[MATH]-gram確率が計算される．
[i:12, score:0.23689] 確率的単語分割コーパスの初期の論文では，単語境界確率は，自動分割により単語境界と推定された箇所で単語分割の精度[MATH]（例えば0.95）とし，そうでない箇所で[MATH]とする単純な方法により与えられている．
[i:20, score:0.28767] これに対して，確率的単語分割コーパスにおいては，全ての連続する[MATH]個の部分文字列（[MATH]文字）に対して，[MATH]回の浮動小数点数の積を実行して期待頻度を計算し，さらに1回の加算を実行する必要がある（\subref{subsection:EF}参照）．

================================================================
[section type  : proposed_method]
[section title : 確率的単語分割コーパスからの言語モデルの推定]
================================================================
[i:31, score:0.23052] 確率的言語モデルを新たな分野に適応する一般的な方法は，適応分野のコーパスを用意し，それを自動的に単語分割し，単語の頻度統計を計算することである．
[i:33, score:0.19734] この解決方法として，適応分野のコーパスを確率的単語分割コーパスとして用いることが提案されている[CITE]．
[i:34, score:0.18615] この節では，確率的単語分割コーパスからの確率的言語モデルの推定方法について概説する．
-----------------------------------------------------
  [subsection title : 確率的単語分割コーパス]
-----------------------------------------------------
  [i:lead, score:0.25893] 確率的単語分割コーパスは，生コーパス[MATH]（以下，文字列[MATH]として参照）とその連続する各2文字[MATH]の間に単語境界が存在する確率[MATH]の組として定義される．
.....
  [i:35, score:0.25893] 確率的単語分割コーパスは，生コーパス[MATH]（以下，文字列[MATH]として参照）とその連続する各2文字[MATH]の間に単語境界が存在する確率[MATH]の組として定義される．
  [i:40, score:0.18809] まず，単語に分割されたコーパスに対して自動単語分割システムの境界推定精度[MATH]を計算しておく．
  [i:41, score:0.20645] 次に，適応分野のコーパスを自動単語分割し，その出力において単語境界であると判定された点では[MATH]とし，単語境界でないと判定された点では[MATH]とする．
-----------------------------------------------------
  [subsection title : 単語$n$-gram頻度]
-----------------------------------------------------
  [i:lead, score:0.22419] 確率的単語分割コーパスに対して単語[MATH]-gram頻度が以下のように定義される．
.....
  [i:50, score:0.23444] したがって，確率的単語分割コーパスの単語1-gram頻度[MATH]は，単語[MATH]の表記の全ての出現[MATH] [MATH]に対する期待頻度の和として以下のように定義される．
  [i:51, score:0.25899] [MATH]文字からなる単語列[MATH]の確率的単語分割コーパス[MATH]における頻度，すなわち単語[MATH]-gram頻度について考える．
  [i:52, score:0.22856] このような単語列に相当する文字列が確率的単語分割コーパスの[MATH]文字目から始まり[MATH]文字目で終る文字列と等しく([MATH])，単語列に含まれる各単語[MATH]に相当する文字列が確率的単語分割コーパスの[MATH]文字目から始まり[MATH]文字目で終る文字列と等しい([MATH]; [MATH]; [MATH]; [MATH])状況を考える（\figref{figure:SSC}参照）．
-----------------------------------------------------
  [subsection title : 単語$n$-gram確率]
-----------------------------------------------------
  [i:lead, score:0.23225] 確率的単語分割コーパスにおける単語[MATH]-gram確率は，単語[MATH]-gram頻度の相対値として計算される．
.....
  [i:63, score:0.23225] 確率的単語分割コーパスにおける単語[MATH]-gram確率は，単語[MATH]-gram頻度の相対値として計算される．
  [i:64, score:0.11764] 以下のように単語1-gram頻度を単語0-gram頻度で除することで計算される．
  [i:65, score:0.12613] 以下のように単語[MATH]-gram頻度を単語[MATH]-gram頻度で除することで計算される．
-----------------------------------------------------
  [subsection title : 単語$n$-gram頻度の計算コスト]
-----------------------------------------------------
  [i:lead, score:0.22364] ある単語列[MATH] ([MATH])のある1箇所の出現位置に対する期待頻度の計算に必要な演算は，\equref{eqnarray:1-gram}や\equref{eqnarray:n-gram}から明かなように，[MATH]回の浮動小数点に対する減算と[MATH]回の浮動小数点に対する乗算である．
.....
  [i:68, score:0.24568] 通常の決定的単語分割コーパスの場合には，単語列の出現回数がそのまま頻度となるので，上述の浮動小数点に対する演算が全て付加的な計算コストであり，言語モデルの応用の実行速度を大きく損ねる．
  [i:74, score:0.26333] このように，確率的単語分割コーパスに対する単語[MATH]-gram頻度の計算のコストは，従来の決定的単語分割コーパスに対する計算コストに比べて非常に大きくなる．
  [i:78, score:0.24976] 確率的単語分割コーパスにおける単語[MATH]-gram頻度の計算のコストによって，これらの改良を試みることが困難になっている．

================================================================
[section type  : proposed_method]
[section title : 最大エントロピー法による単語境界確率の推定]
================================================================
[i:79, score:0.20269] この節では，最大エントロピー法による単語分割器を単語境界確率の推定に用いる方法について述べる．
-----------------------------------------------------
  [subsection title : 単語境界確率の推定]
-----------------------------------------------------
  [i:lead, score:0.17422] 日本語の単語分割の問題は，入力文の各文字間に単語境界が発生するか否かを予測する問題とみなせる[CITE]．
.....
  [i:83, score:0.20616] 各文字間のタグがこのいずれかであるかは，単語境界が明示されたコーパスから学習された点推定の最大エントロピーモデル(ME model; maximum entropy model)により推定する．
  [i:85, score:0.20542] すなわち，以下の式が示すように，最大エントロピーモデルにより，単語境界と推定される確率が非単語境界と推定される確率より高い文字間を単語境界とする．
  [i:87, score:0.22157] 本論文では，以下のように，タグ[MATH]の出現確率を確率的単語分割コーパスにおける単語境界確率[MATH]として用いることを提案する．
-----------------------------------------------------
  [subsection title : 参照する素性]
-----------------------------------------------------
  [i:lead, score:0.14369] 後述する実験においては，[MATH]の間に注目する際の最大エントロピーモデルの素性としては，[MATH]の範囲の文字[MATH]-gramおよび字種[MATH]-gram([MATH])をすべて用いた．
.....
  [i:89, score:0.14369] 後述する実験においては，[MATH]の間に注目する際の最大エントロピーモデルの素性としては，[MATH]の範囲の文字[MATH]-gramおよび字種[MATH]-gram([MATH])をすべて用いた．
  [i:91, score:0.12006] 素性として利用する[MATH]-gramは，先頭文字の字種がその前の文字の字種と同じか否か，および，末尾文字の字種がその次の文字の字種と同じか否かの情報を付加して参照する．
  [i:93, score:0.16613] たとえば，文字列「文字列を単語に分割する」の「語」「に」の文字間の素性は，{[MATH]単[MATH], [MATH]語[MATH], [MATH]に[MATH], [MATH]分[MATH], [MATH]単語[MATH], [MATH]語[MATH]に[MATH], [MATH]に分[MATH], [MATH]単語[MATH]に[MATH], [MATH]語[MATH]に分[MATH], [MATH]K[MATH], [MATH]K[MATH], [MATH]H[MATH], [MATH]K[MATH], [MATH]KK[MATH], [MATH]K[MATH]H[MATH], [MATH]HK[MATH], [MATH]KK[MATH]H[MATH], [MATH]K[MATH]HK[MATH], }となる．

================================================================
[section type  : proposed_method]
[section title : 疑似確率的単語分割コーパス]
================================================================
[i:98, score:0.24389] 確率的単語分割コーパスに対する単語[MATH]-gram頻度は，高いコストの計算を要する．
[i:109, score:0.29068] 上記の方法では，文字列としての出現頻度が低い単語[MATH]-gramの頻度が確率的単語分割コーパスと疑似確率的単語分割コーパスにおいて大きく異なる可能性がある．
[i:116, score:0.27980] 疑似確率的単語分割コーパスにおける単語[MATH]-gram頻度の計算はこの特殊な場合であり，[MATH]の値や文字数によらずに[MATH]に比例する程度の速さで減少する．

================================================================
[section type  : experiment_result]
[section title : 評価]
================================================================
[i:118, score:0.19444] 単語境界確率の推定方法の評価として，言語モデルの適応の実験を行なった．
[i:119, score:0.21092] まず，適応対象文野の大きな生コーパスに既存手法と提案手法のそれぞれで単語境界確率を付与した．
[i:120, score:0.24925] 次に，その結果得られる確率的単語分割コーパスから単語2-gramモデルを推定し，これを一般分野の単語分割済みコーパスから推定された単語2-gramモデルと補間した．
-----------------------------------------------------
  [subsection title : 実験の条件]
-----------------------------------------------------
  [i:lead, score:0.15873] 実験に用いたコーパスは，「現代日本語書き言葉均衡コーパス」モニター公開データ（2008年度版）中の人手による単語分割の修正がなされている文（一般コーパス）と医療文書からなる適応対象のコーパスである．
.....
  [i:127, score:0.24934] 自動単語分割器や単語境界確率の推定のための最大エントロピーモデルはこの学習コーパスから構築される．
  [i:129, score:0.22940] この内の7,000文に入力記号列（読み）を付与しテストコーパスとし，残りを確率的単語分割コーパスとして言語モデルの学習に用いた（\tabref{table:raw-corpus}参照）．
  [i:130, score:0.16395] テストコーパスの内の1,000文には，単語境界情報も付与し，言語モデルの予測力の評価に用いた．
-----------------------------------------------------
  [subsection title : 評価基準]
-----------------------------------------------------
  [i:lead, score:0.17375] 確率的言語モデルの予測力の評価に用いた基準は，テストコーパスにおける単語あたりのパープレキシティである．
.....
  [i:131, score:0.17375] 確率的言語モデルの予測力の評価に用いた基準は，テストコーパスにおける単語あたりのパープレキシティである．
  [i:132, score:0.10683] まず，テストコーパス[MATH]に対して未知語の予測も含む文字単位のエントロピー[MATH]を以下の式で計算する[CITE]．
  [i:133, score:0.21247] ここで，[MATH]は単語[MATH]-gramモデルによる単語列[MATH]の生成確率を，[MATH]はテストコーパス[MATH]の文字数を表す．
-----------------------------------------------------
  [subsection title : 単語境界確率の推定方法の評価]
-----------------------------------------------------
  [i:lead, score:0.28107] 単語境界確率の推定方法の差異を調べるために，以下の2つの確率的単語分割コーパスを作成しそれらから推定された単語2-gramモデルの能力を調べた．
.....
  [i:140, score:0.28107] 単語境界確率の推定方法の差異を調べるために，以下の2つの確率的単語分割コーパスを作成しそれらから推定された単語2-gramモデルの能力を調べた．
  [i:142, score:0.25424] 各単語境界確率は，単語2-gramモデルに基づく自動単語分割器の判断に応じて[MATH]又は[MATH]とする．
  [i:148, score:0.31386] この結果から，最大エントロピー法により推定された単語境界確率を持つ確率的単語分割コーパスを用いることで適応対象分野における単語2-gram確率がより正確に推定されていることがわかる．
-----------------------------------------------------
  [subsection title : 疑似確率的単語分割コーパスの評価]
-----------------------------------------------------
  [i:lead, score:0.17300] 本論文のもう一つの論点は，単語分割済みコーパスによる確率的単語分割コーパスの近似である．
.....
  [i:154, score:0.34545] この評価として，3種類の大きさ(1/1, 1/2, 1/4)の適応分野の疑似確率的単語分割コーパスから推定した言語モデルのテストコーパスに対するパープレキシティと文字正解率を複数の倍率([MATH])に対して計算した．
  [i:159, score:0.29424] 倍率が1の疑似確率的単語分割と決定的単語分割の唯一の違いは，自動単語分割の際に単語境界確率を0.5と比較するか，0から1の乱数と比較するかであり，モデル構築の計算コストはほとんど同じである．
  [i:170, score:0.32337] 同じ計算機で，16倍の疑似確率的単語分割コーパスから単語2-gram頻度をRAMディスク上で計算すると，語彙サイズが46,777語であり，単語2-gram頻度のファイルサイズは9.98 MBであり，計算時間は1009.95秒（約17分）と約61分の1となった．

================================================================
[section type  : conclusion]
[section title : おわりに]
================================================================
[i:180, score:0.21627] 本論文では，確率的単語分割コーパスにおける新しい単語境界確率の推定方法を提案した．
[i:182, score:0.18518] さらに，確率的単語分割コーパスを通常の決定的単語分割コーパスにより模擬する方法を提案した．
[i:183, score:0.20719] 実験の結果，言語モデルの能力を下げることなく，確率的単語分割コーパスの利用において必要となる計算コストが削減可能であることを示した．

