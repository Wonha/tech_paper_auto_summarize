一般的な分野において精度の高い単語分割済みコーパスが利用可能になってきた現在，言語モデルの課題は，言語モデルを利用する分野への適応，すなわち，適応対象分野に特有の単語や表現の統計的振る舞いを的確に捉えることに移ってきている．
この際の標準的な方法では，適応対象のコーパスを自動的に単語分割し，単語[MATH]-gram頻度などが計数される．
この際に用いられる自動単語分割器は，一般分野の単語分割済みコーパスから構築されており，分割誤りの混入が避けられない．
特に，適切に単語分割される必要がある適応対象分野に特有の単語や表現やその近辺において誤る傾向があり，単語[MATH]-gram頻度などの信頼性を著しく損なう結果となる．
上述の単語分割誤りの問題に対処するため，確率的単語分割コーパスという概念が提案されている[CITE]．
この枠組では，適応対象の生コーパスは，各文字の間に単語境界が存在する確率が付与された確率的単語分割コーパスとみなされ，単語[MATH]-gram確率が計算される．
従来の決定的に自動単語分割された結果を用いるより予測力の高い言語モデルが構築できることが確認されている．
また，仮名漢字変換[CITE]や音声認識[CITE]においても，従来手法に対する優位性が示されている．
確率的単語分割コーパスの初期の論文では，単語境界確率は，自動分割により単語境界と推定された箇所で単語分割の精度[MATH]（例えば0.95）とし，そうでない箇所で[MATH]とする単純な方法により与えられている．
実際には，単語境界が存在すると推定される確率は，文脈に応じて幅広い値を取ると考えられる．
例えば，学習コーパスからはどちらとも判断できない箇所では1/2に近い値となるべきであるが，既存手法では1に近い[MATH]か，0に近い[MATH]とする他ない．
この問題に加えて，既存の決定的に単語分割する手法よりも計算コスト（計算時間，記憶領域）が高いことが挙げられる．
その要因は2つある．
1つ目は，期待頻度の計算に要する演算の種類と回数である．
通常の手法では，学習コーパスは単語に分割されており，これを先頭から単語毎に順に読み込んで単語辞書を検索して番号に変換し，対応する単語[MATH]-gram頻度をインクリメントする．
単語辞書の検索は，辞書をオートマトンにしておくことで，コーパスの読み込みと比較して僅かなオーバーヘッドで行える[CITE]．
これに対して，確率的単語分割コーパスにおいては，全ての連続する[MATH]個の部分文字列（[MATH]文字）に対して，[MATH]回の浮動小数点数の積を実行して期待頻度を計算し，さらに1回の加算を実行する必要がある（\subref{subsection:EF}参照）．
2つ目の要因は，学習コーパスのほとんど全ての部分文字列が単語候補になるため，語彙サイズが非常に大きくなることである．
この結果，単語[MATH]-gramの頻度や確率の記憶領域が膨大となり，個人向けの計算機では動作しなくなるなどの重大な制限が発生する．
例えば，本論文で実験に用いた44,915文の学習コーパスに出現する句読点を含まない16文字以下の部分文字列は9,379,799種類あった．
このうち，期待頻度が0より大きい部分文字列と既存の語彙を加えて重複を除いた結果を語彙とすると，そのサイズは9,383,985語となり，この語彙に対する単語2-gram頻度のハッシュによる記憶容量は10.0 GBとなった．
このような時間的あるいは空間的な計算コストにより，確率的単語分割コーパスからの言語モデル構築は実用性が高いとは言えない．
このことに加えて，単語クラスタリング[CITE]や文脈に応じた参照履歴の伸長[CITE]などのすでに提案されている様々な言語モデルの改良を試みることが困難になっている．
本論文では，まず，確率的単語分割コーパスにおける新しい単語境界確率の推定方法を提案する．
さらに，確率的単語分割コーパスを通常の決定的に単語に分割されたコーパスにより模擬する方法を提案する．
最後に，実験の結果，言語モデルの能力を下げることなく，確率的単語分割コーパスの利用において必要となる計算コストが大幅に削減可能であることを示す．
これにより，高い性能の言語モデルを基礎として，既存の言語モデルの改良法を試みることが容易になる．
