サポートベクタマシン
本節では，サポートベクタマシン (SVMs) の理論的な枠組みを簡単に与える．
訓練事例が以下のように与えられるとする:
\begin{displaymath}
(\bmath{x}_{1}, y_{1}), \ldots,
(\bmath{x}_{i}, y_{i}), \ldots,
(\mbox{\boldmath$x$}_{l}, y_{l}), 
\mbox{\boldmath$x$}_{i} \in \mbox{\boldmath$R$}^{n}, y_{i} \in
\{+1, -1\}.
\end{displaymath}
SVM の枠組みにおける決定関数 (decision function) $g$ は次のように定義
される:
\begin{eqnarray}
g(\mbox{\boldmath$x$}) & = & {\rm sgn}(f(\mbox{\boldmath$x$})) \\
f(\mbox{\boldmath$x$}) & = & \sum_{i = 1}^{l}
y_{i}\alpha_{i}K(\mbox{\boldmath$x$}_{i}, \mbox{\boldmath$x$}) + b
\label{eq:fx}
\end{eqnarray}
ここで $K$ はカーネル関数，$b \in \bmath{R}$ は閾値，$\alpha_{i}$ は重
みである．
さらに，重み $\alpha_{i}$ は次の制約も満たす:
\begin{displaymath}
\forall i: 0 \leq \alpha_{i} \leq C \ {\rm and} \ 
 \sum_{i = 1}^{l}\alpha_{i}y_{i} = 0,
\end{displaymath}
ここで $C$ は誤分類のコストである．
ゼロでない $\alpha_{i}$ を持つ事例 $\bmath{x}_{i}$ はサポートベクタと
呼ばれる．
線形 (linear) SVM では，カーネル関数 $K$ は次のように定義される:
\begin{displaymath}
K(\mbox{\boldmath$x$}_{i}, \mbox{\boldmath$x$}) = 
\mbox{\boldmath$x$}_{i} \cdot \mbox{\boldmath$x$}.
\end{displaymath}
このとき，式~(\ref{eq:fx}) は次のように書き直すことができる:
\begin{eqnarray}
f(\bmath{x}) & = & \bmath{w}\cdot\bmath{x} + b 
\end{eqnarray}
ここで $\bmath{w} = \sum_{i = 1}^{l}y_{i}\alpha_{i}\bmath{x}_{i}$ であ
る．
SVM の学習とは，次の最適化問題を解いて $\alpha_{i}$ と $b$ を求めるこ
とである．
\begin{eqnarray*}
\mbox{\rm maximize} & \displaystyle{ \sum_{i = 1}^{l}\alpha_{i} -
\frac{1}{2} \sum_{i, j =
1}^{l}\alpha_{i}\alpha_{j}y_{i}y_{j}K(\mbox{\boldmath$x$}_{i},\mbox{\boldmath$x$}_{j}) 
} \\
\mbox{\rm subject to} & \displaystyle{
\forall i: 0 \leq \alpha_{i} \leq C \  {\rm and} \  \sum_{i =
1}^{l}\alpha_{i}y_{i} = 0 }\label{alphacond}.
\end{eqnarray*}
この解は，最適超平面 (optimal hyperplane) を与える．この超平面は二つのクラス
の決定境界 (decision boundary) である．
図~\ref{fig:sv} に最適超平面とサポートベクタの例を示す．

\begin{figure}
\begin{center}
\epsfile{file=sv-hp-2.eps,width=24em}
\end{center}
\caption{超平面 (太線) とサポートベクタ}\label{fig:sv}
\end{figure}


仮想事例と仮想サポートベクタ\label{sec:vsv}

仮想事例は，ラベル付き事例から生成されるとする\footnote{
ここではラベル付き事例からの生成のみ考える．
ラベルが分からない事例からの生成は考えないとする．
}．
ターゲットとなるタスクの事前知識に基づいて，元になった事例のラベルと同じ
ものを，仮想事例として生成された事例のラベルに設定する．


例えば，手書き数字の認識では，上下左右の方向に1ピクセル移動させても事
例に対するラベルは変化しないとの仮定を置いて，仮想事例を作ることができ
る \cite{Schoelkopf1996,DeCoste2002}．

特にサポートベクタから作られた仮想事例は，{\em 仮想サポートベクタ} 
({\em virtual support vectors}) と呼ばれる．
妥当な仮定に基づいて生成された仮想サポートベクタは，よりよい最適超平面
を与えると期待される．
仮想事例がターゲットとなるタスクにおける事例の自然なバリエーションを表
現していると仮定すると，決定境界はより正確になるはずである．
図~\ref{fig:vsv} は仮想サポートベクタの例を示している．
仮想サポートベクタが与えられた図~\ref{fig:vsv} の例では，最適超平面
が図~\ref{fig:sv} と異なっていることに注意されたい．

\begin{figure}
\begin{center}
\epsfile{file=vsv-hp-4.eps,width=24em}
\end{center}
\caption{超平面と仮想サポートベクタ．図~\ref{fig:sv} に，サポートベクタ
の仮想事例，\\
つまり仮想サポートベクタが追加されている．}\label{fig:vsv}
\end{figure}


文書分類のための仮想事例\label{sec:vx}
本節では，文書分類のための仮想事例の作り方の提案手法を述べる．
まず，文書分類の事前知識から仮定を設定し，次にその仮定に基づく提案手法
を述べる．

ここでは，文書分類について次の仮定を置く:
\begin{assumption}\label{assum1}
ある文書に付けられているカテゴリは，たとえ少量の単語を追加あるいは削除
しても変化しない．
\end{assumption}
この仮定は十分妥当であろう．
文書分類の典型的な適用場面では，大抵の文書は，カテゴリを暗示する数個以上
のキーワードと，一定量のカテゴリによらない単語を含んでいる．
少量の単語の追加削除の影響は多くの場合に限定的だと考えられる．

仮定~\ref{assum1} に従って，文書分類のための仮想事例を生成する方法を
二つ提案する．
第一の方法は，少量の単語を文書から削除する方法である．
仮想事例のラベルは，その仮想事例の元となった事例のラベルと同じであると
する．
もう一つの方法は，少量の単語を文書に追加する方法である．
仮想事例に追加される単語は，元となる文書と同じラベルを持つ文書群から
選ぶ．
仮定~\ref{assum1} に基づく仮想事例の作り方にはいろいろなものが考えられ
るが，本研究では非常に簡単なものをまず提案し，その効果を検証したい．

提案手法を述べる前に，本研究で用いた文書の表現方法 (text
representation) について述べる．
一つの文書は一つの単語ベクタ (word vector) で表現する．
文書を単語に分割し，それらを小文字に統一，ストップワードを削除した．
ストップワードのリストは freeWAIS-sf\footnote{
http:\slash\slash ls6-www.informatik.uni-dortmund.de\slash ir\slash
projects\slash freeWAIS-sf\slash} のものを用いた．
ステミングは行なっていない．
各単語をバイナリ素性として表現している．単語の頻度は利用していない．
このとき，文書集合全体には $m$ 個の異なり単語 $w_{1}, w_{2}, \ldots,
w_{m}$ があるとすると，一つの文書は単語のベクタとして表現できる．
以下では，文書に存在する単語をコンマで区切って並べ，$[ \ ]$ で囲って単
語ベクタを記述することにする．
例えば，ある文書 $\bmath{x}$ が
四つの単語 $w_{1}, w_{3}, w_{4}, w_{6}$ から構成されるとき，
$\bmath{x} = [w_{1}, w_{3}, w_{4}, w_{6}]$ と書く．

それでは，二つの提案手法 \GenerateByDeletion\ と \GenerateByAddition\ 
を述べる．
ある文書を表す単語ベクタ $\bmath{x}$ と，$\bmath{x}$ から生成された単
語ベクタ $\bmath{x'}$ があるとする．
アルゴリズム \GenerateByDeletion\ は次の通り:
\begin{enumerate}
\item $\bmath{x}$ を $\bmath{x'}$ にコピーする．
\item $\bmath{x'}$ のそれぞれの単語 $w$ について，
もし ${\rm rand}() \le t$ なら単語 $w$ を $\bmath{x'}$ から削除する．
ここで ${\rm rand}()$ は $0$ から $1$ の乱数を生成する関数，
$t$ はどの程度の素性を削除するかを決めるパラメータである．
\end{enumerate}
例を示す．表~\ref{tbl:sample} にあるような文書集合があるとする．
\begin{table}
\caption{文書集合の例}\label{tbl:sample}
\begin{center}
\begin{tabular}{c|lc} \hline\hline
Document ID & 単語ベクタ (\bmathsmall{x}) & ラベル ($y$) \\ \hline
1 & $[w_{1}, w_{2}, w_{3}, w_{4}, w_{5}]$ & $+1$ \\
2 & $[w_{2}, w_{4}, w_{5}, w_{6}]$        & $+1$ \\
3 & $[w_{2}, w_{3}, w_{5}, w_{6}, w_{7}]$ & $+1$ \\
4 & $[w_{1}, w_{3}, w_{8}, w_{9}, w_{10}]$ & $-1$ \\
5 & $[w_{1}, w_{8}, w_{10}, w_{11}]$       & $-1$ \\ \hline
\end{tabular}
\end{center}
\end{table}
Document~1 からアルゴリズム \GenerateByDeletion\ で生成される仮想事例
としては 
$([w_{2}, w_{3}, w_{4}, w_{5}], +1)$ や $([w_{1}, w_{3}, w_{4}], +1)$，
$([w_{1}, w_{2}, w_{4}, w_{5}], +1)$ などが考えられる．


アルゴリズム \GenerateByAddition\ は次の通り:
\begin{enumerate}
\item 訓練事例の中から $\bmath{x}$ と同じラベルを持つ文書を集める．
\item それら文書を表す単語ベクタを全てつなげて，単語の配列 $a$ 
を作る．
\item $\bmath{x}$ を $\bmath{x'}$ にコピーする．
\item $\bmath{x}$ のそれぞれの単語 $w$ について，
もし ${\rm rand}() \le t$ なら配列 $a$ からランダムに一つの単語を選び， 
それを $\bmath{x'}$ に加える．
\end{enumerate}
例を示す．
表~\ref{tbl:sample} の Document~2 からアルゴリズム 
\GenerateByAddition\ を用いて仮想事例を作ろうとするとき，まず
配列 $a = (w_{1},
 w_{2}, w_{3}, w_{4}, w_{5}, w_{2}, w_{4}, w_{5}, w_{6}, w_{2}, w_{3},
 w_{5}, w_{6}, w_{7})$ を作る．
このとき，アルゴリズム \GenerateByAddition\ で作られる仮想事例として，
$([w_{1}, w_{2}, w_{4}, w_{5}, w_{6}], +1)$ や $([w_{2}, w_{3}, w_{4},
 w_{5}, w_{6}], +1)$，$([w_{2}, w_{4}, w_{5}, w_{6}, w_{7}], +1)$ などが
考えられる．
逆に，$([w_{2}, w_{4}, w_{5}, w_{6}, w_{10}], +1)$ のような事例は 
Document~2 からは決して作られない．
$+1$ のラベルを持つ文書には，$w_{10}$ は含まれていないからである．


