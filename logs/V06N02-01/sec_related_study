前節の結果得られた各文は局所的に見ると記号ばかりであったり，姓名の列挙部分であったりして学習コーパスには適さないものが含まれている．
また電子会議室のテキストはフィルターのルールでカバーしきれなかった部分で単語ではないトークンが無視できない程度に生じていた．
このような文については人手で採用するかどうかを決める，あるいは当該部分を除くことが望ましいが，多量のコーパスについてそのような作業を行うのは不可能なため，ここでは以下の条件のいずれかに当てはまる文は採用しないことにした．
２単語以下から構成される文
文の単語数に対する記号の数が一定以上の文
44K語彙に対して未知語の数が一定以上の割合で含まれる文
音声認識用のコーパスにおいて句読点や括弧表現をどのように取り扱うべきかについてはさまざまな議論がある．
松岡ら[CITE]はカギ括弧以外の括弧（（）【】など）について内容ごと削除しており，伊藤ら[CITE]は括弧の用いられ方（引用，強調など）に応じて削除すべきかどうかを自動判別している．
括弧による表現には確かに読み上げに適さないものも含まれているが，本研究では文章入力手段としての音声認識システムの構築を重視し，これらを削除しないことにした．
また同じ理由で句読点も削除していない．
その結果得られた文の数をソース別に示す（表[REF_TBL:SOURCE]）．
前節にしたがって単語単位に分割されたテキストを学習データとしてN-gramモデルを学習するわけであるが，生起確率の計算上考慮すべきこととして数字，時刻などとくに各単語に確率上の差をつけるべき理由がないもの，および意味がまったく同じでありながら表記の異なる揺らぎが生じているものの取り扱いがある．
前者については各単語をクラスにまとめて確率を計算することにし，合計36クラス作成した．
後者は新聞の場合，用語統一がなされているため影響は少ないと考えられるが，電子会議室のテキストでは「コンピュータ」と「コンピューター」，「組み合わせ」と「組合せ」といった単語は両者とも多数含まれており明らかに無視できない．
そこで44K語彙について「読み」をもとに同義語の候補を抽出した上でチェックを行い，約1,800エントリの別名リストを作成した．
N-gramをカウントするさい，このリストを参照して１つの表記に統一した上で学習を行っている．
一方，テストデータとして新聞3種類，電子会議室のテキストを別に用意し，被験者(単語分割モデルの学習データを作成した被験者とは異なる）により分割を行なった．
テストデータのそれぞれについて文数，形態素数，単語数，そして44K語彙のカバレージを表[REF_TBL:TESTDATA]に示す．
この表から１文あたりの単語数は形態素数に比較して12-19%程度少なくなることがわかる．
本実験の目的は
単語を単位としたN-gramモデルの有効性，コーパスの必要量を評価する．
新聞と電子会議室において単語N-gramモデルから見た違いを明らかにする．
の２点である．
そこで新聞，電子会議室のそれぞれについてその種類，時期の違いを捨象するため，全学習データを文単位でシャッフルした上で８個に分割したサブセット(新聞: N-1,.
.,8，電子会議室F-1,.
..
,8）を作成した．
そして各サブセットをさらに95%と5%の比率で分割し前者をN-gramカウント，後者をHeld-out補間のパラメータ学習用に用いた．
まず新聞について学習データ(N-1,.
.,8)を順に増加させながら言語モデルを作成し，各モデルをテストセットパープレキシティで評価した．
ただし学習データに１回でも出現したN-gram (trigramまで）はすべて使用しており，また未知語部分については予測を行っていない．
結果を図[REF_FIG:PERPNEWS]に示す（電子会議室に関するデータは「Forum」と表記している）．
予想されるようにいずれのテストデータでも学習コーパスの増加にともなってパープレキシティは緩やかに改善されるが次第に飽和する傾向がみてとれ，いずれの場合も学習データセットを7個から8個に増やしたときのパープレキシティの改善率は1-2%程度でしかない．
パープレキシティの絶対値には相当の差があり，新聞といってもひとくくりにできないことは明らかだが，その値(100-170)は音響識別上対応可能な値であると考えられる[CITE]．
一方電子会議室のテストデータはもっとも良いケースでも400以上のパープレキシティを示しており新聞の学習データだけでは対応できていないことがわかる．
われわれの目的は新聞にとどまらず，より口語体に近い電子会議室に投稿される文にも対応できる言語モデルを作成することである．
そこで新聞データすべてを使用した言語モデルをベースとし，電子会議室の学習データ(F-1,.
.,8)を加えていくことにより各テストデータのパープレキシティがどのようになるかを評価した．
結果を図[REF_FIG:PERPFORUM]に示す．
この結果，電子会議室についてそのパープレキシティは改善される一方，使用したデータ量の範囲(約25M単語)では，新聞に対する影響はほとんどなかった．
一方電子会議室のみから作成した言語モデルで（電子会議室の）テストデータを評価すると152.1であり，若干の差は見られるものの，混合学習データから作成した言語モデルは新聞・電子会議室の双方に対応できることがわかる．
これは双方の統計的異なりが共通しているN-gramの確率が相違しているというよりも，N-gramの種類に，より大きく現れていることを示唆している．
一方コーパスのサイズと結果として得られたモデルのサイズ，すなわちN-gramの異なり数の関係を見たのが図[REF_FIG:NGRAM]である．
これは新聞データ(N-1,.
.,8)の場合であるが，bigram，trigramとも飽和する傾向は見てとれない．
電子会議室テキストを加えた場合も同様でN-1,.
..
,8，F-1,.
..
,8すべてを学習データに使用した場合のN-gram数はtrigramが31M個，bigramが5.6M個に達した．
とくにtrigramは学習データサイズの増分に対しほとんど比例して増加している．
今後主記憶，外部記憶の容量がさらに増加するとしてもこのN-gram数（異なり）のままでは，実装することが難しい．
そこでN-gramの中で低頻度のものを除くことが，パープレキシティにどのような影響を与えるかを検証する実験を行った．
結果を図[REF_FIG:SMALLLM]に示す．
N-gramの異なりの多くを占めるのはtrigramなので，学習データはN-1,.
..
,8，F-1,.
..
,8すべてを使用した上で，言語モデルを作成するときtrigramの最低出現回数を設定することにより，モデルのサイズを変更している．
図からtrigramの異なり数が5M個以下になるとパープレキシティが急速に悪くなる傾向が見てとれるが，一方モデルサイズを1/3〜1/5にした程度ではパープレキシティの差は小さいことがわかる．
このように，本研究では比較的少量の人による分割データから揺らぎを含めた分割傾向を推定する手法について述べ，新聞およびパソコン通信の電子会議室を学習データとして，そのモデルからつくられた単語の集合と言語モデルについて考察した．
結果として，人が単語と意識する単位はその揺らぎを含めても発散することはなく，約44Kで94-98%程度のカバレージが得られること，形態素に比較して１文あたりの要素数が12-19%程度減少すること，電子会議室と新聞では，N-gramモデルからみた統計量に相当の差があり，予想されたように新聞単体では十分に対応できないものの，新聞をベースとして電子会議室のテキストを混合させたデータから作成した言語モデルは新聞のテストデータに対するパープレキシティを増大させることはほとんどなくその双方に対応可能であることがわかった．
分割モデル，N-gramモデルのいずれも，データの種類(新聞，パソコン通信）に依存している．
これ自体は容易に予想できることであるが，その異なりが共通する事象の確率が異なるというよりも事象自体の異なりにより大きく現れていることは興味深い．
形態素との効率比較という意味では，同一学習データから作成した言語モデルを用いて単位長さ（たとえば文）あたりのパープレキシティを比較する必要がある．
これについて学習，テストデータ量は少ないものの，すでに報告を行っており，文あたりパープレキシティがほぼ等しく，したがって単位長が長い分，より有利な単位となっていることを確認している[CITE]．
コーパス量とパープレキシティの関係について，とくに日本語に関して報告された例はほとんどないため，他の研究と比較して議論することが難しい．
本研究の実験からは400万文強のデータではまだパープレキシティが減少するが，その改善率は低く数倍以上のデータがないと意味のある改善が難しいことを示唆している．
人が感覚的にある単位だと判断する日本語トークンについて考察した他の研究との関連についても述べておきたい．
原田[CITE]は人のもつ文節単位の概念に関する調査結果から，「文字列またはモーラ長が一定以上になると分割しようとする動機がたかまる」という仮説を提起している．
われわれの分割モデルでは分割が2形態素の遷移情報のみで独立に起こることを仮定しているが，この独立性については検討が必要であろう．
横田，藤崎[CITE]が短時間に認識できる文字数とその時間との関係から求めた認知単位は，とくに平均長は述べられていないものの，例をみる限りわれわれの単位より明らかに長い．
同論文では「人は文を文字単位で処理しているのではない」と結論しているが，加えて，分割できる最小単位の列として知覚されているのでもないということになる．
今後は，コーパスサイズをより大きくするとともに句読点を削除した場合との比較・考察や，単語分割モデルの分割確率とポーズ位置との関係[CITE]，さらに上記で述べた分割の独立性について検討したいと考える．
本研究にテキストデータ使用を許諾していただいた，産経新聞社，日本経済新聞社，毎日新聞社（CD-毎日新聞91-95），そして(株)ピープルワールドカンパニーに感謝いたします．
