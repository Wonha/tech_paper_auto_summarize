\documentstyle[epsf,jnlpbbl,leqno,fleqn]{jnlp_j_b5}

\makeatletter  \makeatother     \受付{1998}{4}{3} \再受付{1998}{7}{6} \採録{1998}{8}{10}  \oddsidemargin=0cm \evensidemargin=0cm \topmargin=0cm     \affilabel{IBM}{日本アイ・ビー・エム東京基礎研究所} {Tokyo Research Laboratory, IBM Japan, Ltd.}

音声認識技術はその発達にともなって，その適用分野を広げ，日本語においても新聞など一般の文章を認識対象とした研究が行なわれるようになった[CITE]．この要因として，音素環境依存型HMMによる音響モデルの高精度化に加え，多量の言語コーパスが入手可能になった結果，文の出現確率を単語N個組の生起確率から推定するN-gramモデルが実現できるようになったことが挙げられる．日本語をはじめとして単語の概念が明確ではない言語における音声認識を実現する場合，どのような単位を認識単位として採用するかが大きな問題の1つとなる．この問題はユーザーの発声単位に制約を課す離散発声の認識システムの場合に限らない．連続音声の認識においても，ユーザーが適時ポーズを置くことを許容しなければならないため，やはり発声単位を考慮して認識単位を決める必要がある．従来日本語を対象とした自然言語処理では形態素単位に分割することが一般的であり，またその解析ツールが比較的よく整備されていたことからN-gramモデル作成においても「形態素」を単位として採用したものがほとんどである[CITE]．しかしながら，音声認識という立場からあらためてその処理単位に要請される条件を考えなおしてみると，以下のことが考えられる．認識単位は発声単位と同じか，より細かい単位でなければならない．形態素はその本来の定義から言えば必ずこの条件を満たしているが，実際の形態素解析システムにおいては，複合名詞も１つの単位として登録することが普通であるし，解析上の都合から連続した付属語列のような長い単位も採用している場合があるためこの要請が満たされているとは限らない．長い認識単位を採用する方が，音響上の識別能力という観点からは望ましい．つまり連続して発声される可能性が高い部分については，それ自身を認識単位としてもっておく方がよい．言語モデルを構築するためには，多量のテキストを認識単位に分割する必要があり，処理の多くが自動化できなければ実用的ではない．これらは，言い換えれば人間が発声のさいに分割する(可能性がある)単位のMinimum Cover Setを求めることに帰着する．人が感覚的にある単位だと判断する日本語トークンについて考察した研究は過去にも存在する．原田[CITE]は人が文節という単位について一貫した概念を持っているかについて調査し，区切られた箇所の平均一致率が76%であり付属語については多くの揺れがあったと報告している．また横田，藤崎[CITE]は人が短時間に認識できる文字数とその時間との関係から人の認知単位を求め，その単位を解析にも用いることを提案している．しかしながら，これらの研究はいずれも目的が異なり，音声認識を考慮したものではない．そこで，われわれは，人が潜在意識としてもつ単語単位を形態素レベルのパラメータでモデル化するとともに，そのモデルに基づいて文を分割，N-gramモデルを作成する手法を提案し，認識率の観点からみて有効であることを示した[CITE]．本論文では主として言語処理上の観点からこの単語単位N-gramモデルを考察し，必要な語彙数，コーパスの量とパープレキシティの関係を明らかにする．とくに新聞よりも「話し言葉」に近いと考えられるパソコン通信の電子会議室から収集した文章を対象に加え，新聞との違いについて実験結果を述べる．本節ではわれわれが採用した単語単位と，同単位への分割手法について述べる．日本語を分割して発声する場合，その分割点はきわめて安定している点と，人，または時によって分割されたりされなかったりする不安定な点がある．例として「私は計測器のテストを行っています．」という文を考えよう．これは形態素解析により，たとえばと分割されるが，動詞の活用語尾である「っ」や接続助詞の「て」はほぼ確実に「行」と結合して「行って」と発声されるのに対し，接辞である「器」は分割される場合もあれば，結合されることもあるだろう．そこで文がある位置で「分割」される確率を形態素のレベルでモデル化することを考える．そして人が分割した学習用テキストと同じテキストを形態素解析により分割した結果を照合し，各形態素の遷移ごとに当該点で分割される確率を得る．その後，より大量のテキストをそのモデルに基づいて分割すれば(このプログラムを以後セグメントシミュレータと呼ぶ)，人が分割した傾向をもったわかち書きテキストを容易に得られる．「分割」される位置としては，形態素の境界(形態素単位への分割)とさらに細かく形態素の途中(文字単位への分割)がある．ここで分割記号として[MATH]を使用し，「分割」は記号「[MATH]」が生起し，「結合」は「NULL」が生起すると考えれば，前者はある形態素から別の形態素に遷移したときにその間に「[MATH]」が生起する確率としてとなる．後者のそれは[MATH]を文字列[MATH]で表すと，そのj番目の文字の後に[MATH]が生起する確率と考えればと表現できる．モデルのパラメータ(形態素の属性)としては，品詞情報(KoW)，連接属性(Part of Speech: PoS)，，そして表記(String)を採用し，[MATH]と表現する．ここで品詞，連接属性とはわれわれの用いた形態素解析プログラム[CITE]の出力として得られるものであり，品詞は81，連接属性は119に分類されている．したがって形態素単位の分割では6個，文字単位への分割では4個のパラメータで記述されることになるが，そうすると明らかに多量の学習用テキスト(人が分割したもの)が必要となる．そこで頻度が閾値以下であるような場合については，パラメータを特定の順序で縮退させた確率値を用意しセグメントシミュレータの実行時も，確率が記述されているレベルまで同様の順序で縮退し，当該確率値で代用することを考える．縮退の順序にはさまざまなものが考えられるが，モデルのパラメータについてその種類数を考えると表記，連接属性，品詞の順に少なくなることは明らかであり，縮退もそれにしたがうのが妥当であろう．また基本的にはある出現回数を閾値としたときより多くの種類の遷移確率が得られることが望ましい．このような観点からいくつかの予備実験を行い経験的に縮退順序を決定した．この順序と参照される確率値を木構造で表現したのが図[REF_FIG:STATTREE]である．各ノードには形態素の属性とその属性が満たされた場合に分割される確率が対応する．たとえば図[REF_FIG:STATTREE]中は形態素単位への分割に対する記述例で，形態素の属性が動詞活用語尾[29]から接続助詞[69]「て」へ遷移したときに，その間で分割される確率を意味する．1つ上のレベルでは，表記（ここでは「て」）が省略される．ただし品詞が名詞の場合には文字数が分割確率を記述するパラメータとして有効と考えられるので，表記を省略した場合，文字数をパラメータとして残した．さらに上位レベルでは，連接属性番号も省略し，品詞V. infl.からConj. p.p.への遷移に対して，人が分割する確率を記述する．たとえば，「積んで」という文節を形態素に分割するととなるが，その中に現れる「ん」と「で」の間で分割されたカウント等もマージした上で算出された確率となる．このように木はリーフから上位のノードに行くにしたがって縮退されたパラメータ，言い換えればより大まかなパラメータとなる．一方，前節で述べたように人は形態素として定義されたトークンをさらに文字単位で分割する場合もある．これは形態素解析の都合上連続した付属語列を1つの形態素としてとり扱うことが行なわれるためである．たとえばわれわれの用いた形態素解析用文法では「...かどうか」という付属語列が助詞として扱われているが「か」+「どうか」と分割されることもある．そこで形態素レベルの分割よりもさらに詳細なレベルとして，文字レベルの分割をモデル化した．このような確率木はつぎのように構成することができる．つまりもっとも細かい分類における各パラメータについて，人が分割した結果と形態素解析の結果を照合してカウントし，その値をリーフから上位ノードに伝搬させた後，確率値に正規化すればよい．全カウント数が少ないと当該確率(推定値)の信頼性が低いので，カウント，マージ作業を行なって，頻度がある閾値以上のノードを最終的なノードとして採用することにする．このモデル化では学習データの量に応じて，そのデータから得られる情報を最大限に利用することができる．たとえば，2文字漢語から接尾辞への遷移には，非常に多くのものがあるが，その分割されやすさは接尾辞の種類によって異り，それらを捨象してモデル化したのでは，あいまいさが大きくなってしまう．しかし逆にそのすべてを細分化したのでは，頻度が低い接尾辞に対するルールが得られないか，または信頼性の低い確率推定値となってしまう．本手法によれば学習データ中に頻度が高いものについてはより細かい分類でモデル化され，頻度が下るにしたがって統計として信頼にたる単位まで縮退されたパラメータによる確率値が得られることになる．形態素解析システムは，一般に新聞記事に代表される現代語書き言葉を処理できるように開発されてきた．しかし近年，データとして使用されるコーパスの大規模化に伴い，現代語書き言葉以外の表現，特に，会話風の表現（以下，口語体と示す）を扱う試みが増加してきた[CITE]．われわれが従来使用してきた形態素解析の文法規則[CITE]も，原則として現代語書き言葉に対応したもので，口語体への対応は十分ではない．一方本研究で用いる学習用テキストは新聞に限らず，パソコン通信の投稿テキストが含まれており，口語体への対応なくしては充分な精度の解析結果を得ることができない．以下の点を考慮して，より多様な文に対応できるよう形態素解析の文法を記述した．元の文法に対する変更を少なくして派生的な影響を抑える．口語体によく現れる縮退形で，五段活用連用形に接続する「ちゃ」には，接続助詞「て」および係助詞「は」の連なり「ては」の縮退と（例:書いちゃいけない）と，接続助詞「て」および補助動詞「しまう」の語幹の連なり「てしま」の縮退（例:書いちゃう）とがある．前者は直後で文節を切ることができる非活用語，後者はワア行五段活用をするので，ワア行五段活用語尾が接続し，かつ直後で文節末に遷移できる「ちゃ」という形態素の規則を作成すれば形態素解析処理を行うことができる[CITE]．しかし，品詞や活用形を単語分割モデルで利用すると，「ちゃ」に品詞として接続助詞を付与すれば「接続助詞にワア行五段活用語尾が接続する」という一般化が，また動詞を付与すれば「五段動詞語幹が文節末に遷移する」という一般化が行なわれかねない．これを避けるには，「ちゃ」に新たな品詞を付与するか，または「ちゃ」に二種類あるとするという対応が考えられるがわれわれは後者の方法を採った．形態素解析としては前者が望ましいと思われるが，後の単語分割モデルに影響を及ぼす可能性がある場合は，元の文法規則への影響がより少ないものを採用した．また，文語活用の残存形などで，現代語活用に全く同じ形があるものについては，現代語活用の形態素に接続条件を加えて対処した．縮退形の品詞付与では元の形態素列のうち活用語尾や自立語がもつ品詞を優先する．形容詞仮定形活用語尾「けれ」および接続助詞「ば」の連なりの縮退である「きゃ」「けりゃ」の前連接属性は「けれ」，後連接属性は「ば」にほぼ等しい．こうした縮退形の品詞は，元の形態素列のもつ連接属性のうち活用語尾や自立語のものを優先して付与した[CITE]．省略による空文字列は次形態素への遷移を追加して対処する．「勉強しよ」「読も」などのように形態素末が落ちる縮退の場合，前者は助動詞「よう」の縮退「よ」を定義すればよいが，後者は助動詞「う」そのものが脱落しているので，動詞未然形から「う」の次の形態素への遷移を追加して対処する．形態素解析の辞書には，現在までの使用目的に応じて複合語が一語扱いで登録されていることが多いが，単語分割モデル構築のための形態素解析としては短単位に分割されていた方が都合がよい．そこで，複合語の中でも特に多い複合名詞を分割対象として，分割データベースとヒューリスティック規則により，形態素解析で複合名詞分割を行なうことにした．複合名詞の分割データベースは，2カ月分の新聞記事（産経新聞）を形態素解析してその結果から一定以上の頻度で出現する3文字以上の名詞を抜き出した後，人手で，分割する位置の情報を付与することにより作成した．このデータベースには約25,000語の複合名詞が含まれている．ヒューリスティック規則は，以下の条件を満たすように作成した．1語の名詞よりも2語以上の名詞連続のコストが小さい．名詞連続中では，2語のコストがもっとも小さく，次第にコストが増大するように設定する．これは複合名詞を分割する際，あまり細かく切り過ぎないようにするためである．1文字名詞は他の名詞に比べてコストが大きい．上記と同様，過分割を防ぐためである．分割対象は3文字以上の複合名詞とする．1文字ずつに過分割しないためである．未知語のコストは1語の名詞より大きい．また，分割の結果に3文字以上の名詞が含まれている場合は，再帰的にそれを分割し，分割が不可能になるまで繰り返す．分割ルールとその確率を推定するため，計17人の被験者により，新聞5カ月分（日経新聞3カ月および産経新聞2カ月），日本語用例集(合計約26,000文)，そしてパソコン通信「ピープル」の電子会議室（以下電子会議室）から採取した文章(約9,500文）を分割する作業を行った．新聞や日本語用例集はいわゆる「書き言葉」のスタイルであるのに比較して電子会議室の文章はより口語体に近く，これらは分割モデルにも影響を与える可能性がある．そこで両者のデータは別々に取り扱って分割モデル(確率木)を構成した．その結果前者は2,829個，後者は2,269個のノードからなる木が得られた．表[REF_TBL:RULES]に一例を示す．ただしノードとして採用するか否かの閾値には当該ノードの出現回数(カウント)を用い，その値は学習データ中の単語数に比例させた．2つの確率木について得られたノードをいずれに含まれるかで分類し数を示したものが図[REF_FIG:CMPSTAT]である．得られたノードは，かなりの異なりがあることがわかる．たとえば電子会議室データから得られた確率木にのみ存在するノードの中で出現回数の多いものから上位3個をあげると以下のようになる．これらの遷移を含む例文を上げると1.読ん+で+る, 2. ...です+が, 3. ...だ+けどなどであり，明らかに口語体特有の言い回しに伴う遷移が抽出されている．一方新聞データから学習した確率木にのみ存在するノードをみると体言止めに伴う遷移([MATH] i.e.「...を議論+．」)や漢語の接辞([MATH])など直感的にも電子会議室等の文章では比較的頻度が低いと考えられるものが多かった．また両方の確率木に共通して出現しているノード1,607個について分割確率の相関係数を求めたところ0.980となりきわめて高い．したがって共通するノードについてはほとんど違いはなく，２つの確率木の違いはノードつまりルールそのものに現れていることがわかった．これらのモデルに基づいて以下のように多量の(形態素解析された)テキストを分割・統合する．各形態素およびその遷移について，連接属性番号，品詞，形態素の表記を得て，確率木のリーフに記述があるかどうかを調べる．なければ，木作成の説明で述べた順にパラメータ値を縮退させ，確率木に記述があるかどうかを調べる．記述があれば，0から1の範囲の乱数を発生し，その値がノードに付随する確率以下であれば当該位置で分割し，そうでない場合は分割しない．記述がなければ，縮退を繰り返す．もっとも上位のノードにも該当しない場合，形態素の分割点であれば当該位置で分割し，それ以外は分割しない．なおN-gramモデル作成には，乱数による分割処理(セグメントシミュレータ)は必ずしも必要ではなく，形態素解析の結果と分割確率を使って直接各N-gramの生起確率を推定することも可能である．われわれの提案した単語単位に基づく語彙を作成するための予備実験として日経新聞3カ月分(合計446,079文)を用い，前節の手続きを適用して分割，連結を行う実験を行った．西村らの報告[CITE]によれば形態素を単位とした場合，約97%はおよそ3カ月分のテキストで収集できる（言い換えれば飽和する）ことがわかっている．その結果を図[REF_FIG:COVERAGE]に示す．単語は合計で約[MATH]個，のべ216,904種類の単語が生成された．図はそれらを頻度の高いのものから順にとった場合のカバレージを示している．ただし数字表現，姓名はカウントから除いている．一方同じテキストから形態素は132,164個が生成された．これによれば単語単位を採用すると，形態素よりはより多くの種類が必要ではあるものの，決して発散するものではなく，たとえば上位約25,000個(種類)の単語で全トークンの約95%がカバーでき，取り扱いが可能な語彙数であることがわかる．このとき確率木の各ノード（ルール）がどのような割合で使われたかを示したのが表[REF_TBL:USEDRULE]である．表から明らかなように全体の約60%の場合には，一番詳細なレベルのルールが適用されていることがわかる．用意したコーパスのソースは日経新聞（93年から96年），産経新聞（92年10月から97年），毎日新聞（91年と92年），EDRコーパス[CITE]，そしてパソコン通信「ピープル」に投稿された電子会議室の記事である．ただし日経，産経の両紙は示した期間のすべてではなく，月単位で時期が重複しないように選択したサブセットである．新聞についてはその本文を句点単位で文として取り出し，前節で述べた処理を行った．ただし数字については形態素解析で１単語（品詞「数字」）として扱われてしまうので当該トークンをすべて桁付きの漢数字に変換した後，西村[CITE]に記載された数字の読み上げ単位に合わせて分割した．すなわち整数については「十，百，千，万，億」を位と定義し先行する数字と位で１つの単位として取り扱い，小数点以下の位については１桁づつに分割する．たとえば１２３４．５６は「千」「二百」「三十」「四」「・」「五」「六」と変換・分割されることになる．一方ディクテーションのアプリケーションや一般ユーザーが入力するであろう文，言い回しを考えると新聞だけでは明らかに不足である．そこでより口語体に近いデータとしてパソコン通信「ピープル」から約90の電子会議室に投稿されたテキストを用意した．会議室・話題の種類そして投稿時期について特に恣意的な選択は行っていないが，結果としてはパソコン関連の話題が多く，テキスト量でみて約半分を占めている．電子会議室の投稿文は文ばかりではなく，文字を利用した表，絵などが多数含まれている他，他人の記述を引用する場合が多く，これらを含めてしまったのでは学習用コーパスとして不適切であることは明らかである．そこでルールベースでこれらをとり除くフィルターを作成した．主なルールとしては以下のようなものがある．引用記号（「＞＞」など）をもとに引用部分だと判断した行は除く．記号文字(「−」「＊」など）の一定以上の繰り返しを含む行は除く．フェースマーク（「:-)」など）のリストを作成し，それにマッチした箇所は特別な１個の記号に置き換え，未知語の扱いとする．このフィルターを通した後，句点に加え空白行，一定数以上の連続した空白を手がかりとして文を取り出し，形態素解析，セグメントシミュレータの処理を行った．以上の分割済みテキストの内，日経新聞，産経新聞，EDR，そして電子会議室について，95%以上のカバレージをもつ語彙を作成したところ，約44,000語の単語からなるセット(44K語彙)が得られた．このようにして得られた語彙は，人が日本語について単語単位だと感覚的に思うセットを示していると考えられる．たとえば「行う」という動詞とその後続の付属語列からはの計11単語が生成された．また「たい」や「べき」といった単語も生成されており，分割に揺れがある部分では複数の分割に対応した単語が得られることがわかる．

paragraph score: 1.02793687752462
