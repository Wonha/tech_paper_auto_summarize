\documentstyle[epsf,jnlpbbl,leqno,fleqn]{jnlp_j_b5}

 \makeatletter
 \newcommand{\namelistlabel}[1]{}
 \newenvironment{namelist}[1]{}{}
 \makeatother
 \setcounter{page}{9}
 \setcounter{巻数}{6}
 \setcounter{号数}{2}
 \setcounter{年}{1999}
 \setcounter{月}{1}
 \受付{1998}{4}{3}
 \再受付{1998}{7}{6}
 \採録{1998}{8}{10}
 \setcounter{secnumdepth}{2}
 \oddsidemargin=0cm
 \evensidemargin=0cm
 \topmargin=0cm
 \title{単語単位による日本語言語モデルの検討}
 \author{伊東 伸泰\affiref{IBM} \and 西村 雅史\affiref{IBM} \and 荻野 紫穂\affiref{IBM} \and 山崎 一孝\affiref{IBM}}
 \headauthor{伊東 伸泰, 西村 雅史, 荻野 紫穂, 山崎 一孝}
 \headtitle{単語単位による日本語言語モデルの検討}
 \affilabel{IBM}{日本アイ・ビー・エム 東京基礎研究所}
 {Tokyo Research Laboratory, IBM Japan, Ltd.}
 \jabstract{
 日本語では単語の境界があいまいで，活用等のルールに基づいて定義された単位である
 形態素は必ずしも人が認知している単語単位や発声単位と一致しない．
 本研究では音声認識への応用を目的として
 人が潜在意識的にもつ単語単位への分割モデルとその単位を用いた
 日本語の言語 ({\it N}-gram)モデルについて考察した．
 本研究で用いた単語分割モデルは分割確率が2形態素の遷移で決定される
 という仮定を置いたモデルで，人が単語境界と考える点で
 分割した比較的少量のテキストデータと形態素解析による分割結果とを
 照合することにより，パラメータの推定を行った．
 そして多量のテキストを同モデルにしたがって
 分割し，単語単位のセット（語彙）と言語モデルを構築した．
 新聞3誌とパソコン通信の投稿テキストを用いた実験によれば約44,000語で，
 出現した単位ののべ
 94-98\%がカバーでき，1文あたりの単位数は形態素に比べて12\%から19\%
 少なくなった．一方，
 新聞とパソコン通信ではモデルに差があるものの
 その差は単語分割モデル，言語モデル双方とも事象の異なりとして現れ，同一事象に対する
 確率の差は小さい．このため，
 新聞・電子会議室の両データから作成した言語モデルは
 その双方のタスクに対応可能であった．}
 \jkeywords{音声認識，ディクテーション，{\it N}-gram モデル，形態素解析}
 \etitle{A Word-based Japanese Language Model}
 \eauthor{Nobuyasu Itoh \affiref{IBM} \and Masafumi Nishimura \affiref{IBM} \and Shiho Ogino \affiref{IBM} \and Kazutaka Yamasaki \affiref{IBM}}
 \eabstract{
 This paper deals with a word-based language model of Japanese.
 In Japanese, word boundaries are not stable and grammatical units do not
 necessarily coincide with human intuition. For accurate segmentation
 it is therefore necessary to create a vocabulary set that covers human
 utterance units.
 In our word-segmentation method, a model of word boundary is described by morphological
 parameters (i.e. part of speech), which are learned by comparing
 results of human segmentation with those of Japanese morphological
 analyzer. Then by using pseudo-random number and the model,
 it is determined whether each morpheme transition is a word
 boundary. As a result, we obtain a vocabulary set and learning data
 for Japanese language model automatically. According to our experiments using
 articles from three newspaper and appended texts in network-based forums,
 about 44,000 words cover 94-98\% of all words in the test data, and the average numbers of
 words per sentence are 12-19\% smaller than those of morphemes.
 The parameters of word segmentation
 model and language model are quite different in newspaper articles and forum's texts.
 However,
 the difference does not exist in the probabilities of common events, but in the
 kinds of events. Therefore the language
 model, which was created from newspaper articles and forum's text, gave
 the satisfactory results for both test set.}
 \ekeywords{Speech recognition, Dictation, {\it N}-gram model, Morphological analysis}
 \begin{document}
 \maketitle

 \section{はじめに}
   音声認識技術はその発達にともなって，その適用分野を広げ，
 日本語においても新聞など一般の文章を認識対象とした研究が
 行なわれるようになった\cite{MATSUOKA,NISIMURA4}．
 この要因として，音素環境依存型HMMによる音響モデルの高精度化に加え，
 多量の言語コーパスが入手可能になった結果，文の出現確率
 を単語{\it N}個組の生起確率から推定する{\it N}-gramモデルが
 実現できるようになったことが挙げられる．
   日本語をはじ\break
 めとして単語の概念が明確ではない言語における音声認識を
 実現する場合，どのような単位を認識単位として採用する
 かが大きな問題の1つとなる．この問題はユーザーの発声単位
 に制約を課す離散発声の認識システムの場合に限らない．連続
 音声の認識においても，ユーザーが適\break
 時ポーズを置くことを許容
 しなければならないため，やはり発声単位を考慮して認識単位を
 決\break
 める必要がある．従来日本語を対象とした自然言語処理では
 形態素単位に分割することが一般\break
 的であり，またその解析ツールが比較的
 \mbox{よく整備されていたことから{\it N}-gramモデル作成におい}ても「形態素」を
 単位として採用したものがほとんどである\cite{MATSUOKA,ITOHK}．
 しかしながら，音声認識という立場からあらためてその処理単位に要請される
 条件を考えなおしてみると，以下のことが考えられる．
 \begin{itemize}
   \item 認識単位は発声単位と同じか，より細かい単位でなければならない．
 形態素はその本来の定義から言えば必ずこの条件を満たしているが，実際の
 形態素解析システムにおいては，複合名詞も１つの単位として登録することが
 普通であるし，解析上の都合から連続した付属語列
 のような長い単位も採用している場合が
 あるためこの要請が満たされているとは限らない．
   \item 長い認識単位を採用する方が，音響上の
 識別能力という観点からは望ましい．つまり連続して発声される
 可能性が高い部分については，それ自身を認識単位としてもっておく
 方がよい．
   \item 言語モデルを構築するためには，多量のテキストを認識単位に分割する
 必要があり，処理の多くが自動化できなければ実用的ではない．
 \end{itemize}
   これらは，言い換えれば人間が発声のさいに分割する(可能性がある)
 単位のMinimum Cover Set
 を求めることに帰着する．
 人が感覚的にある単位だと判断する
 \mbox{日本語トークンについて考}察した研究は
 過去にも存在する．原田\cite{HARADA}は人が文節という単位について一貫した概念を
 持っているかについて調査し，区切られた箇所の平均一致率が76\%であり
 付属語については多くの揺れがあったと報告している．また
 横田，藤崎\cite{YOKOTA}は人が
 短時間に認識できる文字数とその時間との関係から人の認知単位を求め，その
 単位を解析にも用いることを提案している．しかしながら，これらの研究はいずれも
 目的が異なり，音声認識を考慮したものではない．
 そこで，われわれは，
 人が潜在意識としてもつ単語単位を形態素レベルのパラメータで
 モデル化するとともに，そのモデルに基づいて文を分割，{\it N}-gramモデルを作成する
 手法を提案し，認識率の観点からみて有効であることを示した\cite{NISIMURA3}．
 本論文では主として言語処理上の観点からこの単語単位{\it N}-gramモデルを考察し，
 必要な語彙数，コーパスの量とパープレキシティの関係を明らかにする．
 とくに新聞よりも「話し言葉」に近いと考えられるパソコン通信の電子会議室から収集した
 文章を対象に加え，新聞との違いについて実験結果を述べる．
 \section{単語単位への分割}
   本節ではわれわれが採用した単語単位と，同単位への分割手法について述べる．
 \par
 日本語を分割して発声する場合，その分割点はきわめて安定
 している点と，人，または時によって分割されたりされなかったり
 する不安定な点がある．
 例として「私は計測器のテストを
 行っています．」という文を考えよう．これは形態素解析により，
 たとえば
 \[\hspace{-6mm}
   私\:+\:は\:+\:計測\:+\:器\:+\:の\:+\:テスト\:+\: \\
 を\:+\:行\:+\:っ\:+\:て\:+\:い\:+\:ます\:+\:．
 \]
 と分割されるが，動詞の活用語尾である「っ」や接続助詞の「て」
 はほぼ確実に「行」と結合して「行って」と発声されるのに対し，
 接辞である「器」は分割される場合もあれば，結合されること
 もあるだろう．そこで文がある位置で「分割」される
 確率を形態素のレベルでモデル
 化することを考える．そして人が分割した学習用テキストと同じテキストを
 形態素解析により分割した結果を照合し，各形態素の遷移ごとに
 当該点で分割される確率を得る．その後，
 より大量のテキストをそのモデルに基づいて分割すれば (このプログラムを以後
 セグメントシミュレータと呼ぶ)，人が
 分割した傾向をもったわかち書きテキストを容易に得られる．
 \par
 「分割」される位置としては，形態素の境界
 (形態素単位への分割)と
 \mbox{さらに細かく形態素の}途中 (文字単位への分割)がある．ここで
 分割記号として$\sharp$を使用し，\mbox{「分割」は記号「$\sharp$」が
 生起}し，「結合」は「NULL」が生起すると考えれば，前者は
 ある形態素から別の形態素に遷移したときにその間に「$\sharp$」
 が生起する確率として
 \[\hspace{-5mm}
   P(\sharp _i \mid Morpheme_i \rightarrow Morpheme_{i+1})
 \]
 となる．
 後者のそれは$Morpheme$を文字列 $C_1C_2,\ldots,C_n$で表すと，
 \mbox{その{\it j}番目の文字の後に$\sharp$}が
 生起する確率と考えれば
 \[\hspace{-5mm}
   P(\sharp _j \mid Morpheme, \: C_j \rightarrow C_{j+1})
 \]
 と表現できる．
 モデルのパラメータ (形態素の属性)としては，
 \mbox{品詞情報({\it KoW})，連接属性 (Part} of Speech: {\it PoS})，
 ，そして表記 ({\it String})を採用し，
 $(KoW[PoS], String)$と表現する．
 ここで品詞，連接属性
 とはわれわれの用いた形態素解析プログラム\cite{MARUYAMA}の出力
 として得られるもの
 であり，品詞は81，連接属性は119に分類されている
 \footnote{品詞情報は学校文法でいう品詞分類（「動詞」「助動詞」など）に
 相当するが，解析の都合上一般にその品詞であると認められていない
 形態素に当該品詞を割り
 当てている場合がある．その場合は，後の処理のため同じ品詞でも単に
 「助動詞」とするのではなく「助動詞A」
 のように区別しており，結果的に種類が増大している．また連接属性は品詞を
 活用型などによりさらに詳細分類したもので，たとえば動詞は17種類に分類されている．
 意味からすれば品詞情報を{\it PoS} (Part of Speech)とすべきであろうが，ここでは
 文献\cite{MARUYAMA}の記法にしたがった．}．
 したがって
 \mbox{形態素単位の分割}では6個，文字単位への分割では4個のパラメータで記述さ
 れることになるが，そうすると明らかに
 多量の学習用テキスト (人が分割したもの)が必要となる．
 そこで頻度が閾値以下であるような場合については，
 パラメータを特定の順序で縮退させた確率値を用意し
 セグメントシミュレータの実行時も，確率が記述されているレベルまで
 同様の順序で縮退し，当該確率値で代用することを考える．
 縮退の順序にはさまざまなものが考えられるが，モデルのパラメータについてその種類数
 を考えると表記，連接属性，品詞の順に少なくなることは明らかであり，縮退も
 それにしたがうのが妥当であろう．また基本的にはある出現回数を閾値としたとき
 より多くの種類の遷移確率が得られることが望ましい．このような観点から
 いくつかの予備実験を行い経験的に縮退順序を決定した．
 この順序と参照される確率値を木構造で表現したのが
 図\ref{FIG:STATTREE}である．
 各ノードには
 形態素の属性とその属性が満たされた場合に分割される確率
 が対応する．たとえば
 図\ref{FIG:STATTREE}中
 \[\hspace{-5mm}
   P(\sharp \mid V. \:infl.[29] \rightarrow Conj. \:p.p.[69],\: て)
 \]
 は形態素単位への分割に対する記述例で，
 形態素の属性が動詞活用語尾[29]から接続助詞[69]「て」へ
 遷移したときに，その間で分割される確率を意味する\footnote{
 {\it V. infl.}は Verb inflection， {\it Conj. p.p.}は Conjunctive
 post-positional particleの略．}．
 \begin{figure}[htb]
 \begin{center}
   \epsfile{file=signl96.fig2.ps,width=12cm}
   \caption{セグメントシミュレータにおけるパラメータ縮退の順序}
   \label{FIG:STATTREE}
 \end{center}
 \end{figure}
 \mbox{1つ上のレベルでは，表記（こ}こでは「て」）が省略される．
 ただし品詞が名詞の場合には文字数が分割確率を記述するパラメータと
 して有効と考えられるので\footnote{「誤認識」が「誤」「認識」と分割され
 るよりは「音声認識」が「音声」「認識」となりやすいなど．}，\mbox{表記を省
 略した場合，文字数をパラメータとして残し}た．
 さらに上位レベルでは，連接属性番号も省略し，\mbox{品詞{\it V. infl.}から
 {\it Conj. p.p.}への
 遷移に対}\mbox{して，人が分割する確率を記述する．}
 たとえば，「積んで」という文節を形態素に分割すると
 \[\hspace{-5mm}
   積 (Verb[8]) \:+\: ん(V. \:infl[30]) \:+\: で(Conj. \:p.p.[69])
 \]
 となるが，その中に現れる「ん」と「で」の間で分割された
 カウント等もマージした上で算出された確率
 となる．このように木はリーフから上位のノードに行くにしたがって
 縮退されたパラメータ，言い換えればより大まかなパラメータとなる．
 \par
   一方，前節で述べたように人は形態素として定義されたトークンをさらに
 文字単位で分割する場合もある．これは形態素解析の都合上
 連続した付属語列を1つの形態素としてとり扱うことが行なわれるためである．
 たとえばわれわれの用いた形態素
 解析用文法では「...かどうか」という付属語列が助詞として扱われているが
 「か」+「どうか」と分割されることもある．
 そこで形態素レベルの
 分割よりもさらに詳細なレベルとして，文字レベルの分割をモデル化した．
 このような確率木はつぎのように構成することができる．つまり
 もっとも細かい分類における各パラメータ
 について，人が分割した結果と形態素解析の結果を照合してカウントし，
 その値をリーフから上位ノードに伝搬させた後，確率値に正規化す
 ればよい．
 全カウント数が少ないと当該確率 (推定値)の信頼
 性が低いので，カウント，マージ作業を行なって，頻度がある閾値以上のノードを
 最終的なノードとして採用することにする．
 \par
 このモデル化では学習データの量に応
 じて，そのデータから得られる情報を最大限に利用することができる．
 たとえば，2文字漢語から接尾辞への遷移には，非常に多くのものがあるが，
 その分割されやすさは接尾辞の種類によって異り，それらを捨象してモデル
 化したのでは，あいまいさが大きくなってしまう．
 しかし逆にそのすべてを
 細分化したのでは，頻度が低い接尾辞に対するルールが得られないか，また
 は信頼性の低い確率推定値となってしまう．本手法によれば学習データ中に頻度が
 高いものについてはより細かい分類でモデル化され，頻度が下るにしたがっ
 て統計として信頼にたる単位まで縮退されたパラメータ
 による確率値が得られることになる．
 \section{形態素解析プログラムの変更}
 \subsection{現代語書き言葉以外の表現への文法の対応}
 形態素解析システムは，一般に
 新聞記事に代表される現代語書き言葉を処理できるように開発されてきた．
 しかし近年，データとして使用されるコーパスの大規模化に伴い，
 現代語書き言葉以外の表現，特に，会話風の表現（以下，口語体と示す）
 を扱う試みが増加してきた\cite{KURO}．
 われわれが従来使用してきた形態素解析の\mbox{文法規則\cite{MARUYAMA}}\mbox{も，
 原則として}現代語書き言葉に対応したもので，\mbox{口語体への対応は十分ではない．
 一方本研}究で用いる学習用テキストは新聞に限らず，パソコン通信
 の投稿テキストが含まれており，口語体への対応なくしては
 充分な精度の解析結果を得ることができない．
 以下の点を考慮して，より多様な文に対応できるよう形態素解析の文法を記述した．
 \begin{itemize}
 \item 元の文法に対する変更を少なくして派生的な影響を抑える．\\
 口語体によく現れる縮退形で，五段活用連用形に接続する「ちゃ」
 には，接続助詞「て」および係助詞「は」の連なり「ては」
 の縮退と（例: 書い{\bf ちゃ}いけない）と，
 接続助詞「て」および補助動詞「しまう」の語幹の連なり「てしま」の縮退
 （例: 書い{\bf ちゃ}う）とがある．
 前者は直後で文節を切ることができる非活用語，
 後者はワア行五段活用をするので，ワア行五段活用語尾が接続し，かつ
 直後で文節末に遷移できる「ちゃ」という形態素の規則を作成すれば
 形態素解析処理を行うことができる\cite{KURO}．
 しかし，品詞や活用形を単語分割モデルで利用すると，
 「ちゃ」に品詞として接続助詞を付与すれば「接続助詞にワア行五段活用語尾
 が接続する」という一般化が，また動詞を付与すれば「五段動詞語幹が
 文節末に遷移する」という一般化が行なわれかねない．
 これを避けるには，「ちゃ」に新たな品詞を付与するか，または
 「ちゃ」に二種類あるとするという対応が考えられるが
 われわれは後者の方法を採った．
 形態素解析としては前者が望ましいと思われるが，後の単語分割モデルに
 影響を及ぼす可能性がある場合は，元の文法規則への影響がより少ない
 ものを採用した．
 また，文語活用の残存形などで，現代語活用に全く同じ形があるものに
 ついては，現代語活用の形態素に接続条件を加えて対処した．
 \item 縮退形の品詞付与では元の形態素列のうち活用語尾や自立語がもつ品詞を優先する． \\
 形容詞仮定形活用語尾「けれ」および接続助詞「ば」の連なりの縮退である
 「きゃ」「けりゃ」の前連接属性は「けれ」，後連接属性は「ば」
 にほぼ等しい．こうした縮退形の品詞は，元の形態素列のもつ連接属性のうち
 活用語尾や自立語のものを優先して付与した\cite{OGINO}．
 \item 省略による空文字列は次形態素への遷移を追加して対処する．\\
 「勉強しよ」「読も」などのように形態素末が落ちる縮退の場合，
 前者は助動詞「よう」の縮退「よ」を定義すればよいが，後者は
 助動詞「う」そのものが脱落しているので，動詞未然形から
 「う」の次の形態素への遷移を追加して対処する．
 \end{itemize}
 \subsection{複合名詞の分割}
 形態素解析の辞書には，現在までの使用目的に応じて複合語が
 一語扱いで登録されていることが多いが，単語分割モデル構築の
 ための形態素解析としては短単位に分割されていた方が都合がよい．
 そこで，複合語の中でも特に多い複合名詞を
 分割対象として，分割データベースとヒューリスティック規則
 により，形態素解析で複合名詞分割を行なうことにした．
 複合名詞の分割データベースは，
 2カ月分の新聞記事（産経新聞）を形態素解析して
 \mbox{その結果から一定以上}の頻度で出現する3文字以上の名詞を抜き出した後，
 人手で，分割する位置の情報を付与することにより作成した．
 このデータベースには約25,000語の複合名詞が含まれている．
 ヒューリスティック規則は，以下の条件を満たすように作成した．
 \begin{itemize}
 \item 1語の名詞よりも2語以上の名詞連続のコストが小さい．\\
 名詞連続中では，2語のコストがもっとも小さく，次第にコストが
 \mbox{増大するように設定す}る．これは複合名詞を分割する際，
 あまり細かく切り過ぎないようにするためである．
 \item 1文字名詞は他の名詞に比べてコストが大きい． \\
 上記と同様，過分割を防ぐためである．
 \item 分割対象は3文字以上の複合名詞とする． \\
 1文字ずつに過分割しないためである．
 \item 未知語のコストは1語の名詞より大きい． 
 \end{itemize}
 \vspace*{3mm}
 また，分割の結果に3文字以上の名詞が含まれている場合は，
 再帰的にそれを分割し，分割が不可能になるまで繰り返す．
 \section{分割モデルの作成と分割過程}
 \vspace{-1mm}
 \subsection{分割確率の推定}
   分割ルールとその確率を推定するため，計17人の被験者
 \mbox{により，新聞5カ月分（日経新聞3}カ月および産経新聞2カ月）
 \mbox{，日本語用例集 (合計約26,000文)，そしてパソコン通信「ピープ}ル」
 の電子会議室（以下電子会議室）
 \mbox{から採取した文章 (約9,500文）を分割する作業を行った}
 \footnote{
 文選択は文の長さが一定の範囲に入っていることを除けば無作為に行なった．また
 被験者には
 1. 不自然にならない限り，より細かく分割すること
 2. 書かれた文章ではなく発声する場合の分割点を回答すること
 という指示を与えた．}．
 \par
 新聞や日本語用例集はいわゆる「書き言葉」のスタイルであるのに比較して電子会議室の文章は
 より口語体に近く，これらは分割モデルにも影響を与える可能性がある．そこで両者のデータは
 別々に取り扱って分割モデル (確率木)を構成した．その結果前者は2,829個，後者は2,269個
 のノードからなる木が得られた．表\ref{TBL:RULES}に一例を示す．
 ただしノードとして採用するか否かの閾値には当該ノードの出現回数 (カウント)
 を用い，その値は学習データ中の単語数に比例させた \footnote{新聞データの場合で50である．}．
 \begin{table}[t]
 \begin{center}
 \caption{生成された木に記述された分割確率の例 (新聞データから得られたもの)}
 \label{TBL:RULES}
 \begin{tabular}{lr} \hline
   パラメータ値                              & 分割確率 \\ \hline
   $名詞[19]\rightarrow 名詞[19],「者」$ & 0.33 \\
   $名詞[19]\rightarrow 名詞[19],「人」$ & 0.71 \\
   $名詞[19]\rightarrow 形容動詞[18],「的」$ &  0.36 \\
   $動詞活用語尾[29]\rightarrow 接続助詞[69],「て」$ & 0.03 \\
   $名詞[19]\rightarrow 格助詞[77],「を」$ & 1.0 \\ \hline
 \end{tabular}
 \end{center}
 \end{table}
 \begin{figure}[t]
 \begin{center}
   \epsfile{file=16.eps}
 \vspace{6mm}
   \caption{確率木のノード数}
   \label{FIG:CMPSTAT}
 \end{center}
 \end{figure}
   2つの確率木について得られたノードをいずれに含まれるかで分類し
 数を示したものが図\ref{FIG:CMPSTAT}である．
 得られたノードは，かなりの異なりがあることがわかる．\mbox{たとえば電子会議室}データから
 得られた確率木にのみ存在するノードの中で出現回数の多いものから上位3個
 \footnote{遷移後の表記$String$が縮退していないレベルのものに限った．}をあげると
 以下のようになる．
 \begin{tabbing}
   xxx \= xxxxxxxxxxxxxxxxx \= xxxxxx \= xxxxxxxxxxxxxxxxx \= \kill
   1. \> $接続助詞[69]$ \> $\rightarrow$ \> $活用語尾[31]「る」$   \\
   2. \> $助動詞[62]$   \> $\rightarrow$ \> $接続助詞[73]「が」$   \\
   3. \> $助動詞[48]$   \> $\rightarrow$ \> $接続助詞[73]「けど」$ \\
 \end{tabbing}
 \par
   これらの遷移を含む例文を上げると 1. 読ん+で+{\bf る}, 2. ...です+{\bf が}, 
 3. ...だ+{\bf けど}などであり，明らかに口語体特有の言い回しに伴う遷移が抽出されて
 いる．一方新聞データから学習し\break
 た確率木にのみ存在するノードをみると体言止め
 \mbox{に伴う遷移 ($サ変動名詞[13] \:\rightarrow \: 句点[100]「．」$} i.e.「...を議論 + ．」)
 や漢語の接辞 ($名詞[19] \:\rightarrow\: 接辞[19],「会」$)\mbox{など直感的にも電子会議室}等の
 文章では比較的頻度が低いと考えられるものが多かった．\mbox{また両方の確率木に共通して出}\mbox{現
 しているノード1,607個}について分割確率の\mbox{相関係数を求めたところ0.980となりきわめて高}\mbox{い．
 したがって共通するノードに}ついてはほとんど違いはなく，２つの確率木の違いはノードつまり
 ルールそのものに現れていることがわかった．
 \par
 これらのモデルに基づいて以下のように多量の
 (形態素解析された)テキストを分割・統合する．
 \begin{enumerate}
   \item 各形態素およびその遷移について，連接属性番号，品詞，形態素の表記を
 得て，確率木のリーフに記述があるかどうかを調べる．
   \item なければ，木作成の説明で述べた順にパラメータ値を
 縮退させ，確率木に記述があるかどうかを調べる．
   \begin{itemize}
      \item 記述があれば，0から1の範囲の乱数を発生し，その値が
 ノードに付随する確率以下であれば当該位置で分割し，そうでない場合は
 分割しない．
      \item 記述がなければ，縮退を繰り返す．
   \end{itemize}
   \item もっとも上位のノードにも該当しない場合，形態素の分割点であれ
 ば当該位置で分割し，それ以外は分割しない．
 \end{enumerate}
 なお{\it N}-gramモデル
 作成には，乱数による分割処理 (セグメントシミュレータ)は必ずしも必要では
 なく，形態素解析の結果と分割確率を使って\mbox{直接各{\it N}-gramの生起確率を
 推定することも可}能である．
 \subsection{単語カバレージ}
 \begin{figure}[htb]
 \begin{center}
   \epsfile{file=signl96.fig3.ps,width=14cm}
   \caption{日経新聞3ヵ月のテキストに対する単語数とカバレージ}
   \label{FIG:COVERAGE}
 \end{center}
 \end{figure}
 われわれの提案した単語単位に基づく語彙を作成するための予備実験として
 日経新聞3カ月分 (合計446,079文)を用い，
 前節の手続きを適用して分割，\mbox{連結を行う実験を行った．西村ら}
 の報告\cite{NISIMURA}によれば
 形態素を単位とした場合，
 \mbox{約97\%はおよそ3カ月分のテ}キストで収集できる（言い換えれば飽和する）
 ことがわかっている．
 \mbox{その結果を図\ref{FIG:COVERAGE}に示す．単}語は合計で約$10^{7}$個，
 のべ216,904種類の単語が生成された．図はそれらを頻度の高いのものから
 順にとった場合のカバレージを示している．ただし数字表現，姓名はカウン
 トから除いている．一方同じテキストから形態素は132,164個が生成された．
 これによれば単語単位を採用す\break
 ると，形態素よりはより多くの種類が必要では
 あるものの，決して発散するものではなく，た\break
 とえば
 上位約25,000個
 (種類)の単語で全トークンの約95\%がカバーでき，
 \mbox{取り扱いが可能な語}彙数であることがわかる．
 \par
 このとき確率木の各ノード（ルール）がどのような割合で使われたかを示したのが
 表\ref{TBL:USEDRULE}である．表から明らかなように全体の約60\%
 の場合には，一番詳細なレベルのルールが適用されていることがわかる．
 \begin{table}[htb]
 \begin{center}
 \caption{適用されたノードの比率(階層別)}
 \label{TBL:USEDRULE}
 \begin{tabular}{lr} \hline
   パラメータ値                                              & 
 比率 (\%)\\ \hline
   $P(\sharp \mid KoW_{1}[PoS_{1}] \rightarrow KoW_{2}[PoS_{2}], String)$     &
 59.6 \\
   $P(\sharp \mid KoW_{1}[PoS_{1}] \rightarrow KoW_{2}[PoS_{2}])$             &
 29.2 \\
   $P(\sharp \mid KoW_{1} \rightarrow KoW_{2})$     &  3.9 \\
   $P(\sharp \mid KoW_{2})$                         &  6.6 \\
   該当なし                            &  0.7 \\ \hline
 \end{tabular}
 \end{center}
 \end{table}
 \section{語彙とコーパス}
 \subsection{コーパスの前処理}
 用意したコーパスのソースは日経新聞（93年から96年），\mbox{産経新聞（92年10月から97年），}毎日新聞（91年
 と92年），EDRコーパス\cite{EDR}，\mbox{そしてパソコン通信「ピープル」に投}稿された電子会議室の記事である．
 ただし日経，産経の両紙は示した期間のすべてではなく，月単位で時期が
 重複しないように選択したサブセットである．
 新聞についてはその本文を句点単位で文として取り出し，前節で述べた処理を行った．
 ただし数字については形態素解析で１単語（品詞「数字」）として扱われてしまうので
 当該トークンをすべて桁付きの漢数字に変換した後，西村\cite{NISIMURA3}に記載された
 数字の読み上げ単位に合わせて分割した．すなわち整数については「十，百，千，万，億」を位と定義し
 先行する数字と位で１つの単位として取り扱い，小数点以下の位については１桁づつに分割する
 ．たとえば１２３４．５６は「千」「二百」「三十」「四」「・」「五」「六」と変換・分割される
 ことになる\footnote{電子会議室の文章では電話番号やID番号にともなう数字があり，これらは
 位付きで読むことに適さない．そこでルールでそれらに該当すると判断した場合は１桁づつに
 分割した．}．
 \par
 一方ディクテーションのアプリケーションや一般ユーザーが入力するであろう文，言い回し
 を考えると新聞だけでは明らかに不足である．
 そこでより口語体に近いデータとしてパソコン通信「ピープル」から
 約90の電子会議室に投稿されたテキストを用意した．会議室・話題の種類そして投稿時期について特に
 恣意的な選択は行っていないが，結果としてはパソコン関連の話題が多く，テキスト量でみて
 約半分を占めている．
 電子会議室の投稿文は文ばかりではなく，文字を利用した表，絵などが多数含まれている
 他，他人の記述を引用する場合が多く，これらを含めてしまったのでは学習用コーパスとして
 不適切であることは明らかである．そこでルールベースでこれらをとり除く
 フィルターを作成した．主なルールとしては以下のようなものがある．
 \begin{itemize}
    \item 引用記号（「＞＞」など）をもとに引用部分だと判断した行は除く．
    \item 記号文字(「−」「＊」など）の一定以上の繰り返しを含む行は除く．
    \item フェースマーク（「:-)」など）のリストを作成し，それにマッチした箇所は特別な１個の記号
 に置き換え，未知語の扱いとする．
 \end{itemize}
 このフィルターを通した後，句点に加え空白行，一定数以上の
 連続した空白を手がかりとして文を取り出し，形態素解析，
 セグメントシミュレータの処理を行った．
 \vspace{-2mm}
 \subsection{語彙の作成}
 \vspace{-1mm}
 以上の分割済みテキストの内，日経新聞，産経新聞，EDR，そして電子会議室について，
 95\%以上のカバレージをもつ語彙を作成したところ，約44,000語の単語からなるセット
 (44K語\break
 彙) が得られた．
   このようにして得られた語彙は，人が日本語について単語単位だと感覚的に思う
 セットを示していると考えられる．たとえば「行う」という動詞とその
 後続の付属語列からは
 \smallskip
 \begin{tabbing}
    xxx \= xxxxxxxxxxxxxxx \= xxxxxxxxxxxxxxx \= xxxxxxxxxxxxxxx \= \kill
        \> 行い            \> 行いたい        \> 行う            \\
        \> 行うべき        \> 行え            \> 行えば          \\
        \> 行える          \> 行った          \> 行ったら        \\
        \> 行って          \> 行っても        \>                 \\
 \end{tabbing}
 の計11単語が生成された．また「たい」や「べき」といった単語も生
 成されており，分割に揺れがある部分では複数の分割に対応した単語が
 得られることがわかる．
score of this paragraph is 14
