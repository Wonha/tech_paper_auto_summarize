本節ではわれわれが採用した単語単位と，同単位への分割手法について述べる．
日本語を分割して発声する場合，その分割点はきわめて安定している点と，人，または時によって分割されたりされなかったりする不安定な点がある．
例として「私は計測器のテストを行っています．
」という文を考えよう．
これは形態素解析により，たとえば
と分割されるが，動詞の活用語尾である「っ」や接続助詞の「て」はほぼ確実に「行」と結合して「行って」と発声されるのに対し，接辞である「器」は分割される場合もあれば，結合されることもあるだろう．
そこで文がある位置で「分割」される確率を形態素のレベルでモデル化することを考える．
そして人が分割した学習用テキストと同じテキストを形態素解析により分割した結果を照合し，各形態素の遷移ごとに当該点で分割される確率を得る．
その後，より大量のテキストをそのモデルに基づいて分割すれば(このプログラムを以後セグメントシミュレータと呼ぶ)，人が分割した傾向をもったわかち書きテキストを容易に得られる．
「分割」される位置としては，形態素の境界(形態素単位への分割)とさらに細かく形態素の途中(文字単位への分割)がある．
ここで分割記号として[MATH]を使用し，「分割」は記号「[MATH]」が生起し，「結合」は「NULL」が生起すると考えれば，前者はある形態素から別の形態素に遷移したときにその間に「[MATH]」が生起する確率として
となる．
後者のそれは[MATH]を文字列[MATH]で表すと，そのj番目の文字の後に[MATH]が生起する確率と考えれば
と表現できる．
モデルのパラメータ(形態素の属性)としては，品詞情報(KoW)，連接属性(Part of Speech: PoS)，，そして表記(String)を採用し，[MATH]と表現する．
ここで品詞，連接属性とはわれわれの用いた形態素解析プログラム[CITE]の出力として得られるものであり，品詞は81，連接属性は119に分類されている．
したがって形態素単位の分割では6個，文字単位への分割では4個のパラメータで記述されることになるが，そうすると明らかに多量の学習用テキスト(人が分割したもの)が必要となる．
そこで頻度が閾値以下であるような場合については，パラメータを特定の順序で縮退させた確率値を用意しセグメントシミュレータの実行時も，確率が記述されているレベルまで同様の順序で縮退し，当該確率値で代用することを考える．
縮退の順序にはさまざまなものが考えられるが，モデルのパラメータについてその種類数を考えると表記，連接属性，品詞の順に少なくなることは明らかであり，縮退もそれにしたがうのが妥当であろう．
また基本的にはある出現回数を閾値としたときより多くの種類の遷移確率が得られることが望ましい．
このような観点からいくつかの予備実験を行い経験的に縮退順序を決定した．
この順序と参照される確率値を木構造で表現したのが図[REF_FIG:STATTREE]である．
各ノードには形態素の属性とその属性が満たされた場合に分割される確率が対応する．
たとえば図[REF_FIG:STATTREE]中
は形態素単位への分割に対する記述例で，形態素の属性が動詞活用語尾[29]から接続助詞[69]「て」へ遷移したときに，その間で分割される確率を意味する．
1つ上のレベルでは，表記（ここでは「て」）が省略される．
ただし品詞が名詞の場合には文字数が分割確率を記述するパラメータとして有効と考えられるので，表記を省略した場合，文字数をパラメータとして残した．
さらに上位レベルでは，連接属性番号も省略し，品詞V. infl.からConj. p.p.への遷移に対して，人が分割する確率を記述する．
たとえば，「積んで」という文節を形態素に分割すると
となるが，その中に現れる「ん」と「で」の間で分割されたカウント等もマージした上で算出された確率となる．
このように木はリーフから上位のノードに行くにしたがって縮退されたパラメータ，言い換えればより大まかなパラメータとなる．
一方，前節で述べたように人は形態素として定義されたトークンをさらに文字単位で分割する場合もある．
これは形態素解析の都合上連続した付属語列を1つの形態素としてとり扱うことが行なわれるためである．
たとえばわれわれの用いた形態素解析用文法では「.
..
かどうか」という付属語列が助詞として扱われているが「か」+「どうか」と分割されることもある．
そこで形態素レベルの分割よりもさらに詳細なレベルとして，文字レベルの分割をモデル化した．
このような確率木はつぎのように構成することができる．
つまりもっとも細かい分類における各パラメータについて，人が分割した結果と形態素解析の結果を照合してカウントし，その値をリーフから上位ノードに伝搬させた後，確率値に正規化すればよい．
全カウント数が少ないと当該確率(推定値)の信頼性が低いので，カウント，マージ作業を行なって，頻度がある閾値以上のノードを最終的なノードとして採用することにする．
このモデル化では学習データの量に応じて，そのデータから得られる情報を最大限に利用することができる．
たとえば，2文字漢語から接尾辞への遷移には，非常に多くのものがあるが，その分割されやすさは接尾辞の種類によって異り，それらを捨象してモデル化したのでは，あいまいさが大きくなってしまう．
しかし逆にそのすべてを細分化したのでは，頻度が低い接尾辞に対するルールが得られないか，または信頼性の低い確率推定値となってしまう．
本手法によれば学習データ中に頻度が高いものについてはより細かい分類でモデル化され，頻度が下るにしたがって統計として信頼にたる単位まで縮退されたパラメータによる確率値が得られることになる．
形態素解析システムは，一般に新聞記事に代表される現代語書き言葉を処理できるように開発されてきた．
しかし近年，データとして使用されるコーパスの大規模化に伴い，現代語書き言葉以外の表現，特に，会話風の表現（以下，口語体と示す）を扱う試みが増加してきた[CITE]．
われわれが従来使用してきた形態素解析の文法規則[CITE]も，原則として現代語書き言葉に対応したもので，口語体への対応は十分ではない．
一方本研究で用いる学習用テキストは新聞に限らず，パソコン通信の投稿テキストが含まれており，口語体への対応なくしては充分な精度の解析結果を得ることができない．
以下の点を考慮して，より多様な文に対応できるよう形態素解析の文法を記述した．
元の文法に対する変更を少なくして派生的な影響を抑える．
口語体によく現れる縮退形で，五段活用連用形に接続する「ちゃ」には，接続助詞「て」および係助詞「は」の連なり「ては」の縮退と（例:書いちゃいけない）と，接続助詞「て」および補助動詞「しまう」の語幹の連なり「てしま」の縮退（例:書いちゃう）とがある．
前者は直後で文節を切ることができる非活用語，後者はワア行五段活用をするので，ワア行五段活用語尾が接続し，かつ直後で文節末に遷移できる「ちゃ」という形態素の規則を作成すれば形態素解析処理を行うことができる[CITE]．
しかし，品詞や活用形を単語分割モデルで利用すると，「ちゃ」に品詞として接続助詞を付与すれば「接続助詞にワア行五段活用語尾が接続する」という一般化が，また動詞を付与すれば「五段動詞語幹が文節末に遷移する」という一般化が行なわれかねない．
これを避けるには，「ちゃ」に新たな品詞を付与するか，または「ちゃ」に二種類あるとするという対応が考えられるがわれわれは後者の方法を採った．
形態素解析としては前者が望ましいと思われるが，後の単語分割モデルに影響を及ぼす可能性がある場合は，元の文法規則への影響がより少ないものを採用した．
また，文語活用の残存形などで，現代語活用に全く同じ形があるものについては，現代語活用の形態素に接続条件を加えて対処した．
縮退形の品詞付与では元の形態素列のうち活用語尾や自立語がもつ品詞を優先する．
形容詞仮定形活用語尾「けれ」および接続助詞「ば」の連なりの縮退である「きゃ」「けりゃ」の前連接属性は「けれ」，後連接属性は「ば」にほぼ等しい．
こうした縮退形の品詞は，元の形態素列のもつ連接属性のうち活用語尾や自立語のものを優先して付与した[CITE]．
省略による空文字列は次形態素への遷移を追加して対処する．
「勉強しよ」「読も」などのように形態素末が落ちる縮退の場合，前者は助動詞「よう」の縮退「よ」を定義すればよいが，後者は助動詞「う」そのものが脱落しているので，動詞未然形から「う」の次の形態素への遷移を追加して対処する．
形態素解析の辞書には，現在までの使用目的に応じて複合語が一語扱いで登録されていることが多いが，単語分割モデル構築のための形態素解析としては短単位に分割されていた方が都合がよい．
そこで，複合語の中でも特に多い複合名詞を分割対象として，分割データベースとヒューリスティック規則により，形態素解析で複合名詞分割を行なうことにした．
複合名詞の分割データベースは，2カ月分の新聞記事（産経新聞）を形態素解析してその結果から一定以上の頻度で出現する3文字以上の名詞を抜き出した後，人手で，分割する位置の情報を付与することにより作成した．
このデータベースには約25,000語の複合名詞が含まれている．
ヒューリスティック規則は，以下の条件を満たすように作成した．
1語の名詞よりも2語以上の名詞連続のコストが小さい．
名詞連続中では，2語のコストがもっとも小さく，次第にコストが増大するように設定する．
これは複合名詞を分割する際，あまり細かく切り過ぎないようにするためである．
1文字名詞は他の名詞に比べてコストが大きい．
上記と同様，過分割を防ぐためである．
分割対象は3文字以上の複合名詞とする．
1文字ずつに過分割しないためである．
未知語のコストは1語の名詞より大きい．
また，分割の結果に3文字以上の名詞が含まれている場合は，再帰的にそれを分割し，分割が不可能になるまで繰り返す．
分割ルールとその確率を推定するため，計17人の被験者により，新聞5カ月分（日経新聞3カ月および産経新聞2カ月），日本語用例集(合計約26,000文)，そしてパソコン通信「ピープル」の電子会議室（以下電子会議室）から採取した文章(約9,500文）を分割する作業を行った．
新聞や日本語用例集はいわゆる「書き言葉」のスタイルであるのに比較して電子会議室の文章はより口語体に近く，これらは分割モデルにも影響を与える可能性がある．
そこで両者のデータは別々に取り扱って分割モデル(確率木)を構成した．
その結果前者は2,829個，後者は2,269個のノードからなる木が得られた．
表[REF_TBL:RULES]に一例を示す．
ただしノードとして採用するか否かの閾値には当該ノードの出現回数(カウント)を用い，その値は学習データ中の単語数に比例させた．
2つの確率木について得られたノードをいずれに含まれるかで分類し数を示したものが図[REF_FIG:CMPSTAT]である．
得られたノードは，かなりの異なりがあることがわかる．
たとえば電子会議室データから得られた確率木にのみ存在するノードの中で出現回数の多いものから上位3個をあげると以下のようになる．
これらの遷移を含む例文を上げると1.読ん+で+る, 2. .
..
です+が, 3. .
..
だ+けどなどであり，明らかに口語体特有の言い回しに伴う遷移が抽出されている．
一方新聞データから学習した確率木にのみ存在するノードをみると体言止めに伴う遷移([MATH] i.e.「.
..
を議論+．
」)や漢語の接辞([MATH])など直感的にも電子会議室等の文章では比較的頻度が低いと考えられるものが多かった．
また両方の確率木に共通して出現しているノード1,607個について分割確率の相関係数を求めたところ0.980となりきわめて高い．
したがって共通するノードについてはほとんど違いはなく，２つの確率木の違いはノードつまりルールそのものに現れていることがわかった．
これらのモデルに基づいて以下のように多量の(形態素解析された)テキストを分割・統合する．
各形態素およびその遷移について，連接属性番号，品詞，形態素の表記を得て，確率木のリーフに記述があるかどうかを調べる．
なければ，木作成の説明で述べた順にパラメータ値を縮退させ，確率木に記述があるかどうかを調べる．
記述があれば，0から1の範囲の乱数を発生し，その値がノードに付随する確率以下であれば当該位置で分割し，そうでない場合は分割しない．
記述がなければ，縮退を繰り返す．
もっとも上位のノードにも該当しない場合，形態素の分割点であれば当該位置で分割し，それ以外は分割しない．
なおN-gramモデル作成には，乱数による分割処理(セグメントシミュレータ)は必ずしも必要ではなく，形態素解析の結果と分割確率を使って直接各N-gramの生起確率を推定することも可能である．
われわれの提案した単語単位に基づく語彙を作成するための予備実験として日経新聞3カ月分(合計446,079文)を用い，前節の手続きを適用して分割，連結を行う実験を行った．
西村らの報告[CITE]によれば形態素を単位とした場合，約97%はおよそ3カ月分のテキストで収集できる（言い換えれば飽和する）ことがわかっている．
その結果を図[REF_FIG:COVERAGE]に示す．
単語は合計で約[MATH]個，のべ216,904種類の単語が生成された．
図はそれらを頻度の高いのものから順にとった場合のカバレージを示している．
ただし数字表現，姓名はカウントから除いている．
一方同じテキストから形態素は132,164個が生成された．
これによれば単語単位を採用すると，形態素よりはより多くの種類が必要ではあるものの，決して発散するものではなく，たとえば上位約25,000個(種類)の単語で全トークンの約95%がカバーでき，取り扱いが可能な語彙数であることがわかる．
このとき確率木の各ノード（ルール）がどのような割合で使われたかを示したのが表[REF_TBL:USEDRULE]である．
表から明らかなように全体の約60%の場合には，一番詳細なレベルのルールが適用されていることがわかる．
用意したコーパスのソースは日経新聞（93年から96年），産経新聞（92年10月から97年），毎日新聞（91年と92年），EDRコーパス[CITE]，そしてパソコン通信「ピープル」に投稿された電子会議室の記事である．
ただし日経，産経の両紙は示した期間のすべてではなく，月単位で時期が重複しないように選択したサブセットである．
新聞についてはその本文を句点単位で文として取り出し，前節で述べた処理を行った．
ただし数字については形態素解析で１単語（品詞「数字」）として扱われてしまうので当該トークンをすべて桁付きの漢数字に変換した後，西村[CITE]に記載された数字の読み上げ単位に合わせて分割した．
すなわち整数については「十，百，千，万，億」を位と定義し先行する数字と位で１つの単位として取り扱い，小数点以下の位については１桁づつに分割する．
たとえば１２３４．
５６は「千」「二百」「三十」「四」「・」「五」「六」と変換・分割されることになる．
一方ディクテーションのアプリケーションや一般ユーザーが入力するであろう文，言い回しを考えると新聞だけでは明らかに不足である．
そこでより口語体に近いデータとしてパソコン通信「ピープル」から約90の電子会議室に投稿されたテキストを用意した．
会議室・話題の種類そして投稿時期について特に恣意的な選択は行っていないが，結果としてはパソコン関連の話題が多く，テキスト量でみて約半分を占めている．
電子会議室の投稿文は文ばかりではなく，文字を利用した表，絵などが多数含まれている他，他人の記述を引用する場合が多く，これらを含めてしまったのでは学習用コーパスとして不適切であることは明らかである．
そこでルールベースでこれらをとり除くフィルターを作成した．
主なルールとしては以下のようなものがある．
引用記号（「＞＞」など）をもとに引用部分だと判断した行は除く．
記号文字(「−」「＊」など）の一定以上の繰り返しを含む行は除く．
フェースマーク（「:-)」など）のリストを作成し，それにマッチした箇所は特別な１個の記号に置き換え，未知語の扱いとする．
このフィルターを通した後，句点に加え空白行，一定数以上の連続した空白を手がかりとして文を取り出し，形態素解析，セグメントシミュレータの処理を行った．
以上の分割済みテキストの内，日経新聞，産経新聞，EDR，そして電子会議室について，95%以上のカバレージをもつ語彙を作成したところ，約44,000語の単語からなるセット(44K語彙)が得られた．
このようにして得られた語彙は，人が日本語について単語単位だと感覚的に思うセットを示していると考えられる．
たとえば「行う」という動詞とその後続の付属語列からは
の計11単語が生成された．
また「たい」や「べき」といった単語も生成されており，分割に揺れがある部分では複数の分割に対応した単語が得られることがわかる．
前節の結果得られた各文は局所的に見ると記号ばかりであったり，姓名の列挙部分であったりして学習コーパスには適さないものが含まれている．
また電子会議室のテキストはフィルターのルールでカバーしきれなかった部分で単語ではないトークンが無視できない程度に生じていた．
このような文については人手で採用するかどうかを決める，あるいは当該部分を除くことが望ましいが，多量のコーパスについてそのような作業を行うのは不可能なため，ここでは以下の条件のいずれかに当てはまる文は採用しないことにした．
２単語以下から構成される文
文の単語数に対する記号の数が一定以上の文
44K語彙に対して未知語の数が一定以上の割合で含まれる文
音声認識用のコーパスにおいて句読点や括弧表現をどのように取り扱うべきかについてはさまざまな議論がある．
松岡ら[CITE]はカギ括弧以外の括弧（（）【】など）について内容ごと削除しており，伊藤ら[CITE]は括弧の用いられ方（引用，強調など）に応じて削除すべきかどうかを自動判別している．
括弧による表現には確かに読み上げに適さないものも含まれているが，本研究では文章入力手段としての音声認識システムの構築を重視し，これらを削除しないことにした．
また同じ理由で句読点も削除していない．
その結果得られた文の数をソース別に示す（表[REF_TBL:SOURCE]）．
前節にしたがって単語単位に分割されたテキストを学習データとしてN-gramモデルを学習するわけであるが，生起確率の計算上考慮すべきこととして数字，時刻などとくに各単語に確率上の差をつけるべき理由がないもの，および意味がまったく同じでありながら表記の異なる揺らぎが生じているものの取り扱いがある．
前者については各単語をクラスにまとめて確率を計算することにし，合計36クラス作成した．
後者は新聞の場合，用語統一がなされているため影響は少ないと考えられるが，電子会議室のテキストでは「コンピュータ」と「コンピューター」，「組み合わせ」と「組合せ」といった単語は両者とも多数含まれており明らかに無視できない．
そこで44K語彙について「読み」をもとに同義語の候補を抽出した上でチェックを行い，約1,800エントリの別名リストを作成した．
N-gramをカウントするさい，このリストを参照して１つの表記に統一した上で学習を行っている．
一方，テストデータとして新聞3種類，電子会議室のテキストを別に用意し，被験者(単語分割モデルの学習データを作成した被験者とは異なる）により分割を行なった．
テストデータのそれぞれについて文数，形態素数，単語数，そして44K語彙のカバレージを表[REF_TBL:TESTDATA]に示す．
この表から１文あたりの単語数は形態素数に比較して12-19%程度少なくなることがわかる．
本実験の目的は
単語を単位としたN-gramモデルの有効性，コーパスの必要量を評価する．
新聞と電子会議室において単語N-gramモデルから見た違いを明らかにする．
の２点である．
そこで新聞，電子会議室のそれぞれについてその種類，時期の違いを捨象するため，全学習データを文単位でシャッフルした上で８個に分割したサブセット(新聞: N-1,.
.,8，電子会議室F-1,.
..
,8）を作成した．
そして各サブセットをさらに95%と5%の比率で分割し前者をN-gramカウント，後者をHeld-out補間のパラメータ学習用に用いた．
まず新聞について学習データ(N-1,.
.,8)を順に増加させながら言語モデルを作成し，各モデルをテストセットパープレキシティで評価した．
ただし学習データに１回でも出現したN-gram (trigramまで）はすべて使用しており，また未知語部分については予測を行っていない．
結果を図[REF_FIG:PERPNEWS]に示す（電子会議室に関するデータは「Forum」と表記している）．
予想されるようにいずれのテストデータでも学習コーパスの増加にともなってパープレキシティは緩やかに改善されるが次第に飽和する傾向がみてとれ，いずれの場合も学習データセットを7個から8個に増やしたときのパープレキシティの改善率は1-2%程度でしかない．
パープレキシティの絶対値には相当の差があり，新聞といってもひとくくりにできないことは明らかだが，その値(100-170)は音響識別上対応可能な値であると考えられる[CITE]．
一方電子会議室のテストデータはもっとも良いケースでも400以上のパープレキシティを示しており新聞の学習データだけでは対応できていないことがわかる．
われわれの目的は新聞にとどまらず，より口語体に近い電子会議室に投稿される文にも対応できる言語モデルを作成することである．
そこで新聞データすべてを使用した言語モデルをベースとし，電子会議室の学習データ(F-1,.
.,8)を加えていくことにより各テストデータのパープレキシティがどのようになるかを評価した．
結果を図[REF_FIG:PERPFORUM]に示す．
この結果，電子会議室についてそのパープレキシティは改善される一方，使用したデータ量の範囲(約25M単語)では，新聞に対する影響はほとんどなかった．
一方電子会議室のみから作成した言語モデルで（電子会議室の）テストデータを評価すると152.1であり，若干の差は見られるものの，混合学習データから作成した言語モデルは新聞・電子会議室の双方に対応できることがわかる．
これは双方の統計的異なりが共通しているN-gramの確率が相違しているというよりも，N-gramの種類に，より大きく現れていることを示唆している．
一方コーパスのサイズと結果として得られたモデルのサイズ，すなわちN-gramの異なり数の関係を見たのが図[REF_FIG:NGRAM]である．
これは新聞データ(N-1,.
.,8)の場合であるが，bigram，trigramとも飽和する傾向は見てとれない．
電子会議室テキストを加えた場合も同様でN-1,.
..
,8，F-1,.
..
,8すべてを学習データに使用した場合のN-gram数はtrigramが31M個，bigramが5.6M個に達した．
とくにtrigramは学習データサイズの増分に対しほとんど比例して増加している．
今後主記憶，外部記憶の容量がさらに増加するとしてもこのN-gram数（異なり）のままでは，実装することが難しい．
そこでN-gramの中で低頻度のものを除くことが，パープレキシティにどのような影響を与えるかを検証する実験を行った．
結果を図[REF_FIG:SMALLLM]に示す．
N-gramの異なりの多くを占めるのはtrigramなので，学習データはN-1,.
..
,8，F-1,.
..
,8すべてを使用した上で，言語モデルを作成するときtrigramの最低出現回数を設定することにより，モデルのサイズを変更している．
図からtrigramの異なり数が5M個以下になるとパープレキシティが急速に悪くなる傾向が見てとれるが，一方モデルサイズを1/3〜1/5にした程度ではパープレキシティの差は小さいことがわかる．
本節ではわれわれが採用した単語単位と，同単位への分割手法について述べる．
日本語を分割して発声する場合，その分割点はきわめて安定している点と，人，または時によって分割されたりされなかったりする不安定な点がある．
例として「私は計測器のテストを行っています．
」という文を考えよう．
これは形態素解析により，たとえば
と分割されるが，動詞の活用語尾である「っ」や接続助詞の「て」はほぼ確実に「行」と結合して「行って」と発声されるのに対し，接辞である「器」は分割される場合もあれば，結合されることもあるだろう．
そこで文がある位置で「分割」される確率を形態素のレベルでモデル化することを考える．
そして人が分割した学習用テキストと同じテキストを形態素解析により分割した結果を照合し，各形態素の遷移ごとに当該点で分割される確率を得る．
その後，より大量のテキストをそのモデルに基づいて分割すれば(このプログラムを以後セグメントシミュレータと呼ぶ)，人が分割した傾向をもったわかち書きテキストを容易に得られる．
「分割」される位置としては，形態素の境界(形態素単位への分割)とさらに細かく形態素の途中(文字単位への分割)がある．
ここで分割記号として[MATH]を使用し，「分割」は記号「[MATH]」が生起し，「結合」は「NULL」が生起すると考えれば，前者はある形態素から別の形態素に遷移したときにその間に「[MATH]」が生起する確率として
となる．
後者のそれは[MATH]を文字列[MATH]で表すと，そのj番目の文字の後に[MATH]が生起する確率と考えれば
と表現できる．
モデルのパラメータ(形態素の属性)としては，品詞情報(KoW)，連接属性(Part of Speech: PoS)，，そして表記(String)を採用し，[MATH]と表現する．
ここで品詞，連接属性とはわれわれの用いた形態素解析プログラム[CITE]の出力として得られるものであり，品詞は81，連接属性は119に分類されている．
したがって形態素単位の分割では6個，文字単位への分割では4個のパラメータで記述されることになるが，そうすると明らかに多量の学習用テキスト(人が分割したもの)が必要となる．
そこで頻度が閾値以下であるような場合については，パラメータを特定の順序で縮退させた確率値を用意しセグメントシミュレータの実行時も，確率が記述されているレベルまで同様の順序で縮退し，当該確率値で代用することを考える．
縮退の順序にはさまざまなものが考えられるが，モデルのパラメータについてその種類数を考えると表記，連接属性，品詞の順に少なくなることは明らかであり，縮退もそれにしたがうのが妥当であろう．
また基本的にはある出現回数を閾値としたときより多くの種類の遷移確率が得られることが望ましい．
このような観点からいくつかの予備実験を行い経験的に縮退順序を決定した．
この順序と参照される確率値を木構造で表現したのが図[REF_FIG:STATTREE]である．
各ノードには形態素の属性とその属性が満たされた場合に分割される確率が対応する．
たとえば図[REF_FIG:STATTREE]中
は形態素単位への分割に対する記述例で，形態素の属性が動詞活用語尾[29]から接続助詞[69]「て」へ遷移したときに，その間で分割される確率を意味する．
1つ上のレベルでは，表記（ここでは「て」）が省略される．
ただし品詞が名詞の場合には文字数が分割確率を記述するパラメータとして有効と考えられるので，表記を省略した場合，文字数をパラメータとして残した．
さらに上位レベルでは，連接属性番号も省略し，品詞V. infl.からConj. p.p.への遷移に対して，人が分割する確率を記述する．
たとえば，「積んで」という文節を形態素に分割すると
となるが，その中に現れる「ん」と「で」の間で分割されたカウント等もマージした上で算出された確率となる．
このように木はリーフから上位のノードに行くにしたがって縮退されたパラメータ，言い換えればより大まかなパラメータとなる．
一方，前節で述べたように人は形態素として定義されたトークンをさらに文字単位で分割する場合もある．
これは形態素解析の都合上連続した付属語列を1つの形態素としてとり扱うことが行なわれるためである．
たとえばわれわれの用いた形態素解析用文法では「.
..
かどうか」という付属語列が助詞として扱われているが「か」+「どうか」と分割されることもある．
そこで形態素レベルの分割よりもさらに詳細なレベルとして，文字レベルの分割をモデル化した．
このような確率木はつぎのように構成することができる．
つまりもっとも細かい分類における各パラメータについて，人が分割した結果と形態素解析の結果を照合してカウントし，その値をリーフから上位ノードに伝搬させた後，確率値に正規化すればよい．
全カウント数が少ないと当該確率(推定値)の信頼性が低いので，カウント，マージ作業を行なって，頻度がある閾値以上のノードを最終的なノードとして採用することにする．
このモデル化では学習データの量に応じて，そのデータから得られる情報を最大限に利用することができる．
たとえば，2文字漢語から接尾辞への遷移には，非常に多くのものがあるが，その分割されやすさは接尾辞の種類によって異り，それらを捨象してモデル化したのでは，あいまいさが大きくなってしまう．
しかし逆にそのすべてを細分化したのでは，頻度が低い接尾辞に対するルールが得られないか，または信頼性の低い確率推定値となってしまう．
本手法によれば学習データ中に頻度が高いものについてはより細かい分類でモデル化され，頻度が下るにしたがって統計として信頼にたる単位まで縮退されたパラメータによる確率値が得られることになる．
形態素解析システムは，一般に新聞記事に代表される現代語書き言葉を処理できるように開発されてきた．
しかし近年，データとして使用されるコーパスの大規模化に伴い，現代語書き言葉以外の表現，特に，会話風の表現（以下，口語体と示す）を扱う試みが増加してきた[CITE]．
われわれが従来使用してきた形態素解析の文法規則[CITE]も，原則として現代語書き言葉に対応したもので，口語体への対応は十分ではない．
一方本研究で用いる学習用テキストは新聞に限らず，パソコン通信の投稿テキストが含まれており，口語体への対応なくしては充分な精度の解析結果を得ることができない．
以下の点を考慮して，より多様な文に対応できるよう形態素解析の文法を記述した．
元の文法に対する変更を少なくして派生的な影響を抑える．
口語体によく現れる縮退形で，五段活用連用形に接続する「ちゃ」には，接続助詞「て」および係助詞「は」の連なり「ては」の縮退と（例:書いちゃいけない）と，接続助詞「て」および補助動詞「しまう」の語幹の連なり「てしま」の縮退（例:書いちゃう）とがある．
前者は直後で文節を切ることができる非活用語，後者はワア行五段活用をするので，ワア行五段活用語尾が接続し，かつ直後で文節末に遷移できる「ちゃ」という形態素の規則を作成すれば形態素解析処理を行うことができる[CITE]．
しかし，品詞や活用形を単語分割モデルで利用すると，「ちゃ」に品詞として接続助詞を付与すれば「接続助詞にワア行五段活用語尾が接続する」という一般化が，また動詞を付与すれば「五段動詞語幹が文節末に遷移する」という一般化が行なわれかねない．
これを避けるには，「ちゃ」に新たな品詞を付与するか，または「ちゃ」に二種類あるとするという対応が考えられるがわれわれは後者の方法を採った．
形態素解析としては前者が望ましいと思われるが，後の単語分割モデルに影響を及ぼす可能性がある場合は，元の文法規則への影響がより少ないものを採用した．
また，文語活用の残存形などで，現代語活用に全く同じ形があるものについては，現代語活用の形態素に接続条件を加えて対処した．
縮退形の品詞付与では元の形態素列のうち活用語尾や自立語がもつ品詞を優先する．
形容詞仮定形活用語尾「けれ」および接続助詞「ば」の連なりの縮退である「きゃ」「けりゃ」の前連接属性は「けれ」，後連接属性は「ば」にほぼ等しい．
こうした縮退形の品詞は，元の形態素列のもつ連接属性のうち活用語尾や自立語のものを優先して付与した[CITE]．
省略による空文字列は次形態素への遷移を追加して対処する．
「勉強しよ」「読も」などのように形態素末が落ちる縮退の場合，前者は助動詞「よう」の縮退「よ」を定義すればよいが，後者は助動詞「う」そのものが脱落しているので，動詞未然形から「う」の次の形態素への遷移を追加して対処する．
形態素解析の辞書には，現在までの使用目的に応じて複合語が一語扱いで登録されていることが多いが，単語分割モデル構築のための形態素解析としては短単位に分割されていた方が都合がよい．
そこで，複合語の中でも特に多い複合名詞を分割対象として，分割データベースとヒューリスティック規則により，形態素解析で複合名詞分割を行なうことにした．
複合名詞の分割データベースは，2カ月分の新聞記事（産経新聞）を形態素解析してその結果から一定以上の頻度で出現する3文字以上の名詞を抜き出した後，人手で，分割する位置の情報を付与することにより作成した．
このデータベースには約25,000語の複合名詞が含まれている．
ヒューリスティック規則は，以下の条件を満たすように作成した．
1語の名詞よりも2語以上の名詞連続のコストが小さい．
名詞連続中では，2語のコストがもっとも小さく，次第にコストが増大するように設定する．
これは複合名詞を分割する際，あまり細かく切り過ぎないようにするためである．
1文字名詞は他の名詞に比べてコストが大きい．
上記と同様，過分割を防ぐためである．
分割対象は3文字以上の複合名詞とする．
1文字ずつに過分割しないためである．
未知語のコストは1語の名詞より大きい．
また，分割の結果に3文字以上の名詞が含まれている場合は，再帰的にそれを分割し，分割が不可能になるまで繰り返す．
分割ルールとその確率を推定するため，計17人の被験者により，新聞5カ月分（日経新聞3カ月および産経新聞2カ月），日本語用例集(合計約26,000文)，そしてパソコン通信「ピープル」の電子会議室（以下電子会議室）から採取した文章(約9,500文）を分割する作業を行った．
新聞や日本語用例集はいわゆる「書き言葉」のスタイルであるのに比較して電子会議室の文章はより口語体に近く，これらは分割モデルにも影響を与える可能性がある．
そこで両者のデータは別々に取り扱って分割モデル(確率木)を構成した．
その結果前者は2,829個，後者は2,269個のノードからなる木が得られた．
表[REF_TBL:RULES]に一例を示す．
ただしノードとして採用するか否かの閾値には当該ノードの出現回数(カウント)を用い，その値は学習データ中の単語数に比例させた．
2つの確率木について得られたノードをいずれに含まれるかで分類し数を示したものが図[REF_FIG:CMPSTAT]である．
得られたノードは，かなりの異なりがあることがわかる．
たとえば電子会議室データから得られた確率木にのみ存在するノードの中で出現回数の多いものから上位3個をあげると以下のようになる．
これらの遷移を含む例文を上げると1.読ん+で+る, 2. .
..
です+が, 3. .
..
だ+けどなどであり，明らかに口語体特有の言い回しに伴う遷移が抽出されている．
一方新聞データから学習した確率木にのみ存在するノードをみると体言止めに伴う遷移([MATH] i.e.「.
..
を議論+．
」)や漢語の接辞([MATH])など直感的にも電子会議室等の文章では比較的頻度が低いと考えられるものが多かった．
また両方の確率木に共通して出現しているノード1,607個について分割確率の相関係数を求めたところ0.980となりきわめて高い．
したがって共通するノードについてはほとんど違いはなく，２つの確率木の違いはノードつまりルールそのものに現れていることがわかった．
これらのモデルに基づいて以下のように多量の(形態素解析された)テキストを分割・統合する．
各形態素およびその遷移について，連接属性番号，品詞，形態素の表記を得て，確率木のリーフに記述があるかどうかを調べる．
なければ，木作成の説明で述べた順にパラメータ値を縮退させ，確率木に記述があるかどうかを調べる．
記述があれば，0から1の範囲の乱数を発生し，その値がノードに付随する確率以下であれば当該位置で分割し，そうでない場合は分割しない．
記述がなければ，縮退を繰り返す．
もっとも上位のノードにも該当しない場合，形態素の分割点であれば当該位置で分割し，それ以外は分割しない．
なおN-gramモデル作成には，乱数による分割処理(セグメントシミュレータ)は必ずしも必要ではなく，形態素解析の結果と分割確率を使って直接各N-gramの生起確率を推定することも可能である．
われわれの提案した単語単位に基づく語彙を作成するための予備実験として日経新聞3カ月分(合計446,079文)を用い，前節の手続きを適用して分割，連結を行う実験を行った．
西村らの報告[CITE]によれば形態素を単位とした場合，約97%はおよそ3カ月分のテキストで収集できる（言い換えれば飽和する）ことがわかっている．
その結果を図[REF_FIG:COVERAGE]に示す．
単語は合計で約[MATH]個，のべ216,904種類の単語が生成された．
図はそれらを頻度の高いのものから順にとった場合のカバレージを示している．
ただし数字表現，姓名はカウントから除いている．
一方同じテキストから形態素は132,164個が生成された．
これによれば単語単位を採用すると，形態素よりはより多くの種類が必要ではあるものの，決して発散するものではなく，たとえば上位約25,000個(種類)の単語で全トークンの約95%がカバーでき，取り扱いが可能な語彙数であることがわかる．
このとき確率木の各ノード（ルール）がどのような割合で使われたかを示したのが表[REF_TBL:USEDRULE]である．
表から明らかなように全体の約60%の場合には，一番詳細なレベルのルールが適用されていることがわかる．
用意したコーパスのソースは日経新聞（93年から96年），産経新聞（92年10月から97年），毎日新聞（91年と92年），EDRコーパス[CITE]，そしてパソコン通信「ピープル」に投稿された電子会議室の記事である．
ただし日経，産経の両紙は示した期間のすべてではなく，月単位で時期が重複しないように選択したサブセットである．
新聞についてはその本文を句点単位で文として取り出し，前節で述べた処理を行った．
ただし数字については形態素解析で１単語（品詞「数字」）として扱われてしまうので当該トークンをすべて桁付きの漢数字に変換した後，西村[CITE]に記載された数字の読み上げ単位に合わせて分割した．
すなわち整数については「十，百，千，万，億」を位と定義し先行する数字と位で１つの単位として取り扱い，小数点以下の位については１桁づつに分割する．
たとえば１２３４．
５６は「千」「二百」「三十」「四」「・」「五」「六」と変換・分割されることになる．
一方ディクテーションのアプリケーションや一般ユーザーが入力するであろう文，言い回しを考えると新聞だけでは明らかに不足である．
そこでより口語体に近いデータとしてパソコン通信「ピープル」から約90の電子会議室に投稿されたテキストを用意した．
会議室・話題の種類そして投稿時期について特に恣意的な選択は行っていないが，結果としてはパソコン関連の話題が多く，テキスト量でみて約半分を占めている．
電子会議室の投稿文は文ばかりではなく，文字を利用した表，絵などが多数含まれている他，他人の記述を引用する場合が多く，これらを含めてしまったのでは学習用コーパスとして不適切であることは明らかである．
そこでルールベースでこれらをとり除くフィルターを作成した．
主なルールとしては以下のようなものがある．
引用記号（「＞＞」など）をもとに引用部分だと判断した行は除く．
記号文字(「−」「＊」など）の一定以上の繰り返しを含む行は除く．
フェースマーク（「:-)」など）のリストを作成し，それにマッチした箇所は特別な１個の記号に置き換え，未知語の扱いとする．
このフィルターを通した後，句点に加え空白行，一定数以上の連続した空白を手がかりとして文を取り出し，形態素解析，セグメントシミュレータの処理を行った．
以上の分割済みテキストの内，日経新聞，産経新聞，EDR，そして電子会議室について，95%以上のカバレージをもつ語彙を作成したところ，約44,000語の単語からなるセット(44K語彙)が得られた．
このようにして得られた語彙は，人が日本語について単語単位だと感覚的に思うセットを示していると考えられる．
たとえば「行う」という動詞とその後続の付属語列からは
の計11単語が生成された．
また「たい」や「べき」といった単語も生成されており，分割に揺れがある部分では複数の分割に対応した単語が得られることがわかる．
前節の結果得られた各文は局所的に見ると記号ばかりであったり，姓名の列挙部分であったりして学習コーパスには適さないものが含まれている．
また電子会議室のテキストはフィルターのルールでカバーしきれなかった部分で単語ではないトークンが無視できない程度に生じていた．
このような文については人手で採用するかどうかを決める，あるいは当該部分を除くことが望ましいが，多量のコーパスについてそのような作業を行うのは不可能なため，ここでは以下の条件のいずれかに当てはまる文は採用しないことにした．
２単語以下から構成される文
文の単語数に対する記号の数が一定以上の文
44K語彙に対して未知語の数が一定以上の割合で含まれる文
音声認識用のコーパスにおいて句読点や括弧表現をどのように取り扱うべきかについてはさまざまな議論がある．
松岡ら[CITE]はカギ括弧以外の括弧（（）【】など）について内容ごと削除しており，伊藤ら[CITE]は括弧の用いられ方（引用，強調など）に応じて削除すべきかどうかを自動判別している．
括弧による表現には確かに読み上げに適さないものも含まれているが，本研究では文章入力手段としての音声認識システムの構築を重視し，これらを削除しないことにした．
また同じ理由で句読点も削除していない．
その結果得られた文の数をソース別に示す（表[REF_TBL:SOURCE]）．
前節にしたがって単語単位に分割されたテキストを学習データとしてN-gramモデルを学習するわけであるが，生起確率の計算上考慮すべきこととして数字，時刻などとくに各単語に確率上の差をつけるべき理由がないもの，および意味がまったく同じでありながら表記の異なる揺らぎが生じているものの取り扱いがある．
前者については各単語をクラスにまとめて確率を計算することにし，合計36クラス作成した．
後者は新聞の場合，用語統一がなされているため影響は少ないと考えられるが，電子会議室のテキストでは「コンピュータ」と「コンピューター」，「組み合わせ」と「組合せ」といった単語は両者とも多数含まれており明らかに無視できない．
そこで44K語彙について「読み」をもとに同義語の候補を抽出した上でチェックを行い，約1,800エントリの別名リストを作成した．
N-gramをカウントするさい，このリストを参照して１つの表記に統一した上で学習を行っている．
一方，テストデータとして新聞3種類，電子会議室のテキストを別に用意し，被験者(単語分割モデルの学習データを作成した被験者とは異なる）により分割を行なった．
テストデータのそれぞれについて文数，形態素数，単語数，そして44K語彙のカバレージを表[REF_TBL:TESTDATA]に示す．
この表から１文あたりの単語数は形態素数に比較して12-19%程度少なくなることがわかる．
本実験の目的は
単語を単位としたN-gramモデルの有効性，コーパスの必要量を評価する．
新聞と電子会議室において単語N-gramモデルから見た違いを明らかにする．
の２点である．
そこで新聞，電子会議室のそれぞれについてその種類，時期の違いを捨象するため，全学習データを文単位でシャッフルした上で８個に分割したサブセット(新聞: N-1,.
.,8，電子会議室F-1,.
..
,8）を作成した．
そして各サブセットをさらに95%と5%の比率で分割し前者をN-gramカウント，後者をHeld-out補間のパラメータ学習用に用いた．
まず新聞について学習データ(N-1,.
.,8)を順に増加させながら言語モデルを作成し，各モデルをテストセットパープレキシティで評価した．
ただし学習データに１回でも出現したN-gram (trigramまで）はすべて使用しており，また未知語部分については予測を行っていない．
結果を図[REF_FIG:PERPNEWS]に示す（電子会議室に関するデータは「Forum」と表記している）．
予想されるようにいずれのテストデータでも学習コーパスの増加にともなってパープレキシティは緩やかに改善されるが次第に飽和する傾向がみてとれ，いずれの場合も学習データセットを7個から8個に増やしたときのパープレキシティの改善率は1-2%程度でしかない．
パープレキシティの絶対値には相当の差があり，新聞といってもひとくくりにできないことは明らかだが，その値(100-170)は音響識別上対応可能な値であると考えられる[CITE]．
一方電子会議室のテストデータはもっとも良いケースでも400以上のパープレキシティを示しており新聞の学習データだけでは対応できていないことがわかる．
われわれの目的は新聞にとどまらず，より口語体に近い電子会議室に投稿される文にも対応できる言語モデルを作成することである．
そこで新聞データすべてを使用した言語モデルをベースとし，電子会議室の学習データ(F-1,.
.,8)を加えていくことにより各テストデータのパープレキシティがどのようになるかを評価した．
結果を図[REF_FIG:PERPFORUM]に示す．
この結果，電子会議室についてそのパープレキシティは改善される一方，使用したデータ量の範囲(約25M単語)では，新聞に対する影響はほとんどなかった．
一方電子会議室のみから作成した言語モデルで（電子会議室の）テストデータを評価すると152.1であり，若干の差は見られるものの，混合学習データから作成した言語モデルは新聞・電子会議室の双方に対応できることがわかる．
これは双方の統計的異なりが共通しているN-gramの確率が相違しているというよりも，N-gramの種類に，より大きく現れていることを示唆している．
一方コーパスのサイズと結果として得られたモデルのサイズ，すなわちN-gramの異なり数の関係を見たのが図[REF_FIG:NGRAM]である．
これは新聞データ(N-1,.
.,8)の場合であるが，bigram，trigramとも飽和する傾向は見てとれない．
電子会議室テキストを加えた場合も同様でN-1,.
..
,8，F-1,.
..
,8すべてを学習データに使用した場合のN-gram数はtrigramが31M個，bigramが5.6M個に達した．
とくにtrigramは学習データサイズの増分に対しほとんど比例して増加している．
今後主記憶，外部記憶の容量がさらに増加するとしてもこのN-gram数（異なり）のままでは，実装することが難しい．
そこでN-gramの中で低頻度のものを除くことが，パープレキシティにどのような影響を与えるかを検証する実験を行った．
結果を図[REF_FIG:SMALLLM]に示す．
N-gramの異なりの多くを占めるのはtrigramなので，学習データはN-1,.
..
,8，F-1,.
..
,8すべてを使用した上で，言語モデルを作成するときtrigramの最低出現回数を設定することにより，モデルのサイズを変更している．
図からtrigramの異なり数が5M個以下になるとパープレキシティが急速に悪くなる傾向が見てとれるが，一方モデルサイズを1/3〜1/5にした程度ではパープレキシティの差は小さいことがわかる．
