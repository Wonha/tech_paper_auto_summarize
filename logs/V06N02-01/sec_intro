音声認識技術はその発達にともなって，その適用分野を広げ，日本語においても新聞など一般の文章を認識対象とした研究が行なわれるようになった[CITE]．
この要因として，音素環境依存型HMMによる音響モデルの高精度化に加え，多量の言語コーパスが入手可能になった結果，文の出現確率を単語N個組の生起確率から推定するN-gramモデルが実現できるようになったことが挙げられる．
日本語をはじめとして単語の概念が明確ではない言語における音声認識を実現する場合，どのような単位を認識単位として採用するかが大きな問題の1つとなる．
この問題はユーザーの発声単位に制約を課す離散発声の認識システムの場合に限らない．
連続音声の認識においても，ユーザーが適時ポーズを置くことを許容しなければならないため，やはり発声単位を考慮して認識単位を決める必要がある．
従来日本語を対象とした自然言語処理では形態素単位に分割することが一般的であり，またその解析ツールが比較的よく整備されていたことからN-gramモデル作成においても「形態素」を単位として採用したものがほとんどである[CITE]．
しかしながら，音声認識という立場からあらためてその処理単位に要請される条件を考えなおしてみると，以下のことが考えられる．
認識単位は発声単位と同じか，より細かい単位でなければならない．
形態素はその本来の定義から言えば必ずこの条件を満たしているが，実際の形態素解析システムにおいては，複合名詞も１つの単位として登録することが普通であるし，解析上の都合から連続した付属語列のような長い単位も採用している場合があるためこの要請が満たされているとは限らない．
長い認識単位を採用する方が，音響上の識別能力という観点からは望ましい．
つまり連続して発声される可能性が高い部分については，それ自身を認識単位としてもっておく方がよい．
言語モデルを構築するためには，多量のテキストを認識単位に分割する必要があり，処理の多くが自動化できなければ実用的ではない．
これらは，言い換えれば人間が発声のさいに分割する(可能性がある)単位のMinimum Cover Setを求めることに帰着する．
人が感覚的にある単位だと判断する日本語トークンについて考察した研究は過去にも存在する．
原田[CITE]は人が文節という単位について一貫した概念を持っているかについて調査し，区切られた箇所の平均一致率が76%であり付属語については多くの揺れがあったと報告している．
また横田，藤崎[CITE]は人が短時間に認識できる文字数とその時間との関係から人の認知単位を求め，その単位を解析にも用いることを提案している．
しかしながら，これらの研究はいずれも目的が異なり，音声認識を考慮したものではない．
そこで，われわれは，人が潜在意識としてもつ単語単位を形態素レベルのパラメータでモデル化するとともに，そのモデルに基づいて文を分割，N-gramモデルを作成する手法を提案し，認識率の観点からみて有効であることを示した[CITE]．
本論文では主として言語処理上の観点からこの単語単位N-gramモデルを考察し，必要な語彙数，コーパスの量とパープレキシティの関係を明らかにする．
とくに新聞よりも「話し言葉」に近いと考えられるパソコン通信の電子会議室から収集した文章を対象に加え，新聞との違いについて実験結果を述べる．
音声認識技術はその発達にともなって，その適用分野を広げ，日本語においても新聞など一般の文章を認識対象とした研究が行なわれるようになった[CITE]．
この要因として，音素環境依存型HMMによる音響モデルの高精度化に加え，多量の言語コーパスが入手可能になった結果，文の出現確率を単語N個組の生起確率から推定するN-gramモデルが実現できるようになったことが挙げられる．
日本語をはじめとして単語の概念が明確ではない言語における音声認識を実現する場合，どのような単位を認識単位として採用するかが大きな問題の1つとなる．
この問題はユーザーの発声単位に制約を課す離散発声の認識システムの場合に限らない．
連続音声の認識においても，ユーザーが適時ポーズを置くことを許容しなければならないため，やはり発声単位を考慮して認識単位を決める必要がある．
従来日本語を対象とした自然言語処理では形態素単位に分割することが一般的であり，またその解析ツールが比較的よく整備されていたことからN-gramモデル作成においても「形態素」を単位として採用したものがほとんどである[CITE]．
しかしながら，音声認識という立場からあらためてその処理単位に要請される条件を考えなおしてみると，以下のことが考えられる．
認識単位は発声単位と同じか，より細かい単位でなければならない．
形態素はその本来の定義から言えば必ずこの条件を満たしているが，実際の形態素解析システムにおいては，複合名詞も１つの単位として登録することが普通であるし，解析上の都合から連続した付属語列のような長い単位も採用している場合があるためこの要請が満たされているとは限らない．
長い認識単位を採用する方が，音響上の識別能力という観点からは望ましい．
つまり連続して発声される可能性が高い部分については，それ自身を認識単位としてもっておく方がよい．
言語モデルを構築するためには，多量のテキストを認識単位に分割する必要があり，処理の多くが自動化できなければ実用的ではない．
これらは，言い換えれば人間が発声のさいに分割する(可能性がある)単位のMinimum Cover Setを求めることに帰着する．
人が感覚的にある単位だと判断する日本語トークンについて考察した研究は過去にも存在する．
原田[CITE]は人が文節という単位について一貫した概念を持っているかについて調査し，区切られた箇所の平均一致率が76%であり付属語については多くの揺れがあったと報告している．
また横田，藤崎[CITE]は人が短時間に認識できる文字数とその時間との関係から人の認知単位を求め，その単位を解析にも用いることを提案している．
しかしながら，これらの研究はいずれも目的が異なり，音声認識を考慮したものではない．
そこで，われわれは，人が潜在意識としてもつ単語単位を形態素レベルのパラメータでモデル化するとともに，そのモデルに基づいて文を分割，N-gramモデルを作成する手法を提案し，認識率の観点からみて有効であることを示した[CITE]．
本論文では主として言語処理上の観点からこの単語単位N-gramモデルを考察し，必要な語彙数，コーパスの量とパープレキシティの関係を明らかにする．
とくに新聞よりも「話し言葉」に近いと考えられるパソコン通信の電子会議室から収集した文章を対象に加え，新聞との違いについて実験結果を述べる．
