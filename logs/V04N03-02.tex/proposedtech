\documentstyle[jnlpbbl]{jnlp_j}

\setcounter{page}{27}
\setcounter{巻数}{4}
\setcounter{号数}{3}
\setcounter{年}{1997}
\setcounter{月}{7}
\受付{1996}{8}{29}
\再受付{1996}{10}{30}
\再々受付{1997}{1}{17}
\採録{1997}{3}{27}

\setcounter{secnumdepth}{2}

\title{シソーラス上に動的に構成される標本空間における\\
動詞の多義性解消}
\author{内山 将夫\affiref{U} \and 板橋 秀一\affiref{I}}

\headtitle{シソーラス上に動的に構成される標本空間における動詞の多義性解消}
\headauthor{内山 将夫・板橋 秀一}

\affilabel{U}{筑波大学 大学院 工学研究科}{Doctoral Program in Engineering, University of Tsukuba (現在，信州大学工学部電気電子工学科, Department of Electrical and Electronic Engineering, Faculty of Engineering, Shinshu University)}
\affilabel{I}{筑波大学 電子・情報工学系}{Institute of Information Sciences and Electronics, University of Tsukuba}

\jabstract{本稿では，語義の尤度パラメータの標本空間を，シソーラスに沿っ
て動的に拡張することにより，動詞の多義性を解消する手法を提案する．提案
手法では，尤度1位の語義と2位の語義とを比較し，尤度差が統計的に有意なら
ば，1位の語義を選ぶ．有意でなければ，シソーラスに沿って標本空間を一段
拡張し，多義性解消を試みる．もし，最大の標本空間でも尤度差が有意でなけ
れば，語義は判定しない．本稿の実験では，EDR日本語コーパスから頻度500以
上の動詞74語を抽出し，延べで約89,000の動詞について多義性を解消した．こ
のとき，最頻の語義を常に選ぶ場合の適合率は0.65，判定率は1.00であった．
ただし，判定率とは，多義性の解消を試みたなかで，実際に語義が判定された
割合である．クラスベースの手法と提案手法とを比較すると，分類語彙表を利
用した場合には，適合率は共に0.71であったが，判定率は，クラスベースの手
法が0.68，提案手法が0.73であった．EDR概念体系を利用した場合には，適合
率は共に0.70であったが，判定率は，クラスベースの手法が0.76，提案手法が
0.87であった．両者の判定率を比べると，提案手法の方が統計的に有意に高く，
その有効性が示された．}

\jkeywords{統計的な自然言語処理，事例に基づく自然言語処理，シソーラスの利用，動詞の多義性解消}

\etitle{Verb Sense Disambiguation in Dynamically\\
Constructed Sample Spaces on a Thesaurus}

\eauthor{Masao Utiyama \affiref{U} \and Shuichi Itahashi \affiref{I}}

\eabstract{This paper proposes a method which disambiguates verb
senses using co-occurrence-based likelihood parameters whose sample
spaces are extended according to a thesaurus. The method selects the
most plausible sense if its likelihood is significantly greater than
that of the second most plausible one. If not, the sample space is
extended and the significance test is tried again. If it cannot be
extended anymore, the system gives up disambiguation.  The method was
applied to 74 polysemous verbs (about 89,000 instances) extracted from
the EDR Japanese Corpus.  When the most frequent sense was selected,
the precision was 0.65 and the applicability, i.e. the ratio of the
disambiguated verbs to the treated verbs, was 1.00. The proposed
method was compared with a class-based method.  With {\it
Bunruigoihyou\/}, the precisions of both the methods were 0.71, but
the applicabilities of the proposed method and the class-based method
were 0.73 and 0.68, respectively. With the EDR Concept Classification
Dictionary, the precisions of both the methods were 0.70, but the
applicabilities of the proposed method and the class-based method were
0.87 and 0.76, respectively. The applicability of the proposed method
is significantly higher than that of the class-based method, which
shows the plausibility of the proposed method.  }

\ekeywords{statistically-based NLP, example-based NLP, use of thesaurus, verb sense disambiguation}

\begin{document}
\maketitle
\section{動詞の多義性の解消法}
\label{sec:model}

提案手法では，シソーラスに沿って段階的に入力単語を抽象化し，それぞれの
段階で動詞語義の尤度を計算する．動詞語義の尤度は尤度パラメータ(確率変
数)の値なので，シソーラスに沿った段階的な抽象化は，尤度パラメータの標
本空間(定義域)を，シソーラスに沿って段階的に拡張することで実現する．

このように，提案手法では，尤度パラメータの標本空間は可変なのであるが，
説明の順番としては，まず，固定された標本空間での多義性の解消法について
述べたあとで，可変の標本空間での多義性の解消法について述べ，最後に，
\cite{Dagan94}の手法を変形した手法について述べる．

本章で述べる方法は，一つの係り受け関係において動詞語義を決定する方法で
ある．複数の係り受け関係がある場合には，関係ごとに尤度1位の語義と2位の
語義との尤度差の信頼下限を得て，その値が最大の係り受け関係に従って動詞
の語義を決める．ただし，全ての関係において尤度差の信頼下限が閾値以下で
ある場合には，語義は判定されない．

\subsection{固定された標本空間における動詞の多義性解消}
\label{sec:solid}

関係$r$ にある単語$W$と動詞$V$とが与えられ，それぞれの語義集合が
$W=\{w_1,w_2,\ldots\}$，$V=\{v_1,v_2,\ldots\}$であるとき，語義$v_i$の
尤度パラメータ$F_i$の標本空間を定義する．まず，語義$w_h$と$v_i$とが関
係$r$で共起することを$r(w_h,v_i)$で表し，その共起頻度を$n(r(w_h,v_i))$とする．
このとき，単語$W$と語義$v_i$の共起頻度
$n_i = n(r(W,v_i)) = \sum_{w_h \in W} n(r(w_h,v_i))$に基づいて語義
$v_i$の尤度を決める．$n(r(W,v_i))$は，標本空間が
$\{r(W,v_1),r(W,v_2),\ldots\}$である確率変数$N(r(W,v_i))$の観測値であ
る．$N(r(W,v_i))$の標本空間は$F_i$の標本空間でもある．

本稿では，この標本空間における共起頻度の分布が一般化超幾何分布
\footnote{ある母集団が$k$ 種類の個体からなるとき，それぞれの種類の個体
数を$N_1,N_2,...,N_k$とする$(N=N_1 + \cdots + N_k)$．$n$個の個体を非復
元抽出したとき，それぞれの種類の個体が$n_1,n_2,...,n_k(n=n_1 + \cdots +
n_k)$だけ選ばれる確率は，一般化超幾何分布$ h(n_1 \cdots n_k|N_1 \cdots
N_k) = \left(
      \begin{array}{l}
        N_1 \\
        n_1
      \end{array}
    \right)
    \cdots
    \left(
      \begin{array}{l}
        N_k \\
        n_k
      \end{array}
    \right)
    \left(
      \begin{array}{l}
        N \\
        n
      \end{array}
    \right)^{-1} $で表される．なお，$k=2$の場合が超幾何分布である．}に
従うと仮定する．つまり，標本空間を
$\{r(W,v_1),r(W,v_2),\ldots,r(W,v_k)\}$としたとき，$W$と$v_i$との共起
頻度を表す確率変数$N(r(W,v_i))$の値には$0 \le N(r(W,v_i)) \le N_i$とい
う制限があり，$N_1 + N_2 + \cdots + N_k = N$ であるとする．

このとき，語義$v_i$の尤度パラメータ$F_i$を以下のように定義する．
\begin{equation}
  \label{D1}
  F_i = \frac{N_i - n_i}{N - n}.
\end{equation}
ただし，$n = n_1 + n_2 + \cdots + n_k$．すると，$F_i$の期待値，分散，
および，$F_i$と$F_j$の共分散は以下の通りである(付録A参照)．
\begin{eqnarray}
  \label{D2}
  \langle F_i \rangle & = &(n_i + 1)/(n + k), \\
  \label{D3}
  var(F_i) & = & p_i(1-p_i)/(n + k + 1), \\
  \label{D4}
  cov(F_i,F_j) & = & -p_i p_j /(n + k + 1).
\end{eqnarray}
ただし，$p_i = \langle F_i \rangle$である．

動詞の語義を判定するかしないかは，$D = F_1 - F_2$の信頼下限($Pl$)に基
づいて決める．ここで，$\langle D \rangle = \langle F_1 \rangle -
\langle F_2 \rangle$，$var(D) = var(F_1) + var(F_2) - 2 cov(F_1,F_2)$
である．ただし，動詞の語義を適当に並べかえて，$i \ge j$ ならば $n_i
\ge n_j$であるようにする．

推定精度を左右する$\alpha$と閾値$\theta$を適当に選んで，$Pl(v_1) =
\langle D \rangle - \alpha \sqrt{var(D)} > \theta$である場合には$v_1$
を語義とする．そうでない場合には関係$r$においては語義を判定しない．な
お，$\alpha$と$\theta$の値は\ref{sec:experiment} 章で述べる．

複数の関係がある場合には，前述のように，最大の$Pl$である関係(信頼下限
最大の関係)に基づいて語義の判定を行う．これは，\ref{sec:variable} 節と
\ref{sec:dagan} 節で述べる手法についても同様である．なお，以後，特に断
わらない限り，信頼下限とは，尤度1位の語義と2位の語義との尤度差($D$)の
信頼下限($Pl$)のことである．

\paragraph{例}

「初めて理由を聞いた」における「聞く」の多義性を解消する．「聞く」の語
義としては，「音を耳に感じとる(HEAR)」と「質問する(ASK)」とを考える．
なお，以下では，$F_i$という表記の代りに$F(HEAR)$や$F(ASK)$という表記を
用いる．また，共起頻度はEDR日本語コーパス\cite{EDR95}の一部における共
起頻度である．

「初めて理由を聞いた」には，「副詞(初めて，聞く)」と「を(理由，聞く)」
という二つの係り受け関係\footnote{本稿での係り受け関係の種類は
\ref{sec:data} 節で述べる．}があるので．それぞれについて信頼下限を求め
ると以下のようになる．まず，「初めて」は「聞く」との共起回数は 1 回で，
HEAR と共起している．このとき，$\alpha = 1$とすると，(\ref{D2})式から
$\langle F(HEAR) \rangle = (1 + 1)/(1 + 2) \simeq 0.67$，$\langle
F(ASK) \rangle = (0 + 1)/(1 + 2) \simeq 0.33$である．分散は(\ref{D3})
式から$var(F(HEAR)) = var(F(ASK)) \simeq 0.056 $であり，共分散は
(\ref{D4})式から$cov(F(HEAR),F(ASK)) \simeq - 0.056$である．以上より，
$\langle D \rangle = \langle F(HEAR) \rangle - \langle F(ASK) \rangle
\simeq 0.33$，かつ，$var(D) = var(F(HEAR)) + var(F(ASK)) - 2
cov(F(HEAR),F(ASK)) \simeq 0.22$，$\sqrt{var(D)} \simeq 0.47$である．
よって，$Pl(HEAR) = \langle D \rangle - \sqrt{var(D)} \simeq - 0.14$で
ある．次に，「理由」は関係「を」では「聞く」との共起回数は 5 回で，
HEAR と 0 回，ASK と 5 回共起している．よって，上と同様な計算により，
$Pl(ASK) \simeq 0.47$となる．

「聞く」の語義は，$\theta = 0$とすると，以下のように決まる．まず，「副
詞(初めて，聞く)」では，$Pl(HEAR) \simeq - 0.14 \le \theta$であるので，
語義は判定されない．一方，「を(理由，聞く)」では，$Pl(ASK) \simeq 0.47
> \theta$であるので，ASKが語義として選択される．語義が判定された関係は
「を」のみであるので，全体では ASK が語義として選択される．

この例では，信頼下限最大の係り受け関係に従って語義を判定した結果が成功
している．失敗する例については次節で述べる．

\vspace{\baselineskip}

本節で述べた手法は，単語と動詞語義との共起頻度に基づいて動詞の多義性を
解消する手法であるが，この手法は，容易にクラスベースの手法に拡張できる．
すなわち，あるクラスが与えられたときには，そのクラスと動詞語義との共起
頻度に基づいて動詞の多義性を解消すればよい．このとき，クラスと動詞語義
との共起頻度を得るには，そのクラスに属する語義の全てについて動詞語義と
の共起頻度を得て，それらの和をとればよい．これは，\ref{sec:dagan} 節で
述べる手法についても同様である．

\subsection{可変の標本空間における動詞の多義性解消}
\label{sec:variable}

前節で述べた手法は，標本空間$\{r\} \times \{w_1,w_2,\ldots\} \times
\{v_1,v_2,\dots\}$を縮小した標本空間$\{r(W,v_1),r(W,v_2),\ldots\}$にお
ける共起頻度の分布についての手法である．ここでは，標本空間をシソーラス
に沿って拡張することを考える．標本空間を段階的に拡張し，各段階において
信頼下限を求め，信頼下限が閾値より大となった時点で語義を判定し，判定の
プロセスを終える．

以下では，まず，標本空間の拡張の仕方について述べ，次に，信頼下限の求め
方について述べる．最後に例を示す．

\subsubsection{標本空間の拡張の仕方}

ここで考える標本空間は$\{r\} \times U_i \times \{v_1,v_2,\ldots\}$であ
る．$U_i$は，単語$W$の語義集合$W=\{w_1,w_2,\ldots\}$を，シソーラス
\footnote {本稿では，シソーラスとは，一つの根を有するDAG(Directed
Acyclic Graph)であるとする．シソーラスの節点のうちで，根は，それに接続
する枝の終点となることはなく，かつ，根からは全ての節点に対して有向道が
ある．また，そこから出ていく枝がないような節点を葉と呼ぶ．さらに，ある
節点の支配下の節点とは，その節点から到達できる節点である．}の構造に従っ
て拡張したものである．$U_i$は$U_{ij}$の和集合として定義されるので，
$U_{ij}$を定義してから，$U_i$を定義する．まず，$U_{ij}$は，根から$w_j$
までの道上の節点において，根からの距離が$i$にある節点が支配する葉の集
合\footnote{任意の単語の任意の語義は葉で表現されると仮定する．この場合
には各々の語義は互いに支配関係にない．分類語彙表\cite{Kokken64}とEDR概
念体系とを，\ref{sec:experiment} 章では，実験に用いるのであるが，分類語
彙表の場合には，この仮定が成立する．しかし，EDR概念体系の場合には，語
義にあたる概念が葉であるとは限らないため，その語義にあたる節点が別の語
義にあたる節点を支配している場合がある．その場合には，ある節点における
葉の数が，その節点が支配する語義の数と一致しない．そのため(\ref{U2a})
式や(\ref{U2b})式において考慮されない語義がでる．本稿ではこの問題は無
視し，全ての語義が葉に相当するとして尤度を計算した．}として定義される．
このとき，根から$w_j$までの距離を$l_j$とすると，
\begin{equation}
  \label{U_0j}
  U_{0j} \supseteq U_{1j} \supseteq \cdots \supseteq U_{l_j j} = \{w_j\}
\end{equation}
である．なお，$k>l_j$のときには$U_{kj}=\phi$である．次に，$U_i$を以下の
ように定義する．
\begin{equation}
  \label{U_i}
  U_i = \bigcup_{w_j \in W} U_{ij}.
\end{equation}
$i \le j$のときには，(\ref{U_0j})式と同様に，$U_i \supseteq U_j$が成立
する．このとき，標本空間は，$l=\max_{w_j \in W} l_j$とすると，$U_l$か
ら順に，$U_{l-1}, U_{l-2}, \ldots, U_0$と拡張される．

たとえば，図\ref{fig:U_i} で$W = \{w_4,w_5\}$とすると，$U_0 =
\{w_1,w_2,w_3,w_4,w_5,w_6\}$, $U_1 = U_2 = \{w_4,w_5,w_6\}$, $U_3 =
\{w_5\}$である．このとき，標本空間は，$U_3, U_2, U_1, U_0$の順に拡張さ
れる．

複数の親を持つ節点の場合には，根からの距離として複数の道の中で最長のも
のを選択すれば，$i \le j$のときに$U_i \supseteq U_j$となる．たとえば，
根$a$から葉$e$までの二つの道が，$a \rightarrow b \rightarrow c
\rightarrow d \rightarrow e$と$a \rightarrow b \rightarrow f
\rightarrow e$であるとき，それぞれの節点の根からの距離は，
$a=0,b=1,c=2,d=3,e=4,f=2$，とする．

標本空間の拡張の仕方は他にも考えられるが，DAGを対象とする場合には，上
述の方法が簡明であると考える．

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(88,64)
  \end{center}
  \caption{$U_i$の例}
  \label{fig:U_i}
\end{figure}

\subsubsection{信頼下限の求め方}

関係$r$ にある単語$W$と動詞$V$について，それぞれの語義集合を
$W=\{w_1,w_2,\ldots\}$, $V=\{v_1,v_2,\ldots\}$とする．ここで考える標本
空間は，$I = \{r\} \times U_i \times V$である．

語義$v_j$の尤度パラメータ$F(W^\prime,v_j|I)$は，$W^\prime = W \cap
U_i$とすると，次のように定義される．
\begin{eqnarray}
  \label{U1}
  F(W^\prime,v_j|I) &= &F(v_j | I) F(W^\prime|v_j,I) \nonumber \\
  & = & F(v_j | I) \sum_{w \in W^\prime} F(w | v_j,I).
\end{eqnarray}
$F(W^\prime,v_j|I)$は，図\ref{fig:I} に示されるような標本空間の構造，す
なわち，まず，動詞の語義を選び，次に，その語義のもとで単語の語義を選ぶ
という構造を反映している．

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(108,72)
  \end{center}
  \caption{標本空間$I$の構造}
  \label{fig:I}
\end{figure}

$F(W^\prime,v_j|I)$の期待値は以下の通りである．
\begin{equation}
  \label{U3}
  \langle F(W^\prime,v_j|I) \rangle = \langle F(v_j | I) \rangle \sum_{w \in W^\prime} \langle F(w | v_j,I) \rangle.
\end{equation}
ただし，$F(v_j|I)$と$F(W^\prime | v_j,I)$とは確率的に独立であるとみな
した．なお，分散や共分散は付録Bで与える．

(\ref{U1})式における$F(v_j|I)$や$F(w | v_j,I)$は，前と同じように一般化
超幾何分布に従う確率変数であり， それらの期待値は以下の通りである．
\begin{eqnarray}
  \label{U2a}
  \langle F(v_j | I) \rangle & = & \frac{\sum_{u \in U_i} n(r(u,v_j)) + 1}{\sum_{u \in U_i,v \in V} n(r(u,v)) + |V|}\;\;, \\
  \label{U2b}
  \langle F(w | v_j,I) \rangle & = & \frac{n(r(w,v_j)) + 1}{\sum_{u \in U_i} n(r(u,v_j)) + |U_i|}\;\;.
\end{eqnarray}
ただし，$n(r(u,v))$は関係$r$における共起頻度を表す．分散や共分散も
(\ref{D3})，(\ref{D4})式と同様に得られる．

(\ref{U2a})式と(\ref{U2b})式に使われる数値のなかで，まず，$|V|$は動詞
の語義の数である．また，$|U_i|$は標本空間の可変な部分の大きさを表わす
数値である．$|U_i|$はシソーラスの構造から決まるので，シソーラスの節点
にあらかじめ記録しておくことにより，実行時の計算量を減らす．たとえば，
図\ref{fig:U_i} で$|U_2|$の値を求めるときには，二つの節点$u_{24}$と
$u_{25}$に記録されている$|U_{24}|=1$と$|U_{25}|=2$の和をとる．同様に，
$\sum_{u \in U_i} n(r(u,v_j))$の値も，各節点に，支配下の葉と$v_j$との
共起頻度の和を記録しておき，それを利用して求める\footnote{共起頻度の和
は葉から根に再帰的に共起頻度を伝播することで記録する．たとえば，図
\ref{fig:U_i} では，$u_1$に記録される値は$u_{24}$と$u_{25}$に記録されて
いる値の和である．多重継承があるときには，この伝播の過程で，一つの節点
に複数の親がある場合がある．その場合には，その節点に記録されている共起
頻度の値を均等に親に分ける．}．図\ref{fig:U_i} の例では，$U_2$について，
$\sum_{u \in U_2} n(r(u,v_j)) = n(r(w_4,v_j)) + n(r(w_5,v_j)) +
n(r(w_6,v_j))$を求めるためには，$u_{24}$に記録されている
$n(r(w_4,v_j))$の値と$u_{25}$に記録されている$n(r(w_5,v_j)) +
n(r(w_6,v_j))$の値との和をとる．また，(\ref{U2a})式や(\ref{U2b})式の値
は，$U_i$ごとに計算され，$U_i$は最大でシソーラスの高さだけの数しかない
ので，これらの値を計算することは計算量の面で困難ではない．

動詞の語義を判定するかしないかは，$D = F(W^\prime,v_1|I) -
F(W^\prime,v_2|I)$の信頼下限($Pl$)に基づいて決める．ただし，動詞の語義
を適当に並べかえて，$\langle F(W^\prime,v_1|I) \rangle \ge \langle
F(W^\prime,v_2|I) \rangle \ge,\ldots$であるようにする．なお，$D$の期待
値は$\langle D \rangle = \langle F(W^\prime,v_1|I) \rangle - \langle
F(W^\prime,v_2|I) \rangle$である．また，分散$var(D)$は付録Bで与える．

語義を判定するために，$\alpha$と$\theta$を適当に選んで，$Pl(v_1) =
\langle D \rangle - \alpha \sqrt{var(D)} > \theta$である場合には$v_1$
を語義とし，語義判定のプロセスを終える．そうでない場合には，$U_i$の段
階では語義の判定をせずに，標本空間をシソーラスに沿って拡張した
$U_{i-1}$で再び語義の判定をする．$U_0$においても判定ができないときには
関係$r$においては語義の判定を行わない．なお，$\alpha$と$\theta$の値は
\ref{sec:experiment} 章で述べる．

\paragraph{例}

「私は関係者にいきさつを聞いた」における「聞く」の多義性を解消する．
「聞く」の語義としては，前節と同様に，ASKとHEARとを考える．また，
$\alpha=1,\theta=0$とする．なお，計算に必要なその他の詳細は省略する．

「私は関係者にいきさつを聞いた」には，「は(私，聞く)」「に(関係者，聞
く)」「を(いきさつ，聞く)」という三つの係り受け関係があるので，それぞ
れについて信頼下限を計算し，語義を求める．

「私」は関係「は」では「聞く」との共起回数は6回で，HEARと5回，ASKと1回
共起している．そのため，標本空間を拡張するまでもなく$Pl(HEAR) = 2.1
\times 10^{-1} > \theta$となった．

「関係者」は関係「に」では2回共起し，HEARと1回，ASKと1回共起している．
しかし，このHEARでの共起はタグ付けの誤りであり，ASKと共起すべきもので
あった．とにかく，この段階ではHEARとASKとで尤度差はない．しかし，シソー
ラス(分類語彙表)を2段階あがった時点($U_4$)では，$U_4$全体でHEARとの共
起は2回，ASKとの共起は9回である．このとき，$|U_4| = 69$であり，
$Pl(ASK) = 5.5 \times 10^{-4} > \theta$となり，ASKが語義として選択され
る．これは，タグ付けの誤りを回避した例である．

「いきさつ」は関係「を」では「聞く」との共起頻度は0である．一段シソー
ラスを上ったときの標本空間全体ではASKと1回，HEARと0回共起する．しかし，
この段階では$Pl(ASK) < \theta$であるので語義は判定されない．そのま
ま標本空間を拡張していくと，$U_1$で頻度の分布が逆転し，HEARで171回，
ASKで104回共起している．しかし$|U_1| = 26984$と標本空間の大きさが大き
いので信頼下限は閾値$\theta$を超えない．結局，この係り受け関係では語義
は判定されない．これは，シソーラス上での最短距離の語義に従えば成功して
いた例である．

三つの係り受け関係のうちで「は(私，聞く)」が最も信頼下限$Pl$が大きい．
一般に，シソーラスを上ると標本空間の大きさは指数的に大きくなるので，尤
度は指数的に小さくなる．そのため，標本空間が小さいときの信頼下限は，そ
れが大きいときに比べて大きい．

この例では，「は(私，聞く)」に従って語義を選択するので，HEARが語義に選
ばれる．これは失敗である．なお，最大の信頼下限に基づく語義選択の妥当性
は，\ref{sec:experiment} 章で実験により確かめる．

\subsection{Daganの手法}
\label{sec:dagan}

多義性を解消するときに，語義の判定が可能なものだけを判定するという手法
は，\cite{Dagan94}でも採用されている．\cite{Dagan94}では，機械翻訳にお
ける訳語の選択を目的としているが，ここでは，その手法を，動詞語義の選択
のために修正したものについて述べる．以下では，この手法を単にDaganの手
法と呼ぶ．また，本節で用いられている記号のうちで，新たに定義されていな
い記号については，\ref{sec:solid} 節と同じ意味で用いられている．

標本空間についていえば，Daganの手法は，\ref{sec:solid} 節と同じ，固定さ
れた標本空間を使う．ただし，\cite{Dagan94}では共起頻度は多項分布をして
いると仮定している．

動詞の語義を判定するかどうかは，$\hat{p_1}$と$\hat{p_2}$との対数比
$\ln(\hat{p_1}/\hat{p_2})$に基づいて決める．ただし，
$\hat{p_1},\hat{p_2},\ldots$は，$n_1,n_2,\ldots$から最尤推定される
$r(W,v_1),r(W,v_2),\ldots$の確率であり，$\ln(\hat{p_1}/\hat{p_2}) =
\ln(n_1/n_2)$，$var(\ln(\hat{p_1}/\hat{p_2})) \simeq 1/n_1 + 1/n_2$で
ある．もし，$n_1$，$n_2$で$0$なるものがあれば，$0$の代りに$0.5$を用い
る．

$\alpha$と$\theta$を適当に選んで，$Pl(v_1) = \ln(\hat{p_1}/\hat{p_2}) -
\alpha \sqrt{var(\ln(\hat{p_1}/\hat{p_2}))} > \theta$である場合には
$v_1$を語義とする\footnote{\cite{Dagan94}では`$>$'ではなく`$ \ge$'
であるが，`$ \ge$'の場合には，$\theta = 0$としたときに $\hat{p_1} =
\hat{p_2}$であっても$v_1$が選ばれることになるため`$>$'とした．ただし実
際にはどちらを用いても同じことである．}．そうでない場合には関係$r$では
語義は判定されない．なお，\cite{Dagan94}では$\alpha = 1.282$, $\theta
= 0.2$が選ばれているので，\ref{sec:experiment} 章の実験でもそれに従っ
た．

\subsection{Daganの手法と提案手法との違い}

Daganの手法と提案手法との基本的な違いは，標本空間が固定か可変かという
ことである．提案手法が，尤度の比較に差を用いたり，分布に一般化超幾何分
布を仮定したりしているのは，可変の標本空間を上手く取扱うためである．

まず，尤度の比較に差を用いた場合には，標本空間を拡張するたびに尤度や尤
度差が指数的に小さくなるので，標本空間の大きさを信頼下限に直接反映させ
ることができる．Daganの手法のように(対数)比を用いた場合には，第１位の
語義の尤度と第２位の語義の尤度とはオーダとしては違わないため，標本空間
の大きさは直接には反映されない．

次に，一般化超幾何分布を用いている理由は，標本空間の大きさを明示的に取
扱うためである．提案手法では，標本空間を拡張するたびに，尤度パラメータ
の期待値や分散が変化することが必要である．一般化超幾何分布に従うと，
(\ref{U2b})式で示されるように，標本空間の可変な部分の大きさを$|U_i|$と
して明示的に取り扱える．

標本空間の大きさを信頼下限に反映させる理由は，標本空間が小さいときほど
語義の判定結果が信頼できると考えているためである．

\section{考察}
\label{sec:discussion}

本章では，提案手法とクラスベースの手法や事例ベースの手法との関係，およ
び，今後の課題について述べる．

\subsection{提案手法とクラスベースの手法や事例ベースの手法との関係}

提案手法とクラスベースの手法とを比較すると，クラスベースの手法における
クラスは，本稿での場合のように，先験的に決めるか，あるいは，データに基
づいて決める\cite{Resnik92,Nomiyama93,Tanaka95a}必要がある．データに基
づく場合には，必要が生じた時点でクラスを変更する必要がある．しかし，提
案手法の場合には，入力に応じて動的に標本空間を定めるため，クラスの設定
自体が不要である．

提案手法と事例ベースの手法とを実験的に比較することは今後の課題であるが，
一つの係り受け関係から動詞の語義を決める場合については，定性的には以下
のことが言える．

まず，動詞語義の尤度についていえば，事例ベースの手法では，入力単語が動
詞語義に付与する尤度は，入力単語から，その動詞語義とコーパスで共起した
単語(出現単語)への，シソーラス上での最短距離に基づいている．一方，提案
手法では，動詞語義の尤度は，シソーラスの構造と動詞語義のシソーラス上で
の頻度分布と入力単語とにより決まる．これは，シソーラス上での距離を，コー
パスでの共起情報と入力単語とを利用して尤度に変換しているとみなすことも
できる．なお，これと同様なことは\cite{Shinnou96}でも行なわれている．し
かし，\cite{Shinnou96}は，単語間の一般的な類似性を，シソーラス上での単
語間の距離とコーパスでの共起頻度の分布とから設定することを目的としてい
て，多義性の解消は直接の目的とはしていない．

次に，提案手法において，\ref{sec:variable} 節で述べた$\alpha$の値を0に
すると，第1位と第2位の語義の尤度差が0でなくなった段階で，必ず，動詞の
語義が判定される．このときには，事例ベースの手法のように，シソーラス上
での最短距離の出現単語に基づいて動詞語義を判定していることになる．一方，
$\alpha$の値を大きくすると，動詞語義の頻度分布に大きな偏りがなければ，
語義は判定されない．つまり，$\alpha$の値を大きくすることは，最短距離以
外にある出現単語も考慮することを意味する．

このことは，\ref{sec:introduction} 章で述べた，「入力単語の振舞いを決め
るのに幾つ出現単語を用いるか」という問題を，$\alpha$の設定に帰着させた
と考えることもできる．

事例ベースの手法で，もし複数の出現単語を使うとしても，幾つ使うかを決め
るためには，シソーラスにおける動詞語義の頻度分布などを考慮しなければな
らないであろう．提案手法では，\ref{sec:variable} 節で述べたように，それ
が既に分散として数式中で考慮されているため，$\alpha$の値を決めることは，
幾つ出現単語を使うかを決めるよりは，容易であると考える．

\subsection{今後の課題}

表\ref{tab:contrib} にあるように，関係ごとの1位適合率は一様ではない．た
とえば，「を」は1位適合率が高く，「係：動詞」は1位適合率が低い．そこで，
1位適合率が高いものは，$\alpha$を小さくすることにより，判定率を高くし，
1位適合率が低いものは，$\alpha$を大きくすることにより，判定率を低くす
ることが考えられる．$\alpha$の設定の仕方は今後の課題である．なお，同様
な考え方として，\cite{Fujii96a}では，複数の格要素における語義の尤度を
足し合わせて全体の尤度とするときに，格ごとの曖昧性解消への貢献度に応じ
て，その格での尤度に重みを付ける手法が述べられている．

本稿では，複数の係り受け関係があっても，それらの間の関係は考慮せずに，
信頼下限が最大のものを選んで多義性の解消をしている．この方法は，表
\ref{tab:1and2} に示すように，有効である．しかし，複数の係り受け関係の
間にある依存関係を利用すれば，判定率や適合率が向上すると考えられる．そ
のような依存関係を取扱うことは今後の課題である．依存関係を考慮したもの
としては，既存の格フレームを利用したり\cite{Kurohashi92,Fujii96a}，格
フレームあるいは決定木を獲得したり\cite{Tanaka95b}，対数線型モデル
\cite{Matsuda88}により依存関係を推定する\cite{Bruce94}などの研究がある
ので，これらと提案手法との融合を検討したい．

\end{document}
