認知単位の検出方法

前節における実験の結果から，
人間における文解析過程は，認知単位を検出する処理と，認知単位を組み合わせて
文を認識する処理の2段階に分離できるものとみなせる．
このモデルに従い，機械においても文解析の処理を，認知単位を検出する処理と，
検出した認知単位の取捨選択の処理の2段階に分け
れば，解析の効率を高めることができると期待される．


前者の処理において，通常の形態素の辞書を用いて
文から認知単位を検出するには，どのような
形態素の並びが認知単位になるかという局所的な文法が必要である．
認知単位の内部における形態素の並びには，多重埋め込み的なものは少ない．
従って，この局所的な文法は状態遷移図で記述するのがふさわしい．よって
本研究では，認知単位を有限オートマトンで検出することにした．

\vspace*{-4mm}
\subsection{状態遷移図による認知単位の表現}
本研究では，処理の対象として，NHKラジオの気象通報の始めに放送される
天気概況文を用いた．
この例を図\ref{zu2} に示す．
これらの文に現れる認知単位を表層的な形式から128に分類し，
人手で128の受理状態を持つ状態遷移図を作成した．得られた
状態遷移図の一部を図\ref{zu3} に示す．図中$Z_i$は受理状態，
$S_i$は中間状態である．



\begin{figure}[b]
\small
オホーツク海には，発達中の低気圧があって，北北東へ進んでいます．

一方，中国東北部には高気圧があって，ほとんど停滞しています．

西日本は晴れ，東日本はくもりで，北日本では所々で雨が降っています．

尚，北海道周辺海域と三陸沖では所々濃い霧の為見通しが悪くなっています．

日本近海は，北海道東方海上から関東海域北部にかけて，シケています．

気温は，北海道，北陸，東海で，平年より１度高い他は，平年並か，１度から２度低くなっています．
\caption{天気概況文の例} \label{zu2}
\end{figure}

\begin{figure}[b]
\begin{center}
\epsfile{file=8-3.eps,width=90mm}
\end{center}
\caption{認知単位を受理する状態遷移図} \label{zu3}
\end{figure}
名詞句はあらかじめ地名，海，方角，数字，高気圧・低気圧，台風，波，霧，
天気などに分類してあり，
この状態遷移図においては，品詞の並びが同じでも名詞句の分類が異なる
名詞節は，異なる受理状態に遷移する．
従って，「日本海では」，「日本海は」，「気温は」は，すべて別の
受理状態に遷移する．
これは述語句や，その他の修飾句に関しても同様で，
品詞の並びだけではなく，意味的に気圧配置，気温，気圧，波，
霧のどの状態を示すのに使われるかによっても分類される．
従って，「悪くなっており」，「悪くなっています」，「高くなっています」
はすべて別の受理状態に遷移する．

天気概況文は気圧配置，天気，海上，霧，気温に関する文に大別できるが，
それぞれ表現の形式が限定されているため，比較的厳格な文法によりその文法を
記述できると考えられる．また，出現する形態素の数が限られており，同じ形態素が
何度も反復して現れる．従って，小規模なコーパスから得られたデータでも，
精度の高い解析が行える．


\subsection{誤りを含んだ文からの認知単位の検出}

前節で構成した有限オートマトンにより，文から認知単位を検出する
手順は以下の通りになる．

\newtheorem{tejun}{}
\begin{enumerate}
\item 文全体を走査し，形態素を検出して形態素ラティスを得る．
\item 得られた形態素ラティスに対し，有限オートマトンによる走査を
行い，認知単位を検出する．
\end{enumerate}


誤りを含んだ文に対し，文解析によって誤り訂正を行うタスクは，
通信やOCRなど様々な分野にしばしば求められるタスクである．

認知単位は，意味処理を含む脳内の多段の処理において主に単位と
して利用されているものと考えられるため，このような誤りを含む
テキストから誤りを取り除く場合にも人間はいずれかの段階で認知
単位を用いているものと思われる．従って，機械により誤り訂正を
行う場合にも認知単位は有効であると期待される．


誤りを含む文においては，形態素が近い綴りをもつ別の文字列に置き換わる
ことがある．従って，誤りを含む文に対して誤り訂正を行なうためには，
手順(1)において形態素を検出する際に，
厳密に文の一部に一致する形態素だけでなく，ごく近い形態素についても，
誤りによって文の一部に変化する可能性を推定しながら検出する必要がある．

本研究では，このタスクに対応するため，手順(1)において
図\ref{zu4}(a)のように距離1の形態素も検出することにした．
尚，この検出には DP 照合法を用いた\cite{rabi}．

\begin{figure}
\vspace{3mm}
\begin{center}
\epsfile{file=8-4.eps,width=97mm}
\end{center}
\caption{認知単位の検出} \label{zu4}
\end{figure}
誤りには，挿入，欠落，置換の3種類がある．ここでは，誤りが
図\ref{zu6} に示すように，次のモデルに従って発生するものと考える．
\subsection*{[誤りのモデル]}
\begin{quotation}
情報源の1文字あたり，確率$P_n$で誤りが発生する．誤りが発生した場合，
次のいずれかがそれぞれ条件つき確率$1/3$で起こる．

\begin{description}
\item{\gt 挿入:} 情報源の1文字の前または後ろに1文字が挿入され，2文字となる．
\item{\gt 欠落:} 情報源の1文字が失われる．
\item{\gt 置換:} 情報源の1文字が別の1文字に置き換わる．
\end{description}

\end{quotation}

以降の実験では，このモデルに従い誤りを含んだ文字列を発生させており，
挿入または置換の際必要となる文字としては，簡単のため平仮名46文字
のいずれかをランダムに選んで使用している．


今，$a$ と $b$ をそれぞれ文字列とし，$a$の長さは$l$文字， $a$ と $b$ の距離
は $d$ であるとする．上記の誤りによって $a$ が $b$ に変化する確率$P_e(a,b)$
は，$l - d$文字に誤りが発生せず，$d$文字に誤りが発生したと考えることにより，
次の式で近似できる．

\begin{figure}
\begin{center}
\epsfile{file=8-5.eps,width=63mm}
\end{center}
\vspace*{-2mm}
\caption{誤りのモデル} \label{zu6}
\end{figure}
\begin{equation}
P_e(a,b) \approx (1 - P_n)^{l - d}(\frac{P_n}{3})^d 
\end{equation}
　 実際には$d+1$文字以上の誤りによって，$a$が$b$に変化する場合も考えられるが，
本研究では$P_e$が十分小さいと考え，上式においてはこれらの場合は無視している．

手順(2)においては図\ref{zu4}(b)のように，手順(1)により得た形態素ラティス
に基づいて形態素を組み合わせることにより，認知単位を検出して認知単位
ラティスを作り，上式を用いて変化確率を計算するものとした．
\vspace*{-1mm}
bigram による認知単位の取捨選択
\vspace*{-1mm}
前節に示した手順により，文から認知単位を検出できる．
検出した認知単位は，状態遷移図における受理状態に
より128に分類される，この認知単位に対して
取捨選択を行い，文を組み立てるには，様々な方法が考えられる．
筆者らは既に，自動的に獲得した文法に基づき，認知単位を利用して
構文解析を行う方法を提案している\cite{yoko0}．

本論文では，誤りを含んだテキストから誤りを取り除く実験を行うが，
この実験において上記の構文解析を行うと，探索経路が莫大となって
計算に時間がかかる．
従ってこの実験には，より簡単な処理で効果的に
誤り訂正を行える方法が適している．

このような観点から，本論文では
bigram を用いて解析を行うことにした．bigram による方法は，自然
言語のようなマルコフ性を有する系列に対し効果的に取捨選択を行うことが
でき，特に音声認識などの分野では高い評価を得ている．

本研究では簡単のため意味解析は行わないが，
このように誤りを含むテキストを処理する場合，
構文解析や意味解析など，より高度な解析が必要な場合にも，
あらかじめ認知単位の bigram により不的確な候補を効率的に削除しておく
ことにより処理が効率化するものと思われる．

bigram の出現頻度表により記述できる性質は，系列の単純マルコフ的な性質に
限られるが，状態遷移図は多重マルコフ的な性質をも表すことができる利点を持つ．
しかし，認知単位の境界は分岐数が多いため，認知単位の多重マルコフ的な性質を
調べるのは極めて難しい作業となる．
従って，本研究では状態遷移図による解析は認知単位内にとどめる．

今，認知単位の系列$A={a_1, a_2, a_3,\ldots a_n}$の出現確率を$P(A)$とすれば，
\hspace*{-1mm}bigram モデルでは，\\系列$A$の出現確率は次のように表すことができる．
\vspace*{-1mm}
\begin{eqnarray}
P(A)& = & P(a_1|\mbox{Start}) P(a_2|a_1) P(a_3|a_2) \ldots \nonumber \\
& & P(a_n|a_{n-1})
\end{eqnarray}
　 ここで，\hspace*{1mm}$P(a_i|a_{i-1})$ \hspace*{1mm}は認知単位\hspace*{1mm}$a_{i-1}$\hspace*{1mm}の次に\hspace*{1mm}$a_i$\hspace*{1mm}が生起する条件つき
確率である．\hspace*{1mm}また，$P(a_1|\mbox{Start})$は文頭に$a_1$が生起する条件つき確率
である．従って，$A={a_1, a_2, \ldots a_n}$ が誤りによ\\り変化して
$B={b_1, b_2, \ldots b_n}$ として生起される確率は次のようになる．

\begin{eqnarray}
P_p(A, B) & = & P(a_1|\mbox{Start}) P_e(a_1, b_1) \nonumber \\
& & P(a_2|a_1) P_e(a_2,b_2) \nonumber \\
& & P(a_3|a_2) P_e(a_3,b_3) \ldots  \nonumber \\
& & P(a_n|a_{n-1}) P_e(a_n,b_n)  \label{eqa1}
\end{eqnarray}
\begin{figure}
\begin{center}
\epsfile{file=8-6.eps,width=46mm}
\end{center}
\caption{認知単位の bigram} \label{zu7}
\vspace*{3mm}
\end{figure}

ここでは，$a_i$として認知単位の状態遷移図における受理状態の記号
$z_j(0 \le j \le 127)$を用いる．従って，$P(a_i|a_j)$，$P(a_i|\mbox{Start})$を
あらかじめコーパスにより図\ref{zu7} の形の bigram にして求めておき，
(\ref{eqa1})式の$P_p(A, B)$を最大とする系列$A$を幅優先探索法に
よって探索する．

形態素を単位とした bigram を用いる方法では，
文を局所的に解析することしかできず，
また，より大域的な解析を行うために trigram や 4-gram を用いることにすれば，
必要とされる記憶容量が指数関数的に増大する他，巨大なコーパスを必要と
するなど様々な問題が生じる．この代わりに，このように認知単位を単位とした
bigram を用いることにより，記憶容量を抑えながら，大域的な解析を行うことが
できる．

