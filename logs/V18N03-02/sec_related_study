本節ではまず[REF_sec:intro]節でブートストラッピング手法として挙げたSelf-trainingおよびCo-trainingを用いた語義曖昧性解消の先行研究を概観する．
また，アンサンブル学習に基づく語義曖昧性解消には教師あり学習のアンサンブル，教師なし学習のアンサンブル，半教師あり学習のアンサンブルに基づいた手法が提案されており，これら先行研究を併せて概説する．
Self-trainingに基づいた語義曖昧性解消の先駆けとしてはYarowskyの研究[CITE]が挙げられる．
Yarowskyは「語義はその語の連語より定まる(one sense per collocation)」「語義はその語を含む談話より定まる(one sense per discourse)」という二つのヒューリスティックに基づき，ラベルなしデータに反復的にラベル付けするアルゴリズムを提案した．
この手法は二つの観点からラベル付けをするため，Co-trainingの一種であると見ることもできる．
また，このヒューリスティックに基づいたYarowskyのアルゴリズムはAbney [CITE]により，目的関数の最適化問題として定式化されている．
Co-trainingを用いた語義曖昧性解消の早期の例としては新納の報告[CITE]がある．
新納はCo-trainingを適用するにあたり，二組の素性集合の独立性を高めるため，ラベル付きデータに追加するラベルなしデータを素性間の共起性に基づいて選択する手法を提案した．
結果，日本語の語義曖昧性解消において通常のCo-trainingよりも性能が向上したと報告した．
MihalceaはCo-trainingとSelf-trainingの両方を語義曖昧性解消に適用し，[REF_sec:intro]節にて述べたパラメータの影響について調査した[CITE]．
この報告ではパラメータの自動での最適化はできず，最適な設定と自動による設定に大きな差があったと報告している．
また，Mihalceaは同じ報告の中でスムージングしたCo-trainingおよびSelf-trainingを提案した．
これは手順の反復のたびに生成される各分類器の多数決より語義判定しブートストラッピングするという方式であり，ブートストラッピングとアンサンブル学習の組合せの一種と見ることができる．
この方式は通常のブートストラッピングよりも性能が向上したと報告された．
以上の手法は[REF_sec:intro]節で述べたようなパラメータをタスク（データセット）に合わせ調整しなければならないという大きな課題がある．
NiuらはZhuらの提案したラベル伝播手法[CITE]に基づいた半教師あり手法による語義曖昧性解消について調査した[CITE]．
ラベル伝播は事例を節点とする連結グラフを考え，重み付きの辺を通してラベルありの事例からラベルなしの事例へとラベル情報を反復的に伝播させる．
そして伝播の収束結果よりラベルを推定する．
この手法はSenseval-3 [CITE] English lexical sampleタスクのデータセットに適用した結果，従来の教師あり学習と比較して著しい成果は得られなかったとしている．
PhamらはCo-trainingとSmoothed Co-training [CITE]に加え，Spectral Graph Transduction (SGT) [CITE]およびSGTとCo-trainingを組合せたCo-trainingの語義曖昧性解消への適用を調査した[CITE]．
Transductionとは訓練データから分類器を生成せず，直接テストデータにラベル付けする推論方法である[CITE]．
SGTは[MATH]近傍法のTransductive版であるとされる．
SGTは[MATH]近傍法の応用であるため，[MATH]がパラメータとなり，かつ[MATH]は性能に与える影響が大きいと報告されている．
よってPhamらの調査した手法全てにはパラメータ設定の問題が存在していることになる．
手法のアンサンブルを含まないブートストラッピングによる語義曖昧性解消の研究の最後として小町らの報告[CITE]を挙げる．
小町らはブートストラッピング手法の一つであるEspresso [CITE]に対しグラフ理論に基づいて意味ドリフト[CITE]の解析を行った．
意味ドリフトは，語義曖昧性解消の観点から考えると，どのような語義の語も持つ素性をジェネリックパターンと考え，ジェネリックパターンを持つ（信頼性が低いとされるべき）ラベルなしデータに対し手順の反復過程においてラベルが与えられることにより，反復終了後に生成される分類器の性能が低下してしまう現象と解釈できる．
この問題への対処のため小町らは二つのリンク解析的関連度算出法を適用した．
この手法は意味ドリフトに頑健かつパラメータ数が一つでさらにその調整が比較的容易という利点を持つ．
Senseval-3 English lexical sampleタスクのデータセットに手法を適用した実験の結果，小町らの手法は類似したグラフ理論的手法であるHyperLex [CITE]やPageRank [CITE]と比較して高い性能が得られたと報告している．
教師あり学習のアンサンブルに基づく研究としては，AdaBoostを用いた[CITE]，素性として用いる文脈の大きさを変えて複数のNaive Bayes分類器をアンサンブルした[CITE]，六種の分類器の組合せによる[CITE]，二段階の分類器の出力の選択に基づいた[CITE]，複数のNaive Bayes分類器の出力の比較に基づいた[CITE]が挙げられる．
ここではWangらの手法をより詳しくみる．
WangらはまずPedersonと同様に素性として用いる文脈の大きさ，つまり目標の多義語前後[MATH]語以内の語を素性として用いるとして，[MATH]を変えることで複数のNaive Bayes分類器を作成する．
次にラベル付きデータを各分類器にて分類する．
各要素がこの各分類器による分類結果であるベクトルをdecision trajectoryと呼ぶ．
最後に各ラベル付きデータから得たdecision trajectoryの集合を訓練データとし，これらと入力から得たdecision trajectoryの類似度に基づいて入力の語義を判定する．
Wangらの手法は中国語の語義曖昧性解消実験の結果，Pedersonの手法などと比較して最も良い結果が得られたと報告した．
教師なし手法のアンサンブルの例としてはBrodyらの研究[CITE]が挙げられる．
Brodyらは過去に語義曖昧性解消において有効と報告された教師なし手法であるExtended Gloss Overlap [CITE]，Distributional and WordNet Similarity [CITE]，Lexical Chains [CITE]およびStructural Semantic Interconnections [CITE]を組合せた手法を提案した．
この手法は組合せに用いた各手法と比較し，より良い結果が得られたと報告されている．
最後に本稿で提案する手法に最も関連の深いブートストラッピング手法のアンサンブルを行ったLeらの研究[CITE]について述べる．
Leらは我々と同様に従来のブートストラッピングによる語義曖昧性解消の問題点に対する解決法を提案した．
Leらが解決法を提案した問題点は，(1)ラベル付きデータのラベル毎のデータ数の偏り，(2)ラベル付きデータに追加するラベルなしデータ決定の基準，(3)手順の反復の停止条件および最終分類器の作成法の三つである．
ここで問題(2)は[REF_sec:intro]節にて述べたパラメータ[MATH]の決定法，問題(3)はパラメータ[MATH]の決定法とも換言できる．
Leらはこれらの解決のため，追加するラベルなしデータのリサイズ，複数のデータ追加条件の閾値の設定および対応する複数の追加データ集合の設定，訓練データを用いた追加データの評価および手順反復停止条件の設定，そして追加データと教師あり学習手法別の各分類器のいくつかの組合せ法を提案した．
ここで追加データの評価と手順反復停止条件の設定の手法は，Zhouらが提案したTri-training法[CITE]で用いられた手法を参考に設定している．
Tri-trainingはCo-trainingを発展させた手法であり，Co-trainingと異なりパラメータ設定を不要とする特徴がある．
実験はSenseval-2 [CITE]およびSenseval-3のEnglish lexical sampleタスクのデータセットを用いて行われ，従来の教師あり手法と比較し最大で1.51ポイントの精度向上が見られたとLeらは報告した．
[REF_sec:work1]節と[REF_sec:work2]節を踏まえた上での本研究の位置付けは以下の通りである．
まず，小町らの手法はパラメータ設定が容易という利点があるが，他の教師あり手法と組合せるのが困難なのが問題点である．
高性能な教師あり手法を用いず，さらに性能を向上させるのは難しい．
また，Leらの手法の難点として手順の反復停止条件の設定が挙げられる．
これは，教師あり学習を用い追加データを訓練データとして分類器を作成しオリジナルのラベル付きデータを分類して得られるエラー率，および追加データの総数に基づき設定される．
具体的には次の式を用い追加データの評価値[MATH]を求める．
ここで，[MATH]は追加データの総数，[MATH]はエラー率を示す．
この[MATH]が前回の値よりも小さければ反復は停止する．
しかし反復の停止にこの条件が用いられる具体的根拠は示されていない．
単に反復を自動的に停止するためと述べられているだけである．
このためこの停止条件が最適であるかどうか疑問が残る．
そこで本研究の立場だが，まず本稿ではこの停止条件の追究はしない方針とする．
しかし，[REF_sec:intro]節にて目標としたようにブートストラッピングにおけるパラメータの削減は達成を目指す．
そこで本研究では手順の反復回数（パラメータ[MATH]）を一回に留めるという方針を採る．
この方針には次の利点が考えられる．
手順の回数が固定され計算時間の予測が立てやすい．
ラベル付きデータへのラベルなしデータの追加が一度のみとなるため，追加されたデータに対し分析，考察を加えやすい．
反復1回目の精度を向上させることで，手法を複数反復できるように拡張したとき更なる精度向上が見込める．
以上の検討に基づき，本研究では反復を伴わず，かつ教師あり学習手法を併用した高性能なブートストラッピング的手法を確立する．
また反復回数を一回にすることは，反復回数以外のパラメータを削減することにもつながる．
詳しくは[REF_sec:method]節にて述べる
