}
\footnotetext{野村 浩郷, Hirosato Nomura, 九州工業大学 情報工学部 知能情報工学科, Department of Artificial Intelligence, Kyushu Institute of Technology}

機械翻訳システムの長い歴史の中で、システム評価は常に大きな
課題の一つであった。システムの研究開発が健全に進むためには、
客観的かつ正確な評価法が必要となる。このため、ユーザの立場か
ら評価を行うもの、開発者の立場から評価を行うもの、また、技
術的側面から評価を行うもの、経済的側面から評価を行うものと、
多くの研究者によって様々な視点からの評価法が検討されてきた。
これらの検討に基づいて、(社)日本電子工業振興協会によって一
連の機械翻訳システム評価基準が開発されてきた(野村・井佐原 1992,
Nomura and Isahara 1992a,  
Nomura and Isahara 1992b, 日本電子工業振興協会 1993)。
本稿で提案する機械翻訳システムの評価法は、システムの
改良を続ける開発者の立場から、機械翻訳システムの技術面を翻
訳品質に注目して評価するものである。

機械翻訳システムの訳文の品質面での評価に関しては、従来から
のいわゆるＡＬＰＡＣレポート型の評価法に加えて、近年、いくつ
かの提案がなされている。ある程度まとまった文章を翻訳し、そこ
から得られる理解の度合を評価しようとするものとして、ＡＲＰＡ
による機械翻訳システム評価(White et al 1994)や、ＴＯＥＦＬの
テストを用いる方法(Tomita 1992)が提案されているが、これらは
システム間の現時点での性能の比較評価には用いることが出来ても、
評価結果を直接システム改良に結び付けることは困難である。

これに対し、個別の例文を収集することにより評価用の例文集を
作成し、その各例文の翻訳結果を評価し、対応する言語現象の処理
能力を判定しようとする提案がいくつかなされている。これらのう
ちには、単に文を集めるのみで、その後の例文の利用法(評価過程)
は個別の評価者に任せようというものから、本稿で提案するように、
客観的評価のためにさまざまな情報を付加しようというものまで、
いくつかの段階がある。

わが国においては、(社)日本電子工業振興協会が既に昭和６０
年に機械翻訳例文資料として、翻訳における曖昧性に関する問題点
に着目して、英文および和文を収集分類し公開している(日本電子工業振興協会
1985)。
また同協会は昭和６２年度に、機械翻訳シス 
テムの技術レベルを評価するために、文の複雑さの定量化、文の複
雑さや文体の定性的特徴の抽出、標準的例文の収集を行なった
(日本電子工業振興協会 1988, 石崎・井佐原 1988)。

この他、英語を話す人間と日本語を話す人間との間にある言語理
解法の違い(言い替えると、日本語と英語の発想法の違い)に注目
して日本語の言語表現を分類し、それらの表現の翻訳能力を評価す
る試験文集を作成するもの(池原・小倉 1990, 池原 他 1994)
や、言語学
的観点から日本語および英語の言語表現の構造に注目し、その表現
上の構造的特性を的確に表すような試験文集を作成すること
(成田 1988)が提案されてきた。後者は、個々の言語現象に対する翻訳の
可否を示すことの必要性から、一定の内容の文の言い換えなどによ
って日本語および英語の言語表現と翻訳能力の関係を言語学者の立
場から評価することを提案している。

本稿で論じる機械翻訳システム評価用テストセットは、以上のよ
うな、ＡＬＰＡＣレポート以来の品質評価に関する研究を踏まえて、
誰でも客観的かつ実用的な評価を行なえる評価法の確立を目指し作
成したものである。次節以下では、テストセットを用いた評価法の
全体を流れる基本的な考え方、英日機械翻訳用テストセット、日英
機械翻訳用テストセットについて、順次説明していく。


score: 13349