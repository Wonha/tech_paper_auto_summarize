これまで、機械翻訳システムの品質評価法として種々の方法が提案されているが、それらの方法に関しては一貫して客観的評価が困難であるという指摘が行なわれてきた。
本稿ではまず、従来までの評価法と比べての本評価法の利点を、以下の二つの客観性に基づいて検討する。
(1)評価過程が客観的であること
(2)評価結果の判断が客観的に行なえること
たとえば、ＡＬＰＡＣレポート等に代表される評価法は、評価の軸として「忠実度」「理解容易度」といった、その解釈が評価者の主観的判断に依存する基準を採用している。
その結果、評価結果が評価者によって大きく異なってしまうという問題があり、(１)の客観性を満たしていない。
この評価のばらつきは不完全な翻訳結果を評価する際に特に顕著に現れるが、現実の機械翻訳システムを評価し、開発過程にフィードバックする際には、翻訳に成功した場合よりも失敗した場合についての検討が重要である。
この種の評価法においては評価結果は数値で表現されているため、ある意味では、(２)の客観性を満たしているともいえよう。
しかしながら開発者にとっては、自己のシステムが、どの言語現象をどの原因によって処理できなかったのかを判断することが特に重要であり、言語現象が複雑に絡みあった文の翻訳結果を単純に得点化するだけでは有効とはいえない。
システム改良に用いるためにその評価結果を解釈しようとする場合には主観的な判断に頼らざるを得ないので、実用的にはこの評価法は(２)の客観性を満たしているとはいえない。
一方、我々の開発した評価法においては、これら二つの客観性は共に保たれている。
ここでは、単にそれに答えるだけで、システム開発者が自己のシステムの性能評価を行なえるように作られたyes/no設問を各例文に付加することにより、翻訳結果を評価する手続きを明確化した。
評価過程で必要とされる手順は単純なyes/no疑問文に答えることだけであり、誰でも機械翻訳システムを同様に評価することが出来る。
不完全な翻訳文に対しても、評価者によって評価が大幅に変わるということはない。
さらに、各例文には翻訳処理と言語現象との関係を表す解説が付加されており、これにより、システム開発者はなぜ自己のシステムが問題の言語現象を正しく解析できないのかを知ることが出来る。
すなわち、我々のテストセットに基づく評価結果を用いて、機械翻訳システムの改良法を決定することが出来る。
機械翻訳システムの評価に関しては、既に述べたように、評価すべき言語現象を含む文を集めた評価用例文集の作成という試みもなされている(成田1988,池原・小倉1990,池原他1994)。
このような例文集を用いれば、もしシステムが、ある例文を正しく翻訳できないと評価された場合には、システム開発者はただちにその例文が問題としている言語現象をそのシステムが処理できないということが分かる。
この点において、この手法もまた(２)の客観性を保持している。
しかしながら、この手法には以下の二つの問題がある。
例文の翻訳結果を評価する手順が明示されていない。
評価結果から機械翻訳システムの不備な点を見つけ出す過程が評価者の言語直観に頼っている。
例文を集めただけのもの(テストスゥィート(Test Suite))では、個々のシステムのad hocな評価は可能であっても、評価法としては確立しない。
明確に記述された手続きにしたがって、誰でも同じように機械翻訳システムを評価できることが必要である。
この目的のために各例文に設問や訳出例を付与しているということを明確にする意味で、我々の評価法においては「テストセット(Test Set)」という名称を用いている。
また、評価結果を機械翻訳システムの改良に用いるためには、さまざまな言語現象を単に羅列しておくだけでは不十分である。
文法体系の中での各言語現象の位置づけを明確にしておくことも必要である。
このような考察のもと、我々は上で述べた評価過程の客観性と結果の判断の客観性という二つの客観性を追求した品質評価を可能とする品質評価用テストセットを提案してきた。
ここで用いるテストセットは、考慮すべき文法項目を系統立てて収集し、その各項目に例文を付加して作られた。
各例文に解説や設問を付加することによって評価の手順を明確に記述することが可能となった。
各テストセットには、評価用例文、その人間による模範訳、システムの出力(翻訳結果)を評価するための設問などが記述されており、評価者はテストセット中の例文を翻訳し、その翻訳結果を参照しながら各例文に付与された設問に回答していく。
ここで各設問は判断のポイント(すなわち、例文のどの部分が、どのような役割で、どのような訳文となっていれば良いか)が明示されたyes/no質問文であり、評価者によって判断が異なることがないように作られている。
この判断をさらに容易にするために、既存の機械翻訳システムでの翻訳結果を用いた回答例が付与されている。
以上により評価過程の客観性を実現している。
また、各例文には、その文がどのような言語現象を評価するためのものであるかを説明する解説が付与されており、開発者はその例文に対する翻訳結果から自己のシステムが十分には対応していない言語現象を容易に理解することが出来る。
これにより、評価結果の判断の客観性を実現できる。
この評価用テストセットは、個々の機械翻訳システムに依存しない汎用の品質評価法として作成している。
したがって、対象とするシステムがルールベース・知識ベース・用例ベース・直接型といった機械翻訳のどの手法を採用しているかには依存しない。
このテストセットの目的は機械翻訳システムの開発者が自己のシステムの性能を向上するために、システムの処理できない言語現象を正確に把握することである。
その言語現象を処理可能にするための手法は、個々のシステムあるいは個々の手法によって異なっており、その判断は開発者に任すこととし、評価基準としては、そこには立ち入らない。
用例ベースの手法とルールベースの手法に共通する解決策を評価法が示すということは現実的ではない。
また、個々のシステムによって、対象とする文書が異なっており、各言語現象の出現頻度も異なっている。
したがって、システムの欠点のうちで、どの欠点が最も重大であるかを決定することは、当事者にのみ可能なことである。
本テストセットの目標は、その当事者の判断を可能な限り援助することにある。
ここではテストセット中の各例文には、頻度に関する情報を記述するのではなく、その例文が判断する言語現象を記述してある。
翻訳対象となる文書が特定の言語現象に偏っている場合には、評価者はこのテストセットのうちで、必要な言語現象に対応する部分についてのみ翻訳し、その結果を評価すれば良い。
自分にとって重要な言語現象を取り扱えるかどうかが、個々の開発者あるいはユーザがシステムを評価する場合には重要であり、評価法としての独自の頻度による一般的な得点化を行なうのは、むしろ誤った評価の原因になると考える。
また、評価に例文を用いることについては、その例文に対して高い評価が出るようにシステムを修正することが可能であること、また、全ての言語現象を網羅できるわけではないことなどの問題点が指摘される。
しかしながら、ここで提案する評価法はシステム間の相対的な性能評価のために用いるものではない。
開発者が自己のシステムの改良のために、その欠点を把握することが目的であり、この本来の目的のためには本テストセットに対してチューニングをすることに意味はない。
また、本テストセットは単なる例文集ではなく、各例文にはその対象とする言語現象が解説されており、さらには必要に応じて関連文と、その模範訳が付加されている。
これらの文を翻訳し検討することにより、単に一つの例文を処理できるかどうかを判断するだけではなく、その例文に関連する言語現象についての処理能力も知ることが出来る。
さらに、個々の開発者が処置するべき問題として、テストセット中の例文に存在する未定義語の問題がある。
例文中に(そのシステムにとっての)未定義語があった場合には、評価者は例文中に現れた未定義語を辞書登録するか、あるいは例文中の未定義語を既にシステムに登録されている類似の単語に変更することが要求される。
繰り返すが、この評価法はシステム間の優劣を決めることが目的ではなく開発者が自分のためにシステムの欠陥を見つけて、それを修正することを主たる目的としている。
したがって、評価者(すなわち開発者)は単純に評価結果を受け入れるのではなく、「翻訳に成功しているが偶然良い訳語が記述されていただけだ。
」「翻訳に失敗したが、それはその単語が未定義であったためで、類似の現象自体は取り扱う能力がある。
」等については、各自の(自己のシステムについての)知識に基づいて判断する必要がある。
また、評価の結果、さまざまな欠陥が見つかった場合に、限られた人的資源の中で、どのような順序でそれを解決していくかという問題もある。
しかしながら、各開発者毎に資源の制約や、そのシステムが主として対象とする文書(あるいは、対象とする言語現象)が異なるため、一般的な優先順位を予め定めておくことは現実的ではない。
本テストセットは、比較的近い将来に正しい処理の実現が可能な言語現象に重点をおいて作っているが、取り扱えなかった言語現象の内で、まずどの現象を処理可能にするかという優先順位付けは、個々のシステムの開発者に任せられている。
なお、このように近い将来に対応できるものに重点を置いて言語現象を収集しテストセットとした場合、機械翻訳システムの技術水準の向上に伴って、対象とする言語現象を継続的に追加あるいは削除していくことが望まれる。
常にその時点で機械翻訳において問題となっている言語現象を１０００文程度のテストセットで示すのが理想であろう。
ただし、最低限の解析能力を試すための基本的な構造の文は現在も(そのような基本的な構造の解析は既にほとんどのシステムにおいて解決されている問題であるとしても)テストセット中に含まれている。
このような基本文はシステムの最低水準を保証するものとして、将来に亙ってもテストセットに含まれると想定している。
テストセットの例文は機械翻訳システムや自然言語処理システムを実際に開発してきた経験に基づいて、著者らによって収集された。
例文の収集に当たっては、我々は以下の２点を重視した。
(1)基本的な言語現象を網羅すること。
(2)機械翻訳システムにとって処理することが困難な言語現象を含む例文を選択すること。
ここでは特に曖昧性の問題を重視した。
言い替えると、(１)は評価すべき文法現象を系統立てて収集分類(トップダウンの手法)し、それらの現象に対応する例を集めることである。
一方(２)は機械翻訳システムによって翻訳することが困難であるような例文を収集する(ボトムアップの手法)ことである。
特に我々は処理の困難さが近い将来に解決できるであろうような言語現象に注目した。
そして機械翻訳システムの評価のための例文を系統立てて分類した。
さらに、我々はこれらの例について、いくつかの商用システムを用いて翻訳評価実験を繰り返し、テストセットを以下の点に焦点を当てながら改良した。
これらは全て、評価過程において客観性を維持するために重要な要素である。
設問に曖昧性がないこと
例文に不必要な複雑さがないこと
翻訳結果に曖昧性がないこと
なお、テストセット中の英文は、その英語としての品質を保証するため、英語を母国語とし、日本語を理解する自然言語処理研究者によって、チェックされ修正された。
なお、このテストセットを用いた品質評価法の提案の主旨と、作成の詳しい経緯については、参考文献(井佐原他1992,日本電子工業振興協会1993,日本電子工業振興協会1994, Isahara et al 1994,日本電子工業振興協会1995a, Isahara 1995)を参照されたい。
また、テストセットの全容は、参考文献(日本電子工業振興協会1995b)に示されている。
本節では、英日機械翻訳システムの品質評価用テストセットについて説明する。
このテストセットは、機械翻訳システムが処理すべき様々な言語現象を含んだ英語例文770文とその模範訳、及びシステムの出力(翻訳結果)を評価するための設問などからなる。
我々は、英日機械翻訳システムの評価基準として、システム開発者が自己のシステムの不備をチェックすることを主要目的とした品質評価用テストセットを作成した。
本テストセットにおける例文の収集に際しては、「基本的な言語現象を網羅すること」「機械翻訳システムが取り扱うことが困難な言語現象を、主に曖昧性の解消に注目して収集・分類すること」を試みた。
また、システムの出力(翻訳結果)を見ながら回答していくことで品質に関する客観的な判断が可能となるように、各文に判断のポイントを明示したyes/no疑問文の形式の設問を付与している。
このように本テストセットは客観的な品質評価の実現を目指して作成したものなので、ユーザが各機械翻訳システムの出力品質を比較する際に利用することも可能である。
本テストセットの作成作業は、平成４年度からの３年間で行った。
平成５年度末までに第１段階として、英語の単文を中心に評価すべき項目を抽出して評価基準を設定し、309の基本例文を収集・評価して「電子協平成５年度版テストセット」としてまとめた。
これに加えて、今回さらに以下の作業を行って項目の充実を図った。
平成５年度版テストセットが単文中心だったのに対して、接続詞、関係詞、比較、話法、挿入、並列など、より複雑な構造を持つ複文・重文に関連する項目を重点的に英文法の解説書などから抽出して収集
複数の文法書などを参考にすることにより、単文内の項目に関しても、平成５年度版テストセットでカバー出来ていない項目を収集。
特に、代名詞、前置詞、記号、数量表現などに関して新規の設問を多数作成
文法項目の洩れを防ぐため、英字新聞から英文テキスト300文を選出して市販の英日機械翻訳システムで試訳し、翻訳が困難となる問題点を抽出
上記の作業により、これまでの309項目と併せて延べで約1000の項目を抽出した。
最終的にこれを整理して、770項目からなるテストセットとしてまとめた。
例文と関連文を合わせると、合計で1450文ほどの規模のテストセットとすることが出来た。
また、本テストセットの実用性の検証と設問の修正のために、ハードウェアタイプの異なる８種の市販の英日機械翻訳システムを対象とした評価を行った。
このテストセット中の各項目は、文番号、例文、その模範訳、○×で答えることが出来る質問文、主として機械翻訳システムによる訳出例、例文と関連する言語現象を含む文、関連する項目の番号、解説から成り立っている。
テストセットの例を図１に示す。
以下では、このテストセットを用いた品質評価の手順、対象とする言語現象、テストセットの書式について述べる。
２．
１．
１多品詞（品詞認定）２．
１．
１．
２名詞/助動詞
【番号】2.1.1.2-1【例文】The trash can was thrown away.【訳文】ごみカンは捨てられた。
【質問】"can"が「カン/缶」のように名詞として訳されていますか？【訳出例】○(くず缶/ごみ容器/くず入れ)は(廃棄された/[投げ]捨てられた)。
×ごみは捨てられ得る。
【関連文】The last will was opened.「最後の遺言書は開けられた。
」【参照項目】2.1.1.2-2, 2.1.1.2-3【解説】"can was"の並びから、"can"が助動詞でないことがわかる。
【番号】2.1.1.2-2【例文】The trash can be thrown away.【訳文】ごみは捨てられ得る。
【質問】"can"が「〜できる/得る」のように助動詞として訳されていますか？【訳出例】○(くず/ごみ/くだらない人間)は(廃棄できる/[投げ]捨てられることができる)。
×ごみカンは捨てられた。
【関連文】【参照項目】2.1.1.2-1, 2.1.1.2-3【解説】2.1.1.2-1とは逆に、ここでは"can"は名詞ではなく助動詞。
図１英日機械翻訳システム用テストセットの例
本テストセットは、以下の利用法を想定している。
(1)評価対象となる英日機械翻訳システムを用意する。
(2)そのシステムでテストセット中の【例文】を翻訳する。
(3)【質問】【訳出例】を見て、その翻訳結果が○か×かを判断する。
(4)システム開発者は、○×の分布からシステムの能力、開発段階を評価する。
特に、×と判断した項目に関連する文法・辞書を追加することで、システムの改良を図る。
( (5)ユーザは、各システムの○×の分布から、出力品質面での優劣を比較する。
)
(6)各項目についてさらに詳細に評価を行う場合は、【関連文】を利用する。
原則として翻訳結果と質問文を見るだけで○×を回答出来るようになっているが、【訳出例】(各訳出例には、質問に対する○×が予め付与されている)を参照することによって、さらに容易に判断が出来るようになっている。
本テストセットを用いて○×の分布を見ることで、システムの対応が不十分な(可能性がある)項目を容易に抽出できる。
ただし本テストセットでは、各項目(例文)間の重要度、頻度などの差異は考慮していないので、単純に○の数をカウントして正解率をシステム間で比較することは、本評価法の意図するところではない。
本テストセットは、機械翻訳システムが処理すべき様々な言語現象を含んだ英語例文770文からなる。
内訳と項目ごとの設問数を図２に示す。
品質評価の対象項目の収集に当たっては、網羅性を保証するトップダウンのアプローチと、機械翻訳における問題点を実際の翻訳結果から抽出して、その問題性によって例文の粗密を決定するボトムアップなアプローチを組み合わせている。
把握部においては、英文法の解説書(江川1964, Hornby 1977,小川他1991,荒木他1992,村田1992)などを参考に英語の文法現象を収集し、そのレベルによって、品詞、文の部分構造、文構造の３段階に分類した。
特に動詞、形容詞、名詞に関してその基本的な用法を網羅するために、ホーンビーの分類した文型(Hornby 1977)を設問項目として採用した。
ただしホーンビーのパターンの中でも、機械翻訳システムの品質評価において特に必要でないとみなした区分については分類を省略している。
同様に助動詞等の基本的な用法の中でも、機械翻訳において対象となることが極めて稀であると思われるものについては省略した。
選択部においては、翻訳で実際に問題となる言語現象を、構文構造の曖昧性に関するものと、コロケーション(他の語との共起による訳し分け)に関するものに分類した。
１把握部小計684１．
１品詞小計355１．
１．
１冠詞15１．
１．
２名詞（固有名詞を含む）27１．
１．
３代名詞25１．
１．
４形容詞42１．
１．
５副詞54１．
１．
６前置詞40１．
１．
７動詞類１．
１．
７．
１動詞・準動詞48１．
１．
７．
２助動詞37１．
１．
８関係詞25１．
１．
９接続詞26１．
１．
１０記号16１．
２文の部分構造小計167１．
２．
１不定詞26１．
２．
２分詞、分詞構文19１．
２．
３動名詞23１．
２．
４時制63１．
２．
５数量表現28１．
２．
６慣用表現8１．
３文構造小計162１．
３．
１文種（疑問文、命令文、感嘆文）19１．
３．
２否定16１．
３．
３特殊構文19１．
３．
４比較21１．
３．
５仮定法（条件法）16１．
３．
６態10１．
３．
７話法4１．
３．
８挿入16１．
３．
９省略9１．
３．
１０倒置7１．
３．
１１並列句25２選択部小計86２．
１構文２．
１．
１多品詞（品詞認定）34２．
１．
２係り先認定27２．
２コロケーション25総計７７０
図２テストセットの全体構成、項目別設問数
本テストセットの各項目の書式を図３に示す。
なお、テストセット中で、[ ]で囲まれた部分は挿入可能な表現を、( / )で囲まれた部分はいずれかを選択する表現を示す。
たとえば、"Ａ[Ｂ]Ｃ(Ｄ/Ｅ)Ｆ"という記号列は、``ＡＢＣＤＦ'', ``ＡＢＣＥＦ'', ``ＡＣＤＦ'', ``ＡＣＥＦ''の４種の記号列を表す。
【番号】：例文の番号【例文】：例文（１文のみ）【訳文】：模範訳（例文の日本語訳）【質問】："Ａ"が「Ｂ」のようにＣとして訳されていますか？という形式の質問文・Ａ：英語表現。
" "で囲む。
例文中のどの部分を翻訳することにポイントがあるのかを表す。
文全体の場合、また明らかな場合などは省略する。
・Ｂ：日本語表現。
「」で囲む。
・Ｃ：内容や文法事項の補足説明（「習慣を表す表現」、「選択疑問文」等）を記す。
記述が長くなるものや、原因に言及する場合は、【解説】に記述する。
※必ず○か×か（yes/no）で答えられる形式にする。
本テストセットは作業者が翻訳結果（訳文）を見るだけで○×を与えることを前提としており、解析の詳細に直接言及することは避ける。
【訳出例】：許容される訳出例や誤訳例を列挙。
・正解例（yes）の文頭には○、誤例（no）の文頭には×を付与する。
・１行１文とし、原則として文全体を記述。
・必要ならば正／誤の理由（説明）も示す。
※各例は実際の機械翻訳システムの訳を参考にして作成した。
【関連文】：当該の例文と関連する言語現象を含んだ例文を挙げる。
・文の一部だけの記述は認めない。
必ず文全体を記述する。
・例文の後に、「」で囲んだ訳文を記述する。
・補足事項（may/mightでの丁寧度の違いなど）がある場合は、訳文の後に（）で囲んで記述する。
【参照項目】：本テストセット内の関連項目への参照ポインタ。
原則として、相互参照とする。
※文番号を明示するのみで、文そのものは記述しない。
【解説】：その他の補足事項。
フリーフォーマット。
図３テストセットの書式
日英翻訳システム品質評価用テストセットも英日翻訳システム用と同様に、開発者が自己のシステムの不備な点を発見するための評価法であり、テストセット中の各例文に付与された設問に答えることによって、客観的に評価を下せるように作られている。
しかしながら、英日翻訳と日英翻訳の技術レベルの違いに基づいて、英日用のテストセットとは少し異なった視点でテストセットの開発を行なった。
実際のテストセットの例を図４に示す。
JET140000（１−４）複合述部JEX140000複合述部では、並列用言の認識を行ない、また用言部と格要素・副詞句とをJEX140000区別して翻訳しなければならない。
JEQ141000複合述部の並列用言としての認定JEX141000複合述部の並列用言を認識するには、JEX141000・助詞の種類により判断するJEX141000・助詞の種類と名詞の意味属性により判断するJEX141000・用言性の単語が並んでいれば、並列用言と認定するJEX141000等といった方法がある。
JEG141001私達は研究開発する。
JEE141001 We do research and development. JEE141001 We are carrying out research and development. JEC141001 (失敗例) We study it‖develop it. JEC141001 (失敗例) We develop a research. JEC141001「私達は研究開発する」の「研究開発」が「研究し開発する」という意味にJEC141001訳出されるかを確認する。
JEX141001読点で切られている場合でも、前半はサ変名詞を動詞化する「する」が記述JEX141001されないことがある。
JEG141002検査者は部品を修理、計器を点検する。
JEE141002 The tester repairs the parts and checks the meter. JEQ142000複合述部の要素の格要素としての認定JEX142000複合述部の要素を格要素として認識するには、JEX142000・複合語要素間の関係を用言と格要素への意味的制約により解析するJEX142000・用言性の部分とそれ以外の部分を判断してデフォルト的に格関係を推定するJEX142000等の方法がある。
JEG142001牛乳は栄養豊富である。
JEE142001 Milk is very nutritious. JEE142001 Milk is very rich in nutrition. JEC142001「牛乳は栄養豊富である」の「栄養」と「豊富」からJEC142001「牛乳の栄養が豊富である」という関係を捉える。
JEQ143000複合述部の要素の副詞句としての認定JEX143000複合述部の要素を副詞句として解釈する場合がある。
これを行うには、JEX143000・語の種類により副詞句となりえる要素を複合語より抽出するJEX143000・用言性の部分と副詞句となりえる部分との共起可能性を判断し、決定するJEX143000等の処理が必要となる。
JEG143001資料は当日配布すること。
JEE143001 Distribute materials on the day. JEC143001「当日に配布する」というように「当日」が述部修飾になっているか確認する。
JEG143002渋滞が自然解消する。
JEE143002 The traffic jam dissolved by itself. JEC143002「自然に解消する」のように、「自然」が副詞として解釈されているか確認する。
JEC143002複合述部が同一の要素を含んでいても、述部によりその要素のJEC143002役割が異なってくる場合がある。
JEG143003住民が自然保護する。
JEE143003 The inhabitants conserve nature. JEC143003「自然を保護する」と「自然」が目的格に捉えられているか確認する。
図４日英機械翻訳システム用テストセットの例
我々は、客観的評価を実現するテストセットの採用に加えて、日本語処理システムの開発者の利便を考え、言語現象と処理モジュールとの対応を取ることができる形式の評価方法の開発を行なった。
すなわち、評価用例文と、その翻訳結果を評価する手段(設問)を提供するだけでなく、各言語現象に対応してシステムがどのような処理を行なっているかを把握するための解説も付与している。
解説によって示される言語現象の処理方法を利用して開発者は、そのシステム全体としての言語現象の処理能力を評価するとともに、処理の各段階が充分な能力を持っているかどうかを把握できる。
具体的には言語現象を約４０種類に大別し、その各項目について問題となっている言語現象をどのように処理しているかを調べるための解説が付加されている。
言語現象の項目リストを図５に示す。
ここでは必要に応じて、用いている知識や処理結果の取り扱い等も併せて説明される。
各項目内の個別の言語現象については、その言語現象を含む日本語文、その英訳、ここで確認するべき要素の解説が記述されている。
設問数は約３３０、機能確認のための対訳例は、約４００文の構成となっている。
（１）述部（１−１）述部の訳し分け（１−２）断定文（１−３）体言述語（１−４）複合述部（１−５）訳が一用言となる並列用言（１−６）用言の副詞（句）化（１−７）補助動詞（１−８）基本動詞の訳し分け（２）名詞（２−１）名詞の訳し分け（２−２）複合名詞（２−３）「名詞１の名詞２」という構造を持つ名詞句の処理（２−４）「名詞１の名詞２の〈名詞３〉」という構造を持つ名詞句の処理（２−５）並列構造を持つ名詞句の処理（２−６）疑問表現の名詞節の処理（２−７）用言性名詞（サ変名詞）（２−８）英語における数の扱い（２−９）固有名詞表現（２−１０）形式名詞（２−１１）関係を示す名詞（３）副詞（３−１）副詞のタイプ（３−２）副詞句（３−３）擬音語・擬態語（４）連体修飾語句（４−１）非活用連体修飾（４−２）用言性連体詞（４−３）格助詞相当句（４−４）埋め込み文修飾（５）助詞（５−１）助詞の訳し分け（５−２）深層格の認定（６）接辞（７）テンス、アスペクト、モーダル（７−１）テンスの処理（７−２）アスペクトの処理（７−３）モーダルの処理（７−４）ボイスの処理（８）特殊構造表現（８−１）慣用表現の処理に関して（８−２）四字熟語（８−３）呼応表現（８−４）天候・気象表現（８−５）無生物主語構文（８−６）「はが」構文（８−７）比較表現（８−８）比喩表現（８−９）部分否定、二重否定、倒置文（８−１０）敬語（８−１１）引用・伝聞表現（８−１２）例示・列挙表現
図５テストセットの項目リスト
また、開発者がこのテストセットを使用する際の利便性を考え、テストセットの書式を揃え、各文にインデックスをつけることにより、機械上での検索を容易に行なえるようにした。
上記の各項目に付けられたインデックスは基本的に図６のような構造である。
図６の??????の部分には、数字またはアルファベットが使用される。
最初の２文字がタイトルまたはサブタイトルの章番号を表す。
次の３文字が、各項目中の設問に付与された番号であり、設問は最大３階層になっている。
最後の１文字が例文及び翻訳例の文番号を示す。
解説、コメントはその対象とする項目と同じ文番号となる。
JET??????項目タイトルJEX??????全体的な解説・説明JEQ??????設問（着目すべき主題）JEX??????主題に対する解説（省略されることもある）JEG??????日本文JEE??????英文対訳例JEC??????訳例に対するコメント（チェックすべきポイント,失敗例）
図６インデックスの構造
これらのインデックスを検索のキーとして、各種のＯＳの検索コマンドを使用することにより、機械翻訳にかけるための原文のみの抽出や、項目リストの抽出など、簡単に必要な部分だけを抜きだして使用することが出来る。
使用例を図７に示す。
【使用例】（日英評価基準のファイル名がMT_EVAL_JE.docであるとする）
・まず文法項目の目次を調べる。
$ grep JET MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾JET100000（１）述部JET100000（１−１）述部の訳し分けJET120000（１−２）断定文JET130000（１−３）体言述語JET140000（１−４）複合述部：
・（１−４）の「複合述部」にどのような設問があるかを調べる。
$ grep JEQ14 MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾JEQ141000複合述部の並列用言としての認定JEQ142000複合述部の要素の格要素としての認定JEQ143000複合述部の要素の副詞句としての認定
・次に「副詞句」のところにどのような例文があるか調べる。
$ grep JEG143 MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾JEG143001資料は当日配布すること。
JEG143002渋滞が自然解消する。
JEG143003住民が自然保護する。
・例文の参考訳を調べる。
$ grep JEE143 MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾JEE143001 Distribute materials on the day. JEE143002 The traffic jam dissolved by itself. JEE143003 The inhabitants conserve nature.
・例文JEG143001が何を調べたいの例文なのかを調べる。
$ grep JEC143001 MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾JEC143001「当日に配布する」というように「当日」が述部修飾になっているか確認する。
・例文JEG143001をＭＴで訳させた結果が、コメントJEC143001の確認事項を満たしていれば評価結果を○、さもなければ評価結果を×とする。
（JEE143001は参考訳であり、必ずしもその通りの訳になっていなくとも良い）
図７テストセットの機械上での使用例
これまで、機械翻訳システムの品質評価法として種々の方法が提案されているが、それらの方法に関しては一貫して客観的評価が困難であるという指摘が行なわれてきた。
本稿ではまず、従来までの評価法と比べての本評価法の利点を、以下の二つの客観性に基づいて検討する。
(1)評価過程が客観的であること
(2)評価結果の判断が客観的に行なえること
たとえば、ＡＬＰＡＣレポート等に代表される評価法は、評価の軸として「忠実度」「理解容易度」といった、その解釈が評価者の主観的判断に依存する基準を採用している。
その結果、評価結果が評価者によって大きく異なってしまうという問題があり、(１)の客観性を満たしていない。
この評価のばらつきは不完全な翻訳結果を評価する際に特に顕著に現れるが、現実の機械翻訳システムを評価し、開発過程にフィードバックする際には、翻訳に成功した場合よりも失敗した場合についての検討が重要である。
この種の評価法においては評価結果は数値で表現されているため、ある意味では、(２)の客観性を満たしているともいえよう。
しかしながら開発者にとっては、自己のシステムが、どの言語現象をどの原因によって処理できなかったのかを判断することが特に重要であり、言語現象が複雑に絡みあった文の翻訳結果を単純に得点化するだけでは有効とはいえない。
システム改良に用いるためにその評価結果を解釈しようとする場合には主観的な判断に頼らざるを得ないので、実用的にはこの評価法は(２)の客観性を満たしているとはいえない。
一方、我々の開発した評価法においては、これら二つの客観性は共に保たれている。
ここでは、単にそれに答えるだけで、システム開発者が自己のシステムの性能評価を行なえるように作られたyes/no設問を各例文に付加することにより、翻訳結果を評価する手続きを明確化した。
評価過程で必要とされる手順は単純なyes/no疑問文に答えることだけであり、誰でも機械翻訳システムを同様に評価することが出来る。
不完全な翻訳文に対しても、評価者によって評価が大幅に変わるということはない。
さらに、各例文には翻訳処理と言語現象との関係を表す解説が付加されており、これにより、システム開発者はなぜ自己のシステムが問題の言語現象を正しく解析できないのかを知ることが出来る。
すなわち、我々のテストセットに基づく評価結果を用いて、機械翻訳システムの改良法を決定することが出来る。
機械翻訳システムの評価に関しては、既に述べたように、評価すべき言語現象を含む文を集めた評価用例文集の作成という試みもなされている(成田1988,池原・小倉1990,池原他1994)。
このような例文集を用いれば、もしシステムが、ある例文を正しく翻訳できないと評価された場合には、システム開発者はただちにその例文が問題としている言語現象をそのシステムが処理できないということが分かる。
この点において、この手法もまた(２)の客観性を保持している。
しかしながら、この手法には以下の二つの問題がある。
例文の翻訳結果を評価する手順が明示されていない。
評価結果から機械翻訳システムの不備な点を見つけ出す過程が評価者の言語直観に頼っている。
例文を集めただけのもの(テストスゥィート(Test Suite))では、個々のシステムのad hocな評価は可能であっても、評価法としては確立しない。
明確に記述された手続きにしたがって、誰でも同じように機械翻訳システムを評価できることが必要である。
この目的のために各例文に設問や訳出例を付与しているということを明確にする意味で、我々の評価法においては「テストセット(Test Set)」という名称を用いている。
また、評価結果を機械翻訳システムの改良に用いるためには、さまざまな言語現象を単に羅列しておくだけでは不十分である。
文法体系の中での各言語現象の位置づけを明確にしておくことも必要である。
このような考察のもと、我々は上で述べた評価過程の客観性と結果の判断の客観性という二つの客観性を追求した品質評価を可能とする品質評価用テストセットを提案してきた。
ここで用いるテストセットは、考慮すべき文法項目を系統立てて収集し、その各項目に例文を付加して作られた。
各例文に解説や設問を付加することによって評価の手順を明確に記述することが可能となった。
各テストセットには、評価用例文、その人間による模範訳、システムの出力(翻訳結果)を評価するための設問などが記述されており、評価者はテストセット中の例文を翻訳し、その翻訳結果を参照しながら各例文に付与された設問に回答していく。
ここで各設問は判断のポイント(すなわち、例文のどの部分が、どのような役割で、どのような訳文となっていれば良いか)が明示されたyes/no質問文であり、評価者によって判断が異なることがないように作られている。
この判断をさらに容易にするために、既存の機械翻訳システムでの翻訳結果を用いた回答例が付与されている。
以上により評価過程の客観性を実現している。
また、各例文には、その文がどのような言語現象を評価するためのものであるかを説明する解説が付与されており、開発者はその例文に対する翻訳結果から自己のシステムが十分には対応していない言語現象を容易に理解することが出来る。
これにより、評価結果の判断の客観性を実現できる。
この評価用テストセットは、個々の機械翻訳システムに依存しない汎用の品質評価法として作成している。
したがって、対象とするシステムがルールベース・知識ベース・用例ベース・直接型といった機械翻訳のどの手法を採用しているかには依存しない。
このテストセットの目的は機械翻訳システムの開発者が自己のシステムの性能を向上するために、システムの処理できない言語現象を正確に把握することである。
その言語現象を処理可能にするための手法は、個々のシステムあるいは個々の手法によって異なっており、その判断は開発者に任すこととし、評価基準としては、そこには立ち入らない。
用例ベースの手法とルールベースの手法に共通する解決策を評価法が示すということは現実的ではない。
また、個々のシステムによって、対象とする文書が異なっており、各言語現象の出現頻度も異なっている。
したがって、システムの欠点のうちで、どの欠点が最も重大であるかを決定することは、当事者にのみ可能なことである。
本テストセットの目標は、その当事者の判断を可能な限り援助することにある。
ここではテストセット中の各例文には、頻度に関する情報を記述するのではなく、その例文が判断する言語現象を記述してある。
翻訳対象となる文書が特定の言語現象に偏っている場合には、評価者はこのテストセットのうちで、必要な言語現象に対応する部分についてのみ翻訳し、その結果を評価すれば良い。
自分にとって重要な言語現象を取り扱えるかどうかが、個々の開発者あるいはユーザがシステムを評価する場合には重要であり、評価法としての独自の頻度による一般的な得点化を行なうのは、むしろ誤った評価の原因になると考える。
また、評価に例文を用いることについては、その例文に対して高い評価が出るようにシステムを修正することが可能であること、また、全ての言語現象を網羅できるわけではないことなどの問題点が指摘される。
しかしながら、ここで提案する評価法はシステム間の相対的な性能評価のために用いるものではない。
開発者が自己のシステムの改良のために、その欠点を把握することが目的であり、この本来の目的のためには本テストセットに対してチューニングをすることに意味はない。
また、本テストセットは単なる例文集ではなく、各例文にはその対象とする言語現象が解説されており、さらには必要に応じて関連文と、その模範訳が付加されている。
これらの文を翻訳し検討することにより、単に一つの例文を処理できるかどうかを判断するだけではなく、その例文に関連する言語現象についての処理能力も知ることが出来る。
さらに、個々の開発者が処置するべき問題として、テストセット中の例文に存在する未定義語の問題がある。
例文中に(そのシステムにとっての)未定義語があった場合には、評価者は例文中に現れた未定義語を辞書登録するか、あるいは例文中の未定義語を既にシステムに登録されている類似の単語に変更することが要求される。
繰り返すが、この評価法はシステム間の優劣を決めることが目的ではなく開発者が自分のためにシステムの欠陥を見つけて、それを修正することを主たる目的としている。
したがって、評価者(すなわち開発者)は単純に評価結果を受け入れるのではなく、「翻訳に成功しているが偶然良い訳語が記述されていただけだ。
」「翻訳に失敗したが、それはその単語が未定義であったためで、類似の現象自体は取り扱う能力がある。
」等については、各自の(自己のシステムについての)知識に基づいて判断する必要がある。
また、評価の結果、さまざまな欠陥が見つかった場合に、限られた人的資源の中で、どのような順序でそれを解決していくかという問題もある。
しかしながら、各開発者毎に資源の制約や、そのシステムが主として対象とする文書(あるいは、対象とする言語現象)が異なるため、一般的な優先順位を予め定めておくことは現実的ではない。
本テストセットは、比較的近い将来に正しい処理の実現が可能な言語現象に重点をおいて作っているが、取り扱えなかった言語現象の内で、まずどの現象を処理可能にするかという優先順位付けは、個々のシステムの開発者に任せられている。
なお、このように近い将来に対応できるものに重点を置いて言語現象を収集しテストセットとした場合、機械翻訳システムの技術水準の向上に伴って、対象とする言語現象を継続的に追加あるいは削除していくことが望まれる。
常にその時点で機械翻訳において問題となっている言語現象を１０００文程度のテストセットで示すのが理想であろう。
ただし、最低限の解析能力を試すための基本的な構造の文は現在も(そのような基本的な構造の解析は既にほとんどのシステムにおいて解決されている問題であるとしても)テストセット中に含まれている。
このような基本文はシステムの最低水準を保証するものとして、将来に亙ってもテストセットに含まれると想定している。
テストセットの例文は機械翻訳システムや自然言語処理システムを実際に開発してきた経験に基づいて、著者らによって収集された。
例文の収集に当たっては、我々は以下の２点を重視した。
(1)基本的な言語現象を網羅すること。
(2)機械翻訳システムにとって処理することが困難な言語現象を含む例文を選択すること。
ここでは特に曖昧性の問題を重視した。
言い替えると、(１)は評価すべき文法現象を系統立てて収集分類(トップダウンの手法)し、それらの現象に対応する例を集めることである。
一方(２)は機械翻訳システムによって翻訳することが困難であるような例文を収集する(ボトムアップの手法)ことである。
特に我々は処理の困難さが近い将来に解決できるであろうような言語現象に注目した。
そして機械翻訳システムの評価のための例文を系統立てて分類した。
さらに、我々はこれらの例について、いくつかの商用システムを用いて翻訳評価実験を繰り返し、テストセットを以下の点に焦点を当てながら改良した。
これらは全て、評価過程において客観性を維持するために重要な要素である。
設問に曖昧性がないこと
例文に不必要な複雑さがないこと
翻訳結果に曖昧性がないこと
なお、テストセット中の英文は、その英語としての品質を保証するため、英語を母国語とし、日本語を理解する自然言語処理研究者によって、チェックされ修正された。
なお、このテストセットを用いた品質評価法の提案の主旨と、作成の詳しい経緯については、参考文献(井佐原他1992,日本電子工業振興協会1993,日本電子工業振興協会1994, Isahara et al 1994,日本電子工業振興協会1995a, Isahara 1995)を参照されたい。
また、テストセットの全容は、参考文献(日本電子工業振興協会1995b)に示されている。
本節では、英日機械翻訳システムの品質評価用テストセットについて説明する。
このテストセットは、機械翻訳システムが処理すべき様々な言語現象を含んだ英語例文770文とその模範訳、及びシステムの出力(翻訳結果)を評価するための設問などからなる。
我々は、英日機械翻訳システムの評価基準として、システム開発者が自己のシステムの不備をチェックすることを主要目的とした品質評価用テストセットを作成した。
本テストセットにおける例文の収集に際しては、「基本的な言語現象を網羅すること」「機械翻訳システムが取り扱うことが困難な言語現象を、主に曖昧性の解消に注目して収集・分類すること」を試みた。
また、システムの出力(翻訳結果)を見ながら回答していくことで品質に関する客観的な判断が可能となるように、各文に判断のポイントを明示したyes/no疑問文の形式の設問を付与している。
このように本テストセットは客観的な品質評価の実現を目指して作成したものなので、ユーザが各機械翻訳システムの出力品質を比較する際に利用することも可能である。
本テストセットの作成作業は、平成４年度からの３年間で行った。
平成５年度末までに第１段階として、英語の単文を中心に評価すべき項目を抽出して評価基準を設定し、309の基本例文を収集・評価して「電子協平成５年度版テストセット」としてまとめた。
これに加えて、今回さらに以下の作業を行って項目の充実を図った。
平成５年度版テストセットが単文中心だったのに対して、接続詞、関係詞、比較、話法、挿入、並列など、より複雑な構造を持つ複文・重文に関連する項目を重点的に英文法の解説書などから抽出して収集
複数の文法書などを参考にすることにより、単文内の項目に関しても、平成５年度版テストセットでカバー出来ていない項目を収集。
特に、代名詞、前置詞、記号、数量表現などに関して新規の設問を多数作成
文法項目の洩れを防ぐため、英字新聞から英文テキスト300文を選出して市販の英日機械翻訳システムで試訳し、翻訳が困難となる問題点を抽出
上記の作業により、これまでの309項目と併せて延べで約1000の項目を抽出した。
最終的にこれを整理して、770項目からなるテストセットとしてまとめた。
例文と関連文を合わせると、合計で1450文ほどの規模のテストセットとすることが出来た。
また、本テストセットの実用性の検証と設問の修正のために、ハードウェアタイプの異なる８種の市販の英日機械翻訳システムを対象とした評価を行った。
このテストセット中の各項目は、文番号、例文、その模範訳、○×で答えることが出来る質問文、主として機械翻訳システムによる訳出例、例文と関連する言語現象を含む文、関連する項目の番号、解説から成り立っている。
テストセットの例を図１に示す。
以下では、このテストセットを用いた品質評価の手順、対象とする言語現象、テストセットの書式について述べる。
２．
１．
１多品詞（品詞認定）２．
１．
１．
２名詞/助動詞
【番号】2.1.1.2-1【例文】The trash can was thrown away.【訳文】ごみカンは捨てられた。
【質問】"can"が「カン/缶」のように名詞として訳されていますか？【訳出例】○(くず缶/ごみ容器/くず入れ)は(廃棄された/[投げ]捨てられた)。
×ごみは捨てられ得る。
【関連文】The last will was opened.「最後の遺言書は開けられた。
」【参照項目】2.1.1.2-2, 2.1.1.2-3【解説】"can was"の並びから、"can"が助動詞でないことがわかる。
【番号】2.1.1.2-2【例文】The trash can be thrown away.【訳文】ごみは捨てられ得る。
【質問】"can"が「〜できる/得る」のように助動詞として訳されていますか？【訳出例】○(くず/ごみ/くだらない人間)は(廃棄できる/[投げ]捨てられることができる)。
×ごみカンは捨てられた。
【関連文】【参照項目】2.1.1.2-1, 2.1.1.2-3【解説】2.1.1.2-1とは逆に、ここでは"can"は名詞ではなく助動詞。
図１英日機械翻訳システム用テストセットの例
本テストセットは、以下の利用法を想定している。
(1)評価対象となる英日機械翻訳システムを用意する。
(2)そのシステムでテストセット中の【例文】を翻訳する。
(3)【質問】【訳出例】を見て、その翻訳結果が○か×かを判断する。
(4)システム開発者は、○×の分布からシステムの能力、開発段階を評価する。
特に、×と判断した項目に関連する文法・辞書を追加することで、システムの改良を図る。
( (5)ユーザは、各システムの○×の分布から、出力品質面での優劣を比較する。
)
(6)各項目についてさらに詳細に評価を行う場合は、【関連文】を利用する。
原則として翻訳結果と質問文を見るだけで○×を回答出来るようになっているが、【訳出例】(各訳出例には、質問に対する○×が予め付与されている)を参照することによって、さらに容易に判断が出来るようになっている。
本テストセットを用いて○×の分布を見ることで、システムの対応が不十分な(可能性がある)項目を容易に抽出できる。
ただし本テストセットでは、各項目(例文)間の重要度、頻度などの差異は考慮していないので、単純に○の数をカウントして正解率をシステム間で比較することは、本評価法の意図するところではない。
本テストセットは、機械翻訳システムが処理すべき様々な言語現象を含んだ英語例文770文からなる。
内訳と項目ごとの設問数を図２に示す。
品質評価の対象項目の収集に当たっては、網羅性を保証するトップダウンのアプローチと、機械翻訳における問題点を実際の翻訳結果から抽出して、その問題性によって例文の粗密を決定するボトムアップなアプローチを組み合わせている。
把握部においては、英文法の解説書(江川1964, Hornby 1977,小川他1991,荒木他1992,村田1992)などを参考に英語の文法現象を収集し、そのレベルによって、品詞、文の部分構造、文構造の３段階に分類した。
特に動詞、形容詞、名詞に関してその基本的な用法を網羅するために、ホーンビーの分類した文型(Hornby 1977)を設問項目として採用した。
ただしホーンビーのパターンの中でも、機械翻訳システムの品質評価において特に必要でないとみなした区分については分類を省略している。
同様に助動詞等の基本的な用法の中でも、機械翻訳において対象となることが極めて稀であると思われるものについては省略した。
選択部においては、翻訳で実際に問題となる言語現象を、構文構造の曖昧性に関するものと、コロケーション(他の語との共起による訳し分け)に関するものに分類した。
１把握部小計684１．
１品詞小計355１．
１．
１冠詞15１．
１．
２名詞（固有名詞を含む）27１．
１．
３代名詞25１．
１．
４形容詞42１．
１．
５副詞54１．
１．
６前置詞40１．
１．
７動詞類１．
１．
７．
１動詞・準動詞48１．
１．
７．
２助動詞37１．
１．
８関係詞25１．
１．
９接続詞26１．
１．
１０記号16１．
２文の部分構造小計167１．
２．
１不定詞26１．
２．
２分詞、分詞構文19１．
２．
３動名詞23１．
２．
４時制63１．
２．
５数量表現28１．
２．
６慣用表現8１．
３文構造小計162１．
３．
１文種（疑問文、命令文、感嘆文）19１．
３．
２否定16１．
３．
３特殊構文19１．
３．
４比較21１．
３．
５仮定法（条件法）16１．
３．
６態10１．
３．
７話法4１．
３．
８挿入16１．
３．
９省略9１．
３．
１０倒置7１．
３．
１１並列句25２選択部小計86２．
１構文２．
１．
１多品詞（品詞認定）34２．
１．
２係り先認定27２．
２コロケーション25総計７７０
図２テストセットの全体構成、項目別設問数
本テストセットの各項目の書式を図３に示す。
なお、テストセット中で、[ ]で囲まれた部分は挿入可能な表現を、( / )で囲まれた部分はいずれかを選択する表現を示す。
たとえば、"Ａ[Ｂ]Ｃ(Ｄ/Ｅ)Ｆ"という記号列は、``ＡＢＣＤＦ'', ``ＡＢＣＥＦ'', ``ＡＣＤＦ'', ``ＡＣＥＦ''の４種の記号列を表す。
【番号】：例文の番号【例文】：例文（１文のみ）【訳文】：模範訳（例文の日本語訳）【質問】："Ａ"が「Ｂ」のようにＣとして訳されていますか？という形式の質問文・Ａ：英語表現。
" "で囲む。
例文中のどの部分を翻訳することにポイントがあるのかを表す。
文全体の場合、また明らかな場合などは省略する。
・Ｂ：日本語表現。
「」で囲む。
・Ｃ：内容や文法事項の補足説明（「習慣を表す表現」、「選択疑問文」等）を記す。
記述が長くなるものや、原因に言及する場合は、【解説】に記述する。
※必ず○か×か（yes/no）で答えられる形式にする。
本テストセットは作業者が翻訳結果（訳文）を見るだけで○×を与えることを前提としており、解析の詳細に直接言及することは避ける。
【訳出例】：許容される訳出例や誤訳例を列挙。
・正解例（yes）の文頭には○、誤例（no）の文頭には×を付与する。
・１行１文とし、原則として文全体を記述。
・必要ならば正／誤の理由（説明）も示す。
※各例は実際の機械翻訳システムの訳を参考にして作成した。
【関連文】：当該の例文と関連する言語現象を含んだ例文を挙げる。
・文の一部だけの記述は認めない。
必ず文全体を記述する。
・例文の後に、「」で囲んだ訳文を記述する。
・補足事項（may/mightでの丁寧度の違いなど）がある場合は、訳文の後に（）で囲んで記述する。
【参照項目】：本テストセット内の関連項目への参照ポインタ。
原則として、相互参照とする。
※文番号を明示するのみで、文そのものは記述しない。
【解説】：その他の補足事項。
フリーフォーマット。
図３テストセットの書式
日英翻訳システム品質評価用テストセットも英日翻訳システム用と同様に、開発者が自己のシステムの不備な点を発見するための評価法であり、テストセット中の各例文に付与された設問に答えることによって、客観的に評価を下せるように作られている。
しかしながら、英日翻訳と日英翻訳の技術レベルの違いに基づいて、英日用のテストセットとは少し異なった視点でテストセットの開発を行なった。
実際のテストセットの例を図４に示す。
JET140000（１−４）複合述部JEX140000複合述部では、並列用言の認識を行ない、また用言部と格要素・副詞句とをJEX140000区別して翻訳しなければならない。
JEQ141000複合述部の並列用言としての認定JEX141000複合述部の並列用言を認識するには、JEX141000・助詞の種類により判断するJEX141000・助詞の種類と名詞の意味属性により判断するJEX141000・用言性の単語が並んでいれば、並列用言と認定するJEX141000等といった方法がある。
JEG141001私達は研究開発する。
JEE141001 We do research and development. JEE141001 We are carrying out research and development. JEC141001 (失敗例) We study it‖develop it. JEC141001 (失敗例) We develop a research. JEC141001「私達は研究開発する」の「研究開発」が「研究し開発する」という意味にJEC141001訳出されるかを確認する。
JEX141001読点で切られている場合でも、前半はサ変名詞を動詞化する「する」が記述JEX141001されないことがある。
JEG141002検査者は部品を修理、計器を点検する。
JEE141002 The tester repairs the parts and checks the meter. JEQ142000複合述部の要素の格要素としての認定JEX142000複合述部の要素を格要素として認識するには、JEX142000・複合語要素間の関係を用言と格要素への意味的制約により解析するJEX142000・用言性の部分とそれ以外の部分を判断してデフォルト的に格関係を推定するJEX142000等の方法がある。
JEG142001牛乳は栄養豊富である。
JEE142001 Milk is very nutritious. JEE142001 Milk is very rich in nutrition. JEC142001「牛乳は栄養豊富である」の「栄養」と「豊富」からJEC142001「牛乳の栄養が豊富である」という関係を捉える。
JEQ143000複合述部の要素の副詞句としての認定JEX143000複合述部の要素を副詞句として解釈する場合がある。
これを行うには、JEX143000・語の種類により副詞句となりえる要素を複合語より抽出するJEX143000・用言性の部分と副詞句となりえる部分との共起可能性を判断し、決定するJEX143000等の処理が必要となる。
JEG143001資料は当日配布すること。
JEE143001 Distribute materials on the day. JEC143001「当日に配布する」というように「当日」が述部修飾になっているか確認する。
JEG143002渋滞が自然解消する。
JEE143002 The traffic jam dissolved by itself. JEC143002「自然に解消する」のように、「自然」が副詞として解釈されているか確認する。
JEC143002複合述部が同一の要素を含んでいても、述部によりその要素のJEC143002役割が異なってくる場合がある。
JEG143003住民が自然保護する。
JEE143003 The inhabitants conserve nature. JEC143003「自然を保護する」と「自然」が目的格に捉えられているか確認する。
図４日英機械翻訳システム用テストセットの例
我々は、客観的評価を実現するテストセットの採用に加えて、日本語処理システムの開発者の利便を考え、言語現象と処理モジュールとの対応を取ることができる形式の評価方法の開発を行なった。
すなわち、評価用例文と、その翻訳結果を評価する手段(設問)を提供するだけでなく、各言語現象に対応してシステムがどのような処理を行なっているかを把握するための解説も付与している。
解説によって示される言語現象の処理方法を利用して開発者は、そのシステム全体としての言語現象の処理能力を評価するとともに、処理の各段階が充分な能力を持っているかどうかを把握できる。
具体的には言語現象を約４０種類に大別し、その各項目について問題となっている言語現象をどのように処理しているかを調べるための解説が付加されている。
言語現象の項目リストを図５に示す。
ここでは必要に応じて、用いている知識や処理結果の取り扱い等も併せて説明される。
各項目内の個別の言語現象については、その言語現象を含む日本語文、その英訳、ここで確認するべき要素の解説が記述されている。
設問数は約３３０、機能確認のための対訳例は、約４００文の構成となっている。
（１）述部（１−１）述部の訳し分け（１−２）断定文（１−３）体言述語（１−４）複合述部（１−５）訳が一用言となる並列用言（１−６）用言の副詞（句）化（１−７）補助動詞（１−８）基本動詞の訳し分け（２）名詞（２−１）名詞の訳し分け（２−２）複合名詞（２−３）「名詞１の名詞２」という構造を持つ名詞句の処理（２−４）「名詞１の名詞２の〈名詞３〉」という構造を持つ名詞句の処理（２−５）並列構造を持つ名詞句の処理（２−６）疑問表現の名詞節の処理（２−７）用言性名詞（サ変名詞）（２−８）英語における数の扱い（２−９）固有名詞表現（２−１０）形式名詞（２−１１）関係を示す名詞（３）副詞（３−１）副詞のタイプ（３−２）副詞句（３−３）擬音語・擬態語（４）連体修飾語句（４−１）非活用連体修飾（４−２）用言性連体詞（４−３）格助詞相当句（４−４）埋め込み文修飾（５）助詞（５−１）助詞の訳し分け（５−２）深層格の認定（６）接辞（７）テンス、アスペクト、モーダル（７−１）テンスの処理（７−２）アスペクトの処理（７−３）モーダルの処理（７−４）ボイスの処理（８）特殊構造表現（８−１）慣用表現の処理に関して（８−２）四字熟語（８−３）呼応表現（８−４）天候・気象表現（８−５）無生物主語構文（８−６）「はが」構文（８−７）比較表現（８−８）比喩表現（８−９）部分否定、二重否定、倒置文（８−１０）敬語（８−１１）引用・伝聞表現（８−１２）例示・列挙表現
図５テストセットの項目リスト
また、開発者がこのテストセットを使用する際の利便性を考え、テストセットの書式を揃え、各文にインデックスをつけることにより、機械上での検索を容易に行なえるようにした。
上記の各項目に付けられたインデックスは基本的に図６のような構造である。
図６の??????の部分には、数字またはアルファベットが使用される。
最初の２文字がタイトルまたはサブタイトルの章番号を表す。
次の３文字が、各項目中の設問に付与された番号であり、設問は最大３階層になっている。
最後の１文字が例文及び翻訳例の文番号を示す。
解説、コメントはその対象とする項目と同じ文番号となる。
JET??????項目タイトルJEX??????全体的な解説・説明JEQ??????設問（着目すべき主題）JEX??????主題に対する解説（省略されることもある）JEG??????日本文JEE??????英文対訳例JEC??????訳例に対するコメント（チェックすべきポイント,失敗例）
図６インデックスの構造
これらのインデックスを検索のキーとして、各種のＯＳの検索コマンドを使用することにより、機械翻訳にかけるための原文のみの抽出や、項目リストの抽出など、簡単に必要な部分だけを抜きだして使用することが出来る。
使用例を図７に示す。
【使用例】（日英評価基準のファイル名がMT_EVAL_JE.docであるとする）
・まず文法項目の目次を調べる。
$ grep JET MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾JET100000（１）述部JET100000（１−１）述部の訳し分けJET120000（１−２）断定文JET130000（１−３）体言述語JET140000（１−４）複合述部：
・（１−４）の「複合述部」にどのような設問があるかを調べる。
$ grep JEQ14 MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾JEQ141000複合述部の並列用言としての認定JEQ142000複合述部の要素の格要素としての認定JEQ143000複合述部の要素の副詞句としての認定
・次に「副詞句」のところにどのような例文があるか調べる。
$ grep JEG143 MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾JEG143001資料は当日配布すること。
JEG143002渋滞が自然解消する。
JEG143003住民が自然保護する。
・例文の参考訳を調べる。
$ grep JEE143 MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾JEE143001 Distribute materials on the day. JEE143002 The traffic jam dissolved by itself. JEE143003 The inhabitants conserve nature.
・例文JEG143001が何を調べたいの例文なのかを調べる。
$ grep JEC143001 MT_EVAL_JE.doc‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾JEC143001「当日に配布する」というように「当日」が述部修飾になっているか確認する。
・例文JEG143001をＭＴで訳させた結果が、コメントJEC143001の確認事項を満たしていれば評価結果を○、さもなければ評価結果を×とする。
（JEE143001は参考訳であり、必ずしもその通りの訳になっていなくとも良い）
図７テストセットの機械上での使用例
