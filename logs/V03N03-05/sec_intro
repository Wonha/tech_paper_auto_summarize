機械翻訳システムの長い歴史の中で、システム評価は常に大きな課題の一つであった。
システムの研究開発が健全に進むためには、客観的かつ正確な評価法が必要となる。
このため、ユーザの立場から評価を行うもの、開発者の立場から評価を行うもの、また、技術的側面から評価を行うもの、経済的側面から評価を行うものと、多くの研究者によって様々な視点からの評価法が検討されてきた。
これらの検討に基づいて、(社)日本電子工業振興協会によって一連の機械翻訳システム評価基準が開発されてきた(野村・井佐原1992, Nomura and Isahara 1992a, Nomura and Isahara 1992b,日本電子工業振興協会1993)。
本稿で提案する機械翻訳システムの評価法は、システムの改良を続ける開発者の立場から、機械翻訳システムの技術面を翻訳品質に注目して評価するものである。
機械翻訳システムの訳文の品質面での評価に関しては、従来からのいわゆるＡＬＰＡＣレポート型の評価法に加えて、近年、いくつかの提案がなされている。
ある程度まとまった文章を翻訳し、そこから得られる理解の度合を評価しようとするものとして、ＡＲＰＡによる機械翻訳システム評価(White et al 1994)や、ＴＯＥＦＬのテストを用いる方法(Tomita 1992)が提案されているが、これらはシステム間の現時点での性能の比較評価には用いることが出来ても、評価結果を直接システム改良に結び付けることは困難である。
これに対し、個別の例文を収集することにより評価用の例文集を作成し、その各例文の翻訳結果を評価し、対応する言語現象の処理能力を判定しようとする提案がいくつかなされている。
これらのうちには、単に文を集めるのみで、その後の例文の利用法(評価過程)は個別の評価者に任せようというものから、本稿で提案するように、客観的評価のためにさまざまな情報を付加しようというものまで、いくつかの段階がある。
わが国においては、(社)日本電子工業振興協会が既に昭和６０年に機械翻訳例文資料として、翻訳における曖昧性に関する問題点に着目して、英文および和文を収集分類し公開している(日本電子工業振興協会1985)。
また同協会は昭和６２年度に、機械翻訳システムの技術レベルを評価するために、文の複雑さの定量化、文の複雑さや文体の定性的特徴の抽出、標準的例文の収集を行なった(日本電子工業振興協会1988,石崎・井佐原1988)。
この他、英語を話す人間と日本語を話す人間との間にある言語理解法の違い(言い替えると、日本語と英語の発想法の違い)に注目して日本語の言語表現を分類し、それらの表現の翻訳能力を評価する試験文集を作成するもの(池原・小倉1990,池原他1994)や、言語学的観点から日本語および英語の言語表現の構造に注目し、その表現上の構造的特性を的確に表すような試験文集を作成すること(成田1988)が提案されてきた。
後者は、個々の言語現象に対する翻訳の可否を示すことの必要性から、一定の内容の文の言い換えなどによって日本語および英語の言語表現と翻訳能力の関係を言語学者の立場から評価することを提案している。
本稿で論じる機械翻訳システム評価用テストセットは、以上のような、ＡＬＰＡＣレポート以来の品質評価に関する研究を踏まえて、誰でも客観的かつ実用的な評価を行なえる評価法の確立を目指し作成したものである。
次節以下では、テストセットを用いた評価法の全体を流れる基本的な考え方、英日機械翻訳用テストセット、日英機械翻訳用テストセットについて、順次説明していく。
機械翻訳システムの長い歴史の中で、システム評価は常に大きな課題の一つであった。
システムの研究開発が健全に進むためには、客観的かつ正確な評価法が必要となる。
このため、ユーザの立場から評価を行うもの、開発者の立場から評価を行うもの、また、技術的側面から評価を行うもの、経済的側面から評価を行うものと、多くの研究者によって様々な視点からの評価法が検討されてきた。
これらの検討に基づいて、(社)日本電子工業振興協会によって一連の機械翻訳システム評価基準が開発されてきた(野村・井佐原1992, Nomura and Isahara 1992a, Nomura and Isahara 1992b,日本電子工業振興協会1993)。
本稿で提案する機械翻訳システムの評価法は、システムの改良を続ける開発者の立場から、機械翻訳システムの技術面を翻訳品質に注目して評価するものである。
機械翻訳システムの訳文の品質面での評価に関しては、従来からのいわゆるＡＬＰＡＣレポート型の評価法に加えて、近年、いくつかの提案がなされている。
ある程度まとまった文章を翻訳し、そこから得られる理解の度合を評価しようとするものとして、ＡＲＰＡによる機械翻訳システム評価(White et al 1994)や、ＴＯＥＦＬのテストを用いる方法(Tomita 1992)が提案されているが、これらはシステム間の現時点での性能の比較評価には用いることが出来ても、評価結果を直接システム改良に結び付けることは困難である。
これに対し、個別の例文を収集することにより評価用の例文集を作成し、その各例文の翻訳結果を評価し、対応する言語現象の処理能力を判定しようとする提案がいくつかなされている。
これらのうちには、単に文を集めるのみで、その後の例文の利用法(評価過程)は個別の評価者に任せようというものから、本稿で提案するように、客観的評価のためにさまざまな情報を付加しようというものまで、いくつかの段階がある。
わが国においては、(社)日本電子工業振興協会が既に昭和６０年に機械翻訳例文資料として、翻訳における曖昧性に関する問題点に着目して、英文および和文を収集分類し公開している(日本電子工業振興協会1985)。
また同協会は昭和６２年度に、機械翻訳システムの技術レベルを評価するために、文の複雑さの定量化、文の複雑さや文体の定性的特徴の抽出、標準的例文の収集を行なった(日本電子工業振興協会1988,石崎・井佐原1988)。
この他、英語を話す人間と日本語を話す人間との間にある言語理解法の違い(言い替えると、日本語と英語の発想法の違い)に注目して日本語の言語表現を分類し、それらの表現の翻訳能力を評価する試験文集を作成するもの(池原・小倉1990,池原他1994)や、言語学的観点から日本語および英語の言語表現の構造に注目し、その表現上の構造的特性を的確に表すような試験文集を作成すること(成田1988)が提案されてきた。
後者は、個々の言語現象に対する翻訳の可否を示すことの必要性から、一定の内容の文の言い換えなどによって日本語および英語の言語表現と翻訳能力の関係を言語学者の立場から評価することを提案している。
本稿で論じる機械翻訳システム評価用テストセットは、以上のような、ＡＬＰＡＣレポート以来の品質評価に関する研究を踏まえて、誰でも客観的かつ実用的な評価を行なえる評価法の確立を目指し作成したものである。
次節以下では、テストセットを用いた評価法の全体を流れる基本的な考え方、英日機械翻訳用テストセット、日英機械翻訳用テストセットについて、順次説明していく。
