本節では，文字モデルに基づく単語分割法[CITE]について説明する．
まず，言語モデルとして，文字[MATH]-gramモデルを用いることを考える．
文字[MATH]-gramモデルでは，言語の文字生起は，[MATH]重マルコフモデルで近似される．
長さ[MATH]の文字列[MATH]において，直前の[MATH]文字のみが次の文字の生起確率に影響する．
実際によく用いられるモデルは，[MATH]あるいは[MATH]のモデルであり，これらはbigramモデル，trigramモデルと呼ばれている．
以下では，[MATH]の文字trigramモデルを用いることで，単語分割モデルの定式化を行う．
単語分割モデルの学習データとしては，単語境界位置の付与されたデータを用いる．
図[REF_Fig:training]に学習データの例を示す．
記号[MATH]は単語境界(単語間のスペース)を表す特殊記号であり，[MATH]と[MATH]はそれぞれ文頭と文末を表す特殊記号である．
単語境界位置の付与された学習データから文字trigramモデルの確率値を推定し，これを用いて単語分割を行う．
与えられた「ベタ書き」文を単語列に分割するためには，入力文中の各文字位置に対し，その文字の前で単語分割が起こるか否かを求めればよい．
このために，それぞれの文字位置に対し，2つの状態1と0を仮定する．
状態1はその文字の前が単語境界となることを表す状態であり，状態0は単語境界とならないことを表す状態である．
文字位置[MATH]の状態の推定は次式で与えられる．
なお，[MATH]は文字列[MATH]を生成して状態[MATH]に到達する確率を表す．
また，文字位置[MATH]の場合は，次式で求めることができる．
ここで，学習データ中の文字位置1の前には単語境界記号がないため，式([REF_Eq:WordSegBeginS])を定義する．
入力文[MATH]に対する最適な単語分割は，各文字位置に対する状態1と0の最適な状態遷移系列として与えられる．
単語分割モデルの計算のため，実際の入力文には，文頭記号と文末記号を各々[MATH]番目と[MATH]番目の文字として加えて処理を行う．
学習データ中の文末記号[MATH]の前には単語境界[MATH]がないので，最適な状態遷移系列は
となるような状態遷移系列である．
これを求めるためには，動的計画法の一種であるビタビ・アルゴリズム(Viterbi algorithm)を用いることができる(図[REF_Fig:viterbi]参照)．
求められた最尤状態遷移系列において，状態1である文字位置の前で単語分割を行う．
図[REF_Fig:viterbi]において単語境界を点線で示す．
文字trigramモデルを言語モデルとして用いた場合，以上の単語分割モデルにより，入力文に対して最適な単語分割を求めることができる．
また，同様の考えに基づいて可変長[MATH]-gramモデル(variable-length [MATH]-gram model)を用いた単語分割を行うことも可能である[CITE]．
その場合は，解探索における単語分割候補の指数的増加を避けるために，各文字位置において確率の高い候補のみを後続する文字位置での探索に用いるようにする．
もし文字trigramモデルによる単語分割モデルと同様に，文字位置[MATH]の直前が単語境界である(状態1)か否(状態0)かの2つの仮定に対する各々の最尤解のみに関して解探索を行うならば，その探索空間は，図[REF_Fig:viterbi]に示す探索空間と同じとなる．

[MATH]-gramモデルに，クラスという概念を導入したモデルを[MATH]-gramクラスモデル([MATH]-gram class model)と呼ぶ[CITE]．
ここで，クラスとは[MATH]-gramモデルの予測単位とする文字(あるいは単語)の集合を何らかの基準でクラスタリング(クラス分類)したものを指す．
本節では，特に日本語漢字が表意文字であり，一文字が何らかの意味を担っていることから，類似した文字を自動的にグループ化することを考える．
文字クラス数は文字数に比べると少ないものとなるので，文字[MATH]-gramモデルよりも文字[MATH]-gramクラスモデルの方が推定すべきパラメータ数が少ないという利点がある．
また，文字クラスモデルは，文字クラスを用いた一種のスムージングであり，頑健なモデルを構築することが期待できる．
このため，文字[MATH]-gramクラスモデルは，文字[MATH]-gramモデルよりも必要な学習データ量が少なく，たとえ小さな学習データからでも，より信頼性のある確率値を推定することが容易となる．
文字[MATH]-gramクラスモデルでは，次の文字を直接予測するのではなく，先行する文字クラス列から次の文字クラスを予測した上で次の文字を予測する．
ここで，文字が一つのクラスにしか属さないとすると，文字の生起確率は次の式で表すことができる．
クラス[MATH]は，文字[MATH]の属する文字クラスである．
また，確率[MATH]は次式により最尤推定できる．
ここで，[MATH]は学習データ中で文字[MATH]が出現した回数であり，[MATH]はクラス[MATH]の文字が出現した回数である．
さらに，本論文では，未知文字を考慮するために，未知文字のクラスを考える．
未知文字クラスには，学習データ中に出現しない未知文字と，頻度の小さい文字を含めることとする(未知文字の実例の収集)．
未知文字[MATH]が未知文字クラス[MATH]から生起する確率[MATH]は次式により計算することができる．
ここで，[MATH]は対象言語の文字集合であり，[MATH]は既知文字集合である．
クラス分類法には様々なものが提案されている[CITE]．
優れた文字クラスモデルを獲得するためには，モデルの予測力を向上させる(すなわちクロス・エントロピーの値を小さくする)文字とクラスの対応関係を発見する必要がある．
しかし，クラスタリングに関する多くの先行研究では，確率値の推定に用いる学習データのエントロピーの値を評価基準とすることでクラスタリングの優劣を判定している．
学習データのエントロピーを小さく(学習データを高い精度で予測)することを目的とするのであれば，モデルのパラメータ数は多いほど良いこととなる．
したがって，学習データのエントロピーを評価基準としてクラスタリングの解探索を行う限り，どのような文字の組合せに対しても複数の文字を同一視することで必ず情報の損失が生じるため，文字モデルよりもエントロピーの値が小さい文字クラスモデルは解空間に存在しないこととなる．
以上のように，学習データのエントロピーは，クラスタリングの評価基準としては不適切なものであり，得られた文字クラスモデルが文字モデルより優れた言語モデルであることが期待できないという重大な問題が生じる．
実際，文献[CITE]の手法では，停止基準として人間が決定する閾値(クラス数)を導入し，閾値までパラメータ数を減少させた場合における最も良い(情報の損失の少ない)解を求めているが，得られたモデルの予測力は低下していることが報告されている．
そもそも言語モデルの評価は確率の推定に用いない未知の評価データに対する予測力によって決められる．
したがって，理想的には，対象言語の未知のデータに対してクロス・エントロピーを小さくするように文字をグループ化することが望ましい．
以上の点から，文献[CITE]では，学習データ内の一部を未知の評価データとして扱い，その評価データのクロス・エントロピーが小さくなるようにクラス分類を行うアルゴリズムを提案している．
このクラス分類法には，停止基準を評価基準から導き出せるという利点があり，人間の判断に委ねられる停止基準(閾値)を必要としない(詳細に関しては後述する)．
実際に，得られた単語bigramクラスモデルは単語bigramモデルよりも優れた性能を示すことが実験的に報告されている．
そこで，本論文では日本語文字のクラスタリングに文献[CITE]の手法を適用することを考える．
クラスタリングの評価基準として用いる平均クロス・エントロピーについて説明する．
ここで，言語モデルの性能尺度であるクロス・エントロピー[MATH]は以下の式で定義される．
ここで，[MATH]は言語モデル，[MATH]は評価データ[MATH]中の[MATH]番目の文である．
[MATH]は文[MATH]を構成する文字の数とする．
このとき，文区切りを考慮するために，[MATH]は文末記号までを含むと仮定する．
学習データ内に未知の評価用データを用意して，その評価データによりクラス分類の性能を評価する．
これを実現するために，削除補間(deleted interpolation)のようにクロス・バリデーション法(cross-validation)あるいは交差検定法と呼ばれる技術を用いる．
クロス・バリデーション法とは，データの役割を交替しながら繰り返し学習および評価を行う方法のことを指す．
学習データ[MATH]を[MATH]個の部分データ[MATH]に分割する．
各部分データ[MATH]に対し，ステップ3, 4を行う．
学習データから[MATH]を削除し，残りの[MATH]個のデータから確率値を推定する．
削除されたデータ[MATH]で，式([REF_Eq:Entropy])によりクロス・エントロピーの値を計算する．
以上のようにして，[MATH]個のクロス・エントロピーの値を得ることができるので，それらの値の平均値[MATH] (平均クロス・エントロピー)を全体の評価関数とする．
ここで，[MATH]はステップ3で[MATH]を削除した残りのデータから推定されたモデルである．
平均クロス・エントロピー[MATH]は確率推定に用いないデータにおけるクロス・エントロピーの平均値であるため，文字とクラスの対応関係を変更していくつかの文字を同一視するようにした場合，同一視しなかった場合に比べて[MATH]の値が増加することもあれば減少することもあるという振舞をみせる．
したがって，クラスタリングの解探索は[MATH]が減少する場合のみクラスの変更を施せば良いという極めて自然なものとなる．
以上の[MATH]の値を最小とする文字とクラスの対応関係を求めることが，本論文の文字クラスタリングの最終目的となり，クラスの併合過程においてどのような併合も[MATH]を減少させることができない状態に到達することがアルゴリズムの停止条件となる．
文字クラスモデルを構築するためには，文字クラスタリングにより文字とクラスの対応関係を求めることが必要となる．
文字とクラスの対応関係としては，ある文字が一定の確率で複数のクラスに属するという確率的な関係も考えられるが，解空間が広大になるので，本論文では，文字は一つのクラスのにみ属することを仮定する．
以下では，文字とクラスの対応関係を返すクラス関数[MATH]を用いて説明する．
たとえば，文字[MATH]の属するクラスとして，[MATH]を返す．
このとき，文字[MATH]に対するクラス関数[MATH]も，各々の文字が属するクラスとして同じく文字集合[MATH]を返すこととなる．
ここで，クラスタリング対象文字の集合を[MATH]とすると，[MATH]中のすべての文字のクラス関数[MATH]の和集合は[MATH]となり，[MATH]と未知文字クラスの和集合が対象言語の文字集合[MATH]となる．
さらに，文字のクラス分類に対する解探索を行うために，文字とクラスの対応関係の変更を表す関数[MATH]を定義する．
移動関数[MATH]は，文字とクラスの関係[MATH]に対して，文字[MATH]をクラス[MATH]に移動した結果得られる文字とクラスの関係を返す．
文字は唯一のクラスに属するとしているので，[MATH]は，現在，文字[MATH]が属するクラス[MATH]から，集合の要素[MATH]を取り除き，クラス[MATH]に要素[MATH]を加えることを意味する．
文字クラス分類の最適解を求めるためには，あらゆる可能な文字とクラスの対応関係を調べる必要がある．
クラス分けの総数は有限であるので，理論的には総当たり戦略により最適なクラスを見つけることはできる．
しかし，総当たり法は非現実的であるため，準最適なアルゴリズムを用いることとなる．
文献[CITE]のアルゴリズムを以下に示す．
上記アルゴリズムはボトムアップ型の探索を行っており，初期状態において，各文字を各々一つのクラスとみなしている．
後は，頻度の高い文字の順に他のクラスへの文字の移動を仮定して，平均クロス・エントロピーの値を再計算している．
このとき，平均クロス・エントロピーが減少する文字とクラスの新しい対応関係が発見できれば，クラス関数[MATH]を変更する．
頻度の高い文字から処理を行う理由は，頻繁に出現する文字ほどクロス・エントロピーに与える影響が大きいと考えられるので，早い段階での移動が後の移動によって影響されにくく，収束がより速くなると考えられるからである．
クラスタリングの処理の例を図[REF_Fig:ClusteringImage]に示す．
文字クラスモデルを言語モデルとして，単語分割を行う．
ここで，[REF_Sec:CharClustering]節の文字クラスタリング法では，文字と文字クラスの関係が一意に定まることを考えると，一文を構成する文字列[MATH]がそのまま文字クラス列[MATH]に変換できることが分かる．
単語分割モデルでは，入力文の各文字間において単語境界の有無を仮定して文の生成確率を計算・比較する．
ここで，式([REF_Eq:CharClassProb])および式([REF_Eq:UnknownCharClassProb])から分かるように，確率[MATH]は単語境界の有無には影響を受けない値である．
さらに，一文を構成する文字は不変であるので，[MATH]はどのような分割候補の確率を求める場合でも一定の値の項となる(式([REF_Eq:CharClassModel])参照)．
したがって，文字trigramクラスモデルによる単語分割モデルでは，以下のようにクラス連鎖の確率のみを用いて簡単に計算することができる．
また，文字位置[MATH]の場合は，次式で求めることができる．
上記の単語分割モデルをみれば分かるように，文字クラスモデルを用いた場合は，文字クラスの連鎖により単語境界を予測するという問題に置き換わる．
文字trigramクラスモデルを用いた場合も，[MATH]となる状態遷移系列をビタビ・アルゴリズムを用いて求めることで，入力文に対する最適な単語分割を得ることができる(図[REF_Fig:viterbi]参照)．
また，可変長[MATH]-gramクラスモデルを用いる場合でも，同様に，クラス連鎖における単語境界の出現の有無により確率比較を行い，解探索を行うこととなる．
