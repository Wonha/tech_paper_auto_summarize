実験と議論 \label{sec:exp}

本節では，提案する2つの素性（言い換え素性と翻字素性）が片仮名複合名詞の分
割処理の精度に与える効果について報告を行う．

\begin{table}[b]
    \caption{表\ref{tab:para}の規則をもとに抽出された言い換え表現（の候補）．
    括弧内の数字は頻度を表す}
    \label{fig:extract-para}
\input{02table05.txt}
\end{table}

\subsection{実験設定} \label{sec:setting}

発音モデルのパラメータ推定に必要な翻字対のデータは，外国人の名前を日本語
で表記するときにはほぼ常に翻字が行われることに着目し，Wikipedia
\footnote{http://ja.wikipedia.org/}を用いて自動的に構築した．
構築手順としては，まず「存命人物」のカテゴリに所属するWikipeida記事のタ
イトルを抽出することにより，片仮名表記の人名リストを作成した．そして次
に，Wikipediaの言語間リンクを利用し，各人名に対する原語を抽出した．これ
により17,509の翻字対を収集することができた．このように自動収集したデータ
の中には翻字対として不適切なものも含まれている可能性があるが，大量のデー
タを手軽に用意できるという利点を重視して，この方法を採用している．実際，
このようにパラメータ推定のためのデータを大量に生成するアプローチは，翻字
生成において有効であることが報告されている\cite{Cherry09}．パラメータ推
定時には，EMアルゴリズムの初期値を無作為に10回変化させ，尤度が最大となっ
たモデルを以降の実験で用いた．

\begin{table}[t]
\hangcaption{単語対応付き翻字対の例．スラッシュは抽出時に検出された片仮
    名語の単語境界を示す．単語間の対応関係は自明なので省略する}
    \label{fig:extract-backtrans}
\input{02table06.txt}
\end{table}

平均化パープトロンの学習に必要なラベル付きデータは，日英対訳辞書EDICT
\footnote{http://www.csse.monash.edu.au/\~{}jwb/edict\_doc.html}を利用し
て手作業で作成した．具体的には，まず，EDICTの見出し語から，翻字である片
仮名（複合）名詞を無作為に抽出した．そして，EDICTに記載されている英訳に基
づき，単語境界のラベルを付与した．この結果，5286の片仮名語データを得た．
このデータにおける構成語数の分布を調べたところ，構成語が1語のものが3041，
2語のものが2081，3語以上のものが164となっていた($3041+2081+164=5286$)．ま
た，複合名詞1つあたりの平均文字数および平均構成語数は6.60および1.46であっ
た．以下本節において報告する実験結果は，このラベル付きデータを用いて2分
割交差検定を行ったものである．

言い換え及び逆翻字を抽出するためのテキストには，ウェブから収集した17億文
のブログ記事を用いた．このテキストを用いることによって14,966,205の言い換
え表現と，116,027の単語対応付き翻字対を抽出することができた．表
\ref{fig:extract-para}と\ref{fig:extract-backtrans}に，実際に抽出され
た言い換え表現（の候補）と単語対応付き翻字対の具体例を示す．

単語対応付き翻字対の抽出を行う際には閾値$\theta$を設定する必要がある．
$\theta$は確率の対数に対する閾値であるため，0より小さな任意の値を設定す
ることが可能であるが，ここでは$\{-10,-20,\dots\linebreak
-150\}$の範囲で値を変化さ
せ，実験において最も高いF値が得られた値($\theta=-80$)を採用した．


\subsection{ベースライン手法} \label{sec:baseline}

実験では，3つの教師なし学習手法(Unigram, GMF, GMF2)，2つの教師あり学習手
法(AP, AP $+$ GMF2)，3つの単語分割器(JUMAN, MeCab, KyTea)との比較を行った．
以下ではこれらベースライン手法について簡単に説明を行う．

\begin{description}
 \item[教師なし学習]　
	    \begin{description}
	     \item[Unigram]
			分割結果$\y$に対する$1$-gram言語モデルの尤度
			$p(\y)$が最も大きくなる分割を選択する手法
			\cite{Schiller05,AlfonsecaCICLing08}:
			\[
			 \y^{*}=\argmax_{\y\in\mathcal{Y}(x)}p(\y)=\argmax_{\y\in\mathcal{Y}(x)}\prod_{i}p(y_i)
			\]
			ここで$p(y_i)$は構成語$y_i$の出現確率であり，
			\ref{sec:setting}節で述べたウェブテキストから推
			定をした値を用いた．
	     \item[GMF]
			構成語$y_i$の頻度の幾何平均\mbox{GMF}$(\y)$が最
			大となる分割$\y$を選択する手法\cite{Koehn03}:
			\[
			 \y^{*}=\argmax_{\y\in\mathcal{Y}(x)}\mbox{GMF}(\y)=\argmax_{\y\in\mathcal{Y}(x)}\Bigl\{\prod_{i}f(y_i)\Bigr\}^{1/|\y|}
			\]
			ここで$f(y_i)$は構成語$y_i$の出現頻度であり，
			$p(y_i)$と同様にウェブテキストから推定した値を用
			いた．
	     \item[GMF2]
			頻度の幾何平均に構成語の長さに基づく補正を導入し
			たスコアを用いる手法\cite{Nakazawa05}:\pagebreak
			\[
			 \mbox{GMF2}(\y)=
	     \begin{cases}
	      \mbox{GMF}(\y) & (|\y|=1) \\[10pt]
	      \frac{\mbox{GMF}(\y)}{\frac{C}{N^l}+\alpha} & (|\y|\geq2),
	     \end{cases}
			\]
	    ここで$C$，$N$，$\alpha$は超パラメータ，$l$は構成語の平均文
	    字数を表す．本実験ではNakazawaら\citeyear{Nakazawa05}と同じ
	    く$C$ = 2500，$N$ = 4，$\alpha$ = 0.7とした．
	    \end{description}
 \item[教師あり学習]　
	    \begin{description}
	     \item[AP] 基本素性（\ref{sec:approach}節参照）のみを用いた平
			均化パーセプトロン．
	     \item[AP $+$ GMF2]
			基本素性に加えて{\sc Gmf2}の処理結果を素性として
			用いた平均化パーセプトロン．Alfonsecaら
			\citeyear{AlfonsecaCICLing08}に従って，
			(i) GMF2$(\y)$の値が全分割候補中で最大であるか否
			かを表す2値素性，(ii) 分割を行わない候補
			（i.e., $|\y|=1$となる候補）よりもGMF2の値が大きく
			なるか否かを表す2値素性を追加した．
	    \end{description}
 \item[単語分割器]　
	    \begin{description}
	     \item[JUMAN]
			ルールベースの単語分割器
			\footnote{正確には形態素解析器であるが，本実験で
			は品詞タグ付与の議論は行わないのでこう呼ぶ．}\ JUMAN ver.~6.0 \cite{Kurohashi94}．
	     \item[MeCab]
			対数線形モデルに基づく単語分割器 
			MeCab ver.~0.98 \cite{Kudo04}．解析辞書にはNAIST-jdic
			ver.~0.6.0を用いた．
	     \item[KyTea]
			点推定モデルに基づく単語分割器 KyTea ver.~0.3.1 \cite{Neubig11}．
	    \end{description}
\end{description}


\subsection{ベースライン手法との比較}
\label{sec:compare}

\begin{table}[b]
\hangcaption{ベースライン手法との比較．表中のP，R，F$_1$は，認識された単
   語境界の適合率，再現率，F値を示す．またAccは分割精度，すなわち正しく
   分割された片仮名複合名詞の割合を示す}
   \label{tab:comparison_result}
\input{02table07.txt}
\vspace{-0.5\Cvs}
\end{table}

表\ref{tab:comparison_result}に提案手法(Proposed)とベースライン手法との
比較結果を示す．この表の結果から以下のようなことが分かる．

まず，ProposedとAPの結果の比較から，言い換え素性と逆翻字素性を導入するこ
とにより，分割精度が大きく向上したことが分かる．マクネマー検定を行ったと
ころ，この精度変化は統計的に有意なものであることが確認された($p<0.01$)．
この結果は，提案する2つの素性の有効性を示すものである．

次に，提案手法の精度は，全ての教師なし学習ベースライン(Unigram, GMF, GMF2)及びAP $+$ GMF2の精度を上回っていることが確認できる．これらの結果は，複
合名詞の言い換えや逆翻字の情報が，構成語の頻度情報よりも効果的であること
を示唆している．なお，マクネマー検定を行ったところ，これらの精度向上も全
て統計的に有意であることが確認できた($p<0.01$)．

単語分割器(JUMAN, MeCab, KyTea)の結果は，これまでに単語分割タスクにおい
て報告されている精度\cite{Kudo04,Neubig11}を大きく下回っている．このこと
から，一般的な単語分割と比較して，片仮名複合語の分割処理が困難なタスクで
あることが分かる．さらに，提案手法の精度は，単語分割器の精度を大きく上回っ
ており，提案手法が既存の単語分割器の弱点補強に有用であることが示唆されて
いる．例えば，既存の単語分割器によって「片仮名表記の名詞の連続」と解析さ
れた部分を，提案手法を用いて再分割することにより，解析結果の改善を期待す
ることができる．

\begin{table}[b]
   \caption{MeCabとProposedの出力比較．スラッシュはシステムに認識された
   単語境界を表す}
   \label{tab:example}
\input{02table08.txt}
\end{table}

表\ref{tab:example}に，MeCabでは分割に失敗したが，Proposedでは正しく分割
することができた例を示す．まず最初の例では，片仮名語「ディクショナリー」が
NAIST-jdicに登録されていなかったため，MeCabは分割に失敗している．一方，
Proposedにおいては，以下のような単語対応付き翻字対が学習されており，これ
に基づいて発火した逆翻字素性(1-gram)が有効に働いた結果，正しく分割するこ
とに成功している．

\begin{quote}
 \underline{オックスフォード}$_1$\underline{ディクショナリー}$_2$\ \ \ \
 \underline{oxford}$_1$ \underline{\mbox{dictionary}}$_2$
\end{quote}

\noindent
次の例では「メイン」と「タイトル」が両方ともNAIST-jdicに登録されているに
も関わらず，MeCabは分割に失敗している．これは，MeCabの未知語処理に起因す
る誤りであると考えられる．その一方でProposedが分割に成功しているのは，例
えば「メインのタイトル」といった言い換え表現に基づく素性など，分割を示唆
する素性がより多く発火しているためだと推測できる．最後の例では，
NAIST-jdicに人名「トミー」が登録されているため，MeCabは過分割を行ってし
まっているが，Proposedでは「アナトミー」に対する逆翻字素性が適切に発火し
ており，過分割を防ぐことに成功している．

本論文の趣旨からは外れるが，3つの単語分割器のなかではKyTeaの精度が他の2
つを大きく引き離している点は非常に興味深い．これは，JUMANやMeCabの解析ア
ルゴリズムが，辞書引きによる候補選択に強く依存しているのに対して，KyTea
はそのような候補選択を行っていないことが要因と考えられる．




\subsection{未知語に関する考察}

実験に使用した5286の片仮名複合名詞のうち，2542は少なくとも1つの未知語を
構成語に含んでいた．ただし，ここで言う未知語とは，訓練データに出現せず，
なおかつ外部辞書NAIST-jdicにも登録されていない単語のことを指す．未知語が
分割精度に与える影響について考察するため，提案手法を含む3つの教師あり学
習手法(AP, AP $+$ GMF2, Proposed)と単語分割器MeCabの分類結果を，1つ以上の未
知語を含む2542の片仮名複合名詞と残る2744の片仮名複合名詞に分けて集計した
（表\ref{tab:oov}）．以下では，前者のサブセットを w/ OOVデータ，後者を w/o
OOV データと呼ぶ．

\begin{table}[b]
\hangcaption{未知語を含む片仮名複合名詞(w/ OOV)とそれ以外の片仮名複合名
   詞(w/o OOV)に対する分割結果の比較}
\label{tab:oov}
\input{02table09.txt}
\end{table}

この表から，3つの教師あり学習手法については，w/o OOV データに対しては
90\%を越える高い精度が達成されているのに対して，w/ OOV データの精度は大
きく低下していることが確認できる．同様の傾向はMeCabの結果においても見ら
れる．MeCabは汎用的な単語分割器であるため，複合名詞分割というタスクに特
化して学習された提案手法(Proposed)やその他の教師あり学習手法（APや
AP $+$ GMF2）と比べると，精度自体はどちらのデータにおいても大きく低下している．
しかし，w/ OOV データよりもw/o OOV データのほうが精度が高くなるという傾向
は，依然として確認することができる．これらの結果は，片仮名複合名詞の分割
処理を困難にしている要因は未知語であるという我々の主張を支持するものであ
る．

3つの教師あり学習手法は，w/o OOV データについてはほぼ同じ精度を達成して
いることが分かる．これは，既知語に対しては，基本素性だけを使ってすでに高
い分類精度が達成されているため，これ以上の精度向上が困難であるからだと考
えられる．一方，精度向上の余地が残されている w/ OOV データについては，3
つのシステムの間に大きな精度の差を見てとることができる．そのため，表
\ref{tab:comparison_result}の結果よりも，言い換え素性と翻字素性を導入す
る効果をより直接的に確認することができる．



\subsection{言い換え素性と翻字素性の効果}
 \label{sec:effect}

言い換え素性と翻字素性の有効性について詳細に検証するため，異なる4つの素
性集合を用いたときの平均化パーセプトロンの分割結果の比較を行った（表
\ref{tab:comparison}）．表の1行目は使用した素性集合を表す．{\sc Basic}は
基本素性，{\sc Para}と{\sc Trans}はそれぞれ言い換え素性と翻字素性，{\sc
All}は全ての素性集合を表す．この表より，言い換え素性と翻字素性の両方とも
が分割精度向上に大きく貢献していることを確認することができた．いずれの場
合においても，基本素性だけを使った場合と比較して，精度の向上は統計的に有
意であった（$p<0.01$，マクネマー検定）．

\begin{table}[b]
\hangcaption{言い換え素性と逆翻字素性の効果．表中の{\sc Basic}, {\sc
    Para}, {\sc BackTrans}は，それぞれ基本素性，言い換え素性，逆翻字素性
    を示す．また{\sc All}はそれら全ての素性を示す}
\label{tab:comparison}
\input{02table10.txt}
\end{table}

\begin{figure}[b]
 \begin{center}
 \includegraphics{19-2ia927f1.eps}
 \end{center}
  \hangcaption{言い換えと単語対応付き翻字対の抽出に用いたブログデータのサイズ（横軸）と各素性の発火率（縦軸）の関係}
  \label{fig:feature-coverage}
\end{figure}

次に，各素性の発火率について調査を行った．実験で用いたラベル付きデータに
は7709の構成語が含まれており，そのうち64.0\% (4937/7709)は外部辞書に登録
されていた．これに対して，単語対応付き翻字対に出現していた構成語の割合は
64.0\% (4935/7709)，外部辞書か単語対応付き翻字対のいずれかに出現していた
ものの割合は77.1\% (5941/7709)であった．これにより，翻字素性を導入するこ
とによって，未知語の数が大幅に減少していることが確認された．

一方，ラベル付きデータに含まれる構成語$2$-gramの数は2423であったが，それ
らに対して発火していた言い換え素性と翻字素性の割合は，それぞれ
79.5\% (1926/2423)と12.8\% (331/2423)であった．これらの結果から，提案素性
はいずれも精度向上に寄与しているものの，カバレージにはまだ改善の余地があ
ることが分かった．

続いて，素性の発火率と収集元であるブログデータの大きさの関係を調査した
（図\ref{fig:feature-coverage}）．ここではブログデータの大きさとして，収集
したブログ記事（UTF8エンコーディング）をgzipで圧縮したデータのサイズをギガ
バイト単位で表示している．この図から，大量のブログデータを使うことによっ
て，高い発火率を実現できていることが確認できる．しかし，その一方で，デー
タが増えるにつれて，発火率の向上の度合いは鈍りつつある．このことから，デー
タを単純に増加させるだけでは，ここからの大幅な発火率の改善を期待すること
は難しく，言い換え規則の拡張などの方法も併せて検討していくことが今後重
要になると考えられる．

\subsection{パラメータ$\theta$}
 \label{sec:threshold}

\begin{figure}[b]
 \begin{center}
 \includegraphics[clip]{19-2ia927f2.eps}
 \end{center}
  \caption{パラメータ$\theta$（横軸）と抽出された単語対応付き翻字対の数（縦軸）}
  \label{fig:size}
\end{figure}


最後に，パラメータ$\theta$の値を変化させたときの影響について調査を行った
（図\ref{fig:size}--\ref{fig:threshold}）．図
\ref{fig:size}と\ref{fig:fire}は，様々な値の$\theta$に対する，単語対応
付き翻字対の抽出数および逆翻字素性の発火した割合（\ref{sec:effect}節にお
いて議論したもの）を示している．これらの図から，$\theta$の値をある程度小
さく設定すれば，十分な数の翻字対が抽出され，その結果として多くの事例にお
いて素性が発火するようになることが分かる．図\ref{fig:threshold}は
$\theta$とF値の関係を示している．さきほどの2つの図との比較すると，翻字対
の抽出数と素性の発火数の増加が，F値の向上に直接結びついているこ
とが分かる．

パラメータの値が極端に大きい場合(e.g., $-20$)においては，F値が低下する傾
向が見られたものの，パラメータによらずF値はおおよそ一定であった．この結
果から，提案手法の精度はパラメータ設定に敏感ではなく，パラメータ調整は難
しい作業ではないことが示唆される．また，少なくとも実験において調べた範囲
では，提案手法はパラメータ値によらず，基本素性のみを用いた場合よりも高い
F値を達成することができた．そのため，パラメータの微調整が提案手法の性能
に与える影響は小さいと言うことができる．

\begin{figure}[t]
 \begin{center}
 \includegraphics[clip]{19-2ia927f3.eps}
 \end{center}
  \hangcaption{パラメータ$\theta$（横軸）と逆翻字素性の発火率（縦軸）．グラフ中
  の三角と四角は，それぞれ構成語1-gramと2-gramに対する発火率を表す}
  \label{fig:fire}
\end{figure}

\begin{figure}[t]
 \begin{center}
 \includegraphics[clip]{19-2ia927f4.eps}
 \end{center}
  \hangcaption{パラメータ$\theta$（横軸）とF値（縦軸）の関係．グラフ中の三角と四
  角は，それぞれ全素性を使った場合と，基本素性のみを使った場合に相当する}
  \label{fig:threshold}
\end{figure}


\subsection{誤りの分析} \label{subsec:error}

提案手法が分割を誤った事例を調べたところ，「アップロード」を「アップ」と
「ロード」，「トランスフォーマー」を「トランス」と「フォーマー」に分割す
るなど，単語を過分割している事例が見られた．ここでの「アップ」や「トラン
ス」は接頭辞であると考えられるため，これらの分割結果は形態論的分割
(morphological segmentation)としては正しいものであるかもしれないが，単語
分割としては不適切であると考えられる．

こうした過分割が発生する要因として，接辞と単語の曖昧性を指摘することがで
きる．例えば「アップ」は，確かに接頭辞の1つであるが，文脈によっては「給
料がアップする」のように独立した名詞として使われる場合もある．同じく「ト
ランス」に対しても「トランス状態」のような名詞用法を考えることができる．
このような曖昧性によって引き起こされる最も顕著な問題は，辞書素性（表
\ref{tab:feature}におけるテンプレートID4）が過剰に発火することである．前
述の過分割の事例においては，NAIST-jdicに「アップ」と「トランス」がともに
名詞として登録されていたため，本来不適切な分割結果であるにも関わらず辞書
素性が発火していた．

これと同様の問題は，辞書素性だけでなく，逆翻字素性においても発生しうる．
\ref{sec:trans}節で説明した単語対応付き翻字対の抽出手法は，原語が正しく
分かち書きされていることを前提としていた．しかしながら，実際には接頭辞や
接尾辞の前後に空白区切りを挿入しているテキストも存在するため，不適切な対
応関係が学習されてしまう場合がある．

表\ref{tab:error}は上記の過分割結果に影響を与えたと思われる単語対応付き
翻字対の一部である．この表から，「アップロード」と「トランスフォーマー」
については，それぞれ原語との対応関係が適切に学習されていることが確認でき
る．しかしながら「アップローダー」と「トランスフォーム」については，原語
が接頭辞の直後で分かち書きされていたため，不適切な単語対応が学習されてい
ることが分かる．こうした対応付け結果から導出された逆翻字素性（この例では
特に1-gram）は分割に悪影響を与えている可能性がある．翻字抽出の手法を改善
することにより，こうした誤りを減少させることは，今後の課題の一つであると
考えている．

\begin{table}[t]
 \caption{過分割結果に影響を与えたと思われる単語対応付き翻字対の一部}
 \label{tab:error}
\input{02table11.txt}
\end{table}

過分割が多くみられた別要因としてデータの偏りを考えることもできる．今回使
用したデータの半数以上は構成語数が1つであったため，そもそも過分割が発生
しやすい設定の実験になっていた可能性がある（\ref{sec:setting}節を参照）．
現在のところ，当該タスクに対する別のデータセットを用意することは難しいた
め，この点をすぐに調査することはできないが，今後の研究の中で議論を深めて
いくべきであると考えられる．


