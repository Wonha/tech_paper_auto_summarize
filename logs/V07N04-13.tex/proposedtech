\documentstyle[jnlpbbl,epsf]{jnlp_j}


\setcounter{page}{261}
\setcounter{巻数}{7}
\setcounter{号数}{4}
\setcounter{年}{2000}
\setcounter{月}{10}
\受付{00}{5}{1}
\採録{00}{6}{30}

\setcounter{secnumdepth}{2}

\title{自動要約のための文重要度の比較}
\author{内山 将夫\affiref{U} \and 井佐原 均\affiref{U}}

\headauthor{内山,井佐原}
\headtitle{重要文抽出のための文重要度の比較}

\affilabel{U}{郵政省通信総合研究所}{Communications Research Laboratory, Ministry of Posts and Telecommunications}


\jabstract{本稿では，重要文抽出によるテキスト自動要約のための各
  種重要度を比較した．特に，タイトルとの類似度の高い文から抽出
  するという要約方法を想定し，各種の類似度を比較した．類似度と
  しては，共起関係を利用する方法と利用しない方法とを試みた．そ
  の結果，共起関係を利用する方法の方が高精度な要約が作成できた．
  また，要約の手法としては，他に，本文の先頭数文を抽出する方法
  と，単語の重要度の総和を文の重要度とする方法も試みたが，これ
  らの方法よりも，タイトルとの類似度に基づく方法の方が高精度で
  あった．これらの結果は，共起関係を利用した類似度が自動要約に
  有効であることを示している．}

\jkeywords{テキスト自動要約，重要度，類似度，共起関係}

\etitle{Empirical Comparison of Sentence Importance\\ 
  Measures for Automatic Text Summarization} 
\eauthor{Masao Utiyama\affiref{U} \and Hitoshi Isahara\affiref{U}}

\eabstract{The effectiveness of various statistical measures of
  sentence importance was compared for automatic text
  summarization done by extracting important sentences. We
  focused on comparing various measures of sentence similarity
  on the assumption that important sentences in an article are
  similar to the title. Two types of similarity measures were
  compared: one uses word co-occurrence statistics and the
  other does not. The former proved superior to the latter. 
  Other automatic text summarization methods, such as
  extracting the leading part of an article, or extracting
  sentences with important words, proved inferior to the
  similarity-based method. These results show that similarity
  measurement using word co-occurrence statistics is effective
  for automatic text summarization.}

\ekeywords{automatic text summarization, importance, similarity, co-occurrence statistics}


\begin{document}

\maketitle


\section{文の重要度の定義}
\label{sec:measures}

重要文を抽出するためには，文に重要度を付与する必要がある．その
ための各種の重要度を以下に定義する．なお，以下では，重要度が数
値的に高い文ほど重要な文であるとする．

\subsection{先頭の数文を抽出する手法}
\label{sec:lead}

文章(特に新聞記事)の先頭の数文(冒頭部)は重要と考えられる
\cite{brandow95:_autom_conden_elect_public}．その先頭の数文を抽
出するためには，文$S$の文章中での位置を$p(>=1)$とすると，$S$の
重要度$lead(S)$を，たとえば，
\begin{equation}
  \label{eq:lead}
  lead(S) = \frac{1}{p}
\end{equation}
と定義すれば良い．なお，$lead(S)$としては，その他には，$lead(S)
= -p $などを採用することもできる．

\subsection{単語重要度の和による文重要度}
\label{sec:sum}

重要単語を多く含む文は重要と考えられるので，単語の重要度の和を
文の重要度とすれば良いと考えられる
\cite{zechner96:_fast_gener_abstr_gener_domain}．文の重要度を単
語重要度の和として求めるためには，文$S$を構成する単語集合を
$W(S)$とすると，単語$w$の重要度を$f(w)$とし，$w$の$S$中での頻度
を$n(w,S)$とするとき，文$S$の重要度$sum(S,f)$を，
\begin{equation}
  \label{eq:sum}
  sum(S,f) = \sum_{w \in W(S)} n(w,S) \times f(w)
\end{equation}
と定義すれば良い．

単語$w$の重要度$f(w)$としては，以下のものを定義する．
\begin{eqnarray}
  \label{eq:wimps1}
  one(w) & = & 1\\
  \label{eq:wimps2}
  tf(w) & = & \mbox{要約対象文書中での$w$の頻度}\\
  \label{eq:wimps3}
  idf(w) & = & \log \frac{\mbox{全文書数}}{\mbox{$w$を含む文書数}}\\
  \label{eq:wimps4}
  tfidf(w) & = & tf(w) \times idf(w)
\end{eqnarray}
なお，(\ref{eq:sum})式における$f(w)$を$tfidf(w)$としたときの重
要度が，\cite{zechner96:_fast_gener_abstr_gener_domain}で採用さ
れた文の重要度に相当する．

\subsection{タイトルとの類似度による文重要度}
\label{sec:sim}

タイトルは本文中で最も重要と考えられる
\cite{yoshimi98:_evaluat_impor_senten_connec_title}ので，それと
類似度が高い文は重要と考えられる．そのタイトルとの類似度を文の
重要度とするために，文$S$とタイトル$T$の類似度を以下に定義する．
まず，共起頻度を利用しない方法(以下の$common$, $ip$, $cos$)を定
義する．これらは，類似度の定義として良く知られたものである．

\subsubsection*{共通単語の重みの和}

\begin{equation}
  \label{eq:common}
  common(S,T,f)  =  \sum_{w \in W(S) \cap W(T)} n(w,S) f(w)
\end{equation}

\subsubsection*{内積}

\begin{equation}
  \label{eq:ip}
  ip(S,T,f)  =  \sum_{w \in W(S) \cap W(T)} n(w,S) f(w) \times n(w,T) f(w)
\end{equation}

\subsubsection*{コサイン}

\begin{equation}
  \label{eq:cos}
  cos(S,T,f) = \frac{ip(S,T,f)}{\sqrt{ip(S,S,f)}\sqrt{ip(T,T,f)}}
\end{equation}

ここで，$f$は，(\ref{eq:wimps1})式から(\ref{eq:wimps4})式で定義
された関数のいずれかである．

次に共起頻度を利用する方法(以下の$coProb$, $coIDF$)を定義する．
共起頻度は条件付き確率として式中に現れる．なお，以下で述べる，
比較的簡単な式である$coProb$と，IDF(Inverse Document Frequency)
の拡張としての$coIDF$とは，類似度の尺度として，本稿で新たに提案
するものである．

\subsubsection*{共通単語の条件付き確率の和}

\begin{equation}
  \label{eq:coProb}
  coProb(S,T) = \sum_{w \in W(S)} n(w,S) \Pr(w|T)
\end{equation}

\subsubsection*{IDFの拡張}

\begin{equation}
  \label{eq:coIDF}
  coIDF(S,T) = \sum_{w \in W(S)} n(w,S) \times \Pr(w|T) \log \frac{\Pr(w|T)}{\Pr(w)}
\end{equation}

ここで，
\begin{eqnarray}
  \label{eq:pr1}
  \Pr(w) & = & \frac{\mbox{$w$を含む文書数}}{\mbox{全文書数}}\\
  \label{eq:pr2}
  \Pr(w|T) & = & \max_{v \in W(T)} \Pr(w|v)\\
  \label{eq:pr3}
  \Pr(w|v) & = & \frac{\mbox{$w$と$v$を含む文書数}}{\mbox{$v$を含む文書数}}
\end{eqnarray}
である．

これらの確率の定義によると，$\Pr(x|x) = 1$である．そのため，
(\ref{eq:coIDF})式は，
\begin{eqnarray}
  coIDF(S,T) & = & \sum_{w \in W(S) \cap W(T)} n(w,S) \times idf(w) \nonumber \\
  & + & \sum_{w \in W(S) - W(S) \cap W(T)} n(w,S) \times \Pr(w|T) \log \frac{\Pr(w|T)}{\Pr(w)} \nonumber
\end{eqnarray}
と変形できる．つまり，$coIDF(S,T)$は，共通単語$w$については，
$idf(w)=1\times \log \frac{1}{\Pr(w)}$の和を求め，それ以外につ
いては，共起確率を考慮した値$\Pr(w|T) \log
\frac{\Pr(w|T)}{\Pr(w)}$の和を求めていると言える．これから分か
るように，$\Pr(w|T) \log \frac{\Pr(w|T)}{\Pr(w)}$は，$idf(w)$の
拡張と言える．

また，(\ref{eq:coProb})式は，
\begin{eqnarray}
  coProb(S,T) & = & \sum_{w \in W(S) \cap W(T)} n(w,S) \times one(w) \nonumber \\
  & + & \sum_{w \in W(S) - W(S) \cap W(T)} n(w,S) \times \Pr(w|T)\nonumber
\end{eqnarray}
であるので，共通単語数を確率的に求めたものとも言える．

\end{document}
