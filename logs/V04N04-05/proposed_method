IFT 付きコーパス

対話モデル作成のための基礎データとして，
発話行為タイプ(Illocutionary Force Type; IFT)付きコーパス
\cite{Nagata92,Nagata94,Suzuki93}を用いた．
これは，ATR 対話データベース中の「国際会議参加登録のタスク」の
対話の各発話について，その発語内行為を分析し，
陳述・命令・約束などの発話のタイプが付けられたコーパスである．
このコーパスで用いられている IFT は，
表層の統語的パターンと比較的直接的な対応がとれる
表層 IFT(Surface Illocutionary Force Type)と呼ばれるものである．
また，各発話文には，発話者(事務局または質問者)を示すラベルが付与されている．
IFT 付きコーパスで用いられている表層 IFT の種類
および各 IFT に属する例文を表\ref{Tab:IFTdef} に，
IFT 付きコーパスの例を図\ref{Fig:IFT-Corpus} に示す．
本研究における評価実験では，IFT 付きコーパスの中から，
モデル会話 10 対話(222 文)とキーボード会話 50 対話(1686 文)
を用いた．


\begin{table}[p]
\caption{表層 IFT の分類および例}
\label{Tab:IFTdef}
\begin{center}
\begin{tabular}{@{$\;$}l|p{6cm}|p{5.5cm}@{$\;$}}
\hline
表層 IFT        &  定義         &  例文         \\ \hline
\hline
phatic          & 挨拶などで用いられるイディオム的な表現
                & もしもし，\newline 失礼します \\ \hline
expressive      & 話者の感情表現に関するイディオム的な表現
                & ありがとうございます，\newline
                よろしくお願いします \\ \hline
response        & 質問などに対する応答や合いづち
                & はい，\newline わかりました \\ \hline
promise         & 話し手がある行為をすることを約束する表現
                & 登録用紙を送らせていただきます \\ \hline
request         & 話し手が聞き手に行為をすることを依頼する表現
                & 地下鉄で北大路駅まで行って下さい \\ \hline
inform          & 情報の伝達
                & 今回は割り引きを行なっておりません \\ \hline
questionif      & 真偽疑問文
                & 会議の案内書はお持ちですか \\ \hline
questionref     & 疑問語疑問文
                & どうすればよろしいですか \\ \hline
questionconf    & 確認
                & 既に登録料を振り込まれておられますね \\ \hline
\end{tabular}
\end{center}
\end{table}


\begin{figure}[p]
\begin{verbatim}
質問者
phatic: もしもし
questionif: そちらは会議事務局ですか

事務局
response: はい
response: そうです
questionref: どのようなご用件でしょうか

質問者
inform: 会議に申し込みたいのですが
questionref: どのような手続きをすればよろしいのでしょうか

事務局
request: 登録用紙で手続きをして下さい
questionif: 登録用紙は既にお持ちでしょうか

質問者
response: いいえ
inform: まだです
\end{verbatim}
\caption{IFT 付き対話コーパスの例}
\label{Fig:IFT-Corpus}
\end{figure}


エルゴード HMM による対話構造のモデル化

IFT 付きコーパスの各発話には，話者ラベルおよび IFT が
付与されている．
話者の交替や質問・応答・確認のような対話の基本的な構造を
確率・統計的にモデル化するために，
コーパス中の話者ラベルおよび IFT の系列をエルゴード HMM により
モデル化することを試みた．
なお，エルゴード HMM とは，自己遷移も含めすべての状態間の遷移を許す
全遷移型の HMM である．

本実験では，あらかじめ HMM の状態数を決めておき，
Baum-Welch の再推定アルゴリズムにより，
エルゴード HMM の学習を行なった．
初期モデルとしては，初期状態分布確率を均等確率に，
また状態遷移確率および出力確率は確率値の総計が１になるようなランダムな
値で初期化した．

エルゴード HMM の学習データとして，
モデル会話およびキーボード会話中から，
以下の２つの系列を抽出した．
\begin{itemize}
\item[(1)]      IFT のみの系列
\item[(2)]      話者ラベルと IFT を組み合わせたラベルの系列
\end{itemize}
IFT の総数は９個であり，対話コーパス中の発話者は２名(事務局あるいは質問者)
であるので，上記(2)の場合のシンボル数は 18 個である．

実験では，エルゴード HMM の構造として状態数 2〜14 のものを用いて
学習を行なった．
表\ref{Tab:HMMEntropy} に，状態数 2, 4, 6, 8, 10, 12, 14 の場合のモデルのエントロピーを
示す．
表\ref{Tab:HMMEntropy} で，IFT と示されているのは IFT のみの系列を用いた
ときの結果であり，
SP-IFT は話者ラベルと IFT を組み合わせたラベルの系列を用いたときの結果である．
一般的な傾向として，状態数が増えるに従いエントロピーが小さくなり，
同じ状態数では話者ラベルを併用したものの方がエントロピー値が大きくなっている．

\begin{table}
\caption{Ergodic HMM のエントロピー}
\label{Tab:HMMEntropy}
\begin{center}
\begin{tabular}{c|cc|cc}
\hline
HMMの   & \multicolumn{2}{c|}{モデル会話}       & \multicolumn{2}{c}{キーボード会話} \\
        \cline{2-3} \cline{4-5}
状態数  & IFT   & SP-IFT        & IFT   & SP-IFT        \\ \hline
2       & 2.12  & 2.72          & 2.38  & 3.02          \\ \hline
4       & 1.86  & 2.27          & 1.89  & 2.78          \\ \hline
6       & 1.17  & 1.81          & 1.91  & 2.49          \\ \hline
8       & 1.35  & 1.64          & 1.88  & 2.40          \\ \hline
10      & 1.21  & 1.60          & 1.60  & 2.27          \\ \hline
12      & 0.91  &  1.29  &  1.63  & 1.95  \\ \hline
14      & 0.92  &  1.24  &  1.72  & 2.11  \\ \hline
\end{tabular}
\end{center}
\end{table}


文献\cite{Nagata94}の結果では，trigram モデルを使った場合，
モデル会話での SP-IFT のエントロピー値は 1.26，
キーボード会話での SP-IFT の値は 2.19 と報告している．
本実験では，12〜14 状態のエルゴード HMM の場合が，trigram のエントロピー値と
ほぼ同等になっている．

学習後の HMM の構造(状態数 5 の場合)を図\ref{Fig:HMM-IFT} および
図\ref{Fig:HMM-IFT-SP} に示す．
図\ref{Fig:HMM-IFT} は IFT のみの系列から得られたモデルであり，
図\ref{Fig:HMM-IFT-SP} は話者ラベルと IFT を組み合わせたラベルの系列
から得られたモデルである．
図には，遷移確率および出力確率が 0.1 以上のもののみを記しており，
矢印の太いものほど大きな遷移確率を持っていることを示している．
状態遷移の一番上に書かれている確率が遷移確率であり，
その下に各シンボル(IFT)の出力確率が記されている．
図\ref{Fig:HMM-IFT-SP} で，S で始まるシンボルは
事務局側の発話であることを，
また Q で始まるシンボルは質問者側の発話であることを示している．

例えば，図\ref{Fig:HMM-IFT-SP} では，状態 1 が初期状態であり，
質問者が最初の発話「もしもし」を発話すると Qphatic を出力する遷移を
たどることになる．
これは，状態 1 での自己ループあるいは
状態 1 から 状態 2 への遷移に対応している．
「国際会議参加登録のタスク」では，
事務局の「こちらは会議事務局です」という発話により対話が始まる場合もある．
この場合には Sinform を出力する遷移である状態 1 での自己ループとなる．
また，図\ref{Fig:HMM-IFT-SP} では，
状態遷移が事務局側の発話と質問者側の発話で比較的きれいに分かれている．
例えば，状態 3 から状態 2 への遷移は質問者側の発話によって起こり，
しかもこの遷移は事務局に対する質問や依頼に対応していることが分かる．
この質問や依頼に対し，状態 2 から状態 0 の遷移で事務局が
応答(Sresponse, Sinform)する確率が非常に高いことも読みとることができる．
以上のように，発話行為タイプ付きコーパスから得られたエルゴード HMM は，
質問・応答といった基本的な構造を抽出しているということができる．


\begin{figure}
\begin{center}
\epsfile{file=fig1.eps,width=100mm}
\end{center}
\caption{IFT のみの系列から得られた 5 状態エルゴード HMM}
\label{Fig:HMM-IFT}
\end{figure}

\begin{figure}
\begin{center}
\epsfile{file=fig2.eps,width=100mm}
\end{center}
\caption{話者ラベルと IFT を組み合わせたラベルの系列から得られた 5 状態エルゴード HMM}
\label{Fig:HMM-IFT-SP}
\end{figure}

状態マージング手法による対話構造のモデル化

エルゴード HMM によるモデル化では，
確率モデルの学習に先立ち，モデルの構造(状態数)をあらかじめ決めておく必要がある．
これに対し，近年，状態マージング手法を用いて，学習データに対し最適な
構造を持つモデルを自動的に構築する研究がいくつか行なわれている\cite{Stolcke94a,Stolcke94b}．
我々は，
Carrasco らによる ALERGIA アルゴリズム\cite{Carrasco94} を用いて，
対話構造のモデルを構築することを試みた．

\subsection{ALERGIA アルゴリズム}

ALERGIA アルゴリズムは，与えられた学習データを受理する確率決定性有限オートマトンを
構成するアルゴリズムである．
詳細なアルゴリズムは，文献\cite{Carrasco94}に説明されている．
以下では，ALERGIA アルゴリズムの概要について述べる．


\subsubsection*{(1) 接頭木アクセプタの作成}

学習データから接頭木アクセプタ(Prefix Tree Acceptor；PTA)を作る．
なお，接頭木アクセプタとは，学習データ中のシンボル列を受理する
決定性有限オートマトンであり，
トライのようにシンボル系列の接頭部分が同じものを
共通の状態によって表現したものである．
例えば，学習データ $S = \{ \lambda, 00, 10, 110 \}$ ($\lambda$ は空列)に対する
PTA は，図\ref{Fig:PTA} のようになる．

\begin{figure}[h]
\begin{center}
\epsfile{file=fig3.eps,width=70mm}
\end{center}
\caption{接頭木アクセプタの例}
\label{Fig:PTA}
\end{figure}

\subsubsection*{(2) 状態遷移確率の計算}

$n_{i}$ を学習データが接頭木アクセプタの各状態 $q_{i}$ を訪れた回数とする．
もし学習データが状態 $q_{i}$ で受理されれば，受理されたデータの個数を
$f_{i}(\#)$ とする．
状態 $q_{i}$ で受理されなければ，次の状態へ遷移するが，
このとき状態遷移 $\delta_{i}(a)$
(状態 $q_{i}$ でシンボル $a$ がきたときの遷移)をたどった回数を
$f_{i}(a)$ とする．
状態遷移 $\delta_{i}(a)$ の遷移確率は，次のようにして求められる．
\begin{equation}
P_{i}(a) = \frac{f_{i}(a)}{n_{i}}
\end{equation}
なお，$P_{i}(\#)$ は，データが状態 $q_{i}$ で受理される確率を表している．


\subsubsection*{(3) 状態のマージ}

接頭木アクセプタの状態 $q_{i}$ と $q_{j}$ が等価($q_{i} \equiv q_{j}$)であれば，
これら２つの状態をマージする．
ここで，状態 $q_{i}$ と $q_{j}$ が等価であるとは，
すべてのシンボル $a \in \Sigma$ について，遷移確率 $P_{i}(a)$ と
$P_{j}(a)$ が等しく，遷移後の状態も等価であるときをいう．
即ち，状態 $q_{i}$ と $q_{j}$ が等価であれば，次が成り立つ．
\begin{equation}
q_{i} \equiv q_{j} \Longrightarrow
        \forall a \in \Sigma \left \{ \begin{array}{l}
                P_{i}(a) = P_{j}(a) \\
                \delta_{i}(a) \equiv \delta_{j}(a)
        \end{array}
        \right .
\end{equation}
なお，状態の等価性を判断する場合，学習データに対する統計的な揺れを伴うので，
２つの遷移確率の差が許容範囲にあるときに等価であるとする．

ALERGIA アルゴリズムでは，以下のようにして状態の等価性を決めている．
確率 $p$ のベルヌイ確率変数があり，$n$ 回の試行のうち $f$ 回
この事象が起こったとすると，次式が成り立つ．
\begin{equation}
        P \left ( \left | p - \frac{f}{n} \right | < \sqrt{\frac{1}{2n} \log \frac{2}{\alpha}} \right ) \geq 1 - \alpha
\end{equation}
ALERGIA アルゴリズムでは，学習データから推定された２つの遷移確率の差が，
信頼範囲 $\sqrt{\frac{1}{2n} \log \frac{2}{\alpha}}$
の和の範囲内にあるときに，２つの状態を等価であるとしている．
即ち，状態 $i$ と状態 $j$ が等価であるとは，
すべてのシンボル $a \in \Sigma$ について，
次式が成り立つことである．
\begin{equation}
        \left | \frac{f_{i}(a)}{n_{i}} - \frac{f_{j}(a)}{n_{j}} \right |
        \leq \sqrt{\frac{1}{2} \log \frac{2}{\alpha}}
                \left ( \frac{1}{\sqrt{n_{i}}} + \frac{1}{\sqrt{n_{j}}} \right )
\label{Eq:AlergiaStateEq}
\end{equation}

\subsection{ALERGIA アルゴリズムの動作例}

\begin{figure}
\begin{center}
\epsfile{file=fig4.eps,width=130mm}
\end{center}
\caption{ALERGIA アルゴリズムの動作例}
\label{Fig:ExampleAlergia}
\end{figure}

ALERGIA アルゴリズムの動作を，簡単な例で説明する\cite{Carrasco94}．
いま，学習データとして，次の集合 $S$ が与えられたとする．
\begin{equation}
  S = \{ 110, \lambda, \lambda, \lambda, 0, \lambda, 00, 00,
        \lambda, \lambda, \lambda, 10110, \lambda, \lambda, 100 \}
\end{equation}
また，$\alpha = 0.8$ と仮定する．

\subsubsection*{(1) PTA の作成}

学習データから，図\ref{Fig:ExampleAlergia} (a) の PTA を作成する．
図\ref{Fig:ExampleAlergia} では，各状態の下に，その状態に到達したデータの個数および
その状態で受理されたデータの個数が示されている．
また，各状態遷移には，その遷移を引き起こしたシンボル(0 あるいは 1)と
データ数が示されている．

\subsubsection*{(2) 状態 $(2,1)$ の等価性チェック}

まず，状態 2 と状態 1 の等価性について考える．
２つの状態での受理確率の差は，
\begin{equation}
        \left | \frac{1}{3} - \frac{9}{15} \right |     
        = 0.26 <
        \sqrt{\frac{1}{2} \log \frac{2}{\alpha}}
        \left ( \frac{1}{\sqrt{3}} + \frac{1}{\sqrt{15}} \right )
        = 0.55
\end{equation}
また，シンボル 0 による遷移確率についても，
\begin{equation}
        \left | \frac{2}{3} - \frac{3}{15} \right |
        = 0.46 < 0.55
\end{equation}
となる．
状態 2 と状態 1 が等価であるためには，
更にこれらの状態の遷移先である状態 4 と状態 2 も等価である必要があるが，
同様の計算により，状態 4 と状態 2 の等価性も示すことができる．
状態 4 と状態 2 をマージし，更に状態 2 と状態 1 をマージすると，
図 (b) のオートマトンを得る．

\subsubsection*{(3) 状態 $(3,1)$ の等価性チェック}

次に，状態 3 と状態 1 について考えると，
両者の受理確率の差は，
\begin{equation}
        \left | \frac{0}{3} - \frac{12}{20} \right |
        = 0.6
        >
        \sqrt{\frac{1}{2} \log \frac{2}{\alpha}}
        \left ( \frac{1}{\sqrt{3}} + \frac{1}{\sqrt{20}} \right )
        = 0.53
\end{equation}
となり，等価でないことが分かる．
従って，状態 3 と状態 1 をマージすることはできない．

\subsubsection*{(4) 状態 $(5,1)$ の等価性チェック}

以上の計算と同様にして，状態 5 と状態 1 の等価性も示すことができる．
また，状態 $(5,1)$ の等価性を調べる過程において，
状態 $(7,1)$, $(8,3)$, $(10,6)$, $(11,9)$ の等価性も同時に示される．
これらの状態をマージすると，
図 (c) のオートマトンを得る．

\subsubsection*{(5) 状態 $(6,3)$ の等価性チェック}

同様にして，状態 6 と状態 3 の等価性も示すことができる．
状態 $(6,3)$ をマージすると，図 (d) のオートマトンを得る．
受理確率および遷移確率を計算して，
最終的に図 (e) のオートマトンを得る．

\subsection{対話構造のモデル化}

\begin{figure}
\begin{center}
\epsfile{file=fig5.eps,width=104mm}
\end{center}
\caption{状態数とパープレキシティの関係}
\label{Fig:ALERGIA-STATE-ENTROPY}
\end{figure}

\begin{figure}
\begin{center}
\epsfile{file=fig6.eps,width=140mm}
\end{center}
\caption{ALERGIA アルゴリズムにより得られたオートマトンの一部}
\label{Fig:ALERGIA-IFT}
\end{figure}

上述の ALERGIA アルゴリズムを用いて，
IFT 付きコーパスから対話構造をモデル化する実験を行なった．
学習データとしては，
キーボード会話 50 対話(1686 文)を用いた．

ALERGIA アルゴリズムでは，
状態の等価性は式(\ref{Eq:AlergiaStateEq})により判定されるが，
式(\ref{Eq:AlergiaStateEq})の右辺の値(定数 $\alpha$ の値)を変えることにより，
様々な状態数を持つオートマトンを学習データから構成することができる．
図\ref{Fig:ALERGIA-STATE-ENTROPY} に，
ALERGIA アルゴリズムにより得られたオートマトンの状態数と
パープレキシティの関係を示す．
パープレキシティの値は，学習データに対するテストセット・パープレキシティ
を用いている．
状態数の増加にともないパープレキシティは減少している．

パープレキシティ $P$ とエントロピー $H$ の間には，
\begin{equation}
        P = 2^{H} \label{Eq:PerpEnt}
\end{equation}
なる関係があるが，式(\ref{Eq:PerpEnt})より，
ALERGIA アルゴリズムで得られたモデルのエントロピーを算出してみると，
エルゴード HMM と同程度の精度を達成するためには，
エルゴード HMM の場合よりもはるかに多くの状態が必要となることが分かる．
これは，HMM が非決定性の有限オートマトンと等価であるのに対し，
ALERGIA アルゴリズムにより得られるモデルが決定性の有限オートマトンであるためである．

図\ref{Fig:ALERGIA-IFT} は，
話者ラベルと IFT を組み合わせたラベルの系列から得られた
30 状態のオートマトンの一部(16 個の状態)である．
このオートマトンの初期状態は状態 0 であり，最終状態は状態 22 である．
図の左側には，初期状態 0 から状態遷移する確率の高い 11 個の状態
(状態 0, 4, 7, 9, 10, 11, 12, 17, 20, 27, 28)
が示されている．
状態 0 から始まり再び状態 0 に至る状態遷移系列
(例えば，0 → 7 → 4 や 0 → 7 → 27 → 28 など)
が，質問・応答・確認などの対話の基本サイクルを表していると
考えることができる．

また，図の右側に，最終状態 22 に状態遷移する確率の高い 5 個の状態
(状態 1, 16, 21, 22, 23)が示されている．
例えば，状態 27 あるいは 28 で，
expressive (例：「ありがとうございました」)
に対応する発話が現れると，最終状態へ向かう状態遷移が選択されるということが分かる．
しかし，国際会議参加登録のタスクでは，
expressive や phatic という IFT の出現頻度は IFT 全体の数パーセントにしか
過ぎないので，状態 27 あるいは 28 から状態 21 へ遷移する確率は
低くなっている(図中，遷移確率の小さいものは破線で示されている)．

\vspace*{-3mm}

