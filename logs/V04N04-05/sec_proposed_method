対話モデル作成のための基礎データとして，発話行為タイプ(Illocutionary Force Type; IFT)付きコーパス[CITE]を用いた．
これは，ATR対話データベース中の「国際会議参加登録のタスク」の対話の各発話について，その発語内行為を分析し，陳述・命令・約束などの発話のタイプが付けられたコーパスである．
このコーパスで用いられているIFTは，表層の統語的パターンと比較的直接的な対応がとれる表層IFT(Surface Illocutionary Force Type)と呼ばれるものである．
また，各発話文には，発話者(事務局または質問者)を示すラベルが付与されている．
IFT付きコーパスで用いられている表層IFTの種類および各IFTに属する例文を表[REF_Tab:IFTdef]に，IFT付きコーパスの例を図[REF_Fig:IFT-Corpus]に示す．
本研究における評価実験では，IFT付きコーパスの中から，モデル会話10対話(222文)とキーボード会話50対話(1686文)を用いた．
[p]
質問者phatic:もしもしquestionif:そちらは会議事務局ですか
事務局response:はいresponse:そうですquestionref:どのようなご用件でしょうか
質問者inform:会議に申し込みたいのですがquestionref:どのような手続きをすればよろしいのでしょうか
事務局request:登録用紙で手続きをして下さいquestionif:登録用紙は既にお持ちでしょうか
質問者response:いいえinform:まだです
IFT付きコーパスの各発話には，話者ラベルおよびIFTが付与されている．
話者の交替や質問・応答・確認のような対話の基本的な構造を確率・統計的にモデル化するために，コーパス中の話者ラベルおよびIFTの系列をエルゴードHMMによりモデル化することを試みた．
なお，エルゴードHMMとは，自己遷移も含めすべての状態間の遷移を許す全遷移型のHMMである．
本実験では，あらかじめHMMの状態数を決めておき，Baum-Welchの再推定アルゴリズムにより，エルゴードHMMの学習を行なった．
初期モデルとしては，初期状態分布確率を均等確率に，また状態遷移確率および出力確率は確率値の総計が１になるようなランダムな値で初期化した．
エルゴードHMMの学習データとして，モデル会話およびキーボード会話中から，以下の２つの系列を抽出した．
IFTのみの系列
話者ラベルとIFTを組み合わせたラベルの系列
IFTの総数は９個であり，対話コーパス中の発話者は２名(事務局あるいは質問者)であるので，上記(2)の場合のシンボル数は18個である．
実験では，エルゴードHMMの構造として状態数2〜14のものを用いて学習を行なった．
表[REF_Tab:HMMEntropy]に，状態数2, 4, 6, 8, 10, 12, 14の場合のモデルのエントロピーを示す．
表[REF_Tab:HMMEntropy]で，IFTと示されているのはIFTのみの系列を用いたときの結果であり，SP-IFTは話者ラベルとIFTを組み合わせたラベルの系列を用いたときの結果である．
一般的な傾向として，状態数が増えるに従いエントロピーが小さくなり，同じ状態数では話者ラベルを併用したものの方がエントロピー値が大きくなっている．
文献[CITE]の結果では，trigramモデルを使った場合，モデル会話でのSP-IFTのエントロピー値は1.26，キーボード会話でのSP-IFTの値は2.19と報告している．
本実験では，12〜14状態のエルゴードHMMの場合が，trigramのエントロピー値とほぼ同等になっている．
学習後のHMMの構造(状態数5の場合)を図[REF_Fig:HMM-IFT]および図[REF_Fig:HMM-IFT-SP]に示す．
図[REF_Fig:HMM-IFT]はIFTのみの系列から得られたモデルであり，図[REF_Fig:HMM-IFT-SP]は話者ラベルとIFTを組み合わせたラベルの系列から得られたモデルである．
図には，遷移確率および出力確率が0.1以上のもののみを記しており，矢印の太いものほど大きな遷移確率を持っていることを示している．
状態遷移の一番上に書かれている確率が遷移確率であり，その下に各シンボル(IFT)の出力確率が記されている．
図[REF_Fig:HMM-IFT-SP]で，Sで始まるシンボルは事務局側の発話であることを，またQで始まるシンボルは質問者側の発話であることを示している．
例えば，図[REF_Fig:HMM-IFT-SP]では，状態1が初期状態であり，質問者が最初の発話「もしもし」を発話するとQphaticを出力する遷移をたどることになる．
これは，状態1での自己ループあるいは状態1から状態2への遷移に対応している．
「国際会議参加登録のタスク」では，事務局の「こちらは会議事務局です」という発話により対話が始まる場合もある．
この場合にはSinformを出力する遷移である状態1での自己ループとなる．
また，図[REF_Fig:HMM-IFT-SP]では，状態遷移が事務局側の発話と質問者側の発話で比較的きれいに分かれている．
例えば，状態3から状態2への遷移は質問者側の発話によって起こり，しかもこの遷移は事務局に対する質問や依頼に対応していることが分かる．
この質問や依頼に対し，状態2から状態0の遷移で事務局が応答(Sresponse, Sinform)する確率が非常に高いことも読みとることができる．
以上のように，発話行為タイプ付きコーパスから得られたエルゴードHMMは，質問・応答といった基本的な構造を抽出しているということができる．
エルゴードHMMによるモデル化では，確率モデルの学習に先立ち，モデルの構造(状態数)をあらかじめ決めておく必要がある．
これに対し，近年，状態マージング手法を用いて，学習データに対し最適な構造を持つモデルを自動的に構築する研究がいくつか行なわれている[CITE]．
我々は，CarrascoらによるALERGIAアルゴリズム[CITE]を用いて，対話構造のモデルを構築することを試みた．
ALERGIAアルゴリズムは，与えられた学習データを受理する確率決定性有限オートマトンを構成するアルゴリズムである．
詳細なアルゴリズムは，文献[CITE]に説明されている．
以下では，ALERGIAアルゴリズムの概要について述べる．
学習データから接頭木アクセプタ(Prefix Tree Acceptor；PTA)を作る．
なお，接頭木アクセプタとは，学習データ中のシンボル列を受理する決定性有限オートマトンであり，トライのようにシンボル系列の接頭部分が同じものを共通の状態によって表現したものである．
例えば，学習データ[MATH] ([MATH]は空列)に対するPTAは，図[REF_Fig:PTA]のようになる．
[MATH]を学習データが接頭木アクセプタの各状態[MATH]を訪れた回数とする．
もし学習データが状態[MATH]で受理されれば，受理されたデータの個数を[MATH]とする．
状態[MATH]で受理されなければ，次の状態へ遷移するが，このとき状態遷移[MATH] (状態[MATH]でシンボル[MATH]がきたときの遷移)をたどった回数を[MATH]とする．
状態遷移[MATH]の遷移確率は，次のようにして求められる．
なお，[MATH]は，データが状態[MATH]で受理される確率を表している．
接頭木アクセプタの状態[MATH]と[MATH]が等価([MATH])であれば，これら２つの状態をマージする．
ここで，状態[MATH]と[MATH]が等価であるとは，すべてのシンボル[MATH]について，遷移確率[MATH]と[MATH]が等しく，遷移後の状態も等価であるときをいう．
即ち，状態[MATH]と[MATH]が等価であれば，次が成り立つ．
なお，状態の等価性を判断する場合，学習データに対する統計的な揺れを伴うので，２つの遷移確率の差が許容範囲にあるときに等価であるとする．
ALERGIAアルゴリズムでは，以下のようにして状態の等価性を決めている．
確率[MATH]のベルヌイ確率変数があり，[MATH]回の試行のうち[MATH]回この事象が起こったとすると，次式が成り立つ．
ALERGIAアルゴリズムでは，学習データから推定された２つの遷移確率の差が，信頼範囲[MATH]の和の範囲内にあるときに，２つの状態を等価であるとしている．
即ち，状態[MATH]と状態[MATH]が等価であるとは，すべてのシンボル[MATH]について，次式が成り立つことである．
ALERGIAアルゴリズムの動作を，簡単な例で説明する[CITE]．
いま，学習データとして，次の集合[MATH]が与えられたとする．
また，[MATH]と仮定する．
学習データから，図[REF_Fig:ExampleAlergia] (a)のPTAを作成する．
図[REF_Fig:ExampleAlergia]では，各状態の下に，その状態に到達したデータの個数およびその状態で受理されたデータの個数が示されている．
また，各状態遷移には，その遷移を引き起こしたシンボル(0あるいは1)とデータ数が示されている．
まず，状態2と状態1の等価性について考える．
２つの状態での受理確率の差は，
また，シンボル0による遷移確率についても，
となる．
状態2と状態1が等価であるためには，更にこれらの状態の遷移先である状態4と状態2も等価である必要があるが，同様の計算により，状態4と状態2の等価性も示すことができる．
状態4と状態2をマージし，更に状態2と状態1をマージすると，図(b)のオートマトンを得る．
次に，状態3と状態1について考えると，両者の受理確率の差は，
となり，等価でないことが分かる．
従って，状態3と状態1をマージすることはできない．
以上の計算と同様にして，状態5と状態1の等価性も示すことができる．
また，状態[MATH]の等価性を調べる過程において，状態[MATH], [MATH], [MATH], [MATH]の等価性も同時に示される．
これらの状態をマージすると，図(c)のオートマトンを得る．
同様にして，状態6と状態3の等価性も示すことができる．
状態[MATH]をマージすると，図(d)のオートマトンを得る．
受理確率および遷移確率を計算して，最終的に図(e)のオートマトンを得る．
上述のALERGIAアルゴリズムを用いて，IFT付きコーパスから対話構造をモデル化する実験を行なった．
学習データとしては，キーボード会話50対話(1686文)を用いた．
ALERGIAアルゴリズムでは，状態の等価性は式([REF_Eq:AlergiaStateEq])により判定されるが，式([REF_Eq:AlergiaStateEq])の右辺の値(定数[MATH]の値)を変えることにより，様々な状態数を持つオートマトンを学習データから構成することができる．
図[REF_Fig:ALERGIA-STATE-ENTROPY]に，ALERGIAアルゴリズムにより得られたオートマトンの状態数とパープレキシティの関係を示す．
パープレキシティの値は，学習データに対するテストセット・パープレキシティを用いている．
状態数の増加にともないパープレキシティは減少している．
パープレキシティ[MATH]とエントロピー[MATH]の間には，
なる関係があるが，式([REF_Eq:PerpEnt])より，ALERGIAアルゴリズムで得られたモデルのエントロピーを算出してみると，エルゴードHMMと同程度の精度を達成するためには，エルゴードHMMの場合よりもはるかに多くの状態が必要となることが分かる．
これは，HMMが非決定性の有限オートマトンと等価であるのに対し，ALERGIAアルゴリズムにより得られるモデルが決定性の有限オートマトンであるためである．
図[REF_Fig:ALERGIA-IFT]は，話者ラベルとIFTを組み合わせたラベルの系列から得られた30状態のオートマトンの一部(16個の状態)である．
このオートマトンの初期状態は状態0であり，最終状態は状態22である．
図の左側には，初期状態0から状態遷移する確率の高い11個の状態(状態0, 4, 7, 9, 10, 11, 12, 17, 20, 27, 28)が示されている．
状態0から始まり再び状態0に至る状態遷移系列(例えば，0→7→4や0→7→27→28など)が，質問・応答・確認などの対話の基本サイクルを表していると考えることができる．
また，図の右側に，最終状態22に状態遷移する確率の高い5個の状態(状態1, 16, 21, 22, 23)が示されている．
例えば，状態27あるいは28で，expressive (例：「ありがとうございました」)に対応する発話が現れると，最終状態へ向かう状態遷移が選択されるということが分かる．
しかし，国際会議参加登録のタスクでは，expressiveやphaticというIFTの出現頻度はIFT全体の数パーセントにしか過ぎないので，状態27あるいは28から状態21へ遷移する確率は低くなっている(図中，遷移確率の小さいものは破線で示されている)．
対話モデル作成のための基礎データとして，発話行為タイプ(Illocutionary Force Type; IFT)付きコーパス[CITE]を用いた．
これは，ATR対話データベース中の「国際会議参加登録のタスク」の対話の各発話について，その発語内行為を分析し，陳述・命令・約束などの発話のタイプが付けられたコーパスである．
このコーパスで用いられているIFTは，表層の統語的パターンと比較的直接的な対応がとれる表層IFT(Surface Illocutionary Force Type)と呼ばれるものである．
また，各発話文には，発話者(事務局または質問者)を示すラベルが付与されている．
IFT付きコーパスで用いられている表層IFTの種類および各IFTに属する例文を表[REF_Tab:IFTdef]に，IFT付きコーパスの例を図[REF_Fig:IFT-Corpus]に示す．
本研究における評価実験では，IFT付きコーパスの中から，モデル会話10対話(222文)とキーボード会話50対話(1686文)を用いた．
[p]
質問者phatic:もしもしquestionif:そちらは会議事務局ですか
事務局response:はいresponse:そうですquestionref:どのようなご用件でしょうか
質問者inform:会議に申し込みたいのですがquestionref:どのような手続きをすればよろしいのでしょうか
事務局request:登録用紙で手続きをして下さいquestionif:登録用紙は既にお持ちでしょうか
質問者response:いいえinform:まだです
IFT付きコーパスの各発話には，話者ラベルおよびIFTが付与されている．
話者の交替や質問・応答・確認のような対話の基本的な構造を確率・統計的にモデル化するために，コーパス中の話者ラベルおよびIFTの系列をエルゴードHMMによりモデル化することを試みた．
なお，エルゴードHMMとは，自己遷移も含めすべての状態間の遷移を許す全遷移型のHMMである．
本実験では，あらかじめHMMの状態数を決めておき，Baum-Welchの再推定アルゴリズムにより，エルゴードHMMの学習を行なった．
初期モデルとしては，初期状態分布確率を均等確率に，また状態遷移確率および出力確率は確率値の総計が１になるようなランダムな値で初期化した．
エルゴードHMMの学習データとして，モデル会話およびキーボード会話中から，以下の２つの系列を抽出した．
IFTのみの系列
話者ラベルとIFTを組み合わせたラベルの系列
IFTの総数は９個であり，対話コーパス中の発話者は２名(事務局あるいは質問者)であるので，上記(2)の場合のシンボル数は18個である．
実験では，エルゴードHMMの構造として状態数2〜14のものを用いて学習を行なった．
表[REF_Tab:HMMEntropy]に，状態数2, 4, 6, 8, 10, 12, 14の場合のモデルのエントロピーを示す．
表[REF_Tab:HMMEntropy]で，IFTと示されているのはIFTのみの系列を用いたときの結果であり，SP-IFTは話者ラベルとIFTを組み合わせたラベルの系列を用いたときの結果である．
一般的な傾向として，状態数が増えるに従いエントロピーが小さくなり，同じ状態数では話者ラベルを併用したものの方がエントロピー値が大きくなっている．
文献[CITE]の結果では，trigramモデルを使った場合，モデル会話でのSP-IFTのエントロピー値は1.26，キーボード会話でのSP-IFTの値は2.19と報告している．
本実験では，12〜14状態のエルゴードHMMの場合が，trigramのエントロピー値とほぼ同等になっている．
学習後のHMMの構造(状態数5の場合)を図[REF_Fig:HMM-IFT]および図[REF_Fig:HMM-IFT-SP]に示す．
図[REF_Fig:HMM-IFT]はIFTのみの系列から得られたモデルであり，図[REF_Fig:HMM-IFT-SP]は話者ラベルとIFTを組み合わせたラベルの系列から得られたモデルである．
図には，遷移確率および出力確率が0.1以上のもののみを記しており，矢印の太いものほど大きな遷移確率を持っていることを示している．
状態遷移の一番上に書かれている確率が遷移確率であり，その下に各シンボル(IFT)の出力確率が記されている．
図[REF_Fig:HMM-IFT-SP]で，Sで始まるシンボルは事務局側の発話であることを，またQで始まるシンボルは質問者側の発話であることを示している．
例えば，図[REF_Fig:HMM-IFT-SP]では，状態1が初期状態であり，質問者が最初の発話「もしもし」を発話するとQphaticを出力する遷移をたどることになる．
これは，状態1での自己ループあるいは状態1から状態2への遷移に対応している．
「国際会議参加登録のタスク」では，事務局の「こちらは会議事務局です」という発話により対話が始まる場合もある．
この場合にはSinformを出力する遷移である状態1での自己ループとなる．
また，図[REF_Fig:HMM-IFT-SP]では，状態遷移が事務局側の発話と質問者側の発話で比較的きれいに分かれている．
例えば，状態3から状態2への遷移は質問者側の発話によって起こり，しかもこの遷移は事務局に対する質問や依頼に対応していることが分かる．
この質問や依頼に対し，状態2から状態0の遷移で事務局が応答(Sresponse, Sinform)する確率が非常に高いことも読みとることができる．
以上のように，発話行為タイプ付きコーパスから得られたエルゴードHMMは，質問・応答といった基本的な構造を抽出しているということができる．
エルゴードHMMによるモデル化では，確率モデルの学習に先立ち，モデルの構造(状態数)をあらかじめ決めておく必要がある．
これに対し，近年，状態マージング手法を用いて，学習データに対し最適な構造を持つモデルを自動的に構築する研究がいくつか行なわれている[CITE]．
我々は，CarrascoらによるALERGIAアルゴリズム[CITE]を用いて，対話構造のモデルを構築することを試みた．
ALERGIAアルゴリズムは，与えられた学習データを受理する確率決定性有限オートマトンを構成するアルゴリズムである．
詳細なアルゴリズムは，文献[CITE]に説明されている．
以下では，ALERGIAアルゴリズムの概要について述べる．
学習データから接頭木アクセプタ(Prefix Tree Acceptor；PTA)を作る．
なお，接頭木アクセプタとは，学習データ中のシンボル列を受理する決定性有限オートマトンであり，トライのようにシンボル系列の接頭部分が同じものを共通の状態によって表現したものである．
例えば，学習データ[MATH] ([MATH]は空列)に対するPTAは，図[REF_Fig:PTA]のようになる．
[MATH]を学習データが接頭木アクセプタの各状態[MATH]を訪れた回数とする．
もし学習データが状態[MATH]で受理されれば，受理されたデータの個数を[MATH]とする．
状態[MATH]で受理されなければ，次の状態へ遷移するが，このとき状態遷移[MATH] (状態[MATH]でシンボル[MATH]がきたときの遷移)をたどった回数を[MATH]とする．
状態遷移[MATH]の遷移確率は，次のようにして求められる．
なお，[MATH]は，データが状態[MATH]で受理される確率を表している．
接頭木アクセプタの状態[MATH]と[MATH]が等価([MATH])であれば，これら２つの状態をマージする．
ここで，状態[MATH]と[MATH]が等価であるとは，すべてのシンボル[MATH]について，遷移確率[MATH]と[MATH]が等しく，遷移後の状態も等価であるときをいう．
即ち，状態[MATH]と[MATH]が等価であれば，次が成り立つ．
なお，状態の等価性を判断する場合，学習データに対する統計的な揺れを伴うので，２つの遷移確率の差が許容範囲にあるときに等価であるとする．
ALERGIAアルゴリズムでは，以下のようにして状態の等価性を決めている．
確率[MATH]のベルヌイ確率変数があり，[MATH]回の試行のうち[MATH]回この事象が起こったとすると，次式が成り立つ．
ALERGIAアルゴリズムでは，学習データから推定された２つの遷移確率の差が，信頼範囲[MATH]の和の範囲内にあるときに，２つの状態を等価であるとしている．
即ち，状態[MATH]と状態[MATH]が等価であるとは，すべてのシンボル[MATH]について，次式が成り立つことである．
ALERGIAアルゴリズムの動作を，簡単な例で説明する[CITE]．
いま，学習データとして，次の集合[MATH]が与えられたとする．
また，[MATH]と仮定する．
学習データから，図[REF_Fig:ExampleAlergia] (a)のPTAを作成する．
図[REF_Fig:ExampleAlergia]では，各状態の下に，その状態に到達したデータの個数およびその状態で受理されたデータの個数が示されている．
また，各状態遷移には，その遷移を引き起こしたシンボル(0あるいは1)とデータ数が示されている．
まず，状態2と状態1の等価性について考える．
２つの状態での受理確率の差は，
また，シンボル0による遷移確率についても，
となる．
状態2と状態1が等価であるためには，更にこれらの状態の遷移先である状態4と状態2も等価である必要があるが，同様の計算により，状態4と状態2の等価性も示すことができる．
状態4と状態2をマージし，更に状態2と状態1をマージすると，図(b)のオートマトンを得る．
次に，状態3と状態1について考えると，両者の受理確率の差は，
となり，等価でないことが分かる．
従って，状態3と状態1をマージすることはできない．
以上の計算と同様にして，状態5と状態1の等価性も示すことができる．
また，状態[MATH]の等価性を調べる過程において，状態[MATH], [MATH], [MATH], [MATH]の等価性も同時に示される．
これらの状態をマージすると，図(c)のオートマトンを得る．
同様にして，状態6と状態3の等価性も示すことができる．
状態[MATH]をマージすると，図(d)のオートマトンを得る．
受理確率および遷移確率を計算して，最終的に図(e)のオートマトンを得る．
上述のALERGIAアルゴリズムを用いて，IFT付きコーパスから対話構造をモデル化する実験を行なった．
学習データとしては，キーボード会話50対話(1686文)を用いた．
ALERGIAアルゴリズムでは，状態の等価性は式([REF_Eq:AlergiaStateEq])により判定されるが，式([REF_Eq:AlergiaStateEq])の右辺の値(定数[MATH]の値)を変えることにより，様々な状態数を持つオートマトンを学習データから構成することができる．
図[REF_Fig:ALERGIA-STATE-ENTROPY]に，ALERGIAアルゴリズムにより得られたオートマトンの状態数とパープレキシティの関係を示す．
パープレキシティの値は，学習データに対するテストセット・パープレキシティを用いている．
状態数の増加にともないパープレキシティは減少している．
パープレキシティ[MATH]とエントロピー[MATH]の間には，
なる関係があるが，式([REF_Eq:PerpEnt])より，ALERGIAアルゴリズムで得られたモデルのエントロピーを算出してみると，エルゴードHMMと同程度の精度を達成するためには，エルゴードHMMの場合よりもはるかに多くの状態が必要となることが分かる．
これは，HMMが非決定性の有限オートマトンと等価であるのに対し，ALERGIAアルゴリズムにより得られるモデルが決定性の有限オートマトンであるためである．
図[REF_Fig:ALERGIA-IFT]は，話者ラベルとIFTを組み合わせたラベルの系列から得られた30状態のオートマトンの一部(16個の状態)である．
このオートマトンの初期状態は状態0であり，最終状態は状態22である．
図の左側には，初期状態0から状態遷移する確率の高い11個の状態(状態0, 4, 7, 9, 10, 11, 12, 17, 20, 27, 28)が示されている．
状態0から始まり再び状態0に至る状態遷移系列(例えば，0→7→4や0→7→27→28など)が，質問・応答・確認などの対話の基本サイクルを表していると考えることができる．
また，図の右側に，最終状態22に状態遷移する確率の高い5個の状態(状態1, 16, 21, 22, 23)が示されている．
例えば，状態27あるいは28で，expressive (例：「ありがとうございました」)に対応する発話が現れると，最終状態へ向かう状態遷移が選択されるということが分かる．
しかし，国際会議参加登録のタスクでは，expressiveやphaticというIFTの出現頻度はIFT全体の数パーセントにしか過ぎないので，状態27あるいは28から状態21へ遷移する確率は低くなっている(図中，遷移確率の小さいものは破線で示されている)．
