    \documentclass[japanese]{jnlp_1.4}

\usepackage{jnlpbbl_1.3}
\usepackage[dvipdfm]{graphicx}
\usepackage{amsmath}
\usepackage{dcolumn}

\usepackage{bm}
\usepackage{amssymb} 

\Volume{21}
\Number{5}
\Month{Septermber}
\Year{2014}

\received{2014}{1}{22}
\revised{2014}{4}{1}
\rerevised{2014}{5}{13}
\accepted{2014}{6}{30}

\setcounter{page}{1011}

\jtitle{共変量シフト下の学習による語義曖昧性解消の\\
教師なし領域適応}
\jauthor{新納　浩幸\affiref{Author_1} \and 佐々木　稔\affiref{Author_1}}
\jabstract{
本論文では語義曖昧性解消(Word Sense Disambiguation，WSD)の教師なし領域適応の問題
に対して，共変量シフト下の学習を試みる．共変量シフト下の学習では
確率密度比 $w({\bm x}) =  P_T({\bm x})/P_S({\bm x})$ を重みとした
重み付き学習を行うが，WSD の場合，推定される確率密度比の値が小さくなる傾向がある．
ここでは$P_T({\bm x})$と$P_S({\bm x})$を
それぞれ求めて，その比を取ることで$w({\bm x})$を推定するが，
$P_S({\bm x})$を求める際に，
ターゲット領域のコーパスとソース領域のコーパスを合わせたコーパスを，
新たにソース領域のコーパス$S$と見なすことで，先の問題に対処する．
BCCWJ の3つの領域 OC （Yahoo! 知恵袋），PB（書籍）及び PN（新聞）を選び，
SemEval-2 の日本語 WSD タスクのデータを利用して，多義語 16種類を対象に，
WSD の領域適応の実験を行った．
$w({\bm x})$を推定する手法として，$P_T({\bm x})$と$P_S({\bm x})$を求めずに，
$w({\bm x})$を直接推定する uLSIF も試みた．また確率密度比を上方修正するために
「$p$乗する」「相対確率密度比を取る」という手法も組み合わせて試みた．
それらの実験の結果，提案手法の有効性が示された．
}
\jkeywords{語義曖昧性解消，領域適応，共変量シフト，uLSIF，負の転移}

\etitle{Unsupervised Domain Adaptations for Word Sense Disambiguation by Learning under Covariate Shift}
\eauthor{Hiroyuki Shinnou\affiref{Author_1} \and Minoru Sasaki\affiref{Author_1}} 
\eabstract{
In this paper, we apply the learning under covariate shift
to the problem of unsupervised domain adaptation for word sense disambiguation (WSD).
This learning is a type of weighted learning method, 
in which the probability density ratio $w({\bm x}) =  P_T({\bm x})/P_S({\bm x})$
is used as the weight of an instance. 
However, $w({\bm x})$ tends to be small in WSD tasks.
In order to address this problem, 
we calculate $w({\bm x})$ by estimating $P_T({\bm x})$ and $P_S({\bm x})$,
where $P_S({\bm x})$ is estimating by regarding 
the corpus combining the source domain corpus and target domain corpus
as the source domain corpus.
In the experiment, we use three domains -OC (Yahoo! Chiebukuro), PB (books) and 
PN (news papers)- in BCCWJ,
and 16 target words provided by the Japanese WSD task in SemEval-2.
For calculating $w({\bm x})$, we also use 
uLSIF, which  directly estimates $w({\bm x})$
without estimating $P_T({\bm x})$ or $P_S({\bm x})$.
Moreover, we use the ``$p$ power'' method and the ``relative probability density ratio'' method 
to boost the obtained probability density ratio.
These experiments prove our method to be effective.
}
\ekeywords{word sense disambiguation, domain adaptation, covariate shift, uLSIF，negative transfer}

\headauthor{新納，佐々木}
\headtitle{共変量シフト下の学習による語義曖昧性解消の教師なし領域適応}

\affilabel{Author_1}{茨城大学工学部情報工学科}{Department of Computer and Information Sciences, Ibaraki University}



\begin{document}
\maketitle

\section{期待損失最小化に基づく共変量シフト下の学習}

対象単語$w$の語義の集合を$C$，また
$w$の用例${\bm x}$内の$w$の語義を$c$と識別したときの
損失関数を$l({\bm x},c,d)$で表す．$d$は$w$の語義を識別する分類器である．
$P_T({\bm x},c)$ をターゲット領域上の分布とすれば，
本タスクにおける期待損失$L_0$は以下で表せる．
\[
L_0 = \sum_{{\bm x},c} l({\bm x},c,d) P_T({\bm x},c)
\]
また$P_S({\bm x},c)$ をソース領域上の分布とすると以下が成立する．
\[
L_0 = \sum_{{\bm x},c} l({\bm x},c,d) \frac{P_T({\bm x},c)}{P_S({\bm x},c)} P_S({\bm x},c)
\]
ここで共変量シフトの仮定から
\[
\frac{P_T({\bm x},c)}{P_S({\bm x},c)} = \frac{P_T({\bm x})P_T(c|{\bm x})}{P_S({\bm x})P_S(c|{\bm x})} = \frac{P_T({\bm x})}{P_S({\bm x})}
\]
となり，$w({\bm x}) = P_T({\bm x})/P_S({\bm x})$とおくと以下が成立する．
\[
L_0 = \sum_{{\bm x},c} w({\bm x}) l({\bm x},c,d) P_S({\bm x},c)
\]

訓練データを$D = \{ ({\bm x_i},c_i) \}_{i = 1}^N$とし，
$P_S({\bm x},c)$を経験分布で近似すれば，
\[
 L_0 \approx  \frac{1}{N} \sum_{i=1}^N w({\bm x_i}) l({\bm x_i},c_i,d) 
\]
となるので，期待損失最小化の観点から考えると，共変量シフトの問題は以下の式$L_1$を
最小にする$d$を求めればよいことがわかる．
\begin{equation}
    \label{eq:1}
L_1 = \sum_{i=1}^N w({\bm x_i}) l({\bm x_i},c_i,d) 
\end{equation}

分類器$d$として以下の事後確率最大化推定に基づく識別を考える．
\[
d({\bm x}) = \arg \max_{c} P_T(c|{\bm x})
\]
また損失関数として対数損失$- \log P_T(c|{\bm x})$を用いれば，
\mbox{式(\ref{eq:1})}は以下となる．
\[
L_1 = - \sum_{i=1}^N w({\bm x_i}) \log P_T(c_i|{\bm x_i}) 
\]
つまり，分類問題の解決に$P_T(c|{\bm x},{\bm \lambda})$のモデルを導入するアプローチを取る場合，
共変量シフト下での学習では，確率密度比を重みとした以下に示す
重み付き対数尤度$L({\bm \lambda})$を最大化する
パラメータ${\bm \lambda}$を求める形となる．
\begin{equation}
     \label{eq:2}
    L({\bm \lambda}) = \sum_{i=1}^N w({\bm x_i}) \log P_T(c_i|{\bm x_i},{\bm \lambda})        
\end{equation}


ここではモデルとして以下の式で示される最大エントロピー法を用いる．
\begin{equation}
     \label{eq:3}
P_T(c|{\bm x},{\bm \lambda}) = \frac{1}{Z({\bm x},{\bm \lambda})} \exp \left(
\sum_{j=1}^M \lambda_j f_j({\bm x},c)
\right)
\end{equation}
${\bm x} = (x_1,x_2,\cdots,x_M)$が入力，$c$がクラスである．
関数$f_j({\bm x},c)$は素性関数であり，実質${\bm x}$の真のクラスが
$c$のときに$x_j$を返し，そうでないとき 0 を返す関数に設定される．
$Z({\bm x},{\bm \lambda})$は正規化項であり，以下で表せる．
\begin{equation}
     \label{eq:4}
  Z({\bm x},{\bm \lambda}) = \sum_{c \in C} \exp \left(
\sum_{j=1}^M \lambda_j f_j({\bm x},c) 
\right)
\end{equation}
\noindent
そして${\bm \lambda} = (\lambda_1,\lambda_2,\cdots,\lambda_M)$が
素性に対応する重みパラメータとなる．


\section{確率密度比の算出}

確率密度比$w({\bm x}) =  P_T({\bm x})/P_S({\bm x})$の算出法は大きく2つに分類できる．
1つは$P_S({\bm x})$と$P_T({\bm x})$を各々推定し，その比を取る手法であり，
もう1つは$w({\bm x})$を直接モデル化する手法である．
ここでは前者の方法として論文\cite{shinnou-gengo-14}において提案された手法を利用する．
簡単化のために本論文ではこの手法を NB 法と名付ける．
また後者の方法としては論文\cite{kanamori2009least}において
提案された拘束無し最小二乗重要度適合法 (unconstrained Least-Squares Importance Fitting, uLSIF)
を利用する．


\subsection{NB 法}

対象単語$w$の用例${\bm x}$の素性リストを$\{ f_1,f_2,\cdots, f_n \}$ とする．
求めるのは領域$R \in \{S, T\}$上の${\bm x}$の分布$P_R ({\bm x})$である．
ここで Naive Bayes で使われるモデルを用いる．Naive Bayes のモデルでは以下を仮定する．
\[
P_R ({\bm x}) = \prod_{i=1}^{n} P_R (f_i) 
\]

領域$R$のコーパス内の$w$の全ての用例について素性リストを作成しておく．
ここで用例の数を$N(R)$とおく．
また$N(R)$個の用例の中で，素性$f$が現れた用例数を$n(R,f)$とおく．
MAP 推定でスムージングを行い，$P_R (f)$を以下で定義する\cite{takamura}．
\[
P_R (f) = \frac{n(R,f) + 1}{N(R) + 2}
\]

以上より，ソース領域$S$の用例${\bm x}$に対して，
確率密度比$w({\bm x}) = P_T ({\bm x})/P_S ({\bm x})$が計算できる．
\[
w({\bm x}) = \frac{P_T ({\bm x})}{P_S ({\bm x})} = \prod_{i=1}^n \left( \frac{n(T,f_i) + 1}{N(T) + 2}\cdot\frac{N(S) + 2}{n(S,f_i) + 1} \right)
\]


\subsection{uLSIF}

ソース領域内のデータを$\{{\bm x_i^s}\}_{i=1}^{N_s}$，
ターゲット領域内のデータを$\{{\bm x_i^t}\}_{i=1}^{N_t}$とする
uLSIF では確率密度比$w({\bm x})$を以下の式でモデル化する．
\begin{align*}
w({\bm x}) & = \sum_{l = 1}^b \alpha_l \psi_l ({\bm x}) \\
           & = {\bm \alpha}\cdot {\bm \psi}({\bm x})
\end{align*}
ただしここで，
$
{\bm \alpha} = (\alpha_1, \alpha_2, \cdots, \alpha_b) 
$，
$
{\bm \psi}({\bm x}) = (\psi_1 ({\bm x}), \psi_2 ({\bm x}), \cdots, \psi_b ({\bm x})) 
$ である．
また$\alpha_l$は正の実数であり，$\psi_l ({\bm x})$は基底関数と呼ばれる
ソース領域のデータ${\bm x}$から正の実数値への関数である．
uLSIF では，概略，自然数$b$と基底関数${\bm \psi}({\bm x}) $を定めた後に，
パラメータ${\bm \alpha}$を推定する手順をとる．

説明の都合上，$b$ と ${\bm \psi}({\bm x})$が定まった後の${\bm \alpha}$の推定を
先に説明する．
$w({\bm x})$のモデルを$\hat{w}({\bm x})$とおくと，
パラメータ$\alpha_l$を推定するには，
$w({\bm x})$と$\hat{w}({\bm x})$の平均2乗誤差$J_0({\bm \alpha})$を
最小にするような${\bm \alpha}$を求めれば良い．
$w({\bm x}) = P_T ({\bm x})/P_S ({\bm x})$に注意すると，$J_0({\bm \alpha})$
は以下のように変形できる．
\begin{align*}
J_0({\bm \alpha}) & = \frac{1}{2} \int (\hat{w}({\bm x})  - w({\bm x}))^2 P_S({\bm x}) d{\bm x} \\
 & = \frac{1}{2} \int \hat{w}({\bm x})^2 P_S({\bm x}) d{\bm x}
	- \int \hat{w}({\bm x}) w({\bm x}) P_S({\bm x}) d{\bm x}
	+ \frac{1}{2} \int w({\bm x})^2 P_S({\bm x}) d{\bm x} \\
 & = \frac{1}{2} \int \hat{w}({\bm x})^2 P_S({\bm x})  d{\bm x}
	- \int \hat{w}({\bm x}) P_T({\bm x}) d{\bm x}
  + \frac{1}{2} \int w({\bm x})^2 P_S({\bm x}) d{\bm x} 
\end{align*}

3項目の式は定数なので，$J_0({\bm \alpha})$を最小にするには，
以下の$J({\bm \alpha})$を最小にすればよい．
\[
J({\bm \alpha}) =  \frac{1}{2} \int \hat{w}({\bm x})^2 P_S({\bm x})  d{\bm x}
  - \int \hat{w}({\bm x}) P_T({\bm x}) d{\bm x} 
\]

$J({\bm \alpha})$を経験分布で近似した$\widehat{J}({\bm \alpha})$は
以下となる．
\begin{equation}
\begin{aligned}[b]
\widehat{J}({\bm \alpha}) & = \frac{1}{2 N_s} \sum_{i=1}^{N_s} \widehat{w}({\bm x_i^s})^2 
  -  \frac{1}{N_t} \sum_{j=1}^{N_t} \widehat{w}({\bm x_j^t}) \\
 & = \frac{1}{2} \sum_{l,l'=1}^b \alpha_l \alpha_{l'} 
	 \left( \frac{1}{N_s} \sum_{i=1}^{N_s} \psi_l({\bm x_i^s}) \psi_{l'}({\bm x_i^s}) \right) 
	 - \sum_{l=1}^b  \alpha_l \left( \frac{1}{N_t} \sum_{j=1}^{N_t} \psi_l({\bm x_j^t})  \right) \\
 & =  \frac{1}{2} {\bm \alpha}^T \widehat{H} {\bm \alpha} - \widehat{h}^T {\bm \alpha}
\end{aligned}
\label{jhatalpha}
\end{equation}

ここで$\widehat{H}$は$b \times b$の行列であり，その$l$行$l'$列の要素
$\widehat{H}_{l,l'}$は以下である．
\[
\widehat{H}_{l,l'} = \frac{1}{N_s} \sum_{i=1}^{N_s}  \psi_l({\bm x_i^s}) \psi_{l'}({\bm x_i^s}) 
\]
また$\widehat{h}$は$b$次元のベクトルであり，その$l$次元目の要素
$\widehat{h}_l$は以下である．
\[
\widehat{h}_l = \frac{1}{N_t} \sum_{j=1}^{N_t} \psi_l({\bm x_j^t}) 
\]

$\widehat{J}({\bm \alpha})$の最小値を求める際に正則化を行う．このとき
付加する正則化項を L2 ノルムに設定し，${\bm \alpha} > 0$の条件を外して，
以下の最小化問題を解く．ここでパラメータ$\lambda$が導入されることに
注意する．$\lambda$は基底関数を設定する際に決められる．
\[
\min_{{\bm \alpha}} \left[ \frac{1}{2} {\bm \alpha}^T \widehat{H} {\bm \alpha}
-\widehat{h}^T {\bm \alpha} + \frac{\lambda}{2} {\bm \alpha}^T {\bm \alpha}
\right]
\]
この最小化問題は制約のない凸2次計画問題であるために，唯一の大域解が得られる．
その解は以下である．
\begin{equation}
    \label{eq:alp-kai1}
\tilde{{\bm \alpha}} = (\widehat{H} + \lambda I_b)^{-1} \widehat{h}^T    
\end{equation}
最後に${\bm \alpha} > 0$の条件に合わせるように，以下の調整を行う．
\begin{equation}
\begin{aligned}[b]
 \widehat{{\bm \alpha}} & = \left( (\max(0,\tilde{\alpha_1}),\max(0,\tilde{\alpha_2}), 
	\cdots, \max(0,\tilde{\alpha_b})            \right) \\
  & = \max( 0_b, \tilde{{\bm \alpha}})
\end{aligned}
\label{eq:alp-kai2}
\end{equation}

パラメータ$b$と基底関数の設定であるが，まず，$b$については以下で設定する
\footnote{本実験では$b$の値は最大 100 となるが，この 100 という数値は
オリジナルの論文\cite{kanamori2009least}で使われた値であり，
本論文でのなんらかの予備実験から得た値ではない．
uLSIF の実験結果はこの値を調整することで多少の向上があったかもしれない．}．
\[
b = \min (100, N_t)
\]
次にターゲット領域のデータから重複を許さずに$b$個の点をランダムに取り出す．
それらの点を$\{ {\bm x_j^t} \}_{j=1}^b$ とおく．
そして基底関数$\psi_l ({\bm x})$を以下のガウシアンカーネルで定義する．
\[
\psi_l ({\bm x}) = K({\bm x},{\bm x_l^t}) = \exp \left( - \frac{|| {\bm x} - {\bm x_l^t} ||^2}{\sigma^2} \right)
\]

以上より，確率密度比を求めるために残されているパラメータは正則化項の係数$\lambda$と
ガウシアンカーネルの幅$\sigma$の2つである．これらのパラメータはグリッドサーチの交差検定で求める．
まずソース領域のデータとターゲット領域のデータをそれぞれ交わりのない$R$個の
部分集合に分割する．それらの部分集合の中で$r$番目の部分集合を除き，残りを結合した
集合を作る．それらを新たなソース領域のデータとターゲット領域のデータと見なす．
そして$\lambda$と$\sigma$をある値に設定し，\mbox{式(\ref{eq:alp-kai1})}と
\mbox{式(\ref{eq:alp-kai2})}より${\bm \alpha}$を求め，
\mbox{式(\ref{jhatalpha})}より$\widehat{J}({\bm \alpha})^{(r)}$の値を求める．$r$を 1 から$R$まで
変化させることで，$R$個の$\widehat{J}({\bm \alpha})^{(r)}$の値が求まり，
それらを平均した値を$\lambda$と$\sigma$に対する
$\widehat{J}({\bm \alpha})$の値とする．次に$\lambda$と$\sigma$を変化させ，
上記手順で得られる$\widehat{J}({\bm \alpha})$の値が最小となる
$\hat{\lambda}$と$\hat{\sigma}$を求め，これを
$\lambda$と$\sigma$の推定値とする．


\subsection{$P_S({\bm x})$の補正による確率密度比の算出}

WSD のタスクでは NB 法あるいは uLSIF で算出される確率密度比は小さい値を取る傾向があり，
実際の学習で用いる際には，少し上方に修正した値を取る方が
最終の識別結果が改善されることが多い．
これは以下の2点から生じていると考えられる．
\begin{itemize}
      \item $T$に${\bm x}$が入っているかは確率的であるが，$S$には必ず${\bm x}$が入っている．
      \item $P_S({\bm x})$を推定するために${\bm x} \in S$を用いるため，訓練データである
${\bm x}$に過学習した結果$P_S({\bm x})$は $P_T({\bm x})$に比べて高く見積もられてしまう．
\end{itemize}

このため，求まった確率密度比を上方に修正する手法が存在する．
論文\cite{sugiyama-2006-09-05}では確率密度比$w({\bm x})$を$p$乗($0 < p < 1$)することを
提案している．また論文\cite{yamada2011relative}では以下で示される相対確率密度比$w'({\bm x})$を
確率密度比として利用することを提案している．
\[
w'({\bm x}) = \frac{P_T({\bm x})}{\alpha P_S({\bm x}) + (1-\alpha) P_T({\bm x})}
\]
ここで$0 < \alpha < 1$である．

確率密度比$w({\bm x})$が 1 以下である場合，
$w({\bm x})$を$p$乗すると上方に修正できることは，それらの比の対数を取れば，
\mbox{$\log w({\bm x}) < 0$}であることから明らかである．
\[
\log \frac{w({\bm x})^p}{w({\bm x})} = (p - 1) \log w({\bm x}) > 0
\]
また相対確率密度比$w'({\bm x})$は以下の変形から
$w({\bm x})$を上方に修正していると見なせる．
\begin{align*}
w'({\bm x}) & = \frac{P_T({\bm x})}{\alpha P_S({\bm x}) + (1-\alpha) P_T({\bm x})}    \\
             & = \frac{1}{\alpha  + (1-\alpha) w({\bm x})} w({\bm x})\\
             & > \frac{1}{\alpha  + (1-\alpha)} w({\bm x})\\
             & = w({\bm x})
\end{align*}

確率密度比が 1 以上である場合，これらの手法は確率密度比を下方に修正するので，
正確には確率密度比を 1 に近づける手法である．しかし，ほとんどの訓練データの確率密度比は
1 以下であるために，ここではこれらの手法を上方修正する手法と呼び，提案手法と対比させる．

本論文では確率密度比を上方に修正するために，
ソース領域のデータとターゲット領域のデータを合わせたデータを新たにソース領域のデータとみなし，
NB 法を用いて$P_S({\bm x})$を補正することを提案する．
これは$S$のスパース性を緩和させることを狙ったものである．
確率密度比が真の値よりも低く見積もられる原因の 1 つは，
$P_S({\bm x})$が真の値よりも高く見積もられるからだと考える．
さらにその原因が$S$のスパース性なので，スパース性を緩和するために
$S$にデータを追加するというアイデアである．ただし追加するデータは$S$
と類似の領域のデータであることが望ましい．
WSD の領域適応の場合，$S$と$T$は完全に異なることはなく，
比較的似ているために，追加するデータとして$T$のデータが利用できると考えた．

提案手法の新たなソース領域を$S+T$で表せば，
$P_S ({\bm x}) >  P_{S+T} ({\bm x})$が成立していると考えるのは自然であり，
この不等式が成立していれば，提案手法により確率密度比は上方に修正される．
ただし，ここで提案手法は必ずしも NB 法の確率密度比を上方に修正できるとは限らないことに注意する．
また提案手法は NB 法の確率密度比が 1 以下かどうかには無関係であることにも注意する．
NB 法の確率密度比が 1 以上であっても，上方に修正する可能性がある．
また$P_{S+T} ({\bm x})$は以下の式を利用して求められる．
\begin{align*}
P_{S+T} (f) & = \frac{n(S+T,f) + 1}{N(S+T) + 2}  \\
           & = \frac{n(S,f) +n(T,f) + 1}{N(S) + N(T) + 2}
 \end{align*}


\section{考察}

\subsection{確率密度比を上方修正しないケース}

「$p$乗する」あるいは「相対確率密度比を取る」という手法は，
元の確率密度比が 1 以下である全てのデータに対
してその値を
上方に修正するが，
提案手法は一部のデータに対しては
NB 法の確率密度比が 1 以下であっても，それらを
上方に修正できない．
提案手法により確率密度比の値が大きくならず，逆に小さくなったデータの個数を\mbox{表\ref{tab:down}}に示す．

\begin{table}[b]
\caption{上方修正できなかったデータの個数}
\label{tab:down}
\input{02table04.txt}
\end{table}

ほとんどのデータに対して，その確率密度比を上方に修正しているが，修正できていないデータが極端に多い
ケースも存在する．例えば，PB→PN に関しては「言う」「自分」「見る」「やる」「ゆく」，
OC→PN に関しては「書く」「見る」「やる」「ゆく」である．
これらに関してのみ Base と NB と提案手法の正解率の比較を\mbox{表\ref{tab:down-pre}}に示す．

\mbox{表\ref{tab:down-pre}}からわかるように，上方修正ができないデータが多くなると，
提案手法は NB 法よりも正解率が下がっている．
ただし，下方に修正した場合には必ず正解率が下がるとも言えないことに注意したい．
例えば，確率密度比の値を下げないようにするには提案手法を修正し，
「 NB 法の値を上方に修正できなければ，NB 法の値をそのまま使う」という形にすれば良い．
この修正案の手法も試した結果を\mbox{表\ref{tab:resultsyuusei}}に示す．
修正案の手法の平均正解率は，提案手法よりも若干悪かった．

\begin{table}[t]
\caption{上方修正できなかったデータの正解率(\%)}
\label{tab:down-pre}
\input{02table05.txt}
\end{table}
\begin{table}[t]
\caption{修正版提案手法の平均正解率(\%)}
\label{tab:resultsyuusei}
\input{02table06.txt}
\end{table}

上記の実験は NB 法による確率密度比が 1 以下かどうかは考慮していない．
「p 乗する」や「相対確率密度比を取る」手法では，確率密度比が 1 以上の場合に，
その値を逆に小さくしている．確率密度比が 1 以上の場合に，上方修正する方がよいのか
下方修正する方がよいのかは未解決である．参考として上記の修正案の手法
を更に修正し，「 NB 法の値が 1 以上の場合，あるいは NB 法の値を上方に修正できな場合には
NB 法の値をそのまま使う」という形の実験も行った．結果，平均正解率は 72.14 と
若干改善はされたが，提案手法よりも若干悪いことに変化はなかった．

データの確率密度比（重み）はその値の大きさが重要ではなく，他データとの重みとの関係が本質的である．
例えば全てのデータの重みを 10倍して，値自体を増やしても，推定できるパラメータが
変化しないのは，重み付き対数尤度（\mbox{式\ref{eq:2}}）の最大化する部分が
変化しないことから明らかである．

データの重みは
タスクの背景知識から，その重要度を
設定していくか，
そのデータを数値化した後に
確率密度比という観点から設定していくしか方法はないと考える．
提案手法は後者であり，コーパスのスパース性への対処から NB 法を改良した手法と考えている．
上方修正することに，どのような意味があるかを調べることは今後の課題である．


\subsection{提案手法の重みの上方修正}

\begin{figure}[b]
\begin{center}
\includegraphics{21-5ia2f1.eps}
\end{center}
\caption{$p$ 乗による提案手法値の上方修正}\label{zu1}
\end{figure} 
\begin{figure}[tb]
\begin{center}
\includegraphics{21-5ia2f2.eps}
\end{center}
\caption{相対確率密度比による提案手法値の上方修正}\label{zu2}
\end{figure} 

提案手法は，確率密度比を上方修正する手法と組み合わせて利用することで更なる精度改善も可能である．
提案手法の確率密度比を$p$乗した場合の平均正解率の変化を\mbox{図\ref{zu1}}に示す．
$p = 0.6$のとき最大値 72.54\% をとった．
また提案手法の確率密度比に対してパラメータ$\alpha$の相対確率密度比をとった場合の
平均正解率の変化を\mbox{図\ref{zu2}}に示す．
$\alpha = 0.6$のとき最大値 72.30\% をとった．
ともに確率密度比を上方修正することで平均正解率は改善されている．

本論文の以降の記述において，
提案手法の重みを$p$乗した値を重みにする手法を「P-提案手法」，
提案手法の重みを相対確率密度比により上方修正した値を重みにする手法を「A-提案手法」と名付ける．
ここで$p = 0.6$，$\alpha = 0.6$である．

また前節で行った有意差の検定を「P-提案手法」と「A-提案手法」に対しても行った．
結果，「P-提案手法」は P-NB や提案手法を含む全ての手法に対して有意に優れていた．
ただし「A-提案手法」は P-NB や提案手法とに有意な差はなかった．


\subsection{Misleading データからの評価}

本論文で提案した確率密度比（重み）は NB 法や uLSIF による確率密度比よりも，有効に機能していた．
ただし真の確率密度比の値は未知であるために，真の値に近いかどうかという観点での評価は
不可能である．また重みの設定だけで，どの程度まで平均正解率が向上できるのかも未知である．
一方，Misleading データを削除してから学習を行うことでかなりの精度向上が可能であることが
論文\cite{yoshida}により示されている．Misleading データを削除してから学習することは，
Misleading データの重みを 0，
それ以外のデータの重みを 1 とした重み付き学習と見なせる．
この重み付けが真の確率密度比と類似しているかどうかは不明だが，
Misleading データに対してはできるだけ小さな重みを与える手法が優れているとみなせる．
そこでここでは各手法において Misleading データに付与された重みを調べることで手法を評価する．

まず論文\cite{yoshida}で行ったように，しらみつぶしに Misleading を見つけ出す．
領域$S$から領域$T$の領域適応において，対象単語$w$の
$S$上のラベル付きデータ$D$が存在する．
まず$D$で学習した識別器の$T$に対する正解率$p_0$を測る．
次に$D$から 1 つデータ$x$を取り除き，$D - \{x\}$
から学習した識別器の$T$に対する正解率$p_1$を測る．
$p_1 > p_0$となった場合，データ$x$を Misleading データと見なす．
これを$D$内のすべてのデータに対して行い，
$S$から$T$の領域適応における対象単語$w$の Misleading データを見つける．
この処理によって見つけ出された Misleading データの個数を\mbox{表\ref{tab:mislead}}示す．
括弧内の数値は全データ数である．

また Misleading による重みを用いた学習の
識別結果を\mbox{表\ref{tab:resultmis}}に示す．表中の Mislead がそれにあたる．
本論文の実験で得られている平均正解率よりもかなり高い．
つまり重みの設定のみでも Base の平均正解率 71.71\% を少なくとも 75.42\% まで改善可能である．

\begin{table}[t]
\caption{Misleading データの個数}
\label{tab:mislead}
\input{02table07.txt}
\end{table}
\normalsize
\begin{table}[t]
\caption{Misleading による重みを用いた学習の平均正解率(\%)}
\label{tab:resultmis}
\input{02table08.txt}
\end{table}

次に各手法が Misleading データに付与した重みにより手法を評価する．
領域$S$から領域$T$の領域適応において，対象単語$w$の
$S$上のラベル付きデータを$D = \{ x_i \}_{i=1}^{N_w}$とする．
まず$D$内のデータの重みの平均値$m_w$を調べる．
\[
m_w = \frac{1}{N_w} \sum_{i=1}^{N_w} w(x_i)
\]
次に$D$内の Misleading データを
$\{ x'_j \}_{j=1}^{M_w}$とする．各$x'_j$の重み$w(x'_j)$が$m_w$と比較して
小さな値であればよいので，対象単語$w$に関する Misleading データを用いた評価値$d_w$を以下で測る．
\[
d_w = \frac{1}{M_w} \sum_{j=1}^{M_w} \frac{w(x'_j)}{m_w}
\]
$d_w$は対象単語$w$の訓練データの重みの平均値$m_w$に対して，
Misleading データ$x'_j$の重み$w(x'_j)$の比を取り，
その比の平均を取ったものである．このため$d_w$の値が小さいほど，
適切に重み付けできていると考えられる．
そして
$d_w$の各単語に関して平均を取った値を，
その手法における$S$から$T$の Misleading  データを用いた評価値（小さいほど良い）とする．
これをまとめたものが\mbox{表\ref{tab:miseval}}である．
\mbox{表\ref{tab:miseval}}が示すように，
Misleading  データを用いた評価では，NB法，uLSIF 及び提案手法の3つの中で uLSIF が最も優れている．
ただし提案手法は NB法よりも優れていた．
更に全ての手法において「$p$乗する」，あるいは「相対確率密度比を取る」ことで評価値は改善されており，
重みを上方修正する効果があることがわかる．
また「$p$乗する」と「相対確率密度比を取る」を比較すると，「$p$乗する」方が効果が
あることもわかる．

\begin{table}[t]
\caption{Misleading データからの評価値}
\label{tab:miseval}
\input{02table09.txt}
\end{table}



\subsection{負の転移の有無}

NB法や uLSIF は Base よりも平均正解率が低い．これは確率密度比からの重み付き学習が効果が
なかったことを示している．この原因として，WSD の領域適応では，領域の変化はあるが，
実際には領域適応の問題が生じていない，つまり負の転移\cite{rosenstein2005transfer}が生じていない対象単語が
かなり存在するからだと考える．負の転移が生じていなければ，訓練データを全て利用して
学習する方が有利であることは明らかであり，重みをつけると逆効果になると考えられる．

この点を確認するために，負の転移が生じているものと生じていないものに分けて，
各手法の平均正解率を測ってみる．
まず負の転移が生じている単語の判定であるが，これは\mbox{表\ref{tab:mislead}}で
示した Misleading データの個数から行う．ここでは Misleading データが全データの 1割以下の場合，
負の転移が生じないと判定した．結果を\mbox{表\ref{tab:mislead2}}に示す．
チェックが付いているものが「負の転移が生じない」と判定したものである．

\mbox{表\ref{tab:mislead2}}でチェックがついていない対象単語に限定して，
各手法の平均正解率を測った結果が\mbox{表\ref{tab:del-fu-kekka}}である．
また逆に\mbox{表\ref{tab:mislead2}}でチェックがついている対象単語に限定して，
各手法の平均正解率を測った結果が\mbox{表\ref{tab:del-fu-kekka2}}である．

\begin{table}[t]
\caption{負の転移が生じない単語}
\label{tab:mislead2}
\input{02table10.txt}
\end{table}
\begin{table}[t]
\caption{負の転移が生じる単語に限定した平均正解率(\%)}
\label{tab:del-fu-kekka}
\input{02table11.txt}
\end{table}

\mbox{表\ref{tab:del-fu-kekka}}と\mbox{表\ref{tab:del-fu-kekka2}}からわかるように，
NB 法や uLSIF は負の転移が生じる，生じないに関わらず，Base よりも平均正解率が低く，
本実験においては有効ではなかった．一方，提案手法は負の転移が生じる場合でも，生じない場合でも
Base よりも平均正解率が高く，どちらの場合でも有効であることがわかる．

また負の転移が生じる場合，提案手法の平均正解率は NB 法の平均正解率の 1.09 倍であり，
uLSIF の平均正解率 1.05 倍である．
一方，負の転移が生じない場合，
提案手法の平均正解率は NB 法の平均正解率の 1.02 倍であり，
uLSIF の平均正解率 1.03 倍である．
つまり負の転移が生じるケースで提案手法と既存手法（NB法，uLSIF）との差が大きくなる．

更に確率密度比を上方修正する効果をみてみる．負の転移が生じる場合，NB 法は 
平均正解率 60.69\% が$p$乗することで  65.19\%，相対確率密度比を取ることで 65.35\% まで向上しているので，
平均的には 7.5\% 平均正解率が向上している
\footnote{$((65.19 + 65.35)/2)/60.69 \approx 1.075$ から算出した．}．
同様に計算して uLSIF の平均正解率は 3.6\%，提案手法の平均正解率は 0.5\% 向上している．
負の転移が生じない場合，NB 法は 1.4\%，uLSIF は 2.8\% 平均正解率が向上している．
また提案手法では平均正解率はほとんど変化しない．
つまり確率密度比を上方修正する効果は負の転移が生じるケースで顕著になっている．

\begin{table}[t]
\caption{負の転移が生じない単語に限定した平均正解率(\%)}
\label{tab:del-fu-kekka2}
\input{02table12.txt}
\end{table}

今後の課題としては Misleading データの検出方法を考案することである．
Misleading データを検出し，そのデータに重みを 0 にすることは
かなりの精度向上が期待できる．また Misleading データの割合から負の転移の有無を判定し，
負の転移が生じる問題にだけ，重み付け学習手法を適用するアプローチも効果があると考えられる．


\subsection{トピックモデルの利用}

論文\cite{shinnou-gengo-13}は本論文と同じタスクに対して一部同じデータを用いた
実験結果を示している．ここではそこでの実験結果の値と本論文の実験結果の
値を比較し，手法間の違いを考察する．

論文\cite{shinnou-gengo-13}の核となるアイデアは，ターゲット領域$T$の
トピックモデルを作成し，ターゲット領域に特有のシソーラスを構築することである．
このシソーラスの情報を素性として組み込むことで，識別精度を上げることを狙っている．
実験は OC → PB と
\mbox{PB → OC}の2方向である． また対象単語は
本論文の 16 単語の他「来る」が含まれている
\footnote{本論文では「来る」は PN の領域において曖昧性がないため対象単語から外した．}．

\begin{table}[p]
\caption{正解率(\%)の比較 (OC → PB)}
\label{tm-hikaku1}
\input{02table13.txt}
\end{table}
\begin{table}[p]
\caption{正解率(\%)の比較 (PB → OC)}
\label{tm-hikaku2}
\input{02table14.txt}
\end{table}

OC → PB と PB → OC の領域適応における，本論文の対象単語 16 単語についての識別精度の
比較を\mbox{表\ref{tm-hikaku1}}と\mbox{表\ref{tm-hikaku2}}に示す．
なお表中の SVM-TM-kNN は論文\cite{shinnou-gengo-13}の手法を意味する．

対象単語に応じて最も高い正解率の手法は異なるが，平均的には SVM-TM-kNN が最も高い正解率を
示している．ただし  SVM-TM-kNN はトピックモデルを構築するために，ターゲット領域のコーパスを
利用していることに注意したい．本論文の提案手法はターゲット領域の対象単語の
用例を用いているが，コーパスは利用していない．つまり利用しているリソースが異なるために，
単純に SVM-TM-kNN が提案手法よりも優れているとは結論できない．

また SVM-TM-kNN におけるトピックモデルは素性構築の際に利用されているだけであり，
提案手法と競合するものではない．つまり SVM-TM-kNN の手法を利用して，
WSD での素性を構築し，それに対して本論文の提案手法を適用することも可能である．
今後はこの方向での改良も試みたい．


\end{document}
