表１は品詞をタグづけされたタイ語コーパスの例を示す.
記号`@'や`/'で区分されている記号列（例えばNCMNやPPRS）はその前にある単語が持ち得る品詞を表し,記号`@'の直後の記号列はその文において唯一に決まった品詞を表している.
本稿で用いるタイ語の単語カテゴリの分類法に47種類の品詞が定義されている[CITE].
入力されるタイ語テキストは電子辞書を用いて単語に分割され,各単語の持ち得る品詞もリストアップされるため,品詞のタグづけ問題は以下に示すような文脈を用いた品詞の曖昧性除去あるいは一種のクラス分け問題と見なせる.
ここで, [MATH] ([MATH], [MATH], [MATH])を入力[MATH]の構成部分と呼ぶ.
具体的には, [MATH]は目標単語の取りうる品詞に関するもの, [MATH]と[MATH]はそれぞれ目標単語から左へ[MATH]番目と右へ[MATH]番目の単語の取りうる品詞（文脈）に関するもの,そして, [MATH]は目標単語がその文脈で取る正しい品詞を表すものである.
インフォメーションゲイン（IG）は,特徴ベクトルで定義されるデータセットの情報量がある特定の特徴の値を知ることによってどれだけ増えるかを表す量である[CITE].
より具体的に言えば,ある特徴のIGとはその特徴がデータのクラス同定にどれだけ重要かを反映する量である.
ここで,特徴を入力の構成部分,特徴の値をその構成部分の取りうる品詞,データの属するクラスを目標単語の取りうる品詞にそれぞれ置き換えてやれば,各構成部分のIGはその構成部分の品詞タグづけへの影響度として考えることができる.
従って, (1)における入力の各構成部分[MATH] ([MATH])はそれぞれタグづけへの影響度に応じた重み[MATH]を持つと仮定すれば,その重みは以下のように求められる.
ここで訓練データセットを[MATH], [MATH]番目のクラス,あるいは[MATH]番目の品詞（[MATH],但し, [MATH]は品詞の数）を[MATH]で表す.
セット[MATH]のエントロピー,即ち, [MATH]の中の一つのデータのクラス（品詞）を同定するのに必要とされる情報の平均量は
である.
但し, [MATH]は[MATH]の中のデータの数, [MATH]はそのうちクラス[MATH]に属するデータの数である.
セット[MATH]が構成部分[MATH]の持ちうる品詞によって[MATH]個のサブセット[MATH] ([MATH])に分割されたとき,新しいエントロピーはこれらのサブセットのエントロピーの重みつき総和で求められる.
即ち,
この分割（即ち,構成部分[MATH]の品詞を知ること）による情報の増益（IG）は以下になる.
従って,構成部分[MATH]のタグづけへの影響度に応じた重みは以下のように設定できる.

図１シングルニューロタガー(SNT)
図1は固定長さの文脈を用いて品詞タグづけをするニューラルネット（シングルニューロタガー,略してSNTと呼ぶ）を示す.
単語[MATH]が入力の位置[MATH] ([MATH], [MATH], or [MATH])に与えられた時,入力[MATH]の構成部分[MATH]は以下のように重み付けされたパターンで定義される.
但し, [MATH]は(5)で求められた重み, [MATH]はタイ語に定義された品詞の数, [MATH] ([MATH])である.
もし単語[MATH]が既知のもの,即ち,訓練データに出現するならば,各ビット[MATH]は以下のように得られる.
ここで, [MATH]は単語[MATH]の品詞が[MATH]である確率で,訓練データから以下のように推定される.
ここで, [MATH]は全訓練データを通じ, [MATH]が品詞[MATH]を取る回数で, [MATH]は[MATH]が出現する回数である.
一方,もし単語[MATH]が未知のもの,即ち,訓練データに出現しないならば,各ビット[MATH]は以下のように得られる.
ここで, [MATH]は単語[MATH]が持ちうる品詞の数である.
出力[MATH]は以下のように定義されるパターンである.
[MATH]はデコードされ,目標単語の品詞として最終結果[MATH]が得られる：
文の各単語を左から右へ順にタグづけしていくとき,左側の単語はつねにタグづけ済みと考えられるため,それらの単語に関する入力を構成するとき,より多くの情報が活用できる.
具体的には, (6)-(9)を用いる代わりに,入力は次のように構成される.
ここで, [MATH]は目標単語の文における位置であり, [MATH] for [MATH].
しかしながら,訓練過程においてはタガーの出力はまだ正確ではないため,それらを直接入力にフィードバックして使うことができない.
そのために,訓練過程における入力は以下のように実際の出力と目標出力の重みづき平均を用いて構成する.
ここで, [MATH]は目標出力で, [MATH]と[MATH]はそれぞれ次のように定義される.
ここで, [MATH]と[MATH]はそれぞれ目標誤差と実際の誤差を表す（それらの詳細は4.3節で述べる）.
従って,訓練の始めの入力構成では目標出力の比重が大きく,時間が立つにつれゼロへ減っていく.
逆に,実際の出力の比重は最初小さく,時間が立つにつれて大きくなっていく.
図２に示すように,マルチニューロタガーはエンコーダー/デコーダー,複数のシングルニューロタガーSNT[MATH] ([MATH]),そして最大文脈優先セレクターで構成される.
SNT[MATH]は入力[MATH]を持つ.
入力[MATH]の長さ（即ち,構成部分の数: [MATH]）[MATH]は次の関係を持つ：[MATH] for [MATH].
図２マルチニューロタガー
目標単語[MATH]を中心とした,最大長さ[MATH]の単語列([MATH], [MATH], [MATH], [MATH], [MATH], [MATH])がマルチニューロタガーに与えられた時,それぞれ同じく単語[MATH]を中心とした長さ[MATH]の部分単語列が前節に述べた方法で[MATH]に符号化され,個々のシングルニューロタガーSNT[MATH] ([MATH])に入力される.
それらの入力に対し,個々のSNT[MATH]はそれぞれ独立に品詞タグづけを行ない,出力[MATH]を得る.
出力[MATH]は前節に述べた方法で[MATH]にデコードされる.
[MATH]は更に最大文脈優先セレクターに入力され,最終結果は次のように得られる.
この式は,タグづけの最終結果はできるだけ長い文脈で得られた出力を優先的に用いることを意味する.
図１に示すように,シングルニューロタガーは三層パーセプトロン（詳細は[CITE]を参照）で構成される.
三層パーセプトロンは誤差逆伝播学習アルゴリズム[CITE]を用いて品詞のタグづけ済みの訓練用データを学習することによって品詞タグづけ能力を学習できる.
訓練段階においては,各訓練データーのペア[MATH]は順番にネットワークに与えられる.
但し,上つき記号[MATH]はデータの番号を表し, [MATH] = [MATH], [MATH]は訓練データの数である.
[MATH]番目の訓練データ[MATH]と[MATH]は次のようなパターンとされる.
但し, [MATH]である.
[MATH]の各ビット[MATH]は(6)-(9)或は(12)-(15)を用いて得られる.
[MATH]の各ビット[MATH]は次のように与えられる.
訓練は全訓練データに対し平均出力誤差が目標値以下になるまで繰り返して行なわれる.
ここで[MATH]回目の繰り返し訓練において[MATH]番目の訓練データのペアが提示されたとする.
その時,ネットワークは,入力層に与えられた入力パターン[MATH]を以下の(20)-(24)を用いて出力層へ前向きに伝播しながら変換する.
入力層はまず以下のようにセットされる.
但し, [MATH], [MATH]は入力層のユニット[MATH]の出力, [MATH]はバイアスユニットを表す.
ここで,次の層（中間層或は出力層）のユニット[MATH]の出力は次のように得られる.
但し, [MATH]はユニット[MATH]の内部活動度と呼ばれるもので次のように得られる.
ここで, [MATH]と[MATH]はそれぞれ前の層のユニット[MATH]の出力とユニットの総数である（入力層においては[MATH]）.
また, [MATH]は次のように定義される.
出力層のユニット[MATH]の出力には別な記号[MATH]を用いる,即ち,
出力[MATH]が得られた後,その出力と目標出力間の二乗誤差[MATH]は次のように計算される.
そこで,その誤差を出力から入力へ逆伝播して誤差を減らすようにネットワークの重みを次のように修正する.
但し, [MATH]は重みの更新量を決める学習率で, [MATH]は慣性率である.
[MATH]は最急降下法で次のように計算される.
このように(20)-(27)を通じて[MATH]回目の繰り返し訓練においての[MATH]番目のデータの処理が終る.
訓練は,下の条件が満足されるまで,即ち,各訓練データと各出力ユニットに対する平均誤差[MATH]が目標誤差[MATH]以下になるまで,全訓練データを通じて繰り返して行なわれる：
品詞タグづけ段階においては,入力[MATH]が与えられた時,ネットワークはその入力パターンを(20)-(24)を用いて入力層から出力層へ前向きに伝播しながら変換する.
ここで[MATH]は１にセットされ,上つき記号[MATH]が取り除かれる.
最終的に,出力[MATH]が(24)の代わりに次のように得られる.
ここで[MATH]は出力の閾値で, [MATH]は以下のように定義される.
品詞タグづけにニューラルネットモデルを用いる主な欠点は訓練コストが高い（即ち,訓練に時間がかかる）ことである.
この欠点は複数のニューラルネットの導入によって更に強調されてしまう.
しかしながら,実際,もし短い入力のSNT[MATH]の訓練結果（訓練で獲得した重み）を長い入力のSNT[MATH]（[MATH]）にコピーして初期値として使えば, SNT[MATH]（[MATH]）の訓練時間を大幅に短縮できる.
従って,この方法を用いればマルチニューロタガーをシングルニューロタガーとほとんど変わらないコストで訓練することができる.
図３にSNT[MATH]（入力の長さ3）とSNT[MATH]（入力の長さ4）の場合の例を示す.
この図では実線部分でSNT[MATH]を示し,点線部分を含む全体でSNT[MATH]を示している.
図に示しているように, SNT[MATH]が訓練された後,その重み[MATH]と[MATH]はSNT[MATH]の対応するところにコピーされ, SNT[MATH]の初期値として使われている.
図3シングルニューロタガーSNT[MATH]の訓練
例えば品詞が５０種類ある言語を左右それぞれ三つの単語の情報を文脈としてタグづけを行なう場合, n-gramベースの確率モデルは[MATH]個のn-gram（パラメータ）を推定しなければならない.
それに対し,例えば中間層のユニット数が入力層の半分であるような三層パーセプトロンを用いたニューロタガーの場合,必要とされるパラメータ（ユニット間の結合）の数は僅か[MATH] [MATH] [MATH]である.
ここで, [MATH], [MATH],と[MATH]はそれぞれ入力層,中間層,及び出力層のユニットの数で, [MATH]である.
一般的に,システムに必要とされるパラメータの数が少なければ,それらを正しく同定するのに必要な訓練データの数も少なくてよい.
そのために,ニューラルネットモデルのタグづけ性能は確率モデルのそれに比べ訓練データの数の少なさに影響されにくい[CITE].
また,他モデルに比べ,ニューラルネットモデルは訓練時間がかかる一方,タグづけ速度が非常に速いことも特徴の一つである.
