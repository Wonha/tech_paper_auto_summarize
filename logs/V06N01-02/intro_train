0 一つ一つ の 単語 は しばしば 複数 の 品詞 即ち 品詞 の 曖昧性 を 持ち 得る 
0 しかしながら その 単語 が 一旦 文 に 組み込ま れ ば 持ち 得る 品詞 は その 前後 の 品詞 によって 唯一 に 決まる 場合 が 多い 
0 品詞 の タグ づけ は この よう な 曖昧性 を 文脈 を 用いる こと によって 除去 する こと で ある 
0 品詞タグ づけ の 研究 は 特に 英語 や 日本語 など において 多数 行なわ れ て き た 
0 これら の 研究 を 通じ これ まで 主 に 四つ の アプローチ 即ち ルールベース による ものCITEHMM や ngram を 用い た 確率モデル に 基づい た ものCITEメモリベース の ものCITE そして ニューラルネット を 用い た ものCITE が 提案 さ れ た 
0 これら の 研究 で は 大量 の 訓練データ 例えば CITE において は 1000000個 の データ を 用いれ ば その いずれ の 手法 を 用い て も 未 訓練データ へ の タグ づけ を 95以上 の 正解率 で 行 なえる こと を 示し た 
0 しかしながら 実際 英語 や 日本語 など を 除い た 数多く の 言語 例えば 本稿 で 取り上げ た タイ語 に関して は コーパス自体 も まだ 整備段階 に ある の が 現状 で 予め 大量 の 訓練データ を 得る の が 困難 で ある 
0 従って これら の 言語 にとって は 如何 に 少ない 訓練データ で 十分実用的 で 高い 正解率 の 品詞タグ づけ システム を 構築 する か が 重要 な 課題 と なる 
0 これ まで 提案 さ れ た 確率モデル や ニューラルネットモデル の ほとんど は タグ づけ に 長 さ が 固定 の 文脈 を 用いる もの で あり HMMモデル において も 状態遷移 を 定義 する の に 固定 さ れ た ngramベース の モデル を 用いる 入力 の 各 構成部分 は 同一 の 影響度 を 持つ もの と さ れ て い た 
0 しかし 訓練データ が 少ない 場合タグ づけ 結果 の 確信度 を 高める ため に まず できるだけ 長い 文脈 を 用い 訓練データ の 不足 から 確定的 な 答え が 出 ない 場合 に 順次 文脈 を 短く する といった よう に フレキシブル に タグ づけ する こと が 必要 と さ れよ う 
0 そして 客観的 な 基準 で 入力 の 各 構成部分 の 品詞タグ づけ へ の 影響度 を 計り その 影響度 に 応じ た 重み を それぞれ の 構成部分 に 与えれ ば より 望ましい で あろ う 
0 そこで シンプル で 効果的 と 思わ れる 解決法 は マルチモジュールモデル を 導入 する こと で ある 
1 マルチモジュールモデル と は 複数 の それぞれ 異なっ た 長 さ の 文脈 を 入力 と し た モジュール と それら の 出力 を 選別 する セレクター から 構成 さ れる システム の こと で ある 
0 しかし この よう な システム を 例えば 確率モデル や メモリベースモデル で 実現 しよ う と する と それぞれ以下 に 述べる 不具合 が 生じる 
0 確率モデル は 比較的 短い 文脈 を 用いる 場合 に は 必要 と さ れる パラメター の 数 は それほど 多く なら ない 
0 しかし ここ で 提案 し て いる よう な 複数 の モジュール を 場合 に 応じ て 使い分ける よう な システム で は ある程度 の 長 さ の 文脈 を 用いる こと が 必要 と なり 確率モデル の パラメター の 数 が 膨大 に なる 
0 例えば 品詞 が 50種類 ある 言語 を 左右最大三つ の 単語 の 情報 を 文脈 として タグ づけ を 行なう 場合 その 最長文脈 を 入力 と し た ngramベース確率モデル において は サイズ が MATH の ngramテーブル を 用意 し なけれ ば なら ない 
0 一方 IGTree の よう な メモリベースモデルCITE において は 品詞タグ づけ に 実際 に 用いる 特徴 の 数 は その ツリー を 張る ノード特徴 の 範囲内 で 可変 で あり 各 特徴 の タグ づけ へ の 影響度 も それら を 選択 する 優先順位 で 反映 さ れる 
0 しかしながら 特徴 の 数 を 大きく 取っ た 場合 この 手法 による タグ づけ の 計算コスト が 非常 に かかっ て しまう ケース が 生じる 
0 実際 Daelmansら の モデルCITE において は ノード の 数 は 僅か ４ に 設定 さ れ て おり 実質的 に 固定 長 さ の 文脈 を 用い て いる と 見 て も よい 
0 本稿 で は 複数 の ニューラルネット で 構成 さ れる マルチニューロタガー を 提案 する 
0 品詞 の タグ づけ は 長 さ が 固定 の 文脈 を 用いる の で は なく 最長文脈優先 で フレキシブル に 行なわ れる 
1 個々 の ニューラルネット の 訓練 は それぞれ独立 に 行なわ れる の で は なく 短い 文脈 で の 訓練結果訓練 で 獲得 し た 重み を 長い 文脈 で の 訓練 の 初期値 として 使う 
1 その 結果訓練時間 が 大幅 に 短縮 でき 複数 の ニューラルネット を 用い て も 訓練時間 は ほとんど 変わら ない 
1 タグ づけ において は 目標単語自身 の 影響 が 最も 強く 前後 の 単語 も それぞれ の 位置 に 応じ た 影響度 を 持つ こと を 反映 さ せる ため に 入力 の 各 構成部分 は 情報量最大 を 考慮 し て 訓練データ から 得 られる インフォメーションゲイン 略し て IG と 呼ぶ を 影響度 として 重み付け られる その 結果訓練時間 が 更に 大幅 に 短縮 さ れ タグ づけ の 性能 も 僅か ながら 改善 さ れる 
1 計算機実験 の 結果マルチニューロタガー は 8322文 の 小規模タイ語コーパス を 訓練 に 用いる こと により 未 訓練タイ語データ を 94以上 の 正解率 で タグ づけ する こと が でき た 
1 この 結果 は どの 固定 長 さ の 文脈 を 入力 と し た シングルニューロタガー を 用い た 場合 より も 優れ マルチニューロタガー は タグ づけ 過程 において 動的 に 適切 な 長 さ の 文脈 を 見つけ て いる こと を 示し た 
0 以下２章 で は 品詞タグ づけ 問題 の 定式化３章 で は インフォメーションゲインIG の 求め 方４章 で は マルチニューロタガー の アーキテクチャ そして ５章 で は 計算機実験 の 結果 について 順に 述べ て いく 
