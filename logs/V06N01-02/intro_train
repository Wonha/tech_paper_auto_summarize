0  一つ 一つ の 単語 は しばしば 複数 の 品詞  即ち  品詞 の 曖昧 性  を 持ち 得る  
0  しかしながら  その 単語 が 一旦 文 に 組み込ま れ ば  持ち 得る 品詞 は その 前後 の 品詞 によって 唯一 に 決まる 場合 が 多い  
0  品詞 の タグ づけ は この よう な 曖昧 性 を 文脈 を 用いる こと によって 除去 する こと で ある  
0  品詞 タグ づけ の 研究 は  特に 英語 や 日本語 など において 多数 行なわ れ て き た  
0  これら の 研究 を 通じ  これ まで 主 に 四つ の アプローチ  即ち  ルール ベース による もの  CITE  HMM や n  gram を 用い た 確率 モデル に 基づい た もの  CITE  メモリ ベース の もの  CITE  そして ニューラルネット を 用い た もの  CITE  が 提案 さ れ た  
0  これら の 研究 で は  大量 の 訓練 データ  例えば  CITE  において は 1  000  000 個 の データ  を 用いれ ば  その いずれ の 手法 を 用い て も  未 訓練 データ へ の タグ づけ を 95  以上 の 正解 率 で 行 なえる こと を 示し た  
0  しかしながら  実際  英語 や 日本語 など を 除い た 数 多く の 言語  例えば 本稿 で 取り上げ た タイ 語  に関して は  コーパス 自体 も まだ 整備 段階 に ある の が 現状 で  予め 大量 の 訓練 データ を 得る の が 困難 で ある  
0  従って  これら の 言語 にとって は  如何 に 少ない 訓練 データ で 十分 実用 的 で 高い 正解 率 の 品詞 タグ づけ システム を 構築 する か が 重要 な 課題 と なる  
0  これ まで 提案 さ れ た 確率 モデル や ニューラルネットモデル の ほとんど は タグ づけ に 長 さ が 固定 の 文脈 を 用いる もの で あり  HMM モデル において も 状態 遷移 を 定義 する の に 固定 さ れ た n  gram ベース の モデル を 用いる   入力 の 各 構成 部分 は 同一 の 影響 度 を 持つ もの と さ れ て い た  
0  しかし  訓練 データ が 少ない 場合  タグ づけ 結果 の 確信 度 を 高める ため に  まず できるだけ 長い 文脈 を 用い  訓練 データ の 不足 から 確定 的 な 答え が 出 ない 場合 に 順次 文脈 を 短く する といった よう に フレキシブル に タグ づけ する こと が 必要 と さ れよ う  
0  そして  客観 的 な 基準 で 入力 の 各 構成 部分 の 品詞 タグ づけ へ の 影響 度 を 計り  その 影響 度 に 応じ た 重み を それぞれ の 構成 部分 に 与えれ ば より 望ましい で あろ う  
0  そこで  シンプル で 効果 的 と 思わ れる 解決 法 は マルチモジュールモデル を 導入 する こと で ある  
1  マルチモジュールモデル と は  複数 の それぞれ 異なっ た 長 さ の 文脈 を 入力 と し た モジュール と それら の 出力 を 選別 する セレクター から 構成 さ れる システム の こと で ある  
0  しかし  この よう な システム を 例えば 確率 モデル や メモリベースモデル で 実現 しよ う と する と  それぞれ 以下 に 述べる 不具合 が 生じる  
0  確率 モデル は  比較的 短い 文脈 を 用いる 場合 に は  必要 と さ れる パラメター の 数 は それほど 多く なら ない  
0  しかし  ここ で 提案 し て いる よう な 複数 の モジュール を 場合 に 応じ て 使い分ける よう な システム で は  ある程度 の 長 さ の 文脈 を 用いる こと が 必要 と なり  確率 モデル の パラメター の 数 が 膨大 に なる  
0  例えば  品詞 が 50 種類 ある 言語 を 左右 最大 三つ の 単語 の 情報 を 文脈 として タグ づけ を 行なう 場合  その 最長 文脈 を 入力 と し た n  gram ベース 確率 モデル において は  サイズ が  MATH  の n  gram テーブル を 用意 し なけれ ば なら ない  
0  一方  IGTree の よう な メモリベースモデル  CITE  において は  品詞 タグ づけ に 実際 に 用いる 特徴 の 数 は その ツリー を 張る ノード  特徴  の 範囲 内 で 可変 で あり  各 特徴 の タグ づけ へ の 影響 度 も それら を 選択 する 優先 順位 で 反映 さ れる  
0  しかしながら  特徴 の 数 を 大きく 取っ た 場合  この 手法 による タグ づけ の 計算 コスト が 非常 に かかっ て しまう ケース が 生じる  
0  実際  Daelmans ら の モデル  CITE  において は ノード の 数 は 僅か ４ に 設定 さ れ て おり  実質 的 に 固定 長 さ の 文脈 を 用い て いる と 見 て も よい  
0  本稿 で は  複数 の ニューラルネット で 構成 さ れる マルチニューロタガー を 提案 する  
0  品詞 の タグ づけ は  長 さ が 固定 の 文脈 を 用いる の で は なく  最長 文脈 優先 で フレキシブル に 行なわ れる  
1  個々 の ニューラルネット の 訓練 は それぞれ 独立 に 行なわ れる の で は なく  短い 文脈 で の 訓練 結果  訓練 で 獲得 し た 重み  を 長い 文脈 で の 訓練 の 初期 値 として 使う  
1  その 結果  訓練 時間 が 大幅 に 短縮 でき  複数 の ニューラルネット を 用い て も 訓練 時間 は ほとんど 変わら ない  
1  タグ づけ において は  目標 単語 自身 の 影響 が 最も 強く  前後 の 単語 も それぞれ の 位置 に 応じ た 影響 度 を 持つ こと を 反映 さ せる ため に  入力 の 各 構成 部分 は 情報 量 最大 を 考慮 し て 訓練 データ から 得 られる インフォメーションゲイン  略し て IG と 呼ぶ  を 影響 度 として 重み付け られる  その 結果  訓練 時間 が 更に 大幅 に 短縮 さ れ  タグ づけ の 性能 も 僅か ながら 改善 さ れる  
1  計算 機 実験 の 結果  マルチニューロタガー は  8  322 文 の 小規模 タイ 語 コーパス を 訓練 に 用いる こと により  未 訓練 タイ 語 データ を 94  以上 の 正解 率 で タグ づけ する こと が でき た  
1  この 結果 は  どの 固定 長 さ の 文脈 を 入力 と し た シングルニューロタガー を 用い た 場合 より も 優れ  マルチニューロタガー は タグ づけ 過程 において 動的 に 適切 な 長 さ の 文脈 を 見つけ て いる こと を 示し た  
0  以下  ２ 章 で は 品詞 タグ づけ 問題 の 定式 化  ３ 章 で は インフォメーションゲイン  IG  の 求め 方  ４ 章 で は マルチニューロタガー の アーキテクチャ  そして ５ 章 で は 計算 機 実験 の 結果 について 順に 述べ て いく  
