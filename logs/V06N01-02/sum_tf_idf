================================================================
[section type  : abstract]
[section title : abstract]
================================================================
[i:4, score:219] タグづけにおいては,目標単語自身の影響が最も強く,前後の単語もそれぞれの位置に応じた影響を与えていることを反映させるために,入力の各構成部分は情報量最大を考慮して訓練データから得られるインフォメーションゲイン（略してIGと呼ぶ）を影響度として重み付けられる.
[i:6, score:220] 計算機実験の結果,マルチニューロタガーは, 8,322文の小規模タイ語コーパスを訓練に用いることにより,未訓練タイ語データを94%以上の正解率でタグづけすることができた.
[i:7, score:194] この結果は,固定長さを文脈としたどのシングルニューロタガーを用いた場合よりも優れ,マルチニューロタガーはタグづけ過程において動的に適切な長さの文脈を見つけていることを示した.

================================================================
[section type  : intro]
[section title : はじめに]
================================================================
[i:32, score:249] タグづけにおいては,目標単語自身の影響が最も強く,前後の単語もそれぞれの位置に応じた影響度を持つことを反映させるために,入力の各構成部分は情報量最大を考慮して訓練データから得られるインフォメーションゲイン（略してIGと呼ぶ）を影響度として重み付けられる,その結果,訓練時間が更に大幅に短縮され,タグづけの性能も僅かながら改善される.
[i:33, score:220] 計算機実験の結果,マルチニューロタガーは, 8,322文の小規模タイ語コーパスを訓練に用いることにより,未訓練タイ語データを94%以上の正解率でタグづけすることができた.
[i:34, score:203] この結果は,どの固定長さの文脈を入力としたシングルニューロタガーを用いた場合よりも優れ,マルチニューロタガーはタグづけ過程において動的に適切な長さの文脈を見つけていることを示した.

================================================================
[section type  : proposed_method]
[section title : 品詞タグづけ問題]
================================================================
[i:36, score:109] 表１は品詞をタグづけされたタイ語コーパスの例を示す.
[i:39, score:149] 入力されるタイ語テキストは電子辞書を用いて単語に分割され,各単語の持ち得る品詞もリストアップされるため,品詞のタグづけ問題は以下に示すような文脈を用いた品詞の曖昧性除去あるいは一種のクラス分け問題と見なせる.
[i:41, score:65] 具体的には, [MATH]は目標単語の取りうる品詞に関するもの, [MATH]と[MATH]はそれぞれ目標単語から左へ[MATH]番目と右へ[MATH]番目の単語の取りうる品詞（文脈）に関するもの,そして, [MATH]は目標単語がその文脈で取る正しい品詞を表すものである.

================================================================
[section type  : proposed_method]
[section title : インフォメーションゲイン（IG）]
================================================================
[i:44, score:164] ここで,特徴を入力の構成部分,特徴の値をその構成部分の取りうる品詞,データの属するクラスを目標単語の取りうる品詞にそれぞれ置き換えてやれば,各構成部分のIGはその構成部分の品詞タグづけへの影響度として考えることができる.
[i:45, score:105] 従って, (1)における入力の各構成部分[MATH] ([MATH])はそれぞれタグづけへの影響度に応じた重み[MATH]を持つと仮定すれば,その重みは以下のように求められる.
[i:53, score:94] 従って,構成部分[MATH]のタグづけへの影響度に応じた重みは以下のように設定できる.

================================================================
[section type  : proposed_method]
[section title : マルチニューロタガー]
================================================================
[i:54, score:0] 
-----------------------------------------------------
  [subsection title : シングルニューロタガー]
-----------------------------------------------------
  [i:lead, 121] 図１シングルニューロタガー(SNT)
.....
  [i:55, score:121] 図１シングルニューロタガー(SNT)
  [i:56, score:246] 図1は固定長さの文脈を用いて品詞タグづけをするニューラルネット（シングルニューロタガー,略してSNTと呼ぶ）を示す.
  [i:73, score:97] 従って,訓練の始めの入力構成では目標出力の比重が大きく,時間が立つにつれゼロへ減っていく.
-----------------------------------------------------
  [subsection title : マルチニューロタガー]
-----------------------------------------------------
  [i:lead, 215] 図２に示すように,マルチニューロタガーはエンコーダー/デコーダー,複数のシングルニューロタガーSNT[MATH] ([MATH]),そして最大文脈優先セレクターで構成される.
.....
  [i:75, score:215] 図２に示すように,マルチニューロタガーはエンコーダー/デコーダー,複数のシングルニューロタガーSNT[MATH] ([MATH]),そして最大文脈優先セレクターで構成される.
  [i:79, score:218] 目標単語[MATH]を中心とした,最大長さ[MATH]の単語列([MATH], [MATH], [MATH], [MATH], [MATH], [MATH])がマルチニューロタガーに与えられた時,それぞれ同じく単語[MATH]を中心とした長さ[MATH]の部分単語列が前節に述べた方法で[MATH]に符号化され,個々のシングルニューロタガーSNT[MATH] ([MATH])に入力される.
  [i:80, score:181] それらの入力に対し,個々のSNT[MATH]はそれぞれ独立に品詞タグづけを行ない,出力[MATH]を得る.
-----------------------------------------------------
  [subsection title : 三層パーセプトロン]
-----------------------------------------------------
  [i:lead, 94] 図１に示すように,シングルニューロタガーは三層パーセプトロン（詳細は[CITE]を参照）で構成される.
.....
  [i:85, score:195] 三層パーセプトロンは誤差逆伝播学習アルゴリズム[CITE]を用いて品詞のタグづけ済みの訓練用データを学習することによって品詞タグづけ能力を学習できる.
  [i:107, score:135] 訓練は,下の条件が満足されるまで,即ち,各訓練データと各出力ユニットに対する平均誤差[MATH]が目標誤差[MATH]以下になるまで,全訓練データを通じて繰り返して行なわれる：
  [i:108, score:156] 品詞タグづけ段階においては,入力[MATH]が与えられた時,ネットワークはその入力パターンを(20)-(24)を用いて入力層から出力層へ前向きに伝播しながら変換する.
-----------------------------------------------------
  [subsection title : 訓練]
-----------------------------------------------------
  [i:lead, 163] 品詞タグづけにニューラルネットモデルを用いる主な欠点は訓練コストが高い（即ち,訓練に時間がかかる）ことである.
.....
  [i:112, score:163] 品詞タグづけにニューラルネットモデルを用いる主な欠点は訓練コストが高い（即ち,訓練に時間がかかる）ことである.
  [i:114, score:180] しかしながら,実際,もし短い入力のSNT[MATH]の訓練結果（訓練で獲得した重み）を長い入力のSNT[MATH]（[MATH]）にコピーして初期値として使えば, SNT[MATH]（[MATH]）の訓練時間を大幅に短縮できる.
  [i:119, score:173] 図3シングルニューロタガーSNT[MATH]の訓練
-----------------------------------------------------
  [subsection title : 特徴]
-----------------------------------------------------
  [i:lead, 129] 例えば品詞が５０種類ある言語を左右それぞれ三つの単語の情報を文脈としてタグづけを行なう場合, n-gramベースの確率モデルは[MATH]個のn-gram（パラメータ）を推定しなければならない.
.....
  [i:120, score:129] 例えば品詞が５０種類ある言語を左右それぞれ三つの単語の情報を文脈としてタグづけを行なう場合, n-gramベースの確率モデルは[MATH]個のn-gram（パラメータ）を推定しなければならない.
  [i:124, score:140] そのために,ニューラルネットモデルのタグづけ性能は確率モデルのそれに比べ訓練データの数の少なさに影響されにくい[CITE].
  [i:125, score:136] また,他モデルに比べ,ニューラルネットモデルは訓練時間がかかる一方,タグづけ速度が非常に速いことも特徴の一つである.

================================================================
[section type  : experiment_result]
[section title : 実験結果]
================================================================
[i:130, score:196] マルチニューロタガーは五つの（入力に用いられる左右の単語の数がそれぞれ[MATH]の）シングルニューロタガーSNT[MATH]から構成された.
[i:131, score:181] 個々のタガーSNT[MATH]は入力長さ[MATH]（[MATH]）で入力層[MATH]中間層[MATH]出力層に[MATH]個のユニットを持つ三層パーセプトロンであった.
[i:149, score:212] これを確かめるために, [MATH]でIGなしのシングルニューロタガーSNT[MATH]を改めて前の結果を利用せずに訓練し直した.

================================================================
[section type  : conclusion]
[section title : 結び]
================================================================
[i:157, score:218] マルチニューロタガーは, 8,322文の小規模タイ語コーパスを訓練に用いることにより,未訓練タイ語データを94%以上の正解率でタグづけすることができた.
[i:159, score:192] 効率的な訓練方法,即ち,短い文脈での訓練結果を長い文脈での初期値として使うこと,を用いることにより,マルチニューロタガーをシングルニューロタガーとほとんど変わらないコストで訓練することができた.
[i:160, score:192] インフォメーションゲイン（IG）を導入することにより,訓練時間は更に大幅に短縮され,タグづけの性能も僅かながら改善された.

