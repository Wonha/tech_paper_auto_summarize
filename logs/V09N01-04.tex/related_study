関連研究
\label{sec:rel}

\subsection{複数モデルの出力の混合法}

\ref{sec:intro}~節で述べたように，一般に，
複数のモデル・システムの出力を混合する過程は，大きく以下の二つの部分に
分けて考えることができる．
\begin{enumerate}
\item \label{enum:sub1-rel}
	できるだけ振る舞いの異なる複数のモデル・システムを用意する．
\item \label{enum:sub2-rel}
	用意された複数のモデル・システムの出力を混合する方式を選択・設計し，
	必要であれば学習等を行ない，与えられた現象に対して，
	用意された複数のモデル・システムの出力を混合することを実現する．
\end{enumerate}
ここで，これまで自然言語処理の問題に適用された混合手法においては，
これらの(\ref{enum:sub1-rel})および(\ref{enum:sub2-rel})の過程について，
大体以下のような手法が用いられていた．

まず，(\ref{enum:sub1})については，大きく分けて以下のような手法がある．
\begin{enumerate}
\item[i)] 学習モデルが異なる複数のシステム等
	(原理的には，人手による規則に基づくシステムとデータからの学習に基づくシステム，
	などの組合わせも可能)，
	ある程度振る舞いの異なる既存のシステムを
	用意する~\cite{vanHalteren98a,Brill98a,Henderson99a,KoInui00aj,Sang00a}．
\item[ii)] i)と似ているが，学習モデルは単一のものを用い，データの表現法
	(具体的には，まとめ上げ問題におけるまとめ上げ状態の表現法)として複数のものを
	設定することにより，複数の出力を得る~\cite{Sang00a,TKudo00ajx}．
\item[iii)] 単一の学習モデルを用いるが，訓練データのサンプリングを複数回行なうことにより
	複数のモデルを学習するbagging法~\cite{Breiman96b}を用いる~\cite{Henderson00a}，
	あるいは，単一の学習モデルを用い，誤り駆動型で訓練データ中の訓練事例の重みを操作しながら
	学習と適用を繰り返すことにより，各サイクルの誤りに特化した複数のモデル
	(およびそれらの重み)を学習する
	boosting法~\cite{Freund99aj}を用いる~\cite{Haruno97a,Haruno99a,Abney99a,Henderson00a}．
\end{enumerate}
これに対して，本論文においては，振る舞いの異なる複数のモデルを得る方法として，
学習モデルは単一のものを用い，固有表現まとめ上げの際に考慮する周囲の形態素の個数を
区別することで複数のモデルを得るという方法をとった．
この方法は，上記のうちでは，ii)でとられた方法と比較的似ている．

次に，(\ref{enum:sub2})については，大きく分けて以下のような手法がある
\footnote{
  boostingは，複数のモデルを組合わせる際の重みまで含めて，全体として誤りが減少するように
  複数モデルの生成法が設計されているので，以下の分類には含めない．
}．
\begin{enumerate}
\item[i)] 重み付多数決など，何らかの多数決を行なうもの~\cite{Breiman96b,vanHalteren98a,Brill98a,Henderson99a,KoInui00aj,Sang00a,Henderson00a,TKudo00ajx}．
\item[ii)] 複数のシステム・モデルの重みに応じて採用するシステムの切り替えを行なうもの~\cite{Henderson99a,KoInui00aj}．
\item[iii)] 原理的に，上記のi)およびii)を包含し得る方法として，
	複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段の
	学習器を用いて，複数のシステム・モデルの出力の混合を行なうstacking法~\cite{Wolpert92a}，
	あるいは，それと同等の方法に基づくもの~\cite{vanHalteren98a,Brill98a,Sang00a}．
\end{enumerate}
これらの方法のうち，本論文では，原理的に，i)およびii)を包含し得るiii)のstacking法を用いている．
特に，本論文では，個々のシステムの出力する重みの情報は利用せずstackingを行なっているので，
規則に基づくシステムなどで重みを出力しない場合でも，そのまま本論文の手法を適用することができる．
これに対して，重み付多数決や重みを用いたシステム切り替えの場合は，
システム数が少なく(例えば，二種類のシステムの混合の場合)，かつ，個々のシステムが重みを出力しない
場合などでは，適用が困難になると考えられる．
また，通常のbagging法やboosting法を適用する場合でも，
第一段としては何らかの学習モデルを採用する必要があるが，
本論文の混合法にはそのような制約はないので，原理的には，
第一段として任意のシステムを採用することが可能である．

\subsection{Stacking法}

次に，本節では，stacking法についての関連研究，および，
stacking法と同等の手法を自然言語処理におけるシステム混合の問題に適用している
研究事例について述べる．

stacking法は，\cite{Wolpert92a}によってその枠組みが提案され，その後，機械学習の分野において
いくつかの応用手法が提案されている~\cite{Breiman96a,Ting97a,Gama00a}．
例えば，\cite{Breiman96a}は，回帰法を用いたstackingを提案している．
\cite{Ting97a}は，第一段の学習器として，決定木学習，ナイーブベイズ，最近隣法を用い，
第二段の学習器として，決定木学習，ナイーブベイズ，最近隣法，線形回帰法の一種を用いた
実験を行ない，性能の比較をしている．
一方，\cite{Gama00a}は，それまで提案されたstacking法を，$n$段の学習器の連鎖に拡張し，
第$k$ $(1<k\leq n)$段の学習器は，第一段から第$k-1$段までの全ての学習器の入出力データを
素性として学習を行なうというカスケード法を提案している．
特に，それまでのstacking法は，第一段の学習器の出力のみを入力素性として第二段の
学習器の学習を行なうものがほとんどであったのに対して，カスケード法では，
前段までの学習器の出力だけでなく，入力素性もあわせて利用する点が特徴的である．

一方，自然言語処理におけるシステム混合の問題にstacking法と同等の手法を適用している
研究事例
\footnote{
  ``stacking''という用語を用いていない事例も多い．
}
としては，英語品詞付けにおいて，
最大エントロピー法，変形に基づく学習，トライグラムモデル，メモリベース学習を
第一段の学習器とし，決定木学習，メモリベース学習法などを第二段の学習器として
stackingを行なうもの~\cite{Brill98a,vanHalteren98a}，
英語名詞句まとめ上げにおいて，七種類の学習器を第一段に用い，
決定木学習，メモリベース学習法を第二段の学習器として
stackingを行なうもの~\cite{Sang00a}などがある．
これらの事例においては，いずれも，第一段の入力素性および出力を用いて第二段の学習器の
学習を行なった結果も報告している．
また，\cite{Borthwick98a}は，英語の固有表現抽出において，
単一の最大エントロピーモデルの素性として，通常の固有表現まとめ上げ・タイプ分類に用いる
素性とあわせて，他の既存のシステムの出力を素性として用いて，
個々の単語に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を行なっている．
一方，\cite{Freitag00a}は，情報抽出におけるテンプレート・スロット埋め問題において，
ナイーブベイズ法，帰納的論理プログラミング法などを第一段の学習器とし，
回帰法を第二段の学習器としてstackingを行なっている．
ここでは，第二段の学習器の入力は，第一段の学習器の出力のみとなっている．

これらの事例と比較すると，本論文の日本語固有表現抽出の問題においては，
第一段の学習器は，個々の形態素に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を
行なっているのに対して，第二段の学習器は，個々のシステムの固有表現抽出結果，
および，第一段の学習器の入力となった素性(の一部)を入力として，個々のシステムの固有表現抽出結果の
正誤を判定するための分類器の学習を行なっている．
このように，本論文のstacking法では，第一段と第二段の学習器の学習の単位が異なっている点が
変則的である．
ただし，このような構成をとることにより，第一段としては，
任意の固有表現抽出システムを用いることが可能となっている．
また，\cite{Borthwick98a}と比較すると，\cite{Borthwick98a}では，
本論文の第二段に相当する学習器が，個々の単語に固有表現まとめ上げ状態・タイプ分類を
付与するための分類器の学習を行なっている点が異なっている．


\subsection{統計的手法に基づく日本語固有表現抽出}

統計的手法に基づく日本語固有表現抽出の研究事例としては，
我々がベースとした，最大エントロピー法を用いるもの\cite{Uchimoto00aj}の他に，
決定木学習を用いるもの~\cite{Sekine98a,Nobata99aj}，
最大エントロピー法を用いるもの~\cite{Borthwick99aj}，
決定リスト学習を用いるもの~\cite{Sassano00a}， 
SVM(support vector machines)を用いるもの~\cite{Yamada01ajx}などがある．
これらは，いずれも，単一の学習モデルを用いている．
決定リスト学習を用いる事例~\cite{Sassano00a} 
では，
可変長文脈素性を用いることにより，固定長モデルの性能の上回る結果が得られているが，
ベースとなる決定リスト学習の性能は最大エントロピー法の性能よりも劣っている．
その他の事例では，いずれも，固定長文脈素性を用いている．

また，stacking法の研究事例においては，異なる数種類の学習器を第一段に用いるという構成が
多く見られ，一定の効果が報告されているので，上記の複数の学習器を第一段としてstacking法を
行なうことにより，精度の向上が期待できる可能性がある．
その他には，\cite{Yamada01ajx}で報告されているように，解析の方向を文頭から文末と文末から文頭の
二通り設定し，解析済の固有表現のタグを素性として利用する方法により，
振る舞いの異なった出力が得られる可能性があり，stacking法でその出力を利用することで，
精度の向上が期待できる可能性がある．
また，\cite{Isozaki00ajx}では，
決定木学習により学習された可読性の高い規則や人手による付加制約等を適用して
複数の固有表現候補を生成し，最長一致法により複数の候補の選別を行なっている．
ここで，複数の候補の選別を行なう際に，本論文の混合法を適用することにより，
誤出力の棄却まで含めたより一般的な選別が自然な形で実現できる可能性があると
考えられる．

