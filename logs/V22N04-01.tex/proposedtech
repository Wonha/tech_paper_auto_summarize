    \documentclass[japanese]{jnlp_1.4}
\usepackage{jnlpbbl_1.3}
\usepackage[dvipdfm]{graphicx}
\usepackage{amsmath}
\usepackage{hangcaption_jnlp}
\usepackage{array}

\usepackage{tabularx}
\usepackage{bm}


\Volume{22}
\Number{4}
\Month{December}
\Year{2015}

\received{2015}{2}{27}
\revised{2015}{6}{29}
\accepted{2015}{8}{12}

\setcounter{page}{225}

\jtitle{Deep Belief Networkを用いた検索用語の予測}
\jauthor{馬　　　青\affiref{Author_1} \and 谷河　息吹\affiref{Author_1} \and 村田　真樹\affiref{Author_2}}
\jabstract{
	本稿は機械学習を用いて関連語・周辺語または説明文書から適切な検索用語を予測する手法を提案する．機械学習には深層学習の一種である Deep Belief Network (DBN) を用いる．DBN の有効性を確認するために，用例に基づくベースライン手法，多層パーセプトロン (MLP)，サポートベクトルマシン (SVM) との比較を行った．学習と評価に用いるデータは手動と自動の 2 通りの方法でインターネットから収集した．加えて，自動生成した疑似データも用いた．各種機械学習の最適なパラメータはグリッドサーチと交差検証を行うことにより決定した．
実験の結果，DBN の予測精度はベースライン手法よりはるかに高く MLP と SVM のいずれよりも高かった．また，手動収集データに自動収集のデータと疑似データを加えて学習することにより予測精度は向上した．さらに，よりノイズの多い学習データを加えても DBN の予測精度はさらに向上したのに対し，MLP の精度向上は見られなかった．このことから，DBN のほうが MLP よりもノイズの多い学習データを有効利用できることが分かった．
}
\jkeywords{深層学習，Deep Belief Network，検索用語予測，関連語，周辺語，情報検索支援}

\etitle{Retrieval Term Prediction Using Deep Belief Networks}
\eauthor{Qing Ma\affiref{Author_1} \and Ibuki Tanigawa\affiref{Author_1} \and Masaki Murata\affiref{Author_2}} 
\eabstract{
	This paper presents a method to predict retrieval terms from relevant/surrounding words or descriptive texts in Japanese by using deep belief networks (DBN), one of two typical types of deep learning. To determine the effectiveness of using DBN for this task, we tested it along with baseline methods using example-based approaches and conventional machine learning methods, i.e., multi-layer perceptron (MLP) and support vector machines (SVM), for comparison. The data for training and testing were obtained from the Web in manual and automatic manners. Automatically created pseudo data was also used. A grid search was adopted for obtaining the optimal hyperparameters of these machine learning methods by performing cross-validation on training data. Experimental results showed that (1) using DBN has far higher prediction precisions than using baseline methods and higher prediction precisions than using either MLP or SVM; (2) adding automatically gathered data and pseudo data to the manually gathered data as training data is an effective
measure for further improving the prediction precisions; and (3) DBN is able to deal with noisier training data than MLP, i.e., the prediction precision of DBN can be improved by adding noisy training data, but that of MLP cannot be.
}
\ekeywords{Deep Learning, Deep Belief Network, Retrieval Term Prediction, Relevant Word, Surrounding Word, Information Retrieval Support}

\headauthor{馬，谷河，村田}
\headtitle{Deep Belief Networkを用いた検索用語の予測}

\affilabel{Author_1}{龍谷大学理工学部}{Faculty of Science and Technology, Ryukoku University}
\affilabel{Author_2}{鳥取大学工学研究科}{Graduate School of Engineering, Tottori University}



\begin{document}
\maketitle


\section{関連語・周辺語コーパス}

\begin{table}[b]
\caption{コーパスの入力とラベルのペアの例}
\label{tab:example}
\input{01table01.txt}
\end{table}

機械学習を用いて関連語・周辺語から検索用語を予測・提示する場合，その学習データとして，入力（関連語・周辺語）と正解となるレスポンス（検索用語）のペアからなるコーパスが必要となる．本稿ではこのようなコーパスを「関連語・周辺語コーパス」と呼ぶ．また，教師あり機械学習では，レスポンスをラベルと呼ぶ場合が多いので，本稿では検索用語をラベルと呼ぶ．表~\ref{tab:example}はコーパスの入力（関連語・周辺語とその元となる説明文書）とラベルのペアの例を示す．
本章では，コーパスデータの収集・作成方法について述べる．また，収集・作成したデータからの関連語・周辺語の抽出方法と特徴ベクトルの構成方法について述べる．


\subsection{手動収集と自動収集}

本研究では，ラベルを説明している文書には関連語・周辺語が多く含まれると考え，インターネットからこのようなWebページを手動と自動の 2 通りの方法で収集した．手動収集では人手でラベルを説明するWebページを選別し収集する\footnote{人手でWebページを選別した後，そのWebページから説明文書として該当する箇所を人手で選別する処理を行っている．}．一方，自動収集では，ラベルの後に「とは」「は」「というものは」「については」「の意味は」の 5 語を付けて（たとえば，ラベルが「グラフィックボード」であれば「グラフィックボードとは」「グラフィックボードというものは」などで）Googleで検索したものを説明文書として収集する\footnote{収集したWebページ全体をそのラベルの説明文書として扱っている．}．手動収集データは規模が小さい代わりに精度が高く，自動収集データは精度が低い代わりに規模が大きい．


\subsection{疑似データ}

機械学習の汎化能力を向上させるために，学習データとして，精度は高いが規模が小さい手動収集データに加え，精度はそれほど高くない（つまり，ノイズはある）が相対的に規模の大きい自動収集データを用いることにした．しかし，自動収集したデータには説明文書とラベルがそもそも一致しない，つまり説明文書へのラベルが履き違えられている可能性も考えられる．そのために，手動で収集した説明文書をオリジナルのデータとしてとらえ，それらに適度なノイズを加えて作成した疑似データも用いることにした．このようなデータは自動収集したデータに比べノイズが少なくラベルの履き違いもないと考えることができる．疑似データの具体的な生成手順は以下の通りである．
\begin{enumerate}
\item オリジナルの説明文書からすべての異なり単語を抽出する．
\item 個々のオリジナルの説明文書に対し，追加，削除，または追加\&削除の処理を加える．具体的には，手順(1)で抽出した単語のうち，説明文書にない単語を説明文書の単語数の10\%個ランダムに選んで加える，説明文書から単語を説明文書の単語数の10\%個ランダムに選んで削除する，または上記の（10\%ずつの）追加と削除を同時に施す，という処理を等確率（つまり，それぞれを1/3の確率）で行う\footnote{10\%という値は，予備実験などで精査して決めたものではなく，著者らが適度なノイズとして主観で設定したものである．}．
\item 手順(2)で得られたデータを疑似データとする．
\end{enumerate}
なお，この生成方法においては，1 つのオリジナルの説明文書に対し，疑似データを複数生成することが可能である．


\subsection{評価データ}

評価データは学習データとは別に自動収集したものを用いる．ただし，自動収集データは，ラベルが正確とは限らないため，評価データとして用いても適切な評価とならない可能性がある．そのため，評価データとして自動収集データの中からラベルの正しいものを人手で選別して用いることにした．


\subsection{関連語・周辺語抽出とベクトル変換}

以下の手順(1)〜(4)で説明文書から関連語・周辺語を抽出する．それに手順(5)(6)を加えることにより，機械学習に必要な特徴ベクトルへの変換を行う．
\begin{enumerate}
\item
手動収集のデータを形態素解析し，名詞（固有名詞，サ変接続，一般）を抽出する\footnote{形態素解析にMeCab 0.98を使用した．未知語と表記ゆれについては特別な処理を施しておらず今後の課題となる． ただし，未知語としての複合語については，たとえば「木村製作所」や「株式会社ウエーブ」など大半の日本企業名の場合は（中小企業で社名としては未知語であっても）「木村」と「製作所」などがそれぞれ名詞として解析されているので，手順(2)にしたがって問題なく1つの既知単語として扱われる．一方，たとえば「騰迅公司」のような表現において，最初の漢字が名詞以外の品詞と判断された場合は1つの既知単語として正しく扱うことができない．また，表記ゆれについては，「サーバ」と「サーバー」，「神経回路」と「ニューラルネット」のような形態素解析ツールの辞書に登録されているものはそれぞれ異なる単語として扱われてしまい，予測性能を落とす可能性がある．}．
\item
名詞が連続しているならば，日本語同士なら結合し，英語同士なら空白を間に入れて結合し，1 つの単語と見なす．
\item
各ラベルから出現頻度がトップ50以内の単語を抽出する\footnote{50という値は，手動で収集したデータにおいて各ラベルの関連語・周辺語の数は確実にそれ以下であることを確認した上で，提案手法の拡張性（つまり多少大きい目に）と機械学習の素性選択能力（つまり多少大きい目にしても問題がないこと）も考慮にいれて設定した．なお，この値は多少大きく設定されても手順(4)で絞られるので値をある程度大きい目に設定しておけば40 がよいか60 がよいかといった細かい選択はほとんど意味をなさないと思われる．}．
\item
ラベル間で重複している単語を除外する．本研究では，以下に述べる考えに基づき 2 ラベル間で重複する単語を除外する，または，3 ラベル以上で共通する単語を除外するという 2 通りの方法を採用した．まず，各ラベルにできるだけ特徴的な単語のみを素性にするためには重複単語をできるだけ除外するのが効果的と考える．また，今回は実験規模が小さくあまり問題にならないが，予測用語の数の増加に伴う特徴ベクトル次元の大幅な増加を抑える 1 つの方法として重複単語を除外することが考えられる．特徴ベクトル次元の抑制はまた一般的に，学習におけるデータスパースネス問題の緩和にもつながる．しかし一方，ラベル間の単語重複をまったく認めないと，たとえば「USBメモリ」のような，「USB」や「メモリ」に共通する重要な単語を除外してしまう問題も考えられる．そのため，本研究では 2 ラベル間の重複を許容し 3 ラベル以上で共通する単語を除外する方法も用いる．
\item 上記手順で得られた単語をベクトルの要素とし，個々の要素はその単語が出現していれば1，出現していなければ0の2値を取る．
\item 2.1, 2.2, 2.3節で述べたすべてのデータに対し形態素解析を行い，手順(5)にしたがって特徴ベクトルに変換する．
\end{enumerate}


\section{深層学習}

深層学習とは従来の機械学習より深い層構造をしている機械学習手法全般のことを指す．
その代表的な手法としてDeep Belief Network (DBN)~\cite{Hinton,Lee,Bengio:09,Bengio:13}とStacked Denoising Autoencoder (SdA)~\cite{Bengio:07,Bengio:09,Bengio:13,Vincent:08,Vincent:10}が提案されている．数多くの課題において，その両者の性能がほぼ同じと言われているが，本研究ではよりスマートなアーキテクチャを有するDBNを用いることにした．

深層学習は，本来経験則で行っていた特徴抽出を機械学習に組み込もうとしてできたものである．そのため，DBNは，Restricted Boltzmann Machine (RBM) を複数並べ教師なし学習の特徴抽出器として利用する多層のニューラルネットと，ラベルを出力する教師あり学習の最終層から構成される．特徴抽出器の教師なし学習はPre-training，最終層の教師あり学習はFine-tuningと呼ばれる．


\subsection{Restricted Boltzmann Machine (RBM)}

RBMは制限付きボルツマンマシンとも呼ばれ，学習データの確率分布を教師なし学習で表現する（言い換えれば，学習データの生成モデルを統計的な機械学習の方法で構築する），一種の確率的なグラフィカルモデルである．本来のボルツマンマシンの可視層と隠れ層のユニット間の結合を制限することにより，効率的な教師なし学習を実現している．

RBMの構造は図~\ref{fig_rbm}に示しているように可視層と隠れ層の  2層から構成され，層内ユニット間に結合がなく，層間のユニット，すなわち可視ユニット($v_1, v_2, \cdots, v_m$)と隠れユニット ($h_1, h_2, \cdots, h_n$)， は結合されている． 

\begin{figure}[t]
\begin{center}
\includegraphics{22-4ia1f1.eps}
\end{center}
\caption{Restricted Boltzmann Machineの構造}
\label{fig_rbm}
\vspace{-1\Cvs}
\end{figure}

以下，その学習アルゴリズム~\cite{Bengio:09}を簡潔に述べておく．

学習データ$\bm{v}$が可視層に与えられたとき，まず，式(1)，(2)，そして再度(1)の順で条件付確率に基づくサンプリングを行う．
{\allowdisplaybreaks
\begin{gather}
P(h_i^{(k)}=1|\bm{v}^{(k)})={\rm sigmoid}\bigg(\sum_{j=1}^m w_{ij}v_j^{(k)}+c_i\bigg) \\
P(v_j^{(k+1)}=1|\bm{h}^{(k)})={\rm sigmoid}\bigg(\sum_{i=1}^n w_{ij}h_i^{(k)}+b_j\bigg)
\end{gather}
}
ただし，$k$ ($\geq 1$)はサンプリングの繰り返し回数，$\bm{v}^{(1)}=\bm{v}$，$w_{ij}$はユニット$v_j$と$h_i$間の結合の重み，そして， $b_j$と$c_i$は可視層と隠れ層のユニット$v_j$と$h_i$のオフセット（バイアス）である． 
サンプリングを$k$回行った後，重みとオフセットは以下のように更新される．
\begin{gather}
\bm{W} \leftarrow \bm{W} + \epsilon (\bm{h}^{(1)}\bm{v}^{T}- P(\bm{h}^{(k+1)}=1|\bm{v}^{(k+1)})\bm{v}^{(k+1)T}) \\
\bm{b} \leftarrow \bm{b} + \epsilon(\bm{v}-\bm{v}^{(k+1)}) \\
\bm{c} \leftarrow \bm{c} + \epsilon(\bm{h}^{(1)}-P(\bm{h}^{(k+1)}=1|\bm{v}^{(k+1)}))
\end{gather}
ただし，$\epsilon$は学習率である．$\bm{W}$は微小な乱数\footnote{本研究では，http://deeplearning.net/tutorial/mlp.htmlのチュートリアルに従って，区間[$-4\frac{\sqrt{6}}{\sqrt{m+n}}$, $4\frac{\sqrt{6}}{\sqrt{m+n}}$]内の一様乱数を用いる（ただし，$m$と$n$はそれぞれ可視層と隠れ層のユニット数である）
．その数学的な考えについては\cite{Glorot:10}を参照されたい．}， $\bm{b}$, $\bm{c}$は$\bm{0}$で初期化する． 
サンプリングの繰り返し回数が十分多いときはGibbs samplingと呼ばれており計算コストが非常に高い．そのため，通常，サンプリングを$k$回のみ行う$k$-Contrastive Divergence （略してCD-$k$）と呼ばれる方法が採用される．実際，$k=1$ (CD-1)でも結果が十分よいことが経験的に知られており\cite{Bengio:09}，本研究も$k=1$に設定して学習を行う．

ここで$N$個の学習データに対しCD-$k$と呼ばれるサンプリング方法で$e$回繰り返し学習を行う手順を図~\ref{fig:procedure-RBM}にまとめる．学習が進むにつれ，可視層のサンプル\footnote{ここでは条件付確率の式(1)(2)に基づき生成されたデータをサンプルと呼んでいる．} $\bm{v}^{(k+1)}$が学習データ$\bm{v}$に近づいていく．


\subsection{Deep Belief Network (DBN)}

図~\ref{fig_dbn}は一例として，三つのRBMと教師あり学習器から構成されるDBNを示す．ただし実際，DBNを構成するRBMの数は可変である．それらRBMはPre-trainingとも呼ばれ，教師なしの特徴抽出器として機能する．一方，教師あり学習器はFine-tuningとも呼ばれ，入力（図~\ref{fig_dbn}の場合はその入力から得られたRBM 3の出力）とラベルのペア（つまり正解付学習データ）を学習することにより未知の入力に対しても適切なラベルを出力できるようになる．図に示しているように前方のRBMの隠れ層は後方のRBMの可視層となっている．ここでは簡便化のために，RBMの層（ただし入力層を除く）をDBNの隠れ層と見なす．つまり，図の例は三層の隠れ層のDBNである（隠れ層の数とRBMの数は同じであることに注意されたい）．なお，教師あり学習はいろいろな方法で実現できるが，本稿ではロジスティク回帰を用いることにした．

\begin{figure}[b]
\vspace{-0.5\Cvs}
\begin{center}
\includegraphics{22-4ia1f2.eps}
\end{center}
\caption{RBMの学習手順}
\label{fig:procedure-RBM}
\end{figure}
\begin{figure}[b]
\begin{center}
\includegraphics{22-4ia1f3.eps}
\end{center}
\caption{Deep Belief Networkの例}
\label{fig_dbn}
\end{figure}
\begin{figure}[b]
\begin{center}
\includegraphics{22-4ia1f4.eps}
\end{center}
\caption{三つのRBMを持つDBNの学習手順}
\label{fig:procedure-DBN}
\end{figure}


三つのRBMを持つDBNの学習手順を図~\ref{fig:procedure-DBN}にまとめる． 


\section{本課題の意義について}

本研究では特定の分野の関連語・周辺語または説明文書を入力としたときの
\pagebreak
検索用語の予測・提示を行う検索支援を想定している．まず，説明文書による支援の意義は，たとえばThe 5th NTCIR Workshop Meeting on 
Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Accessのようなワークショップ型共同研究\cite{Ma}における長い文書を検索課題\footnote{検索課題例：AOLとタイムワーナー合併の影響に関する記事を探したい．AOL・タイムワーナー合併がインターネットとエンターテインメントというメディア産業に与える影響に関する意見を適合とする．AOL・タイムワーナー合併の展開についての記述は部分的に適合とする．総額と所有権転換の仕組みに関する情報は不適合とする．}としたタスクからも類推できる．
つまり，たとえばユーザが関連語・周辺語もはっきりわからないときはその支援要求を文書の形で伝える（入力する）ニーズはあると考える．また一方，当然のことではあるが，本研究では，少数キーワード（関連語・周辺語）による検索用語の予測も期待している．実際，表~\ref{tab:keyword-prec}は，DBNについて，各学習データセットを用いた場合の，表~\ref{tab:keyword}に示す 3 関連語・周辺語（$+1$ ノイズ語）\footnote{これらのキーワードは予備実験も含め一切精査せずに著者らの知識に頼って手動収集のデータから関連語・周辺語・ノイズ語としてふさわしいものを主観で選んでいる．しかし当然なことではあるが，これらのキーワードはすべて誰にも知られている用語である保証はない（また，本実験の目的からしてそう保証する必要もない）．}による全検索用語の平均予測精度を示している\footnote{当然のことではあるが，予測精度は用いるキーワードに大きく依存する．試しに10検索用語のうち6検索用語の関連語・周辺語を意識的に関連性の弱いものを選んで実験すると平均精度が8割程度までに下がった．}．実験はまだ小規模ではあるが，この結果は
提案手法が少数キーワードによる支援も可能であることを示唆していると思われる．

\begin{table}[t]
\caption{予測に用いるキーワード}
\label{tab:keyword}
\input{01table13.txt}
\end{table}
\begin{table}[t]
\caption{DBNの少数キーワードによる予測精度（223次元の特徴ベクトルを用いた場合）}
\label{tab:keyword-prec}
\input{01table14.txt}
\end{table}


\section{結び}

本稿では深層学習の代表的な手法であるDeep Belief Network (DBN)を用いて
\pagebreak
関連語・周辺語またはそれらの語から構成される説明文書から適切な検索用語を予測する手法を提案した．DBNの有効性を確認するために，用例に基づくベースライン手法，多層パーセプトロン (MLP)，およびサポートベクトルマシン (SVM) との比較を行った．学習と評価に用いるデータは手動と自動の 2 通りの方法でインターネットから収集した．加えて，自動生成した疑似データも用いた．各種機械学習の最適なパラメータはグリッドサーチと交差検証を行うことにより決めた．実験の結果，DBNの予測精度はベースライン手法よりはるかに高くMLPとSVMのいずれよりも高かった．また，手動収集データに自動収集のデータと疑似データを加えて学習することにより予測精度は向上した．さらに，よりノイズの多い学習データを加えてもDBNの予測精度はさらに向上した．しかしながらこの場合MLPの精度向上は見られなかった．このことから，DBNのほうがMLPよりもノイズの多い学習データを有効利用できることが分かった．なお，まだ少数の実験例しかなかったが，提案手法が少数キーワードによる支援も可能であることを示唆した実験結果も得られた．

今後はより大規模な評価実験を通じ，提案手法の有効性の確認を行うとともに，様々な分野における実用的な検索用語の予測システムを構築していく予定である．


\acknowledgment

本稿に対して丁寧かつ有益なご意見ご指摘をいただきました査読者の方に感謝いたします．
本稿の内容の一部は，The 28th Pacific Asia Conference on Language, Information and Computing (Paclic 28)で発表したものです\cite{Ma:14}．
また，本研究はJSPS科研費25330368の助成を受けています．記して謝意を表します．


\bibliographystyle{jnlpbbl_1.5}
\begin{thebibliography}{}

\bibitem[\protect\BCAY{粟飯原\JBA 長尾\JBA 田中}{粟飯原 \Jetal }{2013}]{Aihara}
粟飯原俊介\JBA 長尾真\JBA 田中久美子 \BBOP 2013\BBCP.
\newblock 意味的逆引き辞書『真言』.\
\newblock \Jem{言語処理学会第19回年次大会発表論文集}, \mbox{\BPGS\ 406--409}.

\bibitem[\protect\BCAY{Auli, Galley, Quirk, \BBA\ Zweig}{Auli
  et~al.}{2013}]{Auli}
Auli, M., Galley, M., Quirk, C., \BBA\ Zweig, G. \BBOP 2013\BBCP.
\newblock \BBOQ Joint Language and Translation Modeling with Recurrent Neural
  Networks.\BBCQ\
\newblock In {\Bem Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2013)}, \mbox{\BPGS\ 1044--1054}.

\bibitem[\protect\BCAY{Bengio}{Bengio}{2009}]{Bengio:09}
Bengio, Y. \BBOP 2009\BBCP.
\newblock \BBOQ Learning Deep Architectures for AI.\BBCQ\
\newblock {\Bem Foundations and Trends in Machine Learning}, {\Bbf 2}  (1),
  \mbox{\BPGS\ 1--127}.

\bibitem[\protect\BCAY{Bengio}{Bengio}{2012}]{Bengio:12}
Bengio, Y. \BBOP 2012\BBCP.
\newblock \BBOQ Practical Recommendations for Gradient-Based Training of Deep
  Architectures.\BBCQ\
\newblock {\Bem eprint arXiv:1206.5533}, \mbox{\BPGS\ 1--33}.

\bibitem[\protect\BCAY{Bengio, Courville, \BBA\ Vincent}{Bengio
  et~al.}{2013}]{Bengio:13}
Bengio, Y., Courville, A., \BBA\ Vincent, P. \BBOP 2013\BBCP.
\newblock \BBOQ Representation Learning: A Review and New Perspectives.\BBCQ\
\newblock {\Bem IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, {\Bbf 35}  (8), \mbox{\BPGS\ 1798--1828}.

\bibitem[\protect\BCAY{Bengio, Lamblin, Popovici, \BBA\ Larochelle}{Bengio
  et~al.}{2007}]{Bengio:07}
Bengio, Y., Lamblin, P., Popovici, D., \BBA\ Larochelle, H. \BBOP 2007\BBCP.
\newblock \BBOQ Greedy Layer-wise Training of Deep Networks.\BBCQ\
\newblock In {\Bem Advances in Neural Information Processing Systems 19 (NIPS
  2006)}, \mbox{\BPGS\ 153--160}.

\bibitem[\protect\BCAY{Billingsley \BBA\ Curran}{Billingsley \BBA\
  Curran}{2012}]{Billingsley}
Billingsley, R.\BBACOMMA\ \BBA\ Curran, J. \BBOP 2012\BBCP.
\newblock \BBOQ Improvements to Training an RNN Parser.\BBCQ\
\newblock In {\Bem Proceedings of the 24th International Conference on
  Computational Linguistics (COLING 2012)}, \mbox{\BPGS\ 279--294}.

\bibitem[\protect\BCAY{Collobert, Weston, Bottou, Karlen, Kavukcuoglu, \BBA\
  Kuksa}{Collobert et~al.}{2011}]{Collobert}
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., \BBA\
  Kuksa, P. \BBOP 2011\BBCP.
\newblock \BBOQ Natural Language Processing (Almost) from Scratch.\BBCQ\
\newblock {\Bem Journal of Machine Learning Research}, {\Bbf 12}, \mbox{\BPGS\
  2493--2537}.

\bibitem[\protect\BCAY{Glorot \BBA\ Bengio}{Glorot \BBA\
  Bengio}{2010}]{Glorot:10}
Glorot, X.\BBACOMMA\ \BBA\ Bengio, Y. \BBOP 2010\BBCP.
\newblock \BBOQ Understanding the Difficulty of Training Deep Feedforward
  Neural Networks.\BBCQ\
\newblock In {\Bem Proceedings of the 13th International Conference on
  Artificial Intelligence and Statistics (AISTATS 2010)}, \mbox{\BPGS\
  249--256}.

\bibitem[\protect\BCAY{Glorot, Bordes, \BBA\ Bengio}{Glorot
  et~al.}{2011}]{Glorot}
Glorot, X., Bordes, A., \BBA\ Bengio, Y. \BBOP 2011\BBCP.
\newblock \BBOQ Domain Adaptation for Large-Scale Sentiment Classification: A
  Deep Learning Approach.\BBCQ\
\newblock In {\Bem Proceedings of the 28th International Conference on Machine
  Learning (ICML 2011)}, \mbox{\BPGS\ 513--520}.

\bibitem[\protect\BCAY{Hashimoto, Miwa, Tsuruoka, \BBA\ Chikayama}{Hashimoto
  et~al.}{2013}]{Hashimoto}
Hashimoto, K., Miwa, M., Tsuruoka, Y., \BBA\ Chikayama, T. \BBOP 2013\BBCP.
\newblock \BBOQ Simple Customization of Recursive Neural Networks for Semantic
  Relation Classification.\BBCQ\
\newblock In {\Bem Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2013)}, \mbox{\BPGS\ 1372--1376}.

\bibitem[\protect\BCAY{Hermann \BBA\ Blunsom}{Hermann \BBA\
  Blunsom}{2013}]{Hermann}
Hermann, K.~M.\BBACOMMA\ \BBA\ Blunsom, P. \BBOP 2013\BBCP.
\newblock \BBOQ The Role of Syntax in Vector Space Models of Compositional
  Semantics.\BBCQ\
\newblock In {\Bem Proceedings of the 51st Annual Meeting of the Association
  for Computational Linguistics (ACL 2013)}, \mbox{\BPGS\ 894--904}.

\bibitem[\protect\BCAY{Hiton, Osindero, \BBA\ Teh}{Hiton et~al.}{2006}]{Hinton}
Hiton, G.~E., Osindero, S., \BBA\ Teh, Y. \BBOP 2006\BBCP.
\newblock \BBOQ A Fast Learning Algorithm for Deep Belief Nets.\BBCQ\
\newblock {\Bem Neural Computation}, {\Bbf 18}, \mbox{\BPGS\ 1527--1554}.

\bibitem[\protect\BCAY{Kalchbrenner \BBA\ Blunsom}{Kalchbrenner \BBA\
  Blunsom}{2013}]{Kalchbrenner}
Kalchbrenner, N.\BBACOMMA\ \BBA\ Blunsom, P. \BBOP 2013\BBCP.
\newblock \BBOQ Recurrent Continuous Translation Models.\BBCQ\
\newblock In {\Bem Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2013)}, \mbox{\BPGS\ 1700--1709}.

\bibitem[\protect\BCAY{Krizhevsky, Sutskever, \BBA\ Hinton}{Krizhevsky
  et~al.}{2012}]{Krizhevsky}
Krizhevsky, A., Sutskever, I., \BBA\ Hinton, G.~E. \BBOP 2012\BBCP.
\newblock \BBOQ ImageNet Classification with Deep Convolutional Neural
  Networks.\BBCQ\
\newblock In {\Bem Advances in Neural Information Processing Systems 25 (NIPS
  2012)}, \mbox{\BPGS\ 1097--1105}.

\bibitem[\protect\BCAY{Lee, Grosse, Ranganath, \BBA\ Ng}{Lee
  et~al.}{2009}]{Lee}
Lee, H., Grosse, R., Ranganath, R., \BBA\ Ng, A.~Y. \BBOP 2009\BBCP.
\newblock \BBOQ Convolutional Deep Belief Networks for Scalable Unsupervised
  Learning of Hierarchical Representations.\BBCQ\
\newblock In {\Bem Proceedings of the 26th International Conference on Machine
  Learning (ICML 2009)}, \mbox{\BPGS\ 609--616}.

\bibitem[\protect\BCAY{Li, Zhao, Jiang, Zhang, Wang, Gonzalez, Valentin, \BBA\
  Sahli}{Li et~al.}{2013}]{Li}
Li, L., Zhao, Y., Jiang, D., Zhang, Y., Wang, F., Gonzalez, I., Valentin, E.,
  \BBA\ Sahli, H. \BBOP 2013\BBCP.
\newblock \BBOQ Hybrid Deep Neural Network - Hidden Markov Model (DNN-HMM)
  Based Speech Emotion Recognition.\BBCQ\
\newblock In {\Bem Proceedings of Humaine Association Conference on Affective
  Computing and Intelligent Interaction (ACII 2013)}, \mbox{\BPGS\ 312--317}.

\bibitem[\protect\BCAY{Liu, Watanabe, Sumita, \BBA\ Zhao}{Liu
  et~al.}{2013}]{Liu}
Liu, L., Watanabe, T., Sumita, E., \BBA\ Zhao, T. \BBOP 2013\BBCP.
\newblock \BBOQ Additive Neural Networks for Statistical Machine
  Translation.\BBCQ\
\newblock In {\Bem Proceedings of the 51st Annual Meeting of the Association
  for Computational Linguistics (ACL 2013)}, \mbox{\BPGS\ 791--801}.

\bibitem[\protect\BCAY{Luong, Socher, \BBA\ Manning}{Luong
  et~al.}{2013}]{Luong}
Luong, T., Socher, R., \BBA\ Manning, C. \BBOP 2013\BBCP.
\newblock \BBOQ Better Word Representations with Recursive Neural Networks for
  Morphology.\BBCQ\
\newblock In {\Bem Proceedings of the 51st Annual Meeting of the Association
  for Computational Linguistics (ACL 2013)}, \mbox{\BPGS\ 104--113}.

\bibitem[\protect\BCAY{Ma, Nakao, \BBA\ Murata}{Ma et~al.}{2005}]{Ma}
Ma, Q., Nakao, K., \BBA\ Murata, M. \BBOP 2005\BBCP.
\newblock \BBOQ Single Language Information Retrieval at NTCIR-5.\BBCQ\
\newblock In {\Bem Proceedings of the Fifth NTCIR Workshop Meeting on
  Evaluation of Information Access Technologies: Information Retrieval,
  Question Answering and Cross-Lingual Information Access}, \mbox{\BPGS\
  39--43}.

\bibitem[\protect\BCAY{Ma, Tanigawa, \BBA\ Murata}{Ma et~al.}{2014}]{Ma:14}
Ma, Q., Tanigawa, I., \BBA\ Murata, M. \BBOP 2014\BBCP.
\newblock \BBOQ Retrieval Term Prediction Using Deep Belief Networks.\BBCQ\
\newblock In {\Bem Proceedings of the 28th Pacific Asia Conference on Language,
  Information and Computing (Paclic 28)}, \mbox{\BPGS\ 338--347}.

\bibitem[\protect\BCAY{内木\JBA 佐藤\JBA 駒谷}{内木 \Jetal }{2013}]{Uchiki}
内木賢吾\JBA 佐藤理史\JBA 駒谷和範 \BBOP 2013\BBCP.
\newblock 日本語クロスワードを解く：性能向上の検討.\
\newblock \Jem{2013年度人工知能学会全国大会}.

\bibitem[\protect\BCAY{Salakhutdinov \BBA\ Hinton}{Salakhutdinov \BBA\
  Hinton}{2009}]{Salakhutdinov}
Salakhutdinov, R.\BBACOMMA\ \BBA\ Hinton, G.~E. \BBOP 2009\BBCP.
\newblock \BBOQ Semantic Hashing.\BBCQ\
\newblock {\Bem International Journal of Approximate Reasoning}, {\Bbf 50}
  (7), \mbox{\BPGS\ 969--978}.

\bibitem[\protect\BCAY{Seide, Li, \BBA\ Yu}{Seide et~al.}{2011}]{Seide}
Seide, F., Li, G., \BBA\ Yu, D. \BBOP 2011\BBCP.
\newblock \BBOQ Conversational Speech Transcription Using Context-Dependent
  Deep Neural Networks.\BBCQ\
\newblock In {\Bem Proceedings of 12th Annual Conference of the International
  Speech Communication Association (INTERSPEECH 2011)}, \mbox{\BPGS\ 437--440}.

\bibitem[\protect\BCAY{Socher, Bauer, Manning, \BBA\ Ng}{Socher
  et~al.}{2013}]{Socher:13a}
Socher, R., Bauer, J., Manning, C.~D., \BBA\ Ng, A.~Y. \BBOP 2013\BBCP.
\newblock \BBOQ Parsing with Computational Vector Grammars.\BBCQ\
\newblock In {\Bem Proceedings of the 51st Annual Meeting of the Association
  for Computational Linguistics (ACL 2013)}, \mbox{\BPGS\ 455--465}.

\bibitem[\protect\BCAY{Socher, Huang, Pennington, Ng, \BBA\ Manning}{Socher
  et~al.}{2011}]{Socher:11}
Socher, R., Huang, E.~H., Pennington, J., Ng, A.~Y., \BBA\ Manning, C.~D. \BBOP
  2011\BBCP.
\newblock \BBOQ Dynamic Pooling and Unfolding Recursive Autoencoders for
  Paraphrase Detection.\BBCQ\
\newblock In {\Bem Advances in Neural Information Processing Systems 24 (NIPS
  2011)}, \mbox{\BPGS\ 801--809}.

\bibitem[\protect\BCAY{Socher, Perelygin, Wu, \BBA\ Chuang}{Socher
  et~al.}{2013}]{Socher:13b}
Socher, R., Perelygin, A., Wu, J.~Y., \BBA\ Chuang, J. \BBOP 2013\BBCP.
\newblock \BBOQ Recursive Deep Models for Semantic Compositionality Over a
  Sentiment Treebank.\BBCQ\
\newblock In {\Bem Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2013)}, \mbox{\BPGS\ 1631--1642}.

\bibitem[\protect\BCAY{Srivastava, Hovy, \BBA\ Hovy}{Srivastava
  et~al.}{2013}]{Srivastava}
Srivastava, S., Hovy, D., \BBA\ Hovy, E.~H. \BBOP 2013\BBCP.
\newblock \BBOQ A Walk-Based Semantically Enriched Tree Kernel Over Distributed
  Word Representations.\BBCQ\
\newblock In {\Bem Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2013)}, \mbox{\BPGS\ 1411--1416}.

\bibitem[\protect\BCAY{Tsubaki, Duh, Shimbo, \BBA\ Matsumoto}{Tsubaki
  et~al.}{2013}]{Tsubaki}
Tsubaki, M., Duh, K., Shimbo, M., \BBA\ Matsumoto, Y. \BBOP 2013\BBCP.
\newblock \BBOQ Modeling and Learning Semantic Co-Compositionality through
  Prototype Projections and Neural Networks.\BBCQ\
\newblock In {\Bem Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2013)}, \mbox{\BPGS\ 130--140}.

\bibitem[\protect\BCAY{Vincent, Larochelle, Bengio, \BBA\ Manzagol}{Vincent
  et~al.}{2008}]{Vincent:08}
Vincent, P., Larochelle, H., Bengio, Y., \BBA\ Manzagol, P.~A. \BBOP 2008\BBCP.
\newblock \BBOQ Extracting and Composing Robust Features with Denoising
  Autoencoders.\BBCQ\
\newblock In {\Bem Proceedings of the 25th International Conference on Machine
  Learning (ICML 2008)}, \mbox{\BPGS\ 1096--1103}.

\bibitem[\protect\BCAY{Vincent, Larochelle, Lajoie, Bengio, \BBA\
  Manzagol}{Vincent et~al.}{2010}]{Vincent:10}
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., \BBA\ Manzagol, P.~A.
  \BBOP 2010\BBCP.
\newblock \BBOQ Stacked Denoising Autoencoders: Learning Useful Representations
  in a Deep Network with a Local Denoising Criterion.\BBCQ\
\newblock {\Bem Journal of Machine Learning Research}, {\Bbf 11}, \mbox{\BPGS\
  3371--3408}.

\bibitem[\protect\BCAY{Zou, Socher, Cer, \BBA\ Manning}{Zou et~al.}{2013}]{Zou}
Zou, W.~Y., Socher, R., Cer, D.~M., \BBA\ Manning, C.~D. \BBOP 2013\BBCP.
\newblock \BBOQ Bilingual Word Embeddings for Phrase-Based Machine
  Translation.\BBCQ\
\newblock In {\Bem Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2013)}, \mbox{\BPGS\ 1393--1398}.

\end{thebibliography}


\begin{biography}
\bioauthor{馬　　　青}{
1983年北京航空航天大学自動制御学科卒業．1987年筑波
  大学大学院修士課程理工学研究科理工学専攻修了．1990年同大学院博士課程工学研究科電子・情報工学専攻修了．工学博士．郵政省通信総合研究所・独立行政法人通信総合研究所（現国立研究開発法人情報通信研究機構）主任研究官・主任研究員を経て， 2003年4月より，龍谷大学理工学部教授．自然言語処理，機械学習の研究に従事．言語処理学会，情報処理学会，電子情報通信学会，日本神経回路学会，各会員．
}
\bioauthor{谷河　息吹}{
2013年龍谷大学数理情報学科卒業．
2015年龍谷大学大学院理工学研究科数理情報学専攻修士課程修了．
人工知能学会員．
}
\bioauthor{村田　真樹}{
1993年京都大学工学部電気工学第
二学科卒業．1997年同大学院工学研究科電子
通信工学専攻博士課程修了．博士（工学）．
同年，京都大学にて日本学術振興会リサーチ
・アソシエイト．1998年郵政省通信総合研究
所入所．独立行政法人情報通信研究機構主任
研究員を経て，2010年4月より，鳥取大学大
学院工学研究科情報エレクトロニクス専攻教
授．自然言語処理，情報抽出の研究に従事．
言語処理学会，情報処理学会，電子情報通信
学会，人工知能学会，計量国語学会，各会員．
}
\end{biography}


\biodate




\end{document}
