実験 \label{sec:experiments}

本章では，提案手法の有効性を調査するために行った実験と，その結果について
報告する．



\subsection{実験データ} \label{ssec:data}

実験は，第$3$回 NTCIR ワークショップ
\footnote{http://research.nii.ac.jp/ntcir/ntcir-ws3/ws-ja.html}で構築され
たウェブ検索評価用テストセット\cite{Eguchi2002}を用いて，これを行った．テ
ストセットは，$11,038,720$ページの日本語ウェブ文書と，$47$個の検索課題か
ら成る．検索課題ごとに，約$2,000$文書に，その課題に対する適合度が付与され
ている．ただし，適合度は「高適合」「適合」「部分適合」「不適合」のいずれ
かである．これらの適合度が付与された文書を用いて，検索結果のランキング精
度を測ることができる．

図\ref{fig:ntcir_subject}に検索課題の一例を示す．各タグが表す意味内容は次
の通りである．
\begin{description}
 \itemsep=-0.1zw
 \item[NUM] 課題番号．
 \item[TITLE] 検索システムに入力するであろう単語．
課題作成者によって$2$〜$3$語がリストアップされている．左から順に重要．
 \item[DESC] 課題作成者の情報要求を一文で表したもの．
 \item[RDOC] 情報要求に適合する代表的な文書の ID．
課題作成者によって$2$〜$3$個がリストアップされている．
\end{description}

\begin{figure}[t]
 \begin{center}
\includegraphics{19-3ia942f2.eps}
\end{center}
\caption{テストセットにおける検索課題の一例}
\label{fig:ntcir_subject}
\end{figure}

実験では，$\langle$TITLE$\rangle$タグの単語をクエリとして使用した．ただし，
提案手法では，検索の質を高めるため，クエリを含む文書（クエリを構成する各
タームが最低でも$1$回以上出現する文書）のみをスコア付けの対象として収集す
る（\ref{ssec:initial_resuls_acquisition}節参照）．そのため，
$\langle$TITLE$\rangle$タグの全ての単語を用いると，多くの検索課題において，
検索される文書数が極端に少なくなってしまった．例えば，課題番号 0027，
0047，0058 などは，それぞれ$17$文書，$5$文書，$14$文書しか検索できなかっ
た．課題番号 0061 に至っては$1$文書も検索できなかった．このように検索され
る文書が少ないと，適合性フィードバックの有効性が検証しにくい．すなわち，
実際に適合性フィードバックによって初期検索結果のランキングが改善されても，
その結果が P@10 などの評価尺度の値に反映されにくく，適合性フィードバック
が有効に働いたかどうかが判断しづらい．そこで，実験では，この問題を避ける
ため，十分な検索結果が得られるように，クエリとして使用する単語を
$\langle$TITLE$\rangle$タグの最初の$2$語のみとした．ただし，「十分」の定
義は「$100$文書以上」とした．

また，$\langle$RDOC$\rangle$タグの ID が付与された文書を，ユーザのフィー
ドバックとして使用した．上で述べた通り，これらは課題作成者本人によって選
択された代表的な適合文書であり，フィードバックとして使用するのに最適と考
えられる．これらの文書は，提案手法の初期検索結果に含まれるとは限らない．
初期検索結果に含まれない場合，これらをユーザのフィードバックとして使用す
るのは奇異に感じられるかもしれない．しかし，これらの文書は，仮に初期検索
結果に含まれていた場合も，リランキング前後のランキング精度を測定・比較す
る際，結局ランキングから取り除かれる（\ref{ssec:evaluation_method}節で後
述）．言い換えれば，これらは，初期検索結果に含まれていた場合も，初期検索
結果に含まれない場合のように，検索結果中に存在していないものとして扱われ
る．このように，どちらの場合でも存在していないものとして扱われることを考
えると，これらの文書が初期検索結果に含まれているか含まれていないかは重要
ではない．以上を踏まえ，実験では，これらが初期検索結果に含まれているか含
まれていないかは問題にしなかった．

$47$個の検索課題のうち，$7$個の検索課題（課題番号: 0011, 0018, 0032,
0040, 0044, 0047, 0061）については，実験で使用しなかった．これは，上で述
べたようにクエリとして使用する単語を$2$語にしても，十分な文書（i.e.,
$100$文書）が検索できなかったためである．さらに，残った$40$課題を，開発デー
タと評価データに分けて使用した．開発データは，提案手法のパラメータを最適
化するために使用した．評価データは，提案手法のランキング精度を測定するた
めに使用した．開発データには$8$課題（課題番号：0008〜0017）を，評価デー
タには$32$課題（課題番号：0018〜0063）を使用した．



\subsection{実験用検索システム}

実験を行うため，提案手法に従って適合性フィードバックを行う検索システムを
作成した．実装の詳細は以下の通りである．

検索対象とする文書セット (i.e., $\bm{D}_{all}$) には，テストセットの
$11,038,720$文書を使用した．また，文書セット中の各文書について，次の手順
に従って文書モデルを構築した．
\begin{enumerate}
 \item Shinzato らの手法\cite{Shinzato2008}を用いて本文を抽出し，JUMAN
       \cite{Kurohashi1994}を用いて各文を解析する．
 \item 解析結果及び式(\ref{equ:dir})を用いて，DIR に基づく文書モデルを構
       築する．ただし，先行研究\cite{Zhai2001,Wei2006,Yi2009}に倣って，
       $\mu = 1,000$とした．
\end{enumerate}

クエリが与えられたら，次の手順に従ってクエリモデルを構築した．
\begin{enumerate}
 \item JUMAN を用いてクエリを解析する．
 \item 解析結果及び式(\ref{equ:mle})を用いて，MLE に基づくクエリモデルを
       構築する．
\end{enumerate}

LDA の実装については次の通りである．パラメータ$\alpha_{k}$ $(k = 1,
\dots, K)$の初期値は$1$とした．また，$\bm{\beta}_{k}$ $(k = 1, \dots,
K)$の初期値にはランダムな値を与えた．$\bm{\gamma}_{i}$と$\bm{\phi}_{i}$を
更新する際の反復回数と，$\alpha_{k}$と$\bm{\beta}_{k}$を更新する際の反復
回数は，それぞれ$10$回とした．LDA で考慮する語彙数$J$は$100$とした．ただ
し，LDA で考慮する語彙は，初期検索結果に対する重要度を基に選出した．ここ
で，初期検索結果$\bm{D}_{\bm{q}}$に対する単語$w$の重要度は，
$df(w,\bm{D}_{\bm{q}}) * \log(H / df(w,\bm{D}_{all}))$と定義した．ただし，
$df(w, \bm{D})$は$\bm{D}$における$w$の文書頻度を表す．



\subsection{ランキング精度の測定方法} \label{ssec:evaluation_method}

適合性フィードバックの効果は，適合性フィードバック前のランキング（i.e.,
初期検索結果のランキング）と，適合性フィードバック後のランキングを比較す
ることで検証できる．このとき，フィードバックとして使用する文書の扱いに気
を付けなければならない \cite{Hull1993}．

例えば，適合性フィードバック前後のランキングをそのまま比較すると，後者が
有利になってしまう．これは，フィードバックとして与えられた文書（適合であ
ることが分かっている文書）が，適合性フィードバック後のランキングの上位に
含まれやすいためである．

そこで，適合性フィードバック前後のランキングを比較する際，フィードバック
として与えられた文書を適合性フィードバック後のランキングから取り除くとい
う方法が考えられる．しかし，この方法だと，適合性フィードバック前のランキ
ングが有利になってしまう．これは，適合文書が少ないときに特に問題となる．

以上を踏まえ，実験では，ランキングの精度を測定する際，フィードバックとし
て使用した文書を各ランキングから取り除いた．これにより，適合性フィードバッ
ク前後のランキングを公平に比較することができる．

ランキング精度の評価尺度には，P@10，Mean Average Precision (MAP)，
Normalized Discounted Cumulative Gain at $10$ (NDCG@10)
\cite{Jarvelin2002} を用いた．ただし，P@10 及び MAP を測定する際は，「高
適合」「適合」「部分適合」の文書を正解，「不適合」及び適合度が付与されて
いない文書を不正解とした．また，NDCG@10 は，「高適合」の文書を$3$点，「適
合」の文書を$2$点，「部分適合」の文書を$1$点として算出した．



\subsection{リランキング性能の調査} \label{ssec:experiment1}

まず，提案手法が初期検索結果のランキング精度をどの程度改善できるか調査し
た．具体的には，初期検索結果のランキング精度と，提案手法によってリランキ
ングを行った後のランキング精度を比較し，提案手法の有効性を検証した．実験
には評価データを使用し，各検索課題の初期検索結果を取得する際は，
\ref{ssec:data}節で述べたように，$\langle$TITLE$\rangle$タグの最初の$2$単
語をクエリとして用いた．また，実験では，$initial\_score$（式
(\ref{equ:initial_score})参照）の上位$100$件を初期検索結果とした．提案手
法を実行する際は，$\langle$RDOC$\rangle$タグの最初の$2$文書をフィードバッ
クとして用いた．なお，これらの文書に含まれる単語数は平均$3,589$語であった．
提案手法に必要な$3$つのパラメータ$a$，$b$，$K$の値は，それぞれ$0.2$，
$0.9$，$50$とした．これらは，\ref{ssec:experiment0}節で述べる実験の結果を
基に決定した．

\begin{table}[b]
\vspace{-0.5\Cvs}
  \caption{リランキング性能の調査結果}
  \label{tbl:experiment1}
\input{01table01.txt}
\end{table}

結果を表\ref{sbtbl:eRF}に示す．INIT は各検索課題に対する初期検索結果のラ
ンキング精度の平均値を，OURS は提案手法実行後のランキング精度の平均値を表
す．比較のため，初期検索結果に対してベースラインとなる手法を実行したとき
の結果も示した．ZHAI は Zhai らの手法 \cite{Zhai2001}を，OURS ($a =
0.0$) は提案手法から潜在情報を除いた手法を表す．ただし，ZHAI と OURS ($a
= 0.0$) は本質的にはほとんど同じ手法である．両手法とも，フィードバックの
表層の単語分布を文書セット全体の単語分布で補正することでフィードバックモ
デルを構築し，これを用いてリランキングを行っている．違うのは単語分布の補
正の仕方だけである（前者は EM アルゴリズムを用い，後者は DIR を用いて補正
を行っている）．OURS ($a = 0.0$) では，$b = 0.5$とした．これも，
\ref{ssec:experiment0}節で述べる実験の結果を基に決定した．

DIC もベースラインとなる手法を表す．提案手法の核となるアイディアは，テキ
スト（フィードバック及び検索結果中の各文書）に潜在的に現れうる単語の情報
を適合性フィードバックに利用することである．同義語辞書や関連語辞書などの
知識リソースを用いても，同様のアイディアを実現することができる．DIC では，
OURS ($a = 0.0$) をベースに，テキスト中の各単語が同義語を持つ場合，その同
義語もそのテキストに出現しているとみなした上でリランキングを行った．ただ
し，同義知識は，Shibata らの手法 \cite{Shibata2008}を用いて例会小学国語辞
典 \cite{Tajika2001}と岩波国語辞典 \cite{Nishio2002}から獲得した．獲得さ
れた同義知識（e.g.,「コンピュータ」＝「電子計算機」，「ポテト」＝「じゃが
芋」＝「ばれいしょ」）は$4,597$個であった．

表\ref{sbtbl:eRF}を見ると，すべての尺度において，OURS が INIT を大きく上
回っている．例えば P@10 は$27.6\%$改善しており，提案手法が初期検索結果を
うまくリランキングできたことが分かる．また，提案手法は，ZHAI や OURS ($a
= 0.0$) より高い性能を示した．ZHAI や OURS ($a = 0.0$) は，テキストの表層
情報だけを用いて適合性フィードバックを行っている．一方，提案手法は，テキ
ストの表層情報に加え，テキストの潜在情報も用いて適合性フィードバックを行っ
ている．提案手法がこれらの手法を上回ったことから，潜在情報が適合性フィー
ドバックに有用であったことが分かる．

さらに，リランキング結果を調査したところ，提案手法が，テキストに表層的に
は出現しないが潜在的には現れうる単語の情報をうまく利用していることが確認
できた．図\ref{fig:ntcir_subject}の検索課題を例に取ると，「宗教」や「祝日」
「聖書」などの単語は，情報要求によく関連するが，フィードバックとして使用
した文書には含まれていなかった．そのため，ZHAI や OURS ($a = 0.0$) では，
これらの単語の情報を使用することができなかった．一方，提案手法では，これ
らの単語がフィードバックにおいてもある程度の確率で現れうると推定できた．
具体的には，「宗教」「祝日」「聖書」は，それぞれ$0.0046$，$0.0037$，
$0.0024$の確率で現れうると推定できた．なお，フィードバックに$1$回出現した
単語として「クリスマス」や「ＥＡＳＴＥＲ」などがあったが，これらの生起確
率の推定値は，それぞれ$0.0093$，$0.0060$であった．提案手法では，これらの
推定結果を用いることで，これらの単語を含む検索結果中の適合文書を上位にリ
ランキングすることができた．

DIC はあまり有効に機能せず，その結果は ZHAI や OURS ($a = 0.0$) の結果を
少し上回る程度であった．この原因は，我々が構築した同義語辞書のカバレッジ
にあると思われる．DIC は，よりカバレッジの高い知識リソースが利用できれば
（同義語や関連語などの知識をより多く利用できれば），より有効に機能する可
能性を持つ．しかし，そのようなリソースを構築するのは容易ではない．一方，
提案手法でも，単語と単語が関連するという知識を必要とする．しかし，DIC と
違って，何のリソースも必要としない．すなわち，提案手法では，LDA を用いる
ことで，単語と単語が関連するという知識を検索結果から動的に獲得することが
できる．\ref{sec:introduction}章の「マック{\textvisiblespace} 価格」 とい
うクエリを例に取ると，このクエリに対する検索結果には「CPU」や「ハードディ
スク」「ハンバーガー」「ポテト」などの単語が含まれると考えられる．提案手
法では，検索結果に対して LDA を実行することで，「CPU」と「ハードディスク」
が関連するという知識や「ハンバーガー」と「ポテト」が関連するという知識を，
トピックという形で動的に獲得することができる．そして，獲得された知識を用
いることで，文書に「ハードディスク」という単語が出現していなくても，
「CPU」という単語が出現していれば，「ハードディスク」も潜在的にはその文書
に現れうると推測できる．このように，DIC と比べると，（カバレッジの高低に
関わらず）何のリソースも必要としないという点で，提案手法の方が優れている．

提案手法は擬似適合性フィードバックにも適用可能である．そこで，これに対す
るリランキング性能も調査した．擬似適合性フィードバックでは，初期検索結果
の上位$n$文書を適合文書とみなし，適合性フィードバックを行う．実験では，
$n = 10$として初期検索結果をリランキングし，リランキング前後のランキング
精度を比較した．ただし，擬似適合性フィードバックでは，明示的なフィードバッ
ク（適合であることが分かっている文書）は存在しない．そのため，ランキング
の精度を測る際，他の実験のように，$\langle$RDOC$\rangle$タグの文書を各ラ
ンキングから除くことはしなかった．

結果を表\ref{sbtbl:pRF}に示す．INIT の値が表\ref{sbtbl:eRF}と違うのは，ラ
ンキング精度を算出する際，$\langle$RDOC$\rangle$タグの文書を除いていない
からである．表\ref{sbtbl:pRF}を見ると，普通の適合性フィードバックに比べる
と改善の度合いは小さいが，P@10 や NDCG@10 の値が上昇している．例えば，
P@10 では$8.2\%$の改善が見られる．このことから，擬似適合性フィードバック
においても提案手法がある程度機能することが分かる．


\subsection{フィードバックが少ない状況でのリランキング性能}
\label{ssec:experiment2}

現実的には，ユーザが多くのフィードバックを与えてくれるとは考えにくい．そ
のため，適合性フィードバックの手法は，フィードバックが少ない状況でも機能
するべきである．この実験では，このような状況をシミュレートし，フィードバッ
クが少なくても提案手法が機能するかを調査した．具体的には，提案手法に与え
るフィードバックを少しずつ減らしていき，リランキング性能がどのように変化
するかを調査した．提案手法に与えるフィードバックの分量$G$は，$G = 2^{1},
2^{0}, 2^{-1}, \dots, 2^{-5}$ とした．ただし，例えば$G = 2^{1}$は，フィー
ドバックとして$2$文書を用いることを意味している．また，例えば$G =
2^{-1}$ は，フィードバックとして$1$適合文書の半分だけを用いることを意味し
ている．この場合，適合文書中の単語をランダムに半分抽出し，それらを用いて
適合性フィードバックを行った．$G < 1$ の場合も調査したのは，フィードバッ
クとして文書より小さい単位（e.g., 文書のタイトル，スニペット）が与えられ
た場合を想定し，このような場合にも提案手法が機能するかを調べたかったから
である．

\begin{figure}[t]
 \begin{center}
  \includegraphics{19-3ia942f3.eps}
 \end{center}
  \caption{$G$によるリランキング性能の変化}
  \label{fig:F}
\end{figure}


結果を図\ref{fig:F}に示す．比較のため，提案手法から潜在情報を除いたとき
(i.e., OURS ($a = 0.0$)) の性能の変化も示した．また，INIT は初期検索結果
のランキング精度を表す．図から，$G$が小さいときでも，提案手法が高い性能を
示すことが分かる．例えば$G = 2^{0}$のとき，提案手法は初期検索結果を
$24.5\%$改善している．さらに，$G = 2^{-5}$ のときでも，$5.3\%$ の改善が見
られた．なお，$G = 2^{-5}$のとき，フィードバック$\bm{F}$に含まれる単語数
は平均$57$語であった．一方，OURS ($a = 0.0$) を見ると，$G$が小さくなるに
つれ，ほとんど改善が見られなくなった．OURS ($a = 0.0$) ではテキストの表層
情報しか利用していない．そのため，$G$が小さくなるにつれて利用できる情報が
少なくなり，初期検索結果を改善できなくなったと考えられる．一方，提案手法
では，表層情報だけでなく潜在情報も利用している．利用できる情報が多い分，
$G$が小さいときでも，初期検索結果のランキングを改善することができたと考え
られる．



\subsection{パラメータとリランキング性能の関係} \label{ssec:experiment0}

提案手法には$3$つのパラメータ$a$，$b$，$K$がある．$a$は
$P^{DIR}_{\bm{d}_{i}}(\cdot)$ と$P^{LDA}_{\bm{d}_{i}}(\cdot)$の混合比を調
整するパラメータ（式(\ref{equ:hdm})及び式(\ref{equ:hfm})参照），$b$は
$P^{MLE}_{\bm{q}}(\cdot)$と$P^{HYB}_{\bm{F}}(\cdot)$の混合比を調整するパ
ラメータ（式(\ref{equ:nqm})参照），$K$は LDA のトピック数である．
\ref{ssec:experiment1}節及び\ref{ssec:experiment2}節で述べた実験では，
OURS のパラメータを$a = 0.2$，$b = 0.9$，$K = 50$とした．また，OURS ($a
= 0.0$)のパラメータを$b = 0.5$とした．これらの値は予備実験の結果を基に決
定した．

提案手法の性能を最大限に発揮するためには，パラメータとリランキング性能の
関係について知る必要がある．予備実験では，この関係を知るため，様々な$(a,
b, K)$の組み合わせについて提案手法のリランキング性能を調査し，その結果を
比較した．ただし，$a = 0.0, 0.1, \dots, 1.0$，$b = 0.0, 0.1, \dots, 1.0$，
$K = 10, 20, \dots, 100$とし，全$1,210$通りの組み合わせについて，調査を行っ
た．開発データを用いて調査した．

ある$(a, b, K)$の組み合わせに対するリランキング性能は，他の実験と同じよう
にして，これを測定した．すなわち，開発データ中の各検索課題について初期検
索結果を取得し，提案手法を用いてこれらをリランキングした後，全課題におけ
る P@10 の平均値を算出した．他の実験と同様，クエリには
$\langle$TITLE$\rangle$タグの最初の$2$単語を，フィードバックには
$\langle$RDOC$\rangle$タグの最初の$2$文書を用いた．

\begin{table}[b]
  \caption{$(a, b)$とリランキング性能の関係}
  \label{tbl:experiment0}
\input{01table02.txt}
\end{table}

結果を表\ref{tbl:experiment0}及び図\ref{fig:K}に示す．表
\ref{tbl:experiment0}は，実験結果を$(a, b)$についてまとめたものである．表
中の各セルの値は，各$(a, b)$の組み合わせについて，各$K$の P@10 を平均した
ものである．例えば，$(a, b) = (0.1, 0.2)$のセルは，$(a, b, K) = (0.1,
0.2, 10),$ $(0.1, 0.2, 20), \dots, (0.1, 0.2, 100)$の P@10 の平均値が
$0.286$であったことを示している．各列においてもっとも P@10 が高いセルは，
その値を太字で装飾した．また，各行においてもっとも P@10 が高いセルは，そ
の値に下線を引いた．

表から，$(a, b) = (0.1, 0.9)$ or $(0.2, 0.9)$のとき，リランキング性能がもっ
とも良いことが分かる．また，$a = 0.0$のとき（潜在情報を考慮しないとき）は，
$b$が大体$0.3$〜$0.5$のとき，リランキング性能が良い．一方，$a \geq
0.1$のとき（潜在情報を考慮したとき）は，$b$が大体$0.8$〜$1.0$のとき，リ
ランキング性能が良い．$a = 0.0$のときより，性能が良くなる$b$の値（及びそ
のときのランキング精度）が大きくなっている．これは，潜在情報を考慮するこ
とで，フィードバックモデルの信頼度が増すことを示唆している．

\begin{figure}[t]
 \begin{center}
  \includegraphics{19-3ia942f4.eps}
 \end{center}
  \caption{$K$によるリランキング性能の変化}
  \label{fig:K}
\end{figure}

図\ref{fig:K}は，$K$によるリランキング性能の変化を示している．図では，表
\ref{tbl:experiment0}においてリランキング性能が良かった$3$つの$(a, b)$の
組み合わせ$(a, b) = (0.1, 0.9),\ (0.2, 0.9),\ (0.3, 0.9)$について，$K$によ
る性能の変化を示した．図から，$K$が大体$50$〜$70$のとき，リランキング性
能が良いことが分かる．

以上の結果をまとめると，提案手法がその性能を発揮するパラメータは，$(a,
b) = (0.1, 0.9)$ or $(0.2, 0.9)$，$K$は大体$50$〜$70$となる．



\subsection{LDA の実行時間} \label{ssec:computation_time}

提案手法では，検索結果中の各文書に対する$P^{LDA}_{\bm{d}_{i}}(\cdot)$を構
築するため，検索結果に対して LDA を実行する．また，フィードバックに対する
$P^{LDA}_{\bm{F}}(\cdot)$を構築する際は，フィードバックに対して LDA を実
行する．本節では，これらの処理に要する時間について考察する．

実験では，各検索課題の検索結果（$100$文書）に対して LDA（Perl と C を組み
合わせて実装）を実行するのに，$13.1$〜$16.0$秒を要した．この程度の時間
であれば，提案手法を実行する上で，問題にはならない．適合性フィードバック
は，(1) システムによる検索結果の提示，(2) ユーザによる検索結果の閲覧，適
合文書の選択，(3) 適合文書を用いた検索結果のリランキングという三つのステッ
プから成る．ここで，一般的に考えて，(2) には$1$分以上はかかると思われる．
従って，まずユーザに検索結果を提示し，ユーザが検索結果を閲覧している裏で
LDA を実行するようなシステムの構成を採れば，(3) に移る前に LDA の実行を終
えることができる．このように，検索結果が$100$文書程度であれば，LDA の実行
時間は問題にならない．

一方，検索結果は，より大きくなり得る．検索結果が大きくなると，LDA の実行
時間も大きくなってしまう．これを解決する一つの方法は，ランキングの上位だ
けを検索結果とすることである．例えば，多くの文書が検索されても，上位
$100$文書だけを検索結果とすれば，上述の通り，LDA の実行時間は問題にならな
い．別の方法として，変分パラメータの推定を並列化することも考えられる．
LDA の実行時間は，変分パラメータの推定に要する時間が多くを占める．ここで，
各文書に対する変分パラメータは，他の文書に対する変分パラメータと独立であ
る．従って，各文書に対する変分パラメータの推定を並列化し，LDA の実行時間
を削減することができる．例えば，Nallapati らは，$50$ノードのクラスタを用
いることで LDA の実行時間を$14.5$倍高速化できたと報告している
\cite{Nallapati2007}．提案手法でも並列化を取り入れることで，LDA の実行時
間を削減することができると思われる．

最後に，フィードバックに対して LDA を実行するのに要した時間を報告する．こ
れは$1$秒にも満たないものであった．例えば，フィードバックが$2$文書の場合，
実行に要した時間は，わずか$0.1$〜$0.2$秒であった．従って，フィードバッ
クに対する LDA の実行時間も問題にはならない．



