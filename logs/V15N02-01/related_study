ここでは，本稿の基礎として，クラス所属確率を推定する代表的な方法であるPlattの方法および，Zadroznyらにより提案されたビニングによる方法とIsotonic回帰による方法について述べる．
これらはいずれも2値分類を想定しているが，Isotonic回帰による方法においては，2値分類を多値分類に対応させる方法についても述べる．
最後に，Plattの方法とIsotonic回帰による方法について，多種類の分類器とデータセットによる実験を行って比較したCaruanaらによる研究[CITE]について述べる．
Platt [CITE]は，分類器をSVMに限定し，分類スコアを事例に対してクラスが予測された際の分離平面からの距離[MATH]として，シグモイド関数[MATH]により[0,1]区間に変換される値[MATH]をクラス所属確率の推定値として用いることを提案した．
ただし，パラメータ[MATH]および[MATH]は，あらかじめ最尤法により推定しておく必要がある．
シグモイド関数による方法の利点は，分類スコアから直接，クラス所属確率の推定値を求めることができるため，パラメータ[MATH]および[MATH]が推定されていれば，手続きが容易であることである．
Plattは，シグモイド関数の過学習を避けるために，out-of-sampleモデルを用いて，Reuters [CITE]を含む5種類のデータセットを用いて実験を行い，この方法の有効性を示した．
データセットがAdultの場合における結果を図[REF_Platt] [CITE]に示す．
図[REF_Platt]において，[MATH]軸は分類スコア，[MATH]軸はクラス所属確率を表し，[MATH]印は分類スコアを0.1の区間に分けた場合に対応するクラス所属確率の実測値，実線は推定値を表す．
しかし，Bennett [CITE]は，Plattの方法は分類器がナイーブベイズの場合にうまくいかないことをReuters 21,578データセットにより示した.
また，Zadroznyら[CITE]も，この方法がデータセットによっては適合しない場合があることを示し，以下に述べる方法を提案した．
Zadroznyらは，分類器としてナイーブベイズを想定し，ビニングによる方法（ヒストグラム法）を提案した[CITE].
ビニングによる方法は，未知の事例のクラス所属確率を直接推定せずに，あらかじめ作成しておいた「ビン」を参照し，そのビンにある正解率を用いて間接的に推定を行う方法である．
ビニングによる方法における処理手順は次の通りである．
まず，訓練事例を分類スコアの値順に並べ，各区間に属する事例数が等しくなるように区切ってビンを決める．
このとき，各ビンに属する事例の分類スコアから，そのビンに所属する事例における分類スコアの最大値と最小値を調査しておく．
ここまでの処理を図[REF_bining]に示す．
図[REF_bining]はナイーブベイズ分類器の例で，数値（斜体）は分類スコアを表す．
次に，各ビンごとに正解の事例を数えてそのビンに属す全事例数で割り，正解率を計算する（表[REF_bining1]を参照のこと）．
最後に，未知の事例の分類スコアから該当するビンを見つけ，そのビンの正解率を未知の事例のクラス所属確率値とする．
実験はKDD'98データセットを用いて行われ，平均二乗誤差や平均対数損失による評価の結果，有効性が示された（ビンの数が10個の場合）．
ビニングによる方法は処理が単純であるという利点があるが，最適なビンの個数をどのようにして決めればよいか（各ビンに含まれる事例数をいくつにするか）という問題がある．
なお，Zadroznyらは，この後に，誤分類に対するコストを考慮した方法として，ビニングによる方法を改良した「Probing」という方法を提案したが，実験の結果，有効性を示さない場合も多かった[CITE].
Zadroznyらは，ビニングによる方法の問題点を解決する方法として，次には，分類スコアと正解率が単調非減少な関係にあるという観察に基づくIsotonic回帰による方法を提案した[CITE].
ここで，Isotonic回帰問題とは，実数の有限集合[MATH]が与えられたとき，制約条件[MATH]の下で目的関数[MATH]を最小化する2次計画問題である[CITE].
ただし，[MATH]は正値重みを表す．
Isotonic回帰問題の解法としては，PAV (pool-adjacent violatorsまたはpair-adjacent violators)アルゴリズム（以下では，PAVと略す）が最も代表的であり[CITE], Zadroznyらが提案したIsotonic回帰による方法もPAVが適用されている．
ここで，PAVとは，単調非減少ではないブロックがある場合に，そのブロック内に存在する値のすべてをブロック内の値の平均値で置き換える処理を繰り返すことにより，全体の単調非減少性を保つ方法である．
例えば，前述の目的関数において重みがすべて1のとき，{1, 3, 2, 4, 5, 7, 6, 8}において，まず{3, 2}のブロックが単調非減少ではないために，ブロック内のすべての値を平均値2.5で置き換えて{1, 2.5, 2.5, 4, 5, 7, 6, 8}に修正する．
次に，{7, 6}のブロックが単調非減少ではないために，同様に平均値6.5で置き換えて{1, 2.5, 2.5, 4, 5, 6.5, 6.5, 8}に修正する方法である[CITE].
PAVを用いたIsotonic回帰による方法も，ビニングによる方法と同様に，最初に訓練事例を分類スコア順にソートする必要があるが，事例をまとめて扱わずに，各事例に対して正解率（正例の場合は1,負例の場合は0となる）を付ける点が異なる（図[REF_Isotonic]における開始時点の表を参照のこと）．
正解率が分類スコアと単調非減少な関係になるまで正解率の修正を繰り返し，最終的に定まった値を正解率とする（図[REF_Isotonic]における終了時点の表を参照のこと）．
図[REF_Isotonic]では1回修正された値が再度修正されることはなかったが，値の並び方によっては再修正される可能性が高く，一般的には何度も修正が繰り返される場合が多い[CITE].
実験は，ナイーブベイズ分類器とSVMにおいてKDD'98データセットなどを用い，ビニングによる方法やシグモイド関数による方法と比較された（ビニングの数は5個から50個まで変えて行われた）．
平均二乗誤差による評価の結果，PAVによる方法はビニングによる方法を常に上回ったが，シグモイド関数による方法との差は少しであった．
Zadroznyらは，次に，多値分類においては，分類器は各々の予測クラスに対して分類スコアを1つずつ出力すると仮定し，多値分類におけるPAVの効果を調査した．
すなわち，2値分類においてPAVにより推定したクラス所属確率値を統合した場合と，PAVを用いずに推定した値を統合した場合との比較を行った[CITE].
Zadroznyらは，この実験の前に，あらかじめ，ナイーブベイズ分類器とブーステッドナイーブベイズにおいて20 Newsgroupsデータセットなどを用いた実験を行って，2値分類への分解法であるall-pairsとone-against-allの間で精度の差がないことを確認し，実験ではすべてone-against-allを用いた．
2値分類における推定値を統合する方法としては，one-against-allに対応した正規化の方法の他に，どちらの分解方法にも対応可能な最小2乗法による方法や対数損失を最小化するカップリングの方法が用いられたが，正規化の方法が最もよい結果を示した．
PAVの有効性については，まず，ナイーブベイズ分類器とブーステッドナイーブベイズによりデータセットPendigitを用いた実験の結果，分類器や統合する方法に関係なく，平均二乗誤差による評価では改善がみられたが，エラー率による評価ではほとんど改善されなかった．
次に，ナイーブベイズ分類器によりデータセット20 Newsgroupsを用いた実験結果も，多値分類への統合方法に関係なく，平均二乗誤差による評価では改善がみられたが，エラー率による評価ではほとんど改善されなかった．
ここで，2値分類における推定値の3種類の統合方法を比較すると，ナイーブベイズ分類器による値をPAVにより修正した値を正規化する方法がよかったが（平均二乗誤差により評価した場合），他の分類器や評価法においては差がなかった．
なお，Zadroznyらは，この後さらに提案したProbingとよばれるクラス所属確率の推定方法を多値分類へ拡張する場合には，ここで述べた統合方法を用いずに，one-against-allにより分解した各組において2値分類として推定した値をそのまま用いるという非常に単純な方法を示した[CITE].
ただし，この方法に対する評価実験は行っていない．
Caruanaら[CITE]は，アンサンブル学習を含めた10種類の分類器（SVM,ニューラルネット，決定木，k近傍法，bagged trees, random forests, boosted trees, boosted stumps,ナイーブベイズ分類器，ロジスティック回帰）を，8種類のデータセット（UCI Repositoryから4種類，医療分野から2種類選んだデータセット，IndianPine92データセット[CITE], Stanford Linear Accelerator）に適用し，Plattの方法とIsotonic回帰による方法(PAV)の比較を行った．
その結果，Plattの方法はデータが少ないとき（約1,000サンプル未満）に効果的であり，Isotonic回帰による方法は過学習しない程度に十分なデータがあるときによかった．
Jonesら[CITE]は，検索を成功させるために，ユーザが入力したクエリから新しくクエリを生成して置き換えるというタスクにおいて，置き換えられたクエリの正確さの程度を予測するために確信度スコアが必要であると考え，Isotonic回帰による方法(PAV)とシグモイド関数による方法についての簡単な比較実験を行った．
その結果，Isotonic回帰による方法は過学習の問題があり，平均二乗誤差および対数損失のいずれにおいてもシグモイド関数による方法の方が上回ったため，彼らのタスクではシグモイド関数による方法が採用された．
