統合的確率言語モデル
\label{sec:model}

まず，本論文で一貫して用いる記号について説明する．

\begin{itemize}
\item 入力文字列 $A=a_1,\cdots,a_m$
\item $A$ を生成する単語列 $W=w_1,\cdots,w_n$
\item $W$ を生成する品詞列 $L=l_1,\cdots,l_n$
\item $L$ を生成する構文構造 $R$
\end{itemize}
本研究では，形態素解析と構文解析を同時に取り扱うことを仮定する．
すなわち，入力文字列$A$ が与えられたときに，
その正しい単語列$W$，正しい品詞列$L$，
正しい構文構造$R$ を求めることを目的とする．
例えば，「彼女がパイを食べた」という入力文に対する
解析結果の候補の例を図\ref{fig:examsent} に示す．

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(91,37)


    \caption{例文``彼女がパイを食べた''とその解析結果}
    \label{fig:examsent}
  \end{center}
\end{figure}

各解析結果の候補に対してその生成確率$P(R,L,W,A)$ を計算し，
これが最大の解析結果を選択することによって
曖昧性解消を行う．
さらに，確率モデル$P(R,L,W,A)$ を以下のように分解する．
\begin{equation}
  \label{eq:model-org}
  P(R,L,W,A) = P(R) \cdot P(L|R) \cdot P(W|L,R) \cdot P(A|W,L,R)
\end{equation}
ここで，構文構造$R$ は最終的に品詞列$L$ を生成するものと
仮定すると，$P(L|R) = 1$ となる(図\ref{fig:examsent} 参照)．
また，単語列$W$ が決まれば入力文字列$A$ は一意に決まるので，
$P(A|W)=1$ となる．
したがって，式(\ref{eq:model-org})は以下のように簡略化できる．
\begin{equation}
  \label{eq:model}
  P(R,L,W,A) = P(R) \cdot P(W|R)
\end{equation}
本研究では，式(\ref{eq:model})に示した通り，
解析結果の生成確率を以下の2つの確率モデルの積として計算する．
\begin{enumerate}
\item 構文モデル$P(R)$

  構文構造$R$ の生成確率である．
  この確率モデルには構文的な統計情報を反映させる．

\item 語彙モデル$P(W|R)$

  構文構造$R$ が与えられたときに，
  それから単語列$W$ を生成する確率である．
  この語彙モデルには語彙的な統計情報を反映させる．
\end{enumerate}
\subsection{構文モデル$P(R)$}
\label{sec:syn-model}

構文モデルとしては，構文的な統計情報を反映し，
かつ構文構造$R$ の生成確率を高い精度で推定するものであれば，
どのような確率モデルを利用してもよい．
構文モデルに利用できる確率モデルとしては，
PCFGや確率一般化LR法(Probabilistic Generalized LR Method，
以下PGLR)
などが挙げられる．

我々は，PGLRを構文モデルの有力な候補として考えている．
PGLRとは，構文解析手法のひとつである一般化LR法を拡張したものである．
PGLRは，LR表に記述された各状態遷移の遷移確率を推定し，
その遷移確率の積によって1つの状態遷移列，すなわち
それに対応する構文構造の生成確率を与えるモデルである\footnote{
  一般化LR法に確率を組み込む試みには様々なものが
  あるが~\cite{wright:90:a,ng:91:a,briscoe:93:a}，
  本研究におけるPGLRとはInuiらによるモデル~\cite{inui:97:d,virach:97:c}
  を指す．
  }．
このPGLRはPCFGに比べて，次のような特長を持つ~\cite{inui:97:d}．

\begin{itemize}
\item 文脈依存性を取り扱うことができる．
\item 隣接する品詞間の共起関係を取り扱うことができる．
\item 距離に関する優先度を取り扱うことができる．
\end{itemize}
ここで，隣接する品詞間の共起関係とは，
品詞bi-gramのような品詞列の出現に関する統計情報であり，
形態素解析の曖昧性解消に有効であると考えられる．
また，距離に関する優先度とは，
単語はなるべく近い単語に係りやすいといった，
係り受け関係にある単語間の距離に関する統計情報である．
\subsection{語彙モデル$P(W|R)$}
\label{sec:lex-model}

語彙モデルは，品詞列$L$ を末端とする構文構造$R$ が与えられたときに，
それから単語列$W$ を生成する確率である．
この語彙モデルは，式(\ref{eq:lex-model-org})のような
各単語$w_i$ の生成確率の積として計算することができる．
\begin{equation}
  \label{eq:lex-model-org}
  P(W|R) = \prod_{w_i} P(w_i | R,w_1,\cdots,w_{i-1})
\end{equation}
例えば，図\ref{fig:examsent} の例において，
単語を文の後ろから順番に生成していくと仮定すると，
語彙モデル$P(W|R)$ は以下のような単語の生成確率の積として計算できる．
\begin{eqnarray}
  P(W|R) &=& P(彼女,が,パイ,を,食べ,た|R) \\
  \label{eq:der1-ta}
  &=& P(た|R) \cdot \\
  \label{eq:der1-tabe}
  && P(食べ|R,た) \cdot \\
  \label{eq:der1-o}
  && P(を|R,食べ,た) \cdot \\
  \label{eq:der1-pai}
  && P(パイ|R,を,食べ,た) \cdot \\
  \label{eq:der1-ga}
  && P(が|R,パイ,を,食べ,た) \cdot \\
  \label{eq:der1-kanojo}
  && P(彼女|R,が,パイ,を,食べ,た)
\end{eqnarray}
\subsubsection{単語生成文脈}
\label{eq:lexical-context}

式(\ref{eq:lex-model-org})の各項
(図\ref{fig:examsent} の例では
式(\ref{eq:der1-ta})〜(\ref{eq:der1-kanojo}))
のパラメタ空間は非常に大きく，
これを直接学習することは一般に不可能である．
ところが，各単語$w_i$ の生成に強く影響するのは
各項の確率の前件$R,w_1,\cdots,w_{i-1}$ 全てではなく，
その一部のみであると考えられる．
例えば，図\ref{fig:examsent} の例文において，
``パイ''は動詞``食べ''のヲ格の格要素となっている．
このとき，``パイ''という単語を生成する際には，
式(\ref{eq:der1-pai})の前件``$R,を,食べ,た$''
(図\ref{fig:lc-pai} の斜線部)のうち，
品詞$N$ と単語``を'',``食べ''
(図\ref{fig:lc-pai} の丸で囲まれた部分)によって
十分近似できると期待できる(式(\ref{eq:lc-pai1}))．
\begin{equation}
  \label{eq:lc-pai1}
  P(パイ | R, を, 食べ, た) \simeq P(パイ | N[\slot{食べ}{を}])
\end{equation}

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
    \atari(78,29)
    
    \caption{``パイ''を生成するときの単語生成文脈}
    \label{fig:lc-pai}
  \end{center}
\end{figure}

\noindent
式(\ref{eq:lc-pai1})において，$N[\slot{食べ}{を}]$ は，
``食べ''という動詞のヲ格の格要素となっている
名詞を表わしている．
すなわち，$P(パイ | N[\slot{食べ}{を}])$ は，
``食べ''という動詞のヲ格の格要素となっている名詞から
``パイ''という単語が生成される確率を表わしている．
したがって，式(\ref{eq:lc-pai1})には，
``パイ''という単語そのものがどれくらい出現しやすいかといった
単語の出現頻度と，
``パイ''と``食べ''がどの程度共起しやすいかといった
単語の共起関係が反映されている．

ここで，単語生成文脈$c_i$ を以下のように定義する．
\begin{quote}
  単語$w_i$ の単語生成文脈$c_i$ とは，
  $w_i$ の生成確率の前件$R,w_1,\cdots,w_{i-1}$ から
  $w_i$ の生成に強く影響する部分のみを取り出したものである．
\end{quote}
先ほどの例においては，単語``パイ''の単語生成文脈は
``$\slot{食べ}{を}$''である．
そして，
各単語$w_i$ の生成確率の前件``$R,w_1,\cdots,w_{i-1}$''を，
その単語の品詞$l_i$ と単語生成文脈$c_i$ に縮退することにより，
語彙モデル$P(W|R)$ を以下のように近似する．
\begin{eqnarray}
  \nonumber
  P(W|R) &=& \prod_{w_i} P(w_i | R,w_1,\cdots,w_{i-1}) \\
  \label{eq:lex-model}
  &\simeq& \prod_{w_i} P(w_i | l_i[c_i])
\end{eqnarray}
\subsubsection{単語生成文脈決定規則}
\label{sec:lc-rule}

単語生成文脈を導入する際に問題となるのは，
どのような単語に対して
どのような単語生成文脈を選べばよいのかということである．
我々は，これを人手で作成した規則によって記述する．
以下，単語$w_i$ の単語生成文脈$c_i$ を決定する規則を
単語生成文脈決定規則と呼ぶ．

単語生成文脈決定規則の例を以下に挙げる．

\begin{itemize}
\item 単語の共起関係を全く考慮しない場合

  単語$w_i$ について，
  周囲の単語との従属関係を考慮しない場合には，
  その単語の生成確率はその単語の品詞$l_i$ のみに依存するとみなす．
  例えば，図\ref{fig:examsent} の例において，
  助動詞``た''と動詞``食べ''を生成する際に
  他の単語との語彙的従属関係を考えない場合には，
  それぞれの生成確率(\ref{eq:der1-ta}),(\ref{eq:der1-tabe})は
  以下のように近似すればよい．
  \begin{eqnarray}
    \label{eq:der2-ta}
    P(た|R) &\simeq& P(た|AUX) \\
    \label{eq:der2-tabe}
    P(食べ|R,た) &\simeq& P(食べ|V)
  \end{eqnarray}

  これに対応した単語生成文脈決定規則を以下に示す．
  この規則は単語生成文脈を決定する際のデフォルト規則でもある．

  \bigskip
  \begin{lcdef}{lc:default}
    単語$w_i$ を生成する際に他の単語との従属関係を考慮しない場合には，
    単語$w_i$ の単語生成文脈$c_i$ を空とする．
  \end{lcdef}
  \bigskip

\item 格要素となる名詞が助詞を介して動詞に係る際の従属関係を
  考慮する場合

  前述のように，格要素となる名詞が助詞を介して動詞に係る際には，
  動詞・助詞の組と名詞との間には語彙的従属関係が存在する．
  このような語彙的従属関係を確率モデルに反映させるために
  \lcrule{lc:filler} を定義する．
  
  \bigskip
  \begin{lcdef}{lc:filler}
    単語$w_i$ の品詞$l_i$が$N$(名詞)であり，
    かつ助詞$p$ を介して動詞$v$ に係っているとき，
    単語$w_i$ の単語生成文脈$c_i$ を
    $\slot{v}{p}$ とする．
    このとき，$w_i$ の生成確率$P(w_i | N[\slot{v}{p}])$ は
    動詞$v$ の格$p$ の格要素となる名詞$N$ から
    単語$w_i$ が生成される確率を表わす．
  \end{lcdef}
  \bigskip

  例えば，図\ref{fig:examsent} の例において，
  名詞``パイ''は動詞``食べ''のヲ格の格要素であり，
  名詞``彼女''は動詞``食べ''のガ格の格要素となっている．
  したがって，これらの単語を生成する際にはこの規則が適用され，
  それぞれの生成確率(\ref{eq:der1-pai}),(\ref{eq:der1-kanojo})は
  以下のように近似される．
  \begin{eqnarray}
    \label{eq:der2-pai}
    P(パイ|R,を,食べ,た) &\simeq& P(パイ|N[\slot{食べ}{を}]) \\
    \label{eq:der2-kanojo}
    P(彼女|R,が,パイ,を,食べ,た) &\simeq&
    P(彼女|N[\slot{食べ}{が}])
  \end{eqnarray}

\item
  助詞とその係り先用言の従属関係，格間の従属関係を考慮する場合

  図\ref{fig:examsent} の例文においては，
  2つの助詞``が''と``を''が動詞``食べ''に係っている．
  このとき，
  これらの生成確率(\ref{eq:der1-o}),(\ref{eq:der1-ga})を
  以下のように近似しても，
  助詞とその係り先用言との間の語彙的従属関係，および
  同じ用言に係る助詞同士の従属関係(以下，これを格間の従属関係と呼ぶ)を
  語彙モデルに反映させることができる．
  \begin{eqnarray}
    \label{eq:der2-o}
    P(を|R,食べ,た) &\simeq&
    P(を|P[\head{食べ}{2}{\phi_1,\phi_2}]) \\
    \label{eq:der2-ga}
    P(が|R,パイ,を,食べ,た) &\simeq&
    P(が|P[\head{食べ}{2}{\phi_1,を}])
  \end{eqnarray}

  式(\ref{eq:der2-o})は，助詞$P$ が
  2つの助詞の係り先となっている動詞``食べ''に係っているときに，
  品詞$P$ から単語``を''が生成される確率を表わしている．
  一方式(\ref{eq:der2-ga})は，助詞$P$ が
  2つの助詞の係り先となりかつそのうちの1つは``を''である
  動詞``食べ''に係っているときに，
  品詞$P$ から単語``が''が生成される確率を表わしている．

  助詞とその係り先用言の従属関係，および
  格間の従属関係を語彙モデルに導入するために，
  \lcrule{lc:marker} を以下のように定義する．

  \bigskip
  \begin{lcdef}{lc:marker}
    単語$w_i$ の品詞$l_i$が$P$(助詞)であり
    かつ用言$h$ に係っているとき，
    単語$w_i$ の単語生成文脈$c_i$ を
    $\head{h}{n}{\phi_1,\cdots,\phi_j,p_{j+1},\cdots,p_n}$ とする．
    このとき，$w_i$ の生成確率
    $P(w_i | P[\head{h}{n}{\phi_1,\cdots,\phi_j,p_{j+1},\cdots,p_n}])$は，
    用言$h$ が$n$ 個の助詞の係り先となりかつ
    用言に近い$p_{j+1},\cdots,p_n$ の助詞が既に生成されているときに，
    $\phi_j$ として$w_i$ が生成される確率を表わす．
  \end{lcdef}
  \bigskip

  \lcrule{lc:marker} において，同じ用言に係る助詞は
  用言に近いものから順番に生成されると仮定している．
  すなわち，助詞が出現する順序も考慮されている．


\item 助詞の係り先が用言か体言かを考慮する場合

  助詞の係り先が用言である場合と体言である場合とでは，
  助詞の生成確率$P(w_i | P)$ の分布は著しく異なると考えられる．
  例えば，係り先が用言の場合には``が''，``を''などの
  助詞は出現しやすいが，助詞``の''は出現しにくい．
  これに対して，係り先が体言の場合，
  すなわちその助詞を含む文節が連体修飾節となっている場合には，
  助詞``の''が出現する場合が圧倒的に多いと予想される．
  したがって，助詞の生成確率$P(w_i | P)$ を学習する際に，
  その助詞の係り先が用言もしくは体言であるかを区別しないで
  学習するのは望ましいことではない．
  これに対応するには，
  以下のような\lcrule{lc:josi} を定義すればよい．

  \bigskip
  \begin{lcdef}{lc:josi}
    単語$w_i$ の品詞$l_i$が$P$(助詞)であり，
    かつその助詞の係り先が体言であるとき，
    単語$w_i$ の単語生成文脈$c_i$ を{\it nd\/} とする．
    {\it nd\/} はその助詞の係り先が体言であることを表わす
    シンボルである．
    このとき，$w_i$ の生成確率$P(w_i | P[nd])$は，
    体言を係り先とする助詞から
    単語$w_i$ が生成される確率を表わす．
  \end{lcdef}
  \bigskip
  助詞の単語生成文脈を決定する際には，
  助詞の係り先が用言である場合には\lcrule{lc:marker} が，
  助詞の係り先が体言である場合には\lcrule{lc:josi} が適用される．
\end{itemize}

ここに挙げた\lcrule{lc:default}〜\#\ref{lc:josi} が
単語生成文脈を決定するための全ての規則というわけではない．
本節では，特に用言の格関係に注目して語彙モデルに反映させるべき
語彙的従属関係(単語の共起関係)の例を挙げたが，
他の種類の語彙的従属関係を語彙モデルに反映させるように
単語生成文脈決定規則を拡張・洗練することもできる．
すなわち，
語彙モデルにおいてどのような語彙的従属関係を考慮するかは，
単語生成文脈決定規則の追加・変更によって柔軟に調整することが
可能である．

単語生成文脈として何を選択するかを自動的に
学習することも考えられる\footnote{
  例えば，
  Magerman は確率の前件としてどのような素性を選択すれば
  よいのかを決定木を用いて自動学習している~\cite{magerman:95:a}．
  }が，
我々は言語学的知見に基づくヒューリスティクス規則によって
単語生成文脈を選択する方向で研究をすすめている．
なぜなら，語彙モデルにどのような種類の語彙的従属関係を反映させるかを
単語生成文脈決定規則によって明確に記述することにより，
モデルに反映された統計情報が曖昧性解消に有効であるかどうかなど，
モデルの特性の分析を容易に行うことができるからである．
\subsubsection{従属係数}
\label{sec:dp}

これまでは単語を生成する際に考える単語生成文脈は
常に一つであると仮定していた．
しかしながら，一般には，一つの単語を生成する際に
複数の単語生成文脈を考慮しなければならない場合もある．
\begin{figure}[tbp]
  \begin{center}
    \leavevmode
    \atari(106,30)

    \caption{並列構造を持つ例文}
    \label{fig:exam-coord}
  \end{center}
\end{figure}
例えば，図\ref{fig:exam-coord} の例文において，
2つの文節``食べ-て''\footnote{
  ``-''は単語の区切りを表わす．
  }と``出かけ-た''は並列の関係にある．
したがって，この例文中の名詞``彼女''は動詞``食べ''の
ハ格の格要素であり\footnote{
  本研究では，名詞が助詞を介して用言に係る場合は常に，
  その名詞を用言の表層格の格要素として取り扱う．
  }，
同時に動詞``出かけ''のハ格の格要素でもある．
したがって，\lcrule{lc:filler} に従えば，
``彼女''という単語を生成する際の単語生成文脈としては
$\slot{食べ}{は}$ と$\slot{出かけ}{は}$ の2つがある．
このとき，``彼女''の生成確率は次のように推定することが望ましい．
\begin{equation}
  \label{eq:der-cor1-kanojo}
  P(彼女 | N[\slot{食べ}{は},\slot{出かけ}{は}])
\end{equation}
同様に，この例文中の助詞``は''は
動詞``食べ''と``出かけ''の両方に係っているとみなすことができる．
したがって，\lcrule{lc:marker} に従えば，
``は''という単語を生成する際の単語生成文脈として
$\head{食べ}{2}{\phi_1,を}$ と$\head{出かけ}{2}{\phi_1,へ}$ の
2つがあると考えられ，
``は''の生成確率も次のように推定することが望ましい．
\begin{equation}
  \label{eq:der-cor1-wa}
  P(は | P[\head{食べ}{2}{\phi_1,を},\head{出かけ}{2}{\phi_1,へ}])
\end{equation}

ところが，
式(\ref{eq:der-cor1-kanojo})や(\ref{eq:der-cor1-wa})の
ように複数の単語生成文脈を前件に持つ確率モデルは，
推定するパラメタの数が爆発的に増大する可能性がある．
そこで本研究では，複数の単語生成文脈を以下のように取り扱う．
まず，説明を簡略化するために，
単語$w_i$ が2つの単語生成文脈$c_1$ と$c_2$ を持つとする．
このとき，単語$w_i$ の生成確率$P(w_i | l_i[c_1,c_2])$ を
以下のように近似する．
\begin{eqnarray}
  P(w_i|l_i[c_1,c_2])&=&
  \frac{P(l_i[c_1,c_2]|w_i) \cdot P(w_i)}{P(l_i[c_1,c_2])} \\
  \label{eq:prove-dp-1}
  &=&
  \frac{P(l_i[c_1]|w_i) \cdot P(l_i[c_2]|l_i[c_1],w_i) \cdot P(w_i)}
       {P(l_i[c_1]) \cdot P(l_i[c_2]|l_i[c_1])} \\
  \label{eq:prove-dp-2}
  &\simeq&
  \frac{P(l_i[c_1]|w_i) \cdot P(l_i[c_2]|l_i,w_i) \cdot P(w_i)}
       {P(l_i[c_1]) \cdot P(l_i[c_2]|l_i)} \\
  &=&
  \frac{P(l_i[c_1]|w_i)}{P(l_i[c_1])} \cdot
  \frac{P(l_i[c_2]|l_i,w_i)}{P(l_i[c_2]|l_i)} \cdot P(w_i) \\
  &=&
  \frac{P(w_i|l_i[c_1])}{P(w_i)} \cdot
  \frac{P(w_i,|l_i,l_i[c_2])}{P(w_i|l_i)} \cdot P(w_i) \\
  \label{eq:prove-dp}
  &=&
  P(w_i|l_i)\cdot
  \frac{P(w_i|l_i[c_1])}{P(w_i|l_i)}\cdot
  \frac{P(w_i|l_i[c_2])}{P(w_i|l_i)}
\end{eqnarray}
式(\ref{eq:prove-dp-1})から式(\ref{eq:prove-dp-2})の変形において，
2つの単語生成文脈$c_1$ と$c_2$ は互いに独立であると
仮定している．
\begin{eqnarray}
   P(l_i[c_2]|l_i[c_1]) &\simeq& P(l_i[c_2]|l_i)\\
   P(l_i[c_2]|l_i[c_1],w_i) &\simeq& P(l_i[c_2]|l_i,w_i)
\end{eqnarray} 

ここで，従属係数$D(w_i|l_i[c_i])$ を
式(\ref{eq:def-dp})のように定義する．
\begin{equation}
  \label{eq:def-dp}
  D(w_i|l_i[c_i]) = \frac{P(w_i|l_i[c_i])}{P(w_i|l_i)}
\end{equation}
この従属係数を用いれば，
式(\ref{eq:prove-dp})から式(\ref{eq:double-dp})が導かれる．
\begin{eqnarray}
  \label{eq:double-dp}
  P(w_i|l_i[c_1,c_2]) &\simeq&
  P(w_i|l_i)\cdot D(w_i|l_i[c_1])\cdot D(w_i|l_i[c_2])
\end{eqnarray}
以上では単語$w_i$ が2つの単語生成文脈を持つ場合を考えていたが，
単語$w_i$ が$n$ 個の単語生成文脈$c_1,\cdots,c_n$ を持つ場合にも
同様の近似が可能であり，最終的に以下の式が得られる．
\begin{equation}
  \label{eq:multi-dp}
  P(w_i | l_i[c_1,\cdots,c_n]) \simeq
  P(w_i | l_i) \cdot \prod_{c_i} D(w_i | l_i[c_i])
\end{equation}

式(\ref{eq:def-dp})で定義した従属係数$D(w_i|l_i[c_i])$ は
単語$w_i$ と単語生成文脈$c_i$ の相関関係を評価する統計量である．
例えば，$w_i$ と$c_i$ に相関関係がない場合，
すなわち$w_i$ と$c_i$ が互いに独立である場合には，
式(\ref{eq:def-dp})の分子$P(w_i|l_i[c_i])$ は
分母$P(w_i|l_i)$ にほぼ等しくなり，
従属係数$D(w_i|l_i[c_i])$ は1に近い値を取る．
これに対し，$w_i$ と$c_i$ に正の相関関係がある場合には，
単語生成文脈$c_i$ を前件に加えた確率$P(w_i|l_i[c_i])$ は
単語生成文脈$c_i$ を無視した確率$P(w_i|l_i)$ よりも
大きくなるので，その従属係数は1より大きい値を取る．
同様に，$w_i$ と$c_i$ に負の相関関係がある場合には
従属係数は1より小さい値を取る．

複数の単語生成文脈$c_1,\cdots,c_n$ の下での
単語$w_i$ の生成確率は，
単語生成文脈を無視した単語の生成確率$P(w_i | l_i)$ と，
$w_i$ と$c_i$ の相関関係を他の単語生成文脈とは独立に
評価した従属係数$D(w_i|l_i[c_i])$ の積によって
計算できることを式(\ref{eq:multi-dp})は示している．
従属係数$D(w_i|l_i[c_i])$ を他の単語生成文脈と
独立に推定・学習することにより，
確率モデルのパラメタ空間を推定可能な大きさに抑制することができる．
例えば，
図\ref{fig:exam-coord} の例において，
``彼女''の生成確率(\ref{eq:der-cor1-kanojo})と
``は''の生成確率(\ref{eq:der-cor1-wa})は
それぞれ以下のように推定される．
\begin{eqnarray}
  \nonumber
  && P(彼女 | N[\slot{食べ}{は},\slot{出かけ}{は}]) \\
  \label{eq:der-cor2-kanojo}
  &\simeq& P(彼女 | N) \cdot
  D(彼女 | N[\slot{食べ}{は}) \cdot 
  D(彼女 | N[\slot{出かけ}{は}]) \\[3mm]
  \nonumber
  && P(は | P[\head{食べ}{2}{\phi_1,を},\head{出かけ}{2}{\phi_1,へ}]) \\
  \label{eq:der-cor2-wa}
  &\simeq& P(は | P) \cdot 
  D(は | P[\head{食べ}{2}{\phi_1,を}]) \cdot
  D(は | P[\head{出かけ}{2}{\phi_1,へ}])
\end{eqnarray}

従属係数を導入する利点として，
単語生成文脈を複数取り扱うことができるという点の他に，
式(\ref{eq:lex-model-der-dp})に示すように，
語彙モデル$P(W|R)$ を
単語の出現頻度のみを反映した$P_{cf}(W|L)$ と
単語の共起関係のみを反映した$D(W|R)$ との積に
分解できるという点が挙げられる．
\begin{eqnarray}
  P(W|R) &\simeq& \prod_{i} P(w_i | l_i[C_{w_i}]) \\
  &\simeq& \prod_{w_i} P(w_i|l_i) \cdot 
      \prod_{c_{ij} \in C_{w_i}} D(w_i | l_i[c_{ij}]) \\
  \label{eq:lex-model-der-dp}
  &=& P_{cf}(W|L) \cdot D(W|R) \\[3mm]
  \label{eq:lex-model-der}
  P_{cf}(W|L) &=& \prod_{w_i} P(w_i|l_i) \\[3mm]
  \label{eq:lex-model-dp}
  D(W|R) &=&
  \prod_{w_i} \prod_{c_{ij} \in C_{w_i}} D(w_i | l_i[c_{ij}])
\end{eqnarray}
上式において，$C_{w_i}$ は単語$w_i$ の単語生成文脈の集合を
表わしている．

式(\ref{eq:lex-model-der})の統計量$P_{cf}(W|L)$ は，
単語生成文脈を無視したときに
品詞$l_i$ から単語$w_i$ が生成される確率の積であり，
単語の出現頻度に関する優先度が反映される．
これに対し，
式(\ref{eq:lex-model-dp})の統計量$D(W|R)$ は
各単語$w_i$ とその単語生成文脈$c_{ij}$ の従属係数の積を
表わしており，
$w_i$ と$c_{ij}$ の相関関係に関する優先度
(すなわち単語の共起関係)が反映される．
このように，
語彙モデルを単語の出現頻度，および
単語の共起関係のみを反映させた2つの統計量の積として
分解することにより，
\ref{sec:intro} 節で述べたように，
曖昧性解消時におけるそれぞれの統計情報の働きを
容易に理解することができる．
