はじめに
\label{sec:intro}

コーパス，辞書，シソーラスなどの機械可読な言語データの
整備が進んだことから，
自然言語処理における様々な問題の解決に
何らかの統計情報を利用した研究が盛んに行われている．
特に構文解析の分野においては，構文的な統計情報だけでなく，
単語の出現頻度や単語の共起関係といった語彙的な統計情報を利用して
解析精度を向上させた研究例が数多く報告されている
\cite{schabes:92:a,magerman:95:a,hogenout:96:a,li:96:a,charniak:97:a,collins:97:a}．
ここで問題となるのは，このような語彙的な統計情報を
構文的な統計情報とどのように組み合わせるかということである．
このとき，我々は以下の2つの点が重要であると考える．

\begin{itemize}

\item
  解析結果の候補に与えるスコアが，
  構文的な統計情報のみを反映したスコアと
  語彙的な統計情報のみを反映したスコアから
  構成的に計算できること

  このことによる利点を以下に挙げる．
  \begin{itemize}
  \item 個々の統計情報を個別に学習できる

    構文的な統計情報を学習する際には，
    学習用言語資源として比較的作成コストの高い
    構文構造が付加されたコーパスが必要となる\footnote{
      Inside-Outsideアルゴリズム\cite{lari:90:a} に代表されるような
      EMアルゴリズムを用いて，
      構文構造が付加されていないコーパスから
      構文的な統計情報を学習する研究も行われている．
      しかしながら，
      このような教師なしの学習は一般に精度が悪く，
      現時点では構文構造が付加されたコーパスを利用した方が
      品質の良い統計情報を学習できると考えられる．
      }．
    しかしながら，
    推定パラメタの数はそれほど多くはないので，
    比較的少ないデータ量で学習することができる．
    これに対して，語彙的な統計情報は，
    単語の共起に関する統計情報を学習しなければならないために
    大量の学習用データを必要とするが，
    構文構造付きコーパスに比べて作成コストの低い
    品詞付きコーパスを用いても学習することが十分可能である．
    このように，
    統計情報の種類によって学習に要する言語資源の質・量は大きく異なる．
    そこで，構文的な統計情報と語彙的な統計情報を
    異なる言語資源を用いて個別に学習できるように，
    それぞれの統計情報の独立性を保持しておくことが望ましい．

  \item
    曖昧性解消時における個々の統計情報の働きを
    容易に理解することができる

    例えば，曖昧性解消に失敗した場合には，
    構文的な統計情報と語彙的な統計情報を独立に取り扱うことにより，
    どちらの統計情報が不適切であるかを容易に判断することができる．
  \end{itemize}

\item 
  個々の統計情報を反映したスコアが確率的意味を持っていること

  構文的な統計情報を反映したスコアと
  語彙的な統計情報を反映したスコアを組み合わせて
  全体のスコアとする場合，
  両者のスコアの和を計算すればいいのか，
  積を計算すればいいのか，
  またどちらか片方に重みを置かなければならないのかなど，
  その最適な組み合わせ方は自明ではない．
  このとき，個々のスコアが確率的意味を持つように学習することにより，
  確率の積としてそれらを自然に組み合わせることができる．
\end{itemize}

ところが，語彙的な統計情報を利用して構文解析の精度を向上させる
過去の研究の多くは以上の条件を満たしていない．
例えば田辺らは，確率文脈自由文法
(Probabilistic Context Free Grammar, 以下PCFG)における
書き換え規則の非終端記号に，
その非終端記号が支配する句の主辞となる単語を付加すること
(以下，これをPCFGの語彙化と呼ぶ)によって
語彙的従属関係をPCFGの確率モデルに反映させる方法を
提案している~\cite{tanabe:95:a}．
一方，英語を対象にPCFGを語彙化した研究としては
Hogenoutら~\cite{hogenout:96:a}，
Charniak~\cite{charniak:97:a}，Collins~\cite{collins:97:a} に
よるものがある．
しかしながら，
PCFGの語彙化によって構文的な統計情報と
語彙的な統計情報を組み合わせる方法は，
非終端記号に単語を付加することによって規則数が組み合わせ的に増大し，
推定するパラメタ数も非常に多くなるといった問題点がある．
また，構文的な統計情報と語彙的な統計情報を同時に学習する
モデルとなっているが，
先ほど述べたように両者は独立に学習できることが望ましい．
PCFGをベースとしないSPATTERパーザ~\cite{magerman:95:a}や
SLTAG~\cite{schabes:92:a,resnik:92:b}にも同様の問題が存在する．
これらの研究は語彙的な統計情報を利用して解析精度の向上を
図ってはいるが，
構文的な統計情報と独立に学習する枠組にはなっていない．

構文的な統計情報と語彙的な統計情報を独立に学習する枠組としては
Liによるものが挙げられる~\cite{li:96:a,li:96:b}．
Liは，解析結果の候補$I$ に対して，
構文的な統計情報を反映させた確率モデル$P_{syn}(I)$ と
単語の共起関係を反映させた確率モデル$P_{lex}(I)$ を
別々に学習する方法を提案している．
そして，
語彙的な制約は構文的な制約に優先するといった心理言語学原理に基づき，
まず$P_{lex}(I)$ を$I$ のスコアとして用い，
一位とそれ以外の候補のスコアの差が十分に大きくなかった場合に限り
$P_{syn}(I)$ をスコアとして用いている．
すなわち，構文的な統計情報と語彙的な統計情報を
それぞれ独立に学習してはいるが，
これらを同時に利用して曖昧性解消を行っているわけではない．
また，この2つのスコアの持つ確率的意味が不明確であり\footnote{
  $P_{syn}(I)$，$P_{lex}(I)$ は確率と呼ばれてはいるが，
  どのような事象に対する確率なのかは不明である．
  }，
その最適な組み合わせ方は自明ではない．


本研究では，
構文的な統計情報と語彙的な統計情報を組み合わせる一方法として，
統合的確率言語モデルを提案する~\cite{inui:97:b,inui:97:e,sirai:96:a}．
この統合的確率言語モデルの特徴は，
単語の出現頻度，および単語の共起関係といった
2つの語彙的な統計情報を局所化し，
構文的な統計情報と独立に取り扱う点にある．
また，構文的な統計情報を構文構造の生成確率として，
語彙的な統計情報を単語列の生成確率としてそれぞれ学習し，
これらの積を解析結果の候補に対するスコアとすることにより，
曖昧性解消に両者を同時に利用することができる．
この統合的確率言語モデルの詳細については\ref{sec:model} 節で述べる．
\ref{sec:exp-stat} 節ではこの統合的確率言語モデルの学習，
およびそれを用いた日本語文の文節の係り受け解析実験について述べる．
最後に\ref{sec:conclusion} 節で結論と今後の課題について述べる．
