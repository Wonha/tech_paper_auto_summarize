-- 抽出した段落の一部(3つの内最後)
提案手法は，分割確率最大化という観点からテキスト分割を定式化した．
これに類似の手法として，訓練データを利用したテキスト分割では，[CITE]が隠れマルコフモデルに基づいて，複数ニュースを個々のニュースに分割しているが，訓練データを利用しないテキスト分割では，類似の研究はない．
また，[CITE]についても，彼等は，テキストの分割確率を直接扱っているのではなく，各単語を生起させるようなトピックを単語毎に求め，同一トピックの単語が連続する部分を同一トピックとする，という間接的アプローチをとっている．
そのため，彼等のアプローチでは，たとえば，トピックの平均の長さなどを直接取り込むことが難しい．
一方，我々のアプローチでは，このことは素直に表現できる．
たとえば，[CITE]と同様に，トピックの長さ[MATH]が，平均長[MATH]，標準偏差[MATH]の正規分布[MATH]に従うと仮定すると，単純な拡張としては，([REF_eq:cS_i])式を，[MATH]として，以下のようにすれば，トピックの長さが平均と同じくなるような分割が優先される．
更に，彼等の手法と我々の手法との大きな違いは，彼等が単語の確率を訓練データから推定しているのに対して，我々は，単語の確率を分割対象のテキストから推定している点である．
なお，訓練データが利用可能な場合に，彼等の手法と我々の手法とを比較することは興味深いであろう．
その場合には，上式で示したような，トピックの長さをコスト関数として取り込むことや，種々の手がかり表現をコスト関数に取り込むことも検討したい．

-- 正解 --
正解文1）本稿で述べる手法も，これらの従来手法と同様 
  に，訓練データを利用せずに，テキスト内の単語分布の
  みを利用してテキストを分割する．
正解文2）本稿で述べる手法は，テキストの分割確率が最
  大となるような分割を選択するというものである．
正解文3）しかし，そのような方法は，訓練データが利用で
  きない分野については適用できないので，我々の目的で
  ある，テキスト分割の結果を利用して，長い文書を要約し
  たり，講演のディクテーション結果を要約するためのテキ
  スト分割手法としては適さない．
正解文4）提案手法は，分割確率最大化という観点からテ
  キスト分割を定式化した．
正解文5）これに類似の手法として，訓練データを利用した
  テキスト分割では，[CITE]が隠れマルコフモデルに基づ
  いて，複数ニュースを個々のニュースに分割しているが，
  訓練データを利用しないテキスト分割では，類似の研究
  はない．
正解文6）そのため，彼等のアプローチでは，たとえば，トピ
  ックの平均の長さなどを直接取り込むことが難しい．
正解文7）一方，我々のアプローチでは，このことは素直
  に表現できる．
正解文8）たとえば，[CITE]と同様に，トピックの長さ[
  MATH]が，平均長[MATH]，標準偏差[MATH]の正規分
  布[MATH]に従うと仮定すると，単純な拡張としては，
  ([REF_eq:cS_i])式を，[MATH]として，以下のようにす
  れば，トピックの長さが平均と同じくなるような分割が優
  先される．
正解文9）更に，彼等の手法と我々の手法との大きな違い
  は，彼等が単語の確率を訓練データから推定しているの
  に対して，我々は，単語の確率を分割対象のテキストか
  ら推定している点である．
正解文10）そのために，我々は，本稿では，大域的な最小
  コスト解よりも細かい分割が必要な場合には，再帰的な
  分割を適用し，それは有効ではあったが，より有効な分
  割方法を考えることは今後の課題としたい．

-- 抽出した4つの文 --
文1）これから分かるように，最小コスト解よりも粒度の細
  かい分割が必要なときには，再帰的分割をした方が精度
  良く分割ができる．
  SCORE: 1.20056017646601, idx: 199

文2）これに類似の手法として，訓練データを利用したテキ
  スト分割では，[CITE]が隠れマルコフモデルに基づい
  て，複数ニュースを個々のニュースに分割しているが，訓
  練データを利用しないテキスト分割では，類似の研究は
 ない．
  SCORE: 1.24540356317881, idx: 202

文3）たとえば，[CITE]と同様に，トピックの長さ[MATH]
  が，平均長[MATH]，標準偏差[MATH]の正規分布
  [MATH]に従うと仮定すると，単純な拡張としては，
  ([REF_eq:cS_i])式を，[MATH]として，以下のようにす
  れば，トピックの長さが平均と同じくなるような分割が優
  先される．
  SCORE: 1.23286529151691, idx: 206

文4）更に，彼等の手法と我々の手法との大きな違いは，
  彼等が単語の確率を訓練データから推定しているのに
  対して，我々は，単語の確率を分割対象のテキストから
  推定している点である．
  SCORE: 1.20800656629584, idx: 207

