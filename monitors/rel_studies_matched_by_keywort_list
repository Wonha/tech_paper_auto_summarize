
************************ [./logs/V03N02-02/related_study] ************************
関連研究

ダイジェストの自動生成を実現する中心技術は，サマリ抽出にある．
本システムでは，記事のサマリとして，記事のカテゴリとサマリ文を抽出した．
これらに関連する研究は，主に，テキスト分類と要約という分野において
研究されてきた．

\subsection{テキスト分類}

テキスト分類とは，ひとまとまりのテキスト(文献，ニュース記事等)を，
その内容に基づいて，分類することである．
通常，あらかじめカテゴリ集合が与えられ，
その中から適切なカテゴリを割り当てることを行なう．

Construe-TIS \cite{Construe-TIS-91}は，
英語の新聞記事を対象とする分類システムで，
キーワードから概念を認識し，認識した概念を組み合わせて最終的なカテゴリを決
定する．このシステムは，キーワードとその前後の文脈情報という表層的な手がか
りを利用し，かなりよい精度(90\%程度)で新聞記事を分類することができる．

一方，Thinking Machine Corporationは，Memory-Based Reasoningを用いて，
Dow Jonesのニュース記事を分類するシステムを開発している\cite{Masand-92}．
このシステムは，すでに分類済みの5万件のニュース記事を用いて，
再現率約80\%，正解率約70\%で，分類コード割り当てを行なうことができる．

これらのシステムは，いずれも英語を対象としたシステムであり，
日本語を対象としたテキスト分類は，それほど試みられていない．
また，前節で述べたように，これらの分類は，いずれもテキストの
内容(分野)による分類であり，主題(目的)による分類は，ほとんど研究されていな
い．

\subsection{要約}

要約とは，あるひとまとまりのテキスト(例えば，論文)が表している意味内容を，
非常に短いテキストで簡潔に表現することを言う．ここ1,2年，日本語を対象とし
た要約研究がいくつか行なわれている．

原ら\cite{Hara-ipsj-nlp-94}は，
複雑な言語解析を避け，項目名と特徴という表層的な情報を利用すること
で，特許広報の抄録を作成する方法を提案している．
一方，GREEN\cite{Yamamoto-nlp-95}は，論説文を対象とした要約システムである．
このシステムは，現状で利用可能な談話要素を取り込み，重要な文を抜き出すこと
とその文から修飾句を削減することによって要約を生成する．

我々の立場は，前者と近いが，以下の二点において今までの研究と異なる．
第一に，ネットニュースのダイジェストの生成の際に必要となる
サマリは，通常の要約よりも非常に短いという点である．
我々がダイジェストに求める機能は，「情報(記事)が必要であるか，
不必要であるか判定できること」であり，これを満たすならば，サ
マリは短ければ短いほど好ましいと考える．
第二に，対象としているテキストの品質が多様であるという点である．
これまでの研究が対象としてきたテキストは，特許広報や新聞の論説記事など
高品質なテキストである．これらのテキストの品質に対し，
ネットニュースの記事のテキスト品質はかなり低い．

\subsection{fj.meetingsダイジェストとの違い}

ネットニュースのダイジェスト自動生成システムは，本システム以外に，
筆者らが先に実現したfj.meetingsのダイジェスト自動生成システム
\cite{Madoka-master-94,Madoka-ipsj-conf-94,Madoka-ipsj95}がある．
このシステムと本システムとの大きな違いは，サマリとして抽出する情報と，
その抽出法にある．

fj.meetingsのダイジェスト作成では，会告記事から，その会議の名称(タイトル）
，開催期日，開催場所，論文締切期日といった情報項目を，
その記事のサマリとして抽出する．
このように，抽出すべき情報をあらかじめ限定できるのは，
対象とする記事が会議に関する記事に限定されるからである．
このため，サマリ抽出には，いわゆる情報抽出の手法を用いることができる．
fj.meetingsのサマリ抽出では，センタリング，箇条書といったスタイル情報と
抽出する情報項目に特有な言語表現パターンを組み合わせて利用する．

これに対して本システムのダイジェスト作成では，
各記事から，その記事の内容を端的に表す1文(サマリ文)を抽出する．
これは，fj.wantedの記事から抽出すべき情報項目を，
fj.meetingsの会告記事のように限定することができないからである．
このため，サマリ抽出の手法は，情報抽出よりは要約に近い形となる．
本システムでは，主に言語表現パターンを利用して，サマリ文を見つける方法を
とっている．


************************ [./logs/V03N03-04/related_study] ************************
関連研究との比較
\label{sec:comparison}

KGW+p\cite{Tsujii88}は，拘束規則に基づいてすべての可能な部分構造を生成
する機構と，優先規則に基づいて構造の良さを比較し，それらの間に有意な差
が生じたときに，一部の構造を選択する機構から構成されている．
KGW+pでは，解析のある時点で優先されなかった部分構造が選択されるのは，
優先された部分構造を構成要素とする構造が生成できなかった場合に限られて
おり，局所的な選択の積み重ねで解析を進める一種のビーム探索が行なわれて
いる．
このため，生成された全体構造が可能な構造のうち最も適切なものであること
が保証されない．
これに対し，本手法では保証される． 

シフト／レデュース法において，1)シフト操作とレデュース操作の適用に競合
が生じた場合には，シフト操作を優先させ，2)レデュース操作同士の競合が生
じた場合には，右辺がより長い構文規則の適用を優先させる，という二つのメ
タレベルの優先方略に従い，右連合(right association)や最小付加(minimal
attachment)などの英語における選好を反映した構造を生成する手法が提案さ
れている\cite{Shieber83}．
また，日本語文の構文的特徴が左枝分かれ構造であることに着目し，これを反
映する構造を最初に生成するために，上記の優先方略を変更した手法も示され
ている\cite{Shimazu89}． 
これらの手法では，優先方略が探索機構の中に組み込まれているため，右連合
や最小付加，左枝分かれ構造以外の構造を優先する必要が生じた場合，探索機
構自体を変更しなければならない．これに対し，本手法では，探索機構と規則
記述の枠組みが分離されているので，探索機構には手を加えずに，構文規則の
適用費用を修正するだけで，優先すべき構造を柔軟に変更できる．
例えば，名詞句の左枝分かれ構造を優先したい場合，本手法では，構文規則
$\NP \rightarrow \NP/1\ \NP/2,\ \ 1$を用いればよい．
この規則に従って生成される不活性弧を図\ref{fig:leftright} に示す．
括弧内の数値が生成費用である．
\begin{figure}
\begin{center}
\epsfile{file=leftright.eps}
\vspace{-0.5mm}
\end{center}
\caption{左枝分かれ構造と右枝分かれ構造}
\label{fig:leftright}
\vspace{-0.5mm}
\end{figure}
逆に，右枝分かれ構造を優先したい場合は，右辺の第一項と第二項の重みを入
れ換えればよい．
また，本手法では，優先すべき構造をメタレベルの優先方略に従って選択する
手法と異なり，個々の規則に付与された費用に基づいて選択するので，優先す
べき構造をきめ細かく指定できる．

PAMPS\cite{Uehara83}は，構文規則に付与された費用に基づいて，優先すべき
構造を選択する枠組みとなっている点では，本手法と同じである．
しかし，本手法と異なり，部分構造から全体構造を得るまでの費用の推定が行
なわれていないので，解析過程で生成される部分構造の数は，本手法で生成さ
れる数よりも多くなる可能性が高い．

確率付き構文規則は，文脈自由文法形式の規則に$0 < p_\alpha \le 1$なる実
数を規則の適用確率として付与したものである．
ただし，左辺の構文範疇が同じである各規則の適用確率の和は1でなければな
らない．
確率付き規則を用いた構文解析では，構文構造にはその構造の生成に関与した
規則の適用確率の積が付与される．
確率付き規則$\alpha \rightarrow \beta_1 \ldots \beta_m,\ \ p_\alpha$
は，適用確率$p_\alpha$をそ\\の逆数の対数$\log \frac{1}{p_\alpha}$に置き
換えれば，費用付き規則とみなせ，本稿の構文解析手法を適用することができ
る．
しかし，その逆の費用付き規則から確率付き規則への変換を行なうことはでき
ない．
確率付き規則では，左辺の構文範疇が異なる規則の間での競合が記述できない
からである．
例えば，図\ref{fig:rule} の構文規則において，規則(f)を(h)より優先させた
い場合，費用付き規則では，(f')~$\A \rightarrow \FAIL/1,\ 1$と
(h')~$\PRP \rightarrow \FAIL/1,\ 2$とすればよいが，確率付き規則では，
規則の適用確率$p_\alpha$は$0 < p_\alpha \le 1$なる実数であり，左辺の構
文範疇が同じである各規則の適用確率の和は1でなければならないので，規則
(h)の適用確率よりも大きな値を規則(f)に付与することはできない．
従って，費用付き構文規則のほうが記述力の点で優れている．

これまでに，不適格文を処理するための種々の手法が提案されている
\cite{Matsumoto94}．
ここでは，本手法が，語句の欠落や語順の誤りなどを含む構文的不適格文を効
率良く処理できることを示す．
これまでに提案されている手法の多くは，適格文用の構文規則を用いて解析を
行なう機構と，この機構による通常の解析が失敗した時点で起動される不適格
文を処理するための別の機構を備えている．
これに対し，本手法を用いれば，Fassらの手法\cite{Fass83}と同じく，適格
文と不適格文を区別せずに，両者の処理を統一的な枠組みで行なうことができ
る．
すなわち，不適格文用の構文規則を，適格文用の規則と同じように記述し，
前者の適用費用を後者のものよりも高く設定しておく．
一般に，適格文と不適格文を区別しないように構文規則を拡張すると，適格文
を解析する際に生成される部分構造の数が多くなり，効率が悪くなるという問
題が生じる．
しかし，本手法では，適用費用が高い不適格文用の規則は，適格文用の規則の
適用が失敗した場合にのみ適用される可能性が高いので，効率が悪化する恐れ
は少ないと考えられる．


************************ [./logs/V05N04-05/related_study] ************************
関連研究

CLIRのシステムは利用するリソースから以下のように分類される\cite{hull97}. 


  \noindent {\bf ・コーパスベースシステム}

コーパスベースシステムは, パラレルコーパスやコンパラブルコーパスをキュ
エリの翻訳のために用いる. LSI(Latent Semantic Indexing)は, 行列の次元
を縮退させる手法で, パラレルコーパスから言語に依存せずに, タームとドキュ
メントを表現することができる\cite{14,17}. LSIは, パラレルコーパスから
抽出したタームとドキュメントの対応をターム/ドキュメント間頻度マトリク
スに, 特異値分析法(singular value decomposition)を適用して, 次元数を縮退
し新たな空間を形成するベクトルを抽出する. LSIが, トレー
ニングコーパス以外のドメインでどの程度有効なのかは明らかではない. ETH
は, 疑似的なパラレルコーパスとシソーラスを用いて類似のキュエリタームを
拡張しながら, ドイツ語キュエリをイタリア語キュエリに変換した\cite{26}. 
シソーラスを用いた類似性判定では, タームの分布状況によって, ドキュメン
トにまたがるタームを関連つける. 拡張されたイタリア語キュエリタームが, 
イタリア語ドキュメントと照合される. この手法は, 平均適合率で単言語の検
索の約半分の精度を得たが, ドメイン依存の問題点がある. 

また, 機械翻訳のためにコンパラブルコーパスから単語レベルの訳語知識を抽
出する手法がいくつか提案されている
\cite{Fung95,Fung97,Rappo95,Kaji96,Tanaka96}. 
訳語集合抽出に活用できる
手法については比較・検討し, GDMAX法の改良のために参考としていく. 

  \noindent {\bf ・辞書ベースシステム}

辞書ベースシステムは, 対訳辞書によってタームやフレーズを翻訳しすべての
翻訳結果を結合してキュエリを生成する. SPIRITは, ターム, 複合語, イディ
オムの辞書を用いてキュエリタームを翻訳しブーリアンモデルによって検索する
\cite{23}. この辞書ベースのシステムは, 単言語検索の75-80 \%の精度を得
たが, 機械翻訳システムでは, 60-65 \% の精度であった. この性能は, ドメ
イン対応辞書を作ってキュエリをマニュアル編集した結果によるものであり, 
機械翻訳においても, ドメイン辞書を用意して比較したものである. 我々も, 
対訳辞書をドメインに適応させた後, GDMAX法に関して同様の比較実験を行な
う予定である. 辞書ベースシステムに関して, David Hullは, 自身のシステム
の実験の中でキュエリの約20\%は, キュエリ翻訳の訳語の曖昧性の問題により, 
不適切なものになっていることを報告している\cite{hull97}. 対訳辞書の改
良とともに, GDMAX法に関しても同様の分析を行なう. 

  \noindent {\bf ・ハイブリッドシステム}

ハイブリッドシステムは, キュエリ翻訳のために, 辞書, コーパス, ユーザイ
ンタラクションなどを組み合わせて用いるもので, GDMAX法は, この範疇に属
する. 日英の言語対の報告ではないが, 以下の評価結果とGDMAX法の結果を比
較検討する予定である. マサチュセッツ大学では, 翻訳前と翻訳後のフィード
バックを用いる手法を提案している\cite{1}. キュエリの拡張には, キュエリ
タームと高頻度で共起するタームを選択する自動フィードバック手法が用いら
れている. キュエリ拡張は, ソース言語で翻訳前に, ターゲット言語で翻訳後
に行なう. 翻訳前のフィードバックは, 検索結果と関係のないコーパスを用い
て, 翻訳後のフィードバックは, 検索結果のコーパスを用いて行なわれる. はじめ
は, 単言語検索の40-50 \%の精度だったものが, キュエリ拡張により, 60-65
\% まで向上した\cite{1}. 
ニューメキシコ州立大学は, 品詞タガーを使って, 同じ品
詞の訳語だけを選択するようにしている. さらに, 各タームの訳語候補の中か
ら, パラレルコーパス中のアラインメントされた文に着目して, 最も対応性の
高い訳語を選択する. CLIRの精度は, 初期翻訳では40-50 \%の精度であったが, 
この2段階の訳語絞り込みにより70-75 \% の精度となった\cite{5}. CNR は, 
コンパラブルコーパスを用いた自動キュエリ翻訳の手法を提案している
\cite{20}. ソース言語のキュエリタームは, 限られた範囲内で共起する単語集合とい
うプロファイルで表現される. ターゲット言語のキュエリタームに関しても同様のプロ
ファイルが作成される. ソース言語のプロファイルは, 対訳辞書を用いて翻訳
され, ターゲット言語のプロファイルと最も類似しているものが認定される. 
その中で上位のターゲット言語のタームが, 翻訳キュエリとして用いられる. 
この手法は, ソース言語キュエリとターゲット言語キュエリをプロファイルと
いう別の形式で表現し比較する点において, GDMAX法と類似しているが, 共起
する単語というインスタンスで表現している点で異なる. GDMAX法は, 共起頻度と
いう数値と共起の次元数も複数とれるので, より一般的な記述と考えられる. 
CNRについては, 具体的な評価結果が報告されていないが, 同様の実験を日英
に関しても行なって比較評価していきたい. 



************************ [./logs/V06N05-02/related_study] ************************
関連研究

コンピュータによる比喩理解に関する研究は最近多く見られる．
扱える比喩の範囲が広いものとしてあげられるのが，Fass\cite{Fass1991}や
Martin\cite{Martin1992}の研究などである．
Fass\cite{Fass1991}は，
比喩理解において概念の階層構造の中で共通の上位概念を持つものに着目
し，動詞や名詞に関する類似した対応関係を発見することによって
比喩理解を行っている．
Martin\cite{Martin1992}は，
比喩文を字義通りの文と同様に扱う立場をとり，
比喩文に関する明示的な知識をあらかじめ知識ベースに与えている．そして
新しい比喩にも対応できるように既知の比喩を拡張できるようにしている．
これらに対し著者らの研究では，
現在は扱える比喩の範囲が「TはVだ」の形に限定されているが，
概念の階層構造や比喩文の明示的な知識をシステムに
あらかじめ与えておかないで比喩理解を行う方法を検討している．

比喩理解のモデル化については，日本でもさまざまな手法が提案されている．
例えば，土井ら\cite{Doi1989}は，階層型のニューラルネットワークを用いて
属性層の中から比喩の意味を選択する方法を検討している．
しかし，その方法では，
顕著な属性を自動的に抽出するまでには至っていない．
諏訪ら\cite{SuwaAndMotoda1994}は，隠喩理解の類推的アプローチとして，
類比関係の効率的な決定を行う手法を示した．そこでは，
属性ではなく構造・関係が見立ての対象となる比喩を扱っ
ている．
森ら\cite{MoriAndNakagawa1991}は，状況理論から展開された視点を導入した
比喩理解のモデル化を行っているが，
そこでは，比喩理解のために重要である
属性に対する数値的な順序づけはなされていない．

また，属性が見立ての対象となる比喩に重点を置いて，属性の顕現性を
数値的に扱っている研究が進んでいる
\cite{Iwayama1992,UtsumiAndSugeno1996,UchiyamaAndItabashi1996}．
岩山\cite{Iwayama1992}は，
ベイジアンネットワーク上の確率分布を
もとに情報理論を用いて顕現性を定式化している．
その計算手法をもとに，内海ら\cite{UtsumiAndSugeno1996}は，
関連性に基づく言語解釈モデルを用いて文脈に応じた
隠喩解釈を行う手法を提案している．また，比喩理解過程において創発される
新たな特徴を考慮した計算モデルの研究も行われている\cite{Utsumi1997}．
しかし，岩山と内海らの研究では，比喩を構成する概念の属性情報として，
属性名とその重要度や，属性値集合(属性値とその確率の対)が
人手によって与えられている．著者らは，その部分の自動化を
目指している．コーパスからの共起情報を用いて比喩理解のための
知識獲得を行う手法も考えられてきているが
\cite{MasuiAndSugioAndTazoeAndShiino1997}，
ここではまず人間による生の属性情報を得るために認知心理実験を行うことにする．
内山ら\cite{UchiyamaAndItabashi1996}は，
SD法の実験を行ってその結果から顕現性を計算しているが，
被喩辞と喩辞の別々の評定値に基づいて比喩表現の顕現性を計算している
わけではない．

本論文では，
被喩辞と喩辞を同時に扱い，それらに共通で顕現性の高い属性を
自動的に抽出する方法について述べる．
その際，SD法実験のデータとして，
被喩辞と喩辞を独立に提示したときの
評定値を用いて計算を行う．



************************ [./logs/V06N05-05/related_study] ************************
関連研究
\label{sec:kanren}
類似用例提示型翻訳支援システムの提案はこれまで多くなされて
いる
\cite{Naka89,Sumi91,Tera92,Sato93,Take94,Hyou94,Kitamu96,Aoya95}
．
ここではこのようなシステムの中で著者らの検索の研究と近い研究について比
較を行う．

中村\cite{Naka89}の研究は著者らの研究の出発点になったものである．この
論文は用例
検索による翻訳支援の考え方と構成を示している．
中村はこの論文で入力表現と用例が共有する自立語の数に基づいて類似性を
計算する手法を提案している．また，検索結果の順位を次の 3 つの条件
で整列している：1）構成語
（本研究のキーワード）がその他の語をはさまない，2）用例中の自立語の個数に
対する構成語の数の比率，3）含んでいる構成語の数．
このシステムを使った小規模な評価実験では長い(複数文節)表現を入力した
場合に検索結果が「あいまい」となって被験者の評価が低くなったと報告してい
る．
これは検索結果の効果的な絞り込みの必要性を示唆した結果である．著者らはこ
れに対して語順と変位を考慮した検索を提案しその有効性を確認した．

隅田ら\cite{Sumi91}は表現辞典の用例文を検索する翻訳支援システムを提案し
ている．著者らとの主要な違いは 2 点ある．1）用例文はニュースの記事に比べて
短く入力も単文に近い用例を想定している点，2）検索では構文的な類似性を
重視している点である．類似検索は入力の自立語を順次無視して最後に付属語
列のパターンまで検索条件を緩和する手法を採用している．ここで語順は考慮
していない．

隅田らは構文情報の把握に助詞を利用している．これは短い用例を対象にした
場合に有効であるが，著者らのように長い用例を扱う場合には不適切である．な
ぜなら長い用例では表層の助詞だけで主要な構文構造を把握することができな
いからである．
また，助詞は極めて多くの場所に出現するためこれを使う処理は遅くなる問題
もある．実際著者らのシステムで助詞をキーワードに含めて検索実験を行ったところ
自立語だけを対象にする場合の約 23 倍の時間がかかることが分かった．

佐藤\cite{Sato93}は文字を連続して多く共有する 2 つの文を近いと考
えた「最適照合検索」を提案している．この論文では 1 文字を照合単位としかつ順
序を考慮した照合手法（CTM1）と 2 文字と 3 文字を照合単位とし，順序を無視
した照合手法（CTM2）を提案している．またどちらの手法も文字列が連続して
出現することを類似性の条件に含めている．

文字と単語の違いを無視すれば，著者
らの語順と変位を考えた手法が CTM1 に，語順を無視した手法が CTM2 に対応
する．またこの二つの手法の検索結果の違いを次のような実験で検討している．
1）100 個の入力をそれぞれ検索し，上位 5 つの類似表現を得る，2）その英
訳の中で最良の英訳の有用性を 4 段階で主観評価する
\footnote{利用者は一つの有用な対訳が得られれば十分という考えによる．一
方，著者らは多様な対訳の検索を重要視している．}
．

実験結果よると二つの手法の有用性は同等もしくは CTM2 の方が若干良かった
となっている．著者らの結果と比べると出現順序を考えない CTM2 の結果が
良いのは意外である．原因は用例データベースの違い，入力表現の違い，評価
法の違いがあるため断定できないが，CTM2 の文字列の連続性の条件が貢献し
ている可能性がある．この条件が著者らの変位と対応したと考えられる．著者
らの実験結果でも語順だけでは効果が薄く変位が有効であった点を考え
ると両者の実験結果に矛盾はない．

************************ [./logs/V07N04-04/related_study] ************************
関連研究\label{kanren}
まず, 言い換えに関する既存の研究について論じる.
{}\ref{hajimeni}節で挙げた研究の他では, 
加藤ら\cite{kato99}は, 原文とその要約文との対応がとれたコーパスを
用いて, 言い換えが行われている部分を照合により特定し, それを言い換えの知 
識として自動的に得る手法を提案している.
また, Hovyら\cite{hovy97}, 近藤ら\cite{kondo96}は, シソーラスを用いて, 
意味が類似した複数の語句を, より抽象的な一つの語句に言い換える手法を提案
している.
また, 近藤ら\cite{kondo00}は, 「犬が彼に噛み付く」から「彼が犬に噛み付か
れる」のような, 単文中の非ガ格要素をガ格化する言い換えを実現するための規
則を提案している.
連体修飾節を対象とした言い換えに関しては, これまで, ほとんど研究されてい
なかったが, 
野上ら\cite{nogami00}によって, 「1742年に創立されたコスタは, スウェデー
ン最古の工場だ.」から「コスタは, スウェデーン最古の工場だ. 1742年に創立
された.」への言い換えのように, 連体修飾節を主文から切り離す言い換えが取
り上げられている.

次に, 本論文における削除可能な動詞, および, その定義に関連する研究につい
て論じる.
田中ら\cite{tanaka98b}は, 
``$N_1$の$N_2$''の意味関係を推定するために,
「一般的な意味関係」\footnote{「一般的な意味関係」は, さらに, 7つに分類
される.}と「名詞固有の意味関係」を定義している.
この中で, 本論文における$NV対$の定義, および, それによる連想される動詞の
判定は, 田中らが「名詞固有の意味関係」を得る際に行う処理とほぼ同一である.
田中らの概念は, 冨浦ら\cite{tomiura95}による意味推定において曖昧性が残る
``$N_1のN_2$''を対象としている.
一方, 本論文では, 言い換えを行う際の削除動詞を決定するという立場から, 言
い換え可能な``$N_1のN_2$''すべてを対象としている.
そのため, 「連想される動詞」によって言い換えられる``$N_1のN_2$''は, 田中
らの「名詞固有の意味関係」と, 必ずしも一致しない.
また田中らの概念では, 意味推定という立場から, 対象としている``$N_1の
N_2$''は, 2つの概念のいずれかに分類される.
一方, 本論文における2つの概念は互いに排他的ではなく, 
\ref{nvhantei}~節で議論したように, 両者の概念によって定義される動詞も存
在する.
これらの相違は, 田中らが意味推定, 本論文においては言い換え, と異なる目的
のために2つの概念を定義し, 利用していることにあるといえる.

また, 村田ら\cite{murata98a}, 山本ら\cite{yamamoto98}は, 名詞や動詞の省
略補完において, 本論文と同様に, コーパスから取得した用例を利用している.
ただし, 村田らの手法\cite{murata98a}では, コーパスに対して形態素解析や構
文解析をせず, 単なる文字列として最長に一致する部分を用例と認定している.
また山本らの手法\cite{yamamoto98}では, 
名詞と動詞との係り受け関係に関する情報は格フレーム辞書から得ている.
これらの手法も, 田中らと同様, 表層的には存在しない動詞を推定することを目
的とする.
よって, 用例を利用している点では本論文と類似するが, 目的は異なる.


************************ [./logs/V07N04-05/related_study] ************************
関連研究との比較
\label{sec:relatedworks}

原文書き換えを，形態素解析で得られる情報と通常よりも簡単な構文解析で得ら
れる情報に基づいて行なう手法としては，金らの方法\cite{Kim94}や加藤らの方
法\cite{KatoTerumasa97}がある．
金らの方法は，長い日本語文の構文解析が失敗しやすいという問題に，長文を複
数の短文に分割し，必要な場合には各短文に主語を補うことによって対処するも
のである．
これに対して本稿の原文書き換え系では，文の分割だけでなく，語句の追加・削
除・置換・移動，前編集記号の付加が可能であり，単に文を分割する場合よりも
品質の高い翻訳が得られる．

加藤らは，英語の複文に着目してその編集方法を示しているが，書き換え方法の
提案に留まっており評価結果は報告されていない．


************************ [./logs/V07N05-04/related_study] ************************
関連研究\label{sec:related}

本節では、これまでに提案されてきた日本語構文解析のための統計的アプローチと、
本研究で構文解析に用いる日本語文法SLUNG、及び
確率モデルの推定に用いる最大エントロピー法を紹介する。

\subsection{従来の統計的構文解析手法}\label{subsec:conventional}

日本語の係り受け解析のための統計的手法として、様々なモデルが考案されており、
次の２つに大別される。

\begin{description}
\item[生起確率を計算するモデル]
文$s$が与えられた時に、ある構文木$T$が生起する条件付き確率を求める方法である。
すなわち、次のような構文木$T$を選択する。
\begin{equation}
\mathop{\rm argmax}_T P(T|s)
\end{equation} \refstepcounter{enums}
\item[文中の係り受け確率の積をとるモデル]
文節$i$と文節$j$が係り受け関係にある確率$P(i \rightarrow j)$を考え、
式(\ref{equ:product})に示すような、
文中にある全ての係り受けの積を最大化する係り受け関数 $dep(i)$ を求める方法である。
\begin{equation}
\mathop{\rm argmax}_{dep}
\displaystyle{\prod_i} P(i \rightarrow dep(i))
\label{equ:product}
\end{equation} \refstepcounter{enums}
\end{description}

前者に属するものとして、確率文脈自由文法を用いたもの\cite{Mori98}や、
確率一般化LR法を用いたもの\cite{Shirai98}などがある。
これらは、数学的に妥当な確率を用いることができ、
形態素解析など様々なレベルとの統合が容易であるという利点があるものの、
現状では係り受け解析の精度は最高でも白井らの85〜86$\%$にとどまっている。

一方、後者の手法は、比較的学習が容易なため、
高い解析精度が得られる手法が多数提案されている。
実際、最大87.9$\%$と、生起確率に基づくものよりも高い精度が報告されている\cite{Uchimoto99b}。
本研究の手法もこのアプローチに基づいており、
以下でいくつかの研究を紹介する。
これらの手法及び本稿で提案する手法は、上記の$P(i \rightarrow j)$の求め方に違いがある。


決定木を用いたモデル\cite{Haruno98}、最大エントロピー法を用いたモデル\cite{Uchimoto99}、
距離確率と語彙確率を用いたモデル\cite{Fujio99}では、
係り元文節$i$の品詞や語彙や読点の有無など、係り先文節$j$の品詞や語彙、
そして二文節間の距離・読点や副助詞「は」の数などを属性として、
ある属性を持った二文節が存在する時に
それが係り受け関係にある確率を二文節$i, j$間の係り受けのしやすさとしている。
英語の統計的構文解析において
二語間の距離が係り受けを決定する重要な要素となる\cite{Collins96}のと同様に、
日本語の解析においても二文節間の距離が重要であるとされ、
上記のモデルではいずれも文節間にある文節数を属性として用いている。
これらのモデルでは、文節$i$と$j$以外の文節の情報は、
文節間の距離などの属性を除いては反映されない。


係り元・係り先とそのまわりの文節を考慮するモデル\cite{Uchimoto99b}では、
係り元文節$i$の係り先文節$j$への係りやすさの計算に、
$i$より右側にある全ての文節の情報を用いている。
そのために、二文節間の関係を、「係る」「係らない」の二値ではなく、
「越えて、遠くの文節に係る」「係る」「手前の文節に係る」の三値を出力するものとして学習する。
そして、$i$が$j$に係る確率を、$i$が$i, j$間の文節を「越える」確率と
$i$が$j$より右側の文節より「手前に係る」確率の積で補正する。
これにより、ある種の文脈情報が取り扱えることになり、
解析精度が\cite{Uchimoto99}より約$1\%$向上したことが報告されている。
但し、このモデルでは、係り元文節がそれより右側にあるそれぞれの文節に
「係る」か「越える」か「手前に係る」かを
互いに独立であると仮定しなければならない。


本研究で用いる３つ組／４つ組モデル\cite{Kanayama99}では、
２つまたは３つの係り先候補の属性を
同時に考慮できるため、文脈情報が扱えるうえ、
さまざまな望ましい点がある。
これに関しては本論文の\ref{sec:ourmodel}~節以降で詳しく解説する。


\subsection{日本語文法SLUNG}\label{subsec:slung}

本論文で提案する手法では、人手で書かれた文法で候補を絞ることが必須である。
我々が用いるSLUNG\cite{Mitsuishi98}は、
HPSG\cite{PollardSag94}の枠組みで記述された日本語文法であり、
8つのスキーマと、48個の語彙項目テンプレート\footnote{主に自立語に対して、品詞毎に振る舞いを記述したもの。
具体的な語彙は区別していない。}、105個の語彙項目\footnote{助詞や接尾辞などに対しては、
それぞれの語に対して文法的性質が記述されている。}からなる。
EDR日本語コーパス\cite{EDR}の文に対して98.4$\%$と、
非常に高い被覆率（構文木を一つでも返した文の割合）を示している。

文法自体は曖昧性解消の機構を持っていないため、SLUNGを用いて構文解析した場合、
文法的に許される全ての構文木が出力される。
本研究では、文節係り受けの統計モデルを用いることにより、
出力された構文木から最も優先度の高いものを選び出すことができるようになる。


\subsection{最大エントロピー法}\label{subsec:me}

統計モデルの推定に、最大エントロピー法(ME法)\cite{Berger96}を用いる。
ME法では、
「学習コーパス中の履歴の特定の条件を満たし、かつ特定の出力値を得る場合」
（素性）の頻度を得て、
様々な素性に対するパラメータを、
出力値の確率分布が最も一様分布に近づくように調整して求める。
別の素性に対し、それぞれ満たす集合に重なりがあってもよく、
抽象度の高い素性と低い素性を任意に混ぜることができるため、
統計モデルを構築する際のデータスパースネスの問題を軽減できる。
日本語係り受け解析でもME法は非常に有用で\cite{Uchimoto99}、
品詞の情報だけでなく、頻度の高い単語に対しては語彙的情報も加えるといった
柔軟な素性の追加が容易である。

本稿での実験における精度は、単純な相対頻度で推定した３つ組／４つ組モデル\cite{Kanayama99}
よりも約1.9$\%$向上しているが、
その要因として、ME法を用いることで以前よりも多くの素性を追加できたことが挙げられる。



************************ [./logs/V08N04-01/related_study] ************************
関連研究


まず，言語学の分野では，Austin\cite{austin} ，Searle\cite{searle} によ
る言語行為の研究が挙げられる．彼らは言語行為を分析するために，主として
単独の発話だけを対象にして言語行為論\footnote{言語学の分野では発話行為
論という呼称が主流となっているが，ここではSearle\cite{searle}における訳語を
採用した．}を展開した．そこでは，意図・状
況・文脈を特定しやすい典型的な発話を分析している．彼らの言語行為論は，単独
の発話ではあるが，言語行為を分析するための基本的な枠組みを与えた．また
Austinは，会話の中の言語現象である了解が言語行為の分析にとって重要であ
ることを指摘した\cite{austin} ．しかし，Austin，Searleともに了解の分析
を行ったわけではない．


一方，言語行為論に代表されるトップダウン的なやり方では限界があるので，
ボトムアップ的なやり方として会話分析が用いられている．会話を書き起こし，
ビデオを観察しながら知見を得ようとするこの分野では，近接ペア
\cite{levinson83a} ・三組のリスト\cite{levinson83a} などの成果が得られ
たが，残念ながら方法論が経験的であり，得られた規則も抽象的すぎて他の分
野への応用は困難である．近年，この流れを汲んで，小磯らは音声的特徴との
関連で了解の一顕現形態であるあいづちの研究を進めている\cite{koiso}．し
かし，了解に関する語用論的な知見は得られていない．

首尾よく欠陥なく遂行された会話では了解の過程が示され，意図・状況・文脈
が確定するので，単独の発話だけに比べて言語行為の分析が容易になり，発話
という複雑な現象の分析が容易になる．この点に着目して，共有知識(相互信
念)の問題については人工知能の分野でプラン認識として研究が試みられてい
る\cite{kato} ．会話に登場する各人物のプランをその会話を観察する人が認
識する問題は「鍵穴認識」\cite{suchman} と呼ばれている問題で，話し手の
感情・知識・信念などを直接把握できないために，これは非常に困難な問題と
されている．この分野の研究では，「鍵穴認識」の性質についていろいろわかっ
てきたものの，発話の了解の過程を十分に分析していない．しかも聞き手がど
のように，どの程度了解したかということが十分に分析されていない．

音声認識の分野では，発話の発声スタイルによる分類として，「読み上げ音声」
(read: 文章を読み上げる)，「自然な発話」(spontaneous: 認識装置を意識す
るが思ったことをそのまま話す)，「会話音声」(conversational: 認識装置を意
識しないでそのまま話す)の三つの分類が知られている\cite{kawahara}．音声
認識技術の進展により，研究対象が「読み上げ音声」から「自然な発話」に移っ
てきている\cite{okada}．
まず自然な発話を客観的に記述する方法を開拓し，研究が進んでいる
\cite{shimazu92a} ．了解に関しても前述のような研究が進んでいる
\cite{shimazu}．また，平沢らはユーザインタフェースにあいづちを用いる研
究を進め\cite{hirasawa99}，さらに人間側の理解について心理学的に研究し
ている\cite{hirasawa00}．

上記のように，了解に関連する研究はこれまで，言語学・心理学・音声学など
の視点から試みられてきた．しかし，了解の語用論的分析は行われていない．



************************ [./logs/V08N04-04/related_study] ************************
関連研究
\label{sec:related-work}

動詞由来名詞句に関する工学的研究としては，
文献\cite{Hobbs76,Dahl87,Somers88,Hull96,Macleod98}などがある．
このうち，本研究と同様に英日機械翻訳の立場からの研究はSomersらの研究であ
る．

Somersらは，動詞由来名詞句を自然な日本語表現に翻訳するための方法として，
英日両言語から中立な内部表現を利用する方法と，変換過程での明示的な構造変
換による方法を検討している．
これらはいずれも必要な処理をシステム内部で行なうものである．
このような方法に比べて，本稿のような前編集による方法は，後編集による方法
\cite{Knight94,Yamamoto99,Ozaki01}と同様に，システムからの独立性が高いた
め，様々なシステムで利用可能であり，実際にシステムと組み合わせる際にシス
テムの既存部分を修正する必要はほとんどない．

Hullらは，動詞由来名詞が動詞的意味を表すデキゴト名詞であるか，非動詞的意
味を表すモノ名詞であるかの判定を行い，さらに，デキゴト名詞であると判定さ
れた場合，動詞由来名詞の語義(対応する動詞の格パターン)を決定する方法を示
している．
これに対して，本稿では，デキゴト名詞かモノ名詞かの判定は行なう(デキゴト
名詞かモノ名詞かをあらかじめ区別しておく)が，動詞由来名詞に対応する動詞
の語義を決定することは機械翻訳システム本体に委ねるという方針を採っている．

Macleodらは，動詞由来名詞と動詞の対応情報を記述した辞書を構築している．
この辞書には，動詞の補足語が動詞由来名詞句では属格となるのかあるいはどの
ような前置詞を伴うのかなどが記述されている．
本稿では書き換え対象を形式(\ref{eq:source-np1})に限定したが，
この形式以外の表現では，
(a) 動詞由来名詞がデキゴト名詞かモノ名詞かの曖昧性や，
(b) 動詞由来名詞とその補足語との結合関係の曖昧性，
(c) 動詞由来名詞の後方に存在する前置詞句の付加の曖昧性
などが生じやすい．
Macleodらの辞書を利用すればこのような問題にある程度対処できるので，
書き換え対象を拡げることも可能であろう．


************************ [./logs/V09N01-01/related_study] ************************
関連研究

英語を対象として，生コーパスから格フレームを学習する方法はいくつか研究さ
れてきた\cite{Brent1991,Manning1993,TedBriscoe1997}．英語は格要素が省略
されることがなく，問題となるのは格要素が用言にとって必須であるか任意であ
るかの判定である．この判定は，統計情報を利用して用言と格フレームの関連度
を計算することによって行われている．学習する格フレームは用例格フレームの
ようなものではなく，動詞が名詞句と前置詞句をとるといったパターンである．
つまり，用言の用法そのものを収集していると考えられるので，用言の用法の多
様性は問題にならない．

日本語では，格フレームを構文情報付きコーパスから学習する方法が提案されて
いる\cite{東優1996,宇津呂武仁1997}．これらの手法は，学習に構文情報付きコー
パスを用いているためカバレージの点で問題がある．春野は，意味素を要素とす
る格フレームをコーパスから学習する方法を提案している\cite{春野雅彦1995}．
11個の動詞を対象とし，新聞1年分から人手で抽出した用例を用いているのでカ
バレージの点では問題ないが，動詞数を増やして実用的な格フレームを作成する
のは難しいと思われる．これらの手法で得られる格フレームは，格要素を汎化し
た意味素を格フレームの個々の要素としたものであり，この点では本研究と異な
る．用言の用法の多様性は，それぞれ次のようにして扱っている．東らはEDR コー
パスを用いており，動詞についている動詞概念ごとに格フレームを作成している．
宇津呂らと春野の手法は，それぞれ機械学習，情報圧縮の手法を用いて意味素の
汎化レベルを決定することによって，用例を直接クラスタリングするものである．
しかし，前節で述べたように，これらの方法は精度の面で格フレームの作成には
適当ではないと考えられる．



************************ [./logs/V09N01-04/related_study] ************************
関連研究
\label{sec:rel}

\subsection{複数モデルの出力の混合法}

\ref{sec:intro}~節で述べたように，一般に，
複数のモデル・システムの出力を混合する過程は，大きく以下の二つの部分に
分けて考えることができる．
\begin{enumerate}
\item \label{enum:sub1-rel}
	できるだけ振る舞いの異なる複数のモデル・システムを用意する．
\item \label{enum:sub2-rel}
	用意された複数のモデル・システムの出力を混合する方式を選択・設計し，
	必要であれば学習等を行ない，与えられた現象に対して，
	用意された複数のモデル・システムの出力を混合することを実現する．
\end{enumerate}
ここで，これまで自然言語処理の問題に適用された混合手法においては，
これらの(\ref{enum:sub1-rel})および(\ref{enum:sub2-rel})の過程について，
大体以下のような手法が用いられていた．

まず，(\ref{enum:sub1})については，大きく分けて以下のような手法がある．
\begin{enumerate}
\item[i)] 学習モデルが異なる複数のシステム等
	(原理的には，人手による規則に基づくシステムとデータからの学習に基づくシステム，
	などの組合わせも可能)，
	ある程度振る舞いの異なる既存のシステムを
	用意する~\cite{vanHalteren98a,Brill98a,Henderson99a,KoInui00aj,Sang00a}．
\item[ii)] i)と似ているが，学習モデルは単一のものを用い，データの表現法
	(具体的には，まとめ上げ問題におけるまとめ上げ状態の表現法)として複数のものを
	設定することにより，複数の出力を得る~\cite{Sang00a,TKudo00ajx}．
\item[iii)] 単一の学習モデルを用いるが，訓練データのサンプリングを複数回行なうことにより
	複数のモデルを学習するbagging法~\cite{Breiman96b}を用いる~\cite{Henderson00a}，
	あるいは，単一の学習モデルを用い，誤り駆動型で訓練データ中の訓練事例の重みを操作しながら
	学習と適用を繰り返すことにより，各サイクルの誤りに特化した複数のモデル
	(およびそれらの重み)を学習する
	boosting法~\cite{Freund99aj}を用いる~\cite{Haruno97a,Haruno99a,Abney99a,Henderson00a}．
\end{enumerate}
これに対して，本論文においては，振る舞いの異なる複数のモデルを得る方法として，
学習モデルは単一のものを用い，固有表現まとめ上げの際に考慮する周囲の形態素の個数を
区別することで複数のモデルを得るという方法をとった．
この方法は，上記のうちでは，ii)でとられた方法と比較的似ている．

次に，(\ref{enum:sub2})については，大きく分けて以下のような手法がある
\footnote{
  boostingは，複数のモデルを組合わせる際の重みまで含めて，全体として誤りが減少するように
  複数モデルの生成法が設計されているので，以下の分類には含めない．
}．
\begin{enumerate}
\item[i)] 重み付多数決など，何らかの多数決を行なうもの~\cite{Breiman96b,vanHalteren98a,Brill98a,Henderson99a,KoInui00aj,Sang00a,Henderson00a,TKudo00ajx}．
\item[ii)] 複数のシステム・モデルの重みに応じて採用するシステムの切り替えを行なうもの~\cite{Henderson99a,KoInui00aj}．
\item[iii)] 原理的に，上記のi)およびii)を包含し得る方法として，
	複数のシステム・モデルの出力(および訓練データそのもの)を入力とする第二段の
	学習器を用いて，複数のシステム・モデルの出力の混合を行なうstacking法~\cite{Wolpert92a}，
	あるいは，それと同等の方法に基づくもの~\cite{vanHalteren98a,Brill98a,Sang00a}．
\end{enumerate}
これらの方法のうち，本論文では，原理的に，i)およびii)を包含し得るiii)のstacking法を用いている．
特に，本論文では，個々のシステムの出力する重みの情報は利用せずstackingを行なっているので，
規則に基づくシステムなどで重みを出力しない場合でも，そのまま本論文の手法を適用することができる．
これに対して，重み付多数決や重みを用いたシステム切り替えの場合は，
システム数が少なく(例えば，二種類のシステムの混合の場合)，かつ，個々のシステムが重みを出力しない
場合などでは，適用が困難になると考えられる．
また，通常のbagging法やboosting法を適用する場合でも，
第一段としては何らかの学習モデルを採用する必要があるが，
本論文の混合法にはそのような制約はないので，原理的には，
第一段として任意のシステムを採用することが可能である．

\subsection{Stacking法}

次に，本節では，stacking法についての関連研究，および，
stacking法と同等の手法を自然言語処理におけるシステム混合の問題に適用している
研究事例について述べる．

stacking法は，\cite{Wolpert92a}によってその枠組みが提案され，その後，機械学習の分野において
いくつかの応用手法が提案されている~\cite{Breiman96a,Ting97a,Gama00a}．
例えば，\cite{Breiman96a}は，回帰法を用いたstackingを提案している．
\cite{Ting97a}は，第一段の学習器として，決定木学習，ナイーブベイズ，最近隣法を用い，
第二段の学習器として，決定木学習，ナイーブベイズ，最近隣法，線形回帰法の一種を用いた
実験を行ない，性能の比較をしている．
一方，\cite{Gama00a}は，それまで提案されたstacking法を，$n$段の学習器の連鎖に拡張し，
第$k$ $(1<k\leq n)$段の学習器は，第一段から第$k-1$段までの全ての学習器の入出力データを
素性として学習を行なうというカスケード法を提案している．
特に，それまでのstacking法は，第一段の学習器の出力のみを入力素性として第二段の
学習器の学習を行なうものがほとんどであったのに対して，カスケード法では，
前段までの学習器の出力だけでなく，入力素性もあわせて利用する点が特徴的である．

一方，自然言語処理におけるシステム混合の問題にstacking法と同等の手法を適用している
研究事例
\footnote{
  ``stacking''という用語を用いていない事例も多い．
}
としては，英語品詞付けにおいて，
最大エントロピー法，変形に基づく学習，トライグラムモデル，メモリベース学習を
第一段の学習器とし，決定木学習，メモリベース学習法などを第二段の学習器として
stackingを行なうもの~\cite{Brill98a,vanHalteren98a}，
英語名詞句まとめ上げにおいて，七種類の学習器を第一段に用い，
決定木学習，メモリベース学習法を第二段の学習器として
stackingを行なうもの~\cite{Sang00a}などがある．
これらの事例においては，いずれも，第一段の入力素性および出力を用いて第二段の学習器の
学習を行なった結果も報告している．
また，\cite{Borthwick98a}は，英語の固有表現抽出において，
単一の最大エントロピーモデルの素性として，通常の固有表現まとめ上げ・タイプ分類に用いる
素性とあわせて，他の既存のシステムの出力を素性として用いて，
個々の単語に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を行なっている．
一方，\cite{Freitag00a}は，情報抽出におけるテンプレート・スロット埋め問題において，
ナイーブベイズ法，帰納的論理プログラミング法などを第一段の学習器とし，
回帰法を第二段の学習器としてstackingを行なっている．
ここでは，第二段の学習器の入力は，第一段の学習器の出力のみとなっている．

これらの事例と比較すると，本論文の日本語固有表現抽出の問題においては，
第一段の学習器は，個々の形態素に固有表現まとめ上げ状態・タイプ分類を付与するための分類器の学習を
行なっているのに対して，第二段の学習器は，個々のシステムの固有表現抽出結果，
および，第一段の学習器の入力となった素性(の一部)を入力として，個々のシステムの固有表現抽出結果の
正誤を判定するための分類器の学習を行なっている．
このように，本論文のstacking法では，第一段と第二段の学習器の学習の単位が異なっている点が
変則的である．
ただし，このような構成をとることにより，第一段としては，
任意の固有表現抽出システムを用いることが可能となっている．
また，\cite{Borthwick98a}と比較すると，\cite{Borthwick98a}では，
本論文の第二段に相当する学習器が，個々の単語に固有表現まとめ上げ状態・タイプ分類を
付与するための分類器の学習を行なっている点が異なっている．


\subsection{統計的手法に基づく日本語固有表現抽出}

統計的手法に基づく日本語固有表現抽出の研究事例としては，
我々がベースとした，最大エントロピー法を用いるもの\cite{Uchimoto00aj}の他に，
決定木学習を用いるもの~\cite{Sekine98a,Nobata99aj}，
最大エントロピー法を用いるもの~\cite{Borthwick99aj}，
決定リスト学習を用いるもの~\cite{Sassano00a}， 
SVM(support vector machines)を用いるもの~\cite{Yamada01ajx}などがある．
これらは，いずれも，単一の学習モデルを用いている．
決定リスト学習を用いる事例~\cite{Sassano00a} 
では，
可変長文脈素性を用いることにより，固定長モデルの性能の上回る結果が得られているが，
ベースとなる決定リスト学習の性能は最大エントロピー法の性能よりも劣っている．
その他の事例では，いずれも，固定長文脈素性を用いている．

また，stacking法の研究事例においては，異なる数種類の学習器を第一段に用いるという構成が
多く見られ，一定の効果が報告されているので，上記の複数の学習器を第一段としてstacking法を
行なうことにより，精度の向上が期待できる可能性がある．
その他には，\cite{Yamada01ajx}で報告されているように，解析の方向を文頭から文末と文末から文頭の
二通り設定し，解析済の固有表現のタグを素性として利用する方法により，
振る舞いの異なった出力が得られる可能性があり，stacking法でその出力を利用することで，
精度の向上が期待できる可能性がある．
また，\cite{Isozaki00ajx}では，
決定木学習により学習された可読性の高い規則や人手による付加制約等を適用して
複数の固有表現候補を生成し，最長一致法により複数の候補の選別を行なっている．
ここで，複数の候補の選別を行なう際に，本論文の混合法を適用することにより，
誤出力の棄却まで含めたより一般的な選別が自然な形で実現できる可能性があると
考えられる．


************************ [./logs/V09N03-04/related_study] ************************
関連研究との比較
\label{sec:hikaku}

江原と金\citeyear{ehar96}は，日英機械翻訳の前編集において，日本語の原
文（長文）を短文に分割した際に発生する主格の欠落をゼロ主語（ガ格のゼロ
代名詞）と見なし，確率モデルを用いて補完する手法を提案した．彼らは，ニュー
ス原稿108文を対象に評価実験を行ない，オープンテストで80.6\%の正解率を
得ている．しかし，この手法は同一文内に指示対象がある場合のみが対象であ
り，それ以前の文脈に指示対象が現れる状況を考慮していない．ゼロ主語の照
応先は同文内とは限らず，照応先としてゼロ主語以前の文脈を考慮するほど指
示対象候補が増えて照応解析が難しくなる．事実，彼らの実験においてゼロ主
語ごとの指示対象候補は平均3.9個であり，本研究に比べると極端に少ない
（\ref{sec:result}~節参照）．

AoneとBennett~\citeyear{aone95}は，ゼロ代名詞とそれ以外の照応詞（固有
名詞，限定詞）を対象に，決定木を用いた照応解析手法を提案した．彼らは，
合弁事業に関する新聞記事を用いて評価実験を行ない，ゼロ代名詞に関して，
オープンテストで80\%前後の正解率を得ている．しかし，この実験で対象となっ
たゼロ代名詞は，会社名等の組織を照応するものに限定されている．よって，
指示対象候補として会社名等のみを考慮すればよく，この制約により大幅に候
補を絞り込める．

以上二つの先行研究と比較して，本研究は前方照応のゼロ代名詞全般を対象に
しており，適用範囲が広い．また，以上の研究が学習データとして人手で照応
関係を付与したコーパスを必要とするのに対し，本提案手法は，照応関係が付
与されていないコーパスを併用することで，大規模なコーパスを容易に学習に
利用できる．また本手法では，確信度を利用することで正解の確信が高いゼロ
代名詞のみ選択的に結果を出力し，利用目的に応じて照応解析の正解率を向上
させることができる．



************************ [./logs/V09N03-07/related_study] ************************
関連研究



要約の内容に関する評価について，Jingら\cite{jing:98:a}は，典型的な評価方
法の1つであるF-measureをとりあげ，その問題点をいくつか指摘している．Jing 
らは，システムの要約と人間の被験者の作成した抜粋との比較による評価と，要
約を利用して人間がタスクを行なう場合のタスクの達成率による評価の2つの評
価方法を分析し，評価結果に影響を与える要因を同定することを試みているが，
その結果少なくとも次の2つの点において，これまでの人間の抜粋を用いた評価
方法は問題であるとの知見を得ている．

\begin{itemize}
 \item {\bf 問題点1(要約率の変化に伴う評価値の変化):}\\
       人間の抜粋との比較による評価では，要約率を変化させると，システム
       の評価がかなり変化する．このため，特定の要約率でシステム間の性能
       の比較をする意味がどの程度あるのかは疑問が残る．
 \item {\bf 問題点2(テキスト中の複数類似個所の選択問題):}\\
       テキスト中に類似の内容を含む文が複数存在する場合，どちらの文が正
       解として選択されるかにより，システムの評価は大きく変化する．
\end{itemize}


これまで，問題点1(要約率の変化に伴う評価値の変化)を解消するいくつかの方
法が提案されている．
Mittalら\cite{mittal:99:a}は，要約率の違いによるシステムの評価の違いに関
して，さまざまな要約率における精度を求めた上で，情報検索の評価で用いられ
ている11点平均精度(11 point average precision)のように，複数の要約率での
精度の平均として結果を示すべきであるとしている．

また，コーパスとするテキスト集合の違いが精度に影響を与えることから，コー
パスの要約のしやすさを計る指標として，ランダムに文を選択して要約を作成し
た場合の精度をベースラインとして示すべきであると主張している．そして，シ
ステムの性能を評価する場合，

\[ p' = \frac{p-b}{1-b} \]
(ここで，p，b，p'はそれぞれシステム，ベースライン，補正後のシステムの精
度)のように，ベースラインを用いて補正した精度を用いるべきであるとしてい
る．

一般に，F-measureで要約の精度を評価する場合，ベースライン値＝要約率と
考えることができるため，要約率が大きくなるにつれ，F-measure値は大きく
なる傾向にある．従って，ベースラインを用いて評価値を補正する上記の評価
方法は，Jingらの指摘する問題点1の解消には有用であると考えられる．

一方，被験者間の一致の度合をJとすると，Jは要約システムの精度の上限と考え
られ，また，ランダムに選択した時の精度Rは下限と言える．そのため，Radevら
\cite{radev:00:a}も，Mittalらと同様に，システムの性能を計る値を示す際，
普通に計算された値Sを単に用いるのではなく，これらの値で正規化した値\[ S'
= \frac{S-R}{J-R} \]を示すべきであるとしている．

問題点2(テキスト中の複数類似個所の選択問題)を解消する方法もいくつ
か提案されている．Jingら\cite{jing:98:a}は，人間が選択した重要文を用いて
評価を行なう際，正解と一致した場合正解数1，一致しない場合0として再現率，
精度を計算するのではなく，正解数を被験者間の一致の度合として計算する方法
を提案している．たとえば，5人の被験者中3人，2人がそれぞれ一致して選択し
た文が存在する場合，これまでの評価方法では，前者をシステムが選択した場合
正解数1(過半数以上の被験者が選択しているので)，後者では0となるが，提案す
る方法では，システムの正解数は，前者では3/5，後者では2/5となる．

Radevら\cite{radev:00:a}は，文のutilityという概念を用いた評価方法を示している．文の
utilityは，文がそのテキストの話題に対してどの程度適合した内容であるかを
示す尺度であり，[0-10]の値をとる\footnote{genericな要約を考えた場合，テ
キスト中での文の重要度を示していると考えて良い．}．人間が選択した重要文
を用いたこれまでの評価方法は，正解と一致した場合正解数1，一致しない場
合0として再現率，精度を計算していたが，utilityに基づく評価値は，システム
が選択した文に対して人間が割り当てたutilityの総和を，正解の文のutilityの
総和で割った値として計算する．これまでの評価方法では，システムが選択した
不正解の文は，全く評価が得られなかったのに対し，utilityに基づく評価の場
合，Jingらの方法と同様に，たとえ不正解でもその文がある程度の重要度を持
つ場合，その重要度に対する部分的な評価が得られる点が異なる．ただ一つ正解
が存在し，それとまさに一致することを要求されていたこれまでの評価に比べ，
正解の文のutilityにどのくらい近いutilityの文を選択できるかで評価を行なう．

Donawayら\cite{Donaway:2000}は，2種類の評価方法を提案している．一つは，
人間にも，システムにも，テキスト中の文にすべて順位をつけさせるようにし
て，その文の序列を比較して評価を行なう方法である．これは，これまでの方
法がテキスト中の文を重要/非重要の2つに分類して評価に利用していたのに対
し，テキスト中の文数に分類して利用することに相当する．
Donawayらが提案するもう一つの評価尺度は，人間の作成した正解要約の単語頻
度ベクトルとシステムの要約の単語頻度ベクトルの間のコサイン距離で評価する
方法(以後，content-basedな評価)である．

Donawayらは，この2種類の評価尺度にこれまでの評価方法である再現率に基づ
いた評価を加え，これらを実験により比較，検討している．正解の抜粋に含ま
れる個所が要約作成者毎に異なっていても，内容の類似した個所を抜き出して
いるのであれば，どの要約作成者の抜粋を用いても似たような評価値が得られ
る必要がある．Donawayらは，4人の要約作成者の作った抜粋を用いて，上で述
べたいくつかの尺度で要約を評価し，尺度毎に評価値の相関を調べている．そ
の結果，content-basedな評価が人間の要約との比較による評価方法としては，
Jingらの指摘する問題点2に対する解決策ともなっており，もっとも優れてい
ると結論づけている．



************************ [./logs/V09N04-01/related_study] ************************
関連研究
\label{Sec:関連研究}

節\ref{Sec:検索文書要約の特質}でも述べたように，検索結果の文書要約は，
通常の文書要約とは次の点で異なる．
\begin{enumerate}
 \item 検索要求文が与えられている．\label{step:query}
 \item 複数の文書が同時に得られている．そして，ある一度の検索の結果と
       いう点において文書間に類似性が認められる．\label{step:doc}
\end{enumerate}

本研究においては(\ref{step:doc})の情報を用いたが，
(\ref{step:query})の情報を利用した手法もある．

Tombrosら\cite{Tombros:AdvantagesOfQueryBiasedSummariesInInformationRetrieval}は, 文書中のタイトル情報，文書中での位置情報，
文書中での単語の出現頻度に基づいた，従来通りの文の重要度に，
検索要求文中の単語が文中に出現する頻度に応じたスコアを加味することで，
検索要求文に依存した重要文抽出を実現している．
塩見ら\cite{塩見:視点を考慮した文書要約手法の提案}も，文書中の単語の出現頻度に基づいた文の重要度に，
検索要求中の単語が文中に出現する頻度に応じた
スコアを加味する手法を提案している．
しかし，これらの手法は，スコアの制御が難しいことが問題点である．
また，節\ref{Sec:検索文書要約の特質}で述べた通り，各種フィードバック，検索要求中の単語のシソーラスによる拡張などといった
情報検索システムにおける工夫が反映されないという問題点がある．


また，(\ref{step:doc})の情報を利用するという点では，
Eguchiら
\cite{Eguchi:AdaptiveQueryExpansionBasedOnClusteringSearchResults}，
Fukuharaら
\cite{Fukuhara:Multiple-textSummarizationForCollectiveKnowledgeFormation}，
Radevら\cite{Radev:Centroid-basedSummarizationOfMultipleDocuments,Radev:Centroid-basedSummarizationOfMultipleDocuments}の手法が関連する．
Eguchiらは，適合性フィードバックに基づく検索システムを構築している．
このシステムでは，検索結果を文書間の類似度に基づいてクラスタリングし，
各クラスタごとにクラスタに多く含まれる語と，そのクラスタを代表する
文書のタイトルを，そのクラスタの要約として出力する．ユーザに，出力された
クラスタの中から選択してもらい，そのクラスタに含まれる文書を用いて
適合性フィードバックを行なっている．
FukuharaらやRadevらも，Eguchiらと同様に検索結果を文書間の類似度に基づいて
クラスタリングし，各クラスタごとに要約を出力している．
Fukuharaらの手法では，まず，文書中の単語の出現頻度を考慮し，
クラスタごとのトピックを表す語を抽出する．
そして，それらトピックを含む文を抽出し，焦点−主題連鎖を考慮して並べ替え，
各クラスタの要約としている．
Radevらの手法では，各クラスタについて，その重心ベクトルをTF・IDF値を用いて計
算し，その重心における各語の成分を語の重みの主要な成分としている．

これらの手法では，クラスタリングを文書のグループ分けのみに利用しており，
グループ化された後では，各クラスタにおける語の分布だけを用いて重みを計算し
ている．また，語の重要度としては単純にクラスタ内の
TFやIDFを用いているだけである．直接の比較は今後の課題とするが，我々の手法
においては，文書間の類似性構造の情報も取り入れて重みづけしているので，より
高い分解能ならびに精度が得られていることが期待される．

TSC Task B における評価では，検索文書集合が与えられてはいるものの，
最終的には個別文書の要約になっていた．一方，
\cite{Mani:SummarizingSimilaritiesAndDifferencesAmongRelatedDocuments,McKeown:GeneratingSummariesOfMultipleNewsArticles}
に代表されるように，複数文書から一つの要約文書を生成するという研究も近年注
目を集めている．
特にCarbonellら\cite{Carbonell:TheUseOfMMR:Diversity-BasedRerankingforReorderingDocumentsAndProducingSummaries}
は，極大限界適合度(Maximal Marginal Relevance,MMR)という概念を導入し，検索質問
と検索文書の類似度ならびに，ある文書とそれよりも上位の文書との間の冗長性に基づいて，
検索文書の再順位づけを行なうとともに，これを，パッセージ検索に利用すること
によって要約生成を行なう手法を提案している．

我々の重み付け手法は複数文書を一つの要約にする場合にでも，効果を発揮す
ることが期待されるが，文書間の融合過程においては，冗長性の制御を陽に行なう
MMRのような手法との組み合わせも必要になってくるであろう．



************************ [./logs/V09N04-04/related_study] ************************
関連研究
\cite{zechner96}をはじめとして，多くの研究が重要文選択という手法を採用しており，その中で，どのように重要文を選択するかを問題にしている~\cite{okumura98}．我々は，重要文選択では長い文が選択されがちで，読む負荷が大きいことを問題にし，関係を組み合わせて句を合成することによる要約の生成手法を提案した．ここでは，我々の視点で類似研究をまとめる．

まず，短い文にするという点では，文の言い替え，不要な修飾語の削除で文を短縮するという方向がある．\cite{wakao98}，\cite{mikami98}は，ＴＶニュースにおいて，聞かせるための原稿から，読ませるための字幕を作成することを目的としている．このinformativeという性質上，情報をなるべく落とさないようにすることが必要で，あまり短くはできない．Indicativeを目的とする場合はもっと短くできるはずであるが，文の中心構造を残し，修飾部分を減らす方向では，句表現要約ほど短い要約を作ることはできない．

\cite{boguraev97}による要約手法は，文またはパラグラフではなく，句表現(``phrasal expression'')を採用している．この要約の目的は速読であり，そのため段落ごとにトピックを選定し，そのトピックがどのように扱われるかを提示するのに用いられている．方法としては，単語で表したトピック(``topic stamp'')から句を作っていくもので，ひとつのトピックから複数の句を構成するため，検索結果のふるいわけに適した要約にはなっていない．また，アークの役割や重要性を使っていないため，あまり重要でない語も同様にコアに付加される．

文を合成するという立場が似ている研究としては，\cite{hovy97}，\cite{kondou96}などがある．これらは，シソーラスなどを用いて複数の単語を上位概念で置き換えた文を構成することをねらっている．文章全体の意味を短い表現で置き換えることは要約の目指すところではあるが，単語レベルでの置換では適用範囲が限られるし，より大きな単位の置換が可能になると，知りたかったことが抽象化されすぎて見えなくなる問題も生じるだろう．

我々の句表現要約と同じく，語と語の関係をベースに要約を作るものには，\cite{nagao97}がある．これは，GDA(Global Document Annotation)という意味構造をあらかじめ文書中にタグとして付与しておくことにより，要約など文書の種々の機械的処理を可能にしようという試みである．必須格などの重要な関係を追加していく点などで手法に類似性があるが，At-a-glanceを目的とした短い句を出すものではなく，文の形式を保持しながら長さを柔軟に変更できる要約を目指したものになっている．検索対象文書全てに適切な意味構造が与えられる状況は当面期待できず，検索結果のふるいわけへの応用は難しい．



************************ [./logs/V09N05-02/related_study] ************************
関連研究

２言語間の構造同士の対応を取ることにより，句レベルの対応を階層的に取得
する方法としては，先行研究では以下のものが発表されている．

まず，句構造を基本とする研究としては，
\shortciteA{Kaji:PhraseAlignment1992}の方法がある．単語レベルの対応を
基に，句構造のノード間の対応を取るもので，筆者の研究の基となったもので
ある．しかし，構文カテゴリ制約を使用していないため，単語リンクの両端
が異なる品詞を持つ場合，不当に短い単位で同等句を抽出する．

依存構造を基本とする研究としては，
\shortciteA{Matsumoto:PhraseAlignment1993,Kitamura:PhraseAlignment1997j,Yamamoto:PhraseAlignment2001j,Watanabe:PhraseAlignment2000,Meyers:PhraseAlignment1996}
が挙げられる．依存構造を基にする場合，ノードそのものが最小単位の名詞句，
動詞句，副詞句等を表しているため，構文カテゴリ情報を用いなくともある程
度の句レベル対応を取ることができる．しかし，
\shortciteA{Kaji:PhraseAlignment1992} の手法と同様の問題があると考えら
れる．

\shortciteA{Wu:SimultaneousGrammar1995}は，構文解析後に構造の対応を取
るのではなく，予め２言語間で対応づけられた構文解析規則を用意しておき，
２言語同時に解析を行うことにより，構文解析と句レベルの対応づけを同時に行う
手法を提案している．この方法で予め用意する必要があるのは，１単語
同士の対応規則(終端記号同士の対応)である．言い換えると，単語アライメン
トのみを必要とする．しかし，単語同士の対応が十分つくような直訳文では機
能するが，構文制約が弱いため，意訳等を含む一般的な対訳文，特に本稿で目
指している話し言葉には向かないと考える．

また，いずれの方法も，構文解析に失敗する文の救済策については述べられて
いない．構文解析は，文法設計者が意図したドメインでの性能は高いが，別ド
メインに移行した場合，精度が落ちるものが多い．本手法は，文法のカバレッ
ジが低いパーザであっても，部分解析結果を組み合わせることにより同等句を
抽出しているため，話し言葉以外でも同等句抽出数という点で有利である．

単語アライメントの使用方法という観点で上記研究を俯瞰すると，
\shortciteA{Kaji:PhraseAlignment1992,Watanabe:PhraseAlignment2000} 
は，単語アライメントを決定的に行っている．一方，
\shortciteA{Matsumoto:PhraseAlignment1993,Kitamura:PhraseAlignment1997j,Meyers:PhraseAlignment1996}は，単語類似度を導入し，
構造比較時のスコアとしている．

\shortciteA{Yamamoto:PhraseAlignment2001j}の研究は，単語アライメントを
必要としないという点が特徴的である．これは，句レベルの対応候補を作成し，
重み付きダイス係数という統計量を用いて，最良優先に対応を決定して行くも
のである．我々の提案手法は，統計量をまったく用いていないので，これと類
似の統計量を後処理的に導入することにより，同等句の精度はさらに向上でき
るだろうと推測される．




************************ [./logs/V10N04-10/related_study] ************************
関連研究
\label{sec:relwork}

自動的に記事対応を得ることを目的とする研究はいくつかある．その
うち，\cite{collier98:_machin_trans}は，言語横断検索に機械翻訳
を利用した場合と辞書引きを利用した場合とを比較しており，再現率
が高いとき(多くの記事対応を得たいとき)には，辞書引きの方が有利
だとしている．我々も，表\ref{tab:prec1}のデータの1996-2001につ
いてのみ，シャープ株式会社の機械翻訳支援システムを利用して精度
評価をしてみたが，その結果は，統計的に有意ではないが，辞書引き
の結果の精度の方が高かった\footnote {ただし，このときには，英語
  記事を日本語に翻訳し，その翻訳結果を質問として日本語記事から
  なるデータベースを検索した．これは，本稿でこれまで説明してき
  た方法である，日本語記事を英語単語集合に変換する方法の逆であ
  るので，厳密な比較ではない．}．これらのことから，辞書引きの方
が記事対応を得るには適しているのではないかと考えられる．

また，\cite{matsumoto02:_autom_align_japan_englis_newsp}は，日
経産業新聞について，英語記事と日本語記事との対応付けをしていて，
その精度は，97\,\%と非常に高精度である．しかし，彼らは，同じ方法
を，NHKの報道記事の対応付けに対しても適用しているが，その場合の
精度は69.8\,\%であり，彼らの方法が，全ての場合で高精度であるわけ
ではないということも示している．そのため，彼らの方法を読売新聞
の記事対応付けに利用した場合にも同様に高い精度が得られるかは明
かではない．

これらの従来の記事対応を得る研究と我々の研究との主要な違いは次
の2点である．
\begin{enumerate}
\item まず，記事対応の評価尺度について，我々は，DPマッチングに
  よる文対応付けの結果を利用した信頼性の高い尺度を提案した．そ
  れに対して従来の研究は  bag-of-words に基づいた尺度を利用して
  いる．なお，情報検索において，質問文と文書との類似度を求める
  際にDPマッチングを利用する方法が
  \cite{yamamoto00:_dynam_progr}により提案されているが，彼らの
  研究対象と我々の研究対象とは異なるし，かつ，DPマッチングの方
  法や，評価尺度の定義も異なる．
\item 次に，我々は，記事対応の結果から文対応までを，実際に，大
  規模に得た．\cite{大和99}は，記事対応の結果から文対応を得るこ
  とを構想してはいるが，実際に文対応を得ているわけではない．加
  えて，我々は，対応付けの結果が一般に研究および教育目的に利用
  できるようにしているが，これは日英対応付けコーパスとしては初
  めての試みである．
\end{enumerate}


************************ [./logs/V10N05-01/related_study] ************************
関連研究
\label{ss:related}

\subsection{共通記号列のまとめ処理}

生成規則($A \rightarrow \alpha$)に対し右辺記号列($\alpha$)中のある位置
にドット`・'を付けたデータ構造($A \rightarrow \beta \cdot \gamma$,  た
だし$\alpha = \beta \gamma$)は，項(Item)と呼ばれ，構文解析中に規則のど
こまで解析が進んだかを表すために，本稿で述べたLR項の他，Earley法
\cite{earley1970}やチャート法\cite{key1980}など，種々の構文解析アルゴ
リズムで共通に利用されている．本稿で示した手法は，LR法においてItem以降
の解析がドットの左側(右辺記号列のprefix，$\beta$)には依存しないことを
利用し，ドットの右側の記号列(右辺記号列のsuffix，$\gamma$)が共通なもの
をまとめあげることによって，LR表の圧縮を実現したと考えることができる．
同様に，Itemのドット左右の記号列について複数の規則の間で共通する記号列
をまとめて処理することによる，解析の効率化手法が知られている．

本稿の提案法のように，ドットの右側の記号列($\gamma$)が共通なItemをまと
めて扱う手法が提案されている．文献\cite{leermakers1992}では，Earley法
においてItem以降の解析がドットの右側の記号列($\gamma$)のみに依存し，ドッ
トの左側($\beta$)や生成規則左辺の記号($A$)には依存しないことを利用して，
これらを重複処理しないことによる効率化手法が示されている．文献
\cite{moore2000}では，同様の手法をチャート法に適用している．

逆に，ドットの左側の記号列($\beta$)が共通なItemをまとめて処理する手法
としては，LR法が挙げられる．LR法では，共通なprefixを持つ複数のItemをま
とめて解析の一状態とするようにLR表を作成することで，解析の効率化を実現
している．文献\cite{nederhof1994}では，この考え方を進めて，共通の
prefixをもつ規則をすべてまとめて処理する手法が示されている．また，共通
したprefixを持つ2つ以上の規則を持たないように文法を変形することによっ
て効率化を行なう手法も提案されている\cite{moore2000}．

\subsection{可変な規則長の扱い}

本稿の提案法2では，ドット左側の記号数情報を捨象した可変長LR項の導入の
ため，reduce動作時にスタックからポップする記号数を動的に求める必要があっ
た．そのために，GOTO手続きを規則の解析開始か途中かによって別々に計算す
る手法を示した．同様の考え方は，規則の右辺に記号の正規表現を許した拡張
CFG(正規右辺文法)を扱うLR構文解析法として提案されている
\cite{nederhof1994,purdom1981}．正規右辺文法では，規則の右辺に合
致する記号数を予め知ることができないので，解析時に動的に求める必要があ
るためである．



************************ [./logs/V10N05-07/related_study] ************************
関連研究\label{sec:3}
コールセンターのオペレータの代替をコンピュータが行なう理想的な解決方法としては，自然な対話による高精度な質問応答が重要なキーとなる．

対話の研究としては，古くはELIZA~\cite{eliza}があるが，ユーザの問題解決という目的に作られたものではない．対話による情報検索~\cite{oddy}では，システムがユーザに対していくつかの選択肢を提示して検索を進めるもので，ユーザの意図が反映されにくい．近年，意味や文脈を考慮した対話モデル~\cite{katou,iida}が提案されているが，実用レベルには至っていない．

一方，質問応答システムでは，ユーザの問い合わせに対して，一般的な情報源から応答を生成する方法がある．これは，TRECのQAタスクで用いられるような精密な検索クエリを作成することによる検索問題に置き換えるもの~\cite{sanda,sanda2}と，限定された世界において，その世界モデルとユーザモデルの対応により，ゴールを明確にするプランニングの問題に置き換えるもの~\cite{allen}がある．前者は，「米国の第23代大統領の名前は？」というようなピンポイント的な知識を答えとして求めているのに対し，パソコンなどの使用法に関する質問応答は，使用のプロセスのように過去の蓄積した事例を答えとして求めるものが多く，そのアプローチが異なる．後者は応用システムごとに分野知識やモデルの構築が必要で作成コストがかかり過ぎ，変化の激しい現実の問題には向かない，といった問題点がある．

また，質問応答システムのもう一つのアプローチとして，蓄積された以前の質問と応答の検索によって，問題を解決する方法がある．この方法は，いくつかのタイプに分けられる．第一のタイプは，ユーザの問い合わせをオペレータが仲介し，そのオペレータが問題を解決する時の支援として用いられるものである．これは，ユーザとシステムの間を人間が仲介することから，システムには完全性は求められず，たとえ不完全であってもオペレータを支援する意味で効果を挙げているもの~\cite{yanase}もある．第二のタイプは，コンピュータが直接応答を行なうもので， ニュースグループのFAQを対象としたFAQ-Finder~\cite{bruke}や，これを参考にして作られたらのソフトウェア製品を対象としたヘルプデスク~\cite{kurohashi}がある．FAQ-Finderは，ユーザの問い合わせに対して類似したいくつかの質問文をリストとして提示し選択させているが，ユーザとシステムが対話を行ないながら問題を解決していく仕組みはない．ヘルプデスクは，対話の仕組みは取り入れたが，限定された一部の内容の選択にとどまっているのが現状である．

本研究は，モデルに沿って構造化したデータを対象にするのではなく，コールセンターのオペレータの応答記録のような質問応答データを活用して，ユーザが自ら問題を解決できるように，蓄積された質問応答データへナビゲーションするための基礎技術の開発を試みるものである．



************************ [./logs/V11N05-07/related_study] ************************
関連研究

本節では関連研究について述べる．

まず，本稿と同様に同義なテキスト対を照合することによって，
同義表現を抽出する研究について説明する．
この種の研究に用いられる同義なテキスト対には，
以下のようなものがある．それぞれについて説明する．
\begin{itemize}
\item 
  同じ原文からの複数の翻訳を利用する

  同じ原文から作成された翻訳を複数集めると，
  その翻訳同士は同義なテキスト対となる．
  このテキスト対を照合することで
  同義表現を獲得するのである\cite{imamura_nlp2001ws_true,Barzilay:ACL01,shimohata_ipsj_iikae_mt}．
  今村ら\cite{imamura_nlp2001ws_true}はこの方法で同等表現を抽出し，
  60\%程度の精度で正しい言い換え文の生成を行なっている．
  Barzilayら\cite{Barzilay:ACL01}も同様の方法を利用して言い換え表現の候補を9483対抽出し，
  精度はほぼ意味的に等価な言い換え対を正解として85〜92\%の精度であったとしている．
  また，20回以上出現した112個の言い換え表現では，
  WordNetにおいて35\%が同義表現で
  32\%が上位下位関係であったと報告している．
  下畑ら\cite{shimohata_ipsj_iikae_mt}は抽出した同義表現を
  実際に機械翻訳の研究に役立てる研究もしており，
  抽出した同義表現の利用により翻訳可能な文を8\%向上させ，
  正しい翻訳を出力する割合も2.5\%向上させたと報告している．
  
\item 
  同内容の記事対を利用する

  複数の新聞社の記事を収集し，
  同じ事柄を記述している記事群を抽出する．
  この同じ事柄を記述している記事群が
  同義なテキスト対となるのである．
  このテキスト対を照合することで
  同義表現を獲得するのである\cite{sekine_nlp2001ws_true}．
  関根\cite{sekine_nlp2001ws_true}は，
  固有表現抽出の技術を使い，
  固有表現は対応づけのキーとしてテキスト対を照合する工夫をしている．
  この手法は，同じ事柄を記述している記事群を探すところから，
  処理をする必要がある手間の多い手法ではあるが，
  新聞データはたくさんあるので，
  うまくいくと多くの同義表現を抽出できる可能性がある．
  関根の実験では1日の新聞記事から8つの言い換え表現を抽出し
  そのうちの4つが正しい言い換え表現であったとしている．

\item 
  関連のある話し言葉と書き言葉のデータを利用する

  例えば，同内容の講演発表とその予稿を利用するのである．
  学会などでの発表を文字に書き起こしたデータと，
  その発表と同時に出す予稿の論文を利用して同義表現を
  抽出する研究として文献\cite{murata_kaiho_2001,murata_nl2001_henkei,Murata_spoken_written_lrec}がある．
  発表とそれの元となった論文は，同内容のことを同一の著者が
  言った，また，書いたものなので，
  これらも同義なテキスト対と見なせるのである．
  このテキスト対を照合することでも
  同義表現を抽出することができる．
  論文の講演発表に限らず，ある発表とその発表の元になった
  書き言葉のテキストの対も同様に，
  同義なテキスト対と見なせるので，それらからも
  同義表現を抽出することができる．
  この論文では定量的な評価はなされていないが，
  実際に抽出された話し言葉と書き言葉の対が示されている．

\item 
  同じ文書中の同内容の部分を利用する

  例えば，ある論文の要約の部分と，その論文全体は，
  要約がその論文の中身の要約であるので，
  文章の長さはかなり違うが内容は同じであるので，
  同義なテキストとみなせるのである．
  また，ある特許の請求項の節の内容と，その特許の
  実施例の節の内容も，同じ内容が記述されているので，
  同様に同義なテキストとみなせるのである．
  実際に，文献\cite{Murata_ntcir3_patent}では特許の請求項と実施例の対を利用して
  同義表現の抽出を試みている．
  この論文でも定量的な評価はなされていないが，
  実際に抽出された表現の対がいくつか示されている．
  
\item 
  要約前のテキストと要約後のテキストを利用する
  
  要約の研究においては
  要約前のテキストと要約後のテキストを用意して，
  それを比較することで
  要約に関する同義表現を抽出する場合がある．
  このとき，要約前のテキストと要約後のテキストは同内容であるので，
  それらは同義なテキスト対とみなせるのである．
  このテキスト対からも同義表現を抽出することができる\cite{Kato1999}．
  この論文での言い換え表現の評価では，
  上位100個のものはすべて要約用の言い換えとしては
  妥当なものであったとされている．
  同義表現としての評価はなされていない．

\end{itemize}

以上のように，同義なテキスト対を照合することで
同義表現を抽出する研究には多くのものがある．
ところで，本稿で提案している式(\ref{eq:murata_method})で抽出結果をソートする
村田法は，これらの研究にも利用できるものであり，
村田法の適用範囲は広い．

次に以上の方法以外によって同義な表現を抽出する先行研究について述べる．
これには，共起語が類似している単語同士を同義語とする研究がある\cite{shimomura93,lin_coling98,yamamoto_kazu_nlp2002,Dekang_Lin_IJCAI03}．
共起語が類似していればその単語同士も類似している可能性が高く，
類義語の抽出\cite{hindle90}にはよく使われる方法だが，この方法を同義語の抽出にも使うのである．
しかし，この方法では，同義語以外に反義語や類義語を多く抽出してしまう
ことが知られており，同義語の抽出には種々の工夫が必要な方法となっている．
下村ら\cite{shimomura93}は類似度で取り出した上位5208対から
178対の同義語を取り出したとしている．
山本\cite{yamamoto_kazu_nlp2002}は類似度で抽出する手法に加えて
種々のヒューリスティックを利用することで，
66\%の精度で1117個の言い換え可能な表現と
114個の双方向言い換え可能な表現を抽出したとしている．
Linら\cite{Dekang_Lin_IJCAI03}は類似度で抽出する手法に加えて，
同義表現対が出現することのない
パターンを利用して同義表現対かどうかの判断をすることにより，
80個の同義表現を用いた実験で適合率86\%，再現率95\%
の精度でその同義表現を抽出できたとしている．


************************ [./logs/V12N01-01/related_study] ************************
関連研究とその問題点
\label{sec:Related}

本節では，文法をコーパスから抽出する主な類似研究を紹介する．

\begin{figure}[tp]
  \centering
  \epsfxsize=.6\textwidth
  \epsfbox{Related/figure/grammar_extraction.eps}
  \caption{Penn Treebankコーパスからの文法抽出}
  \label{fig:grammar_extraction_from_penntree}
\end{figure}

英語の大規模な構文構造付きコーパスとしてPenn Treebankコーパスがある
\cite{marcus:93}．
Charniakはこのコーパスから``tree-bank grammar''と呼ばれるCFGを抽出し，
人手で作成した文法との比較を行っている\cite{charniak:96}．
tree-bank grammarは，各中間ノードについて，そのラベルを左辺に，
子ノードのラベルを右辺に持つCFG規則を獲得することで抽出できる
(図\ref{fig:grammar_extraction_from_penntree})．
これまで，コーパスから抽出した文法では，
構文解析はうまくいかないと言われていたが，
人手で作成した文法との比較実験の結果，
特に単語数の多い長い文では，コーパスから
自動抽出した文法の解析精度が良くなることを示し，
それまでの一般的な見識が誤りであることを明らかにしている．

一方，日本語では，Penn Treebankコーパスのような大規模な
構文構造付きコーパスが存在しない．
大規模なコーパスとしてEDRコーパス\cite{edr:2001}と
京大コーパス\cite{kurohashi:97}がある．
しかし，EDRコーパスは括弧付きコーパスであり，
付与されている構文木の内部ノードにラベルが付いていない．
京大コーパスは，二つの文節間の依存関係が付与されているだけで，
文節内の構造は付与されていないので，tree-bank grammarのような
CFGは抽出できない．
白井らはEDRコーパスからのCFGの自動抽出を試みている\cite{shirai:97}．
構文木の内部ノードにラベルが付与されていないので，
各内部ノードに対して適当なラベル(非終端記号)を付与する方法を
提案している．

しかし，
日本語，英語いずれの場合にも，
構文構造付きコーパスから抽出した大規模なCFGで
構文解析を行うと，膨大な数の構文解析結果が出力される．
この問題に対し，
Charniakは，コーパス中の出現頻度の低い文法規則を削除し，
確率文脈自由文法(PCFG)で得られる生成確率に基づく最良優先解析
(best-first parsing)を行い，解析途中で曖昧性を抑えている．
これは，出現頻度の低い文法規則は構文解析における曖昧性を
増大させるだけで，解析精度にほとんど影響を与えないという
仮定に基づいている．
しかし，
詳細は後述するが，出現頻度の低い文法規則だけが
構文解析結果の曖昧性を増大させるわけではない．
労力は要するが，構文解析における曖昧性を増大させる要因を人手で分析する
必要があると我々は考えている．

白井らは，構文解析結果の曖昧性を増大させる要因を分析し，
多数の曖昧性を作り出す文法規則を機械的に変更することで，
曖昧性の削減を図っている．
しかし，機械的な変更だけで曖昧性を削減することには限界があり，
人手による変更も必要になる．
人手による変更が必要となる例を以下に挙げる．

\if0
\begin{description}
\item[人間の直観に反する規則: ] 白井らは，
  括弧付きコーパスであるEDRコーパスからCFGを抽出するために，
  内部ノードに付与するラベルを機械的に推定している．
  しかし，抽出される文法規則が人間の直観に合わない場合がある．
  例えば，「変化/し/まし/た/か」という単語列をカバーするノードのラベルを
  考えると(スラッシュは単語区切りを示す)，
  白井らのアルゴリズムでは，右端の「か」が助詞であるため，
  ``後置詞句''となり，次のCFG規則が得られる．
  しかし，直観的には，後置詞句ではなく動詞句の方が適切である．
\end{description}
\fi

\begin{description}
\item[機能による助詞の細分化: ] 白井らは，助詞を
  形態素ごとに細分化することで曖昧性を抑えている．
  しかし，格助詞，終助詞，並列助詞など機能による細分化も
  曖昧性の削減には必要である．
  EDRコーパス中の助詞に付与されている品詞はすべて``助詞''であり，
  機能による細分化は人手を要する．
\item[意図しない非終端記号の割り当て: ] 白井らは，
  括弧付きコーパスであるEDRコーパスからCFGを抽出するために，
  内部ノードに付与するラベルを機械的に推定している．
  しかし，機械的な推定では，アルゴリズムで想定していない
  文法規則を生成することがある．
  例えば，「変化/し/まし/た/か」という単語列をカバーするノードのラベルを
  考えると(スラッシュは単語区切りを示す)，
  白井らのアルゴリズムでは，右端の「か」が助詞であるため，
  ``後置詞句''となり，次のCFG規則が得られる．
  \[\mbox{後置詞句 → 動詞 語尾 助動詞 助動詞 助詞}\]
  直観的には，後置詞句ではなく動詞句の方が適切であるが，
  機械的な推定では，意図しない非終端記号の割り当てを
  細かく除外していくことは困難である．
\end{description}

\if0
例えば，白井らは助詞を形態素ごとに細分化することで
曖昧性を抑えている．
しかし，格助詞，終助詞，並列助詞など機能による細分化も
曖昧性の削減には必要である．
EDRコーパス中の助詞に付与されている品詞はすべて``助詞''であり，
機能による細分化は人手を要する．
さらに，括弧付きコーパスであるEDRコーパスからCFGを抽出するために，
内部ノードに付与するラベルを機械的に推定している．
しかし，抽出される文法規則が人間の直観に合わない場合がある．
例えば，「変化/し/まし/た/か」という単語列をカバーするノードのラベルを
考えると(スラッシュは単語区切りを示す)，
白井らのアルゴリズムでは，右端の「か」が助詞であるため，
``後置詞句''となり，次のCFG規則が得られる．
\[\mbox{後置詞句} \to \mbox{動詞}~~\mbox{語尾}~~\mbox{助動詞}~~\mbox{助動詞}
~~\mbox{助詞}\]
しかし，直観的には，後置詞句ではなく動詞句の方が適切である．
\fi

後者は曖昧性の増減と直接は関係のないことである．
しかし，人間が見て妥当なCFGを作成するためには，
機械的に内部ノードのラベルを推定するのではなく，
(Penn Treebankコーパスのような)構文構造付きコーパスを用意し，
そこから文法を抽出すべきであると考えている．



************************ [./logs/V12N01-04/related_study] ************************
関連研究

\subsection{チャット対話を対象とした対話構造解析研究との関係}
Khanらは，チャット対話から話題や参加者の興味等を抽出することを目的とし
て，本論文で言う応答関係の同定を試みた\cite{Khan:02}．具体的には，質問
とそれに対する応答のまとまりからなるスレッドという単位を定義し，スレッ
ドの始まりの発言を同定する実験を行なった．スレッドの始まりとそうではな
い発言の特徴を分析し，人手でルールを作成した．ルールは表層表現と発言内
の単語数の組合せである．実験対象は，チャットアプリケーションAOL
Instant Messenger(AIM)\footnote{http://www.jp.aol.com/aim/}を利用して
収集した約1500発言で，対話の参加人数の異なりは12名である．

小倉らは，本論文で言う応答関係の同定を目的とし，チャット対話特
有の表現等を手がかりとした同定実験を行なった\cite{Ogura:03}．具体的に
は，引用記号を使用した次話者の指定表現「＞Aさん」や，関連する発言を明
示する表現「ほんとに？＞りんごと蜂蜜」等の表層表現だけで判断できる5 種
類の同定要素を組合せたルールを用いて，関係同定のためのにどのような表
層表現が有効かを調査した．実験対象は，独自に作成したチャット対話データ
であり，参加者の異なりは16名，2人対話と3人対話合わせて870発言である．

これらの先行研究は，本論文では応答関係と定義した関係の同定に必要な手が
かりに焦点を当てたものであり，対話構造の解析及び継続関係の同定には踏み
込んでいない．ルールベースの関係同定手法は，チャット特有の言語表現が発
言中に存在するもののみを主な対象としたものであり，定量的な性能比較は難
しい．しかし，我々のアプローチには，こうした既知の知見を素性としてモデ
ルに組み込める柔軟性を持っているという利点がある．実際，小倉らの知見は，
4.3節で述べたムーブの末尾の表層表現などの素性としてモデルに反映させて
ある．

小林は，多人数が参加するチャット会議のログからの議事録自動生成を目的と
し，本論文で言う継続関係及び応答関係の同定を試みた
\cite{Kobayashi:03}．発話文という単位を処理の基本単位とし，それらの間
の関係を同定した．具体的には，基点となる発話文の前後の発話文を探索し関
係先候補を複数抽出する．その後，発話文内の表層情報と品詞情報を手がかり
として関係先を決定する．発話文を可能な限りつなげていくことで，ある話題
に対する継続した対話部分を抽出する．関係先の決定には，関係先の候補発話
文にヒューリスティックを用いてポイントを与える． ポイントの計算は指定
した表層表現や品詞の有無で加算される．実験対象は，会議の進行役のいる
チャット会議ログであり，関係先を同定するためのヒューリスティクスもこの
対話スタイルに特化したものになっている．

彼らは，ある発話文と二項関係にある発話文を特定する際に，その2 つの発話
文の素性のみを利用している．本手法では，発言間の二項関係同定のために，
関係を同定したい発言とそのまわりの発言の素性も利用した．継続関係につい
て触れられているが，その同定方法については説明されていない．このため，
本論文の解析手法と比較することはできなかった． 

\subsection{音声対話を対象とした対話構造解析研究との関係}
質問とそれに対する応答のような対話構造をある種の文法規則によって捉えよ
うとする考えとして談話分析がある．

Groszらは，談話中の発話の処理を記述するための枠組となる理論を提示して
いる\cite{Grosz:86}．この理論では，談話の構造は，相互に関係する3種の要
素，言語構造(linguistic structure)，意図構造(intentional structure)，
注視状況(attentional structure)から構成される．この理論に基づく談話単
位は，一貫したゴールを持つ部分を1つの単位としている．しかし，本研究で
求めようとしている対話構造の単位は，質問と応答のようなまとまりからなる
単位であり，より小さな単位となる．

Sinclairらは，より局所的な視点から対話構造の階層モデル提案している
\cite{Sinclair:92}．これは，Hallidayのランク尺度の考え方
\cite{Halliday:61}に基づいている．彼らは，談話に対して相互作用，交渉，
エクスチェンジ，ムーブ，アクトの5つのランクからなるランク尺度を定義し，
それに基づいた対話構造のモデルを提案している．

本研究で対象としたチャット対話は，質問とそれに対する応答のような意味的
につながりを持つ発言が必ずしも隣接しない．また，質問とそれに対する応答
を構成する発言自体も区切り送信される場合があり，対話構造を構成する基本
単位が異なる．このような特徴ため，既存の対話構造モデルをそのままチャッ
ト対話に適用することはできない．我々は，対話構造の基本単位をチャット対
話の基本単位に変更し，意味的につながりを持つ発言が隣接しない現象を表現
できるよう，交差を許すモデルに拡張した．

厳寺らは，Sinclairらのモデル\cite{Sinclair:92}を参考にして対話構造の形
成を試みた\cite{Iwadera:98}．彼らは，2人対話を対象とし，発言がなされる
毎に漸次的に対話構造を認識する手法を提案している．具体的には，文末表現
等から構成される表層表現パタンと対話特有の現象である話者交替に関する情
報を用い，現在処理している発言のアクトを同定する．アクト情報を基に，直
前の発言との関係を同定し，ムーブ，エクスチェンジまで発言をまとめあげて
対話構造を構築する．彼らは，音声対話の書き起こしテキストを実験対象とし，
対話構造の同定の際に現在処理している発言とその直前の発言との関係だけを
利用した．

高梨らは，独話を対象に話し言葉の処理の基本単位を節と定義し，節の同定を試
みた\cite{Takanashi:03}．節の同定には，発言に含まれる形態素といった局所
的な情報を利用している．局所的な情報だけでは節の同定が難しい倒置のような
表現は同定対象としていない．

本研究では，意味的につながりを持つ発言が必ずしも隣接しないという特徴を
持つチャット対話を対象としている． このため，現在処理している発言の直
前の発言だけでなく，それよりも前の発言との関連も調べる解析手法を提案し
た．また，倒置等の局所的な情報だけでは判断できない関係の同定も対象とし
た．

************************ [./logs/V12N05-02/related_study] ************************
関連研究
\label{sec:related}

本節では，コーパスを用いて訳語対応等の翻訳知識を獲得する手法に関連する
研究のうち，
言語横断関連報道記事検索に関する関連研究，および，
訳語対応推定に関する関連研究について述べる．

\subsection{言語横断関連報道記事検索}

\ref{sec:clir}~節で述べた言語横断関連報道記事検索の手法に
関連する研究として，内容的に対応した二言語文書を収集する
手法に関する研究がいくつか行なわれている．
二言語文書の種類としては，同一の期間の報道記事を対象として，
内容が対応した二言語の記事を収集するという手法が
いくつか提案されている．
言語を横断して記事の内容の類似性を測定する手法を分類する観点としては，
主として，
i) 言語を横断する際に用いる対訳情報の情報源の種類，
ii) 記事間の類似度を測定する際に，文レベルの対応まで考慮するか否か，
という二点が挙げられる．
i)に関しては，
翻訳ソフト，既存の対訳辞書，あるいは，内容的に対応する既知の
二言語文書から学習した翻訳モデル，等の情報が用いられる．
また，ii)に関しては，
既存の多くの研究においては，文レベルの対応までは考慮せず，
文書全体での類似性を測定している．
そのような事例としては，
例えば，数値表現や名前等の訳語対応を情報源として
用いるもの~\cite{Takahashi97a,Xu99a}，
翻訳システムおよび会社名の対訳辞書を情報源として用いる
もの~\cite{KMatsumoto02a}，
翻訳システムおよび既存の対訳辞書を情報源として用い，
両者の性能比較を行なったもの~\cite{Collier98a}などがある．
また，\cite{Masuichi00a}は，特許文書を対象として，
小規模な対訳文書を初期データとして，
ブートストラップにより言語横断情報検索モデルを学習しながら，
内容的に対応する二言語文書を収集する手法を提案している．
一方，\cite{Hasan01a}は，日中二言語間で内容的に対応する
文書を収集するタスクにおいて，翻訳ソフトおよび漢字を利用した
いくつかの統計量を情報源として用いている．
以上の事例においては，いずれも，文レベルの対応までは考慮せず，
記事全体で類似性を測定している．
それに対して，
\cite{Uchiyama03aj}は，読売新聞およびThe Daily Yomiuriという，
完全な対訳に近い二言語文書対の収集がある程度期待できる文書集合を
対象として，既存の対訳辞書を情報源として，
記事中の文の対応まで考慮した日英記事間の類似度を用いて，
内容的に対応する記事を収集する手法を提案している．

これらの関連研究と比較すると，本論文で述べた
言語横断関連報道記事検索の手法は，
i)の，言語を横断する際に用いる対訳情報の情報源の種類に関しては，
\ref{sec:clir}~節で述べたように，翻訳ソフト，対訳辞書，
数値表現翻訳規則の三種類のうち，単独の情報源としては翻訳ソフトを
用いている．
また，ii)に関しては，文レベルの対応までは考慮せず，
記事全体で類似性を測定している．
したがって，本論文の言語横断関連報道記事検索の手法は，
既存の研究事例で用いられた手法と比較すると，
相対的に簡便な手法であると言える．
本論文における評価実験は，
言語横断関連報道記事検索の手法として，最も簡便な手法を採用した場合に，
どの程度の記事検索性能，および，訳語対応推定性能が達成できるかを
示しているということができる．
本論文の評価実験において，
関連研究で用いられた技術を導入すれば，
言語横断関連報道記事検索の性能が向上することが期待できる．
具体的には，
i)の，言語を横断する際に用いる対訳情報の情報源の種類に関しては，
複数の情報源を併用すること，
また，ii)に関しては，文レベルの対応まで考慮して記事間の類似度を
測定することが考えられる．
ただし，本論文の手法は，
厳密な文対応付けが困難であるような
粗い関連記事群に対しても有効であるという点が長所の一つであると言えるので，
ii)の点に関しては，綿密な分析が必要であると思われる．

また，その他の関連研究として，ウェブ上の二言語文書を対象として，
URLおよびHTML文書の構造における手がかりを利用することにより，
対訳で書かれた文書対を収集する手法も
提案されている~\cite{Resnik03a,Nie99a}．

\subsection{訳語対応推定}

二言語コーパスからの訳語対応推定の手法の研究においては，
これまでに，様々な手法が提案されている．
本節では，いくつかの観点からそれらの手法を整理するとともに，
同一内容の記事組を抽出した後，何らかの形で訳語の対応を推定するという
本論文の問題設定に比較的近い研究事例について，
本論文の手法との比較を行なう．
また，この本論文の問題設定とは独立な観点として，
訳語対応を推定する際にどのような情報を用いるかという観点のもとでの
整理を行ない，関連研究，および，本論文の手法の間の関係について述べる．

まず，訳語対応推定において用いる要素技術は，
大きく分けて，
文対応がつけられた対訳コーパスからの訳語対応推定手法，
および，
コンパラブルコーパスからの訳語対応推定手法という
二種類の技術に分けることができる．
文対応がつけられた対訳コーパスからの訳語対応推定においては，
訳語候補となる語の組に対して，分割表を用いて統計的な相関を測定する
という手法がよく知られている
\cite{Gale91a,Kumano94b,Haruno98dj,Smadja96a,Kitamura96a,Melamed00a}．
一方，コンパラブルコーパスからの訳語対応推定においては，
一般に，訳語候補となる語の組に対して，何らかの方法で文脈の類似性を測定し，
訳語候補の順位付けを行なう．
特に，初期の研究~\cite{Fung95b,Rapp95a}においては，
基本的な語についての既存の対訳辞書を用いずに，
文脈の類似性を測定することが試みられたが，
以後の研究
\cite{KTanaka96a,Fung98a,Rapp99a,Kaji01aj,Chiao02a,TTanaka02a,Gaussier04a}
では，基本的な語についての既存の対訳辞書を用いて，
文脈の類似性を測定している．
また，訳語対応推定の研究に関連した研究としては，
コンパラブルコーパスを用いて，複数の訳語を持つ語の訳語選択を
行なう手法を提案しているものもある~\cite{Dagan94c,HNakagawa01a}．

一方，比較的最近の研究においては，
要素技術としては，特に，コンパラブルコーパスからの訳語対応推定に
おいて用いられた，文脈の類似性に基づく手法を用いるものが多いが，
問題設定そのものとして，
1) ウェブ上のテキストを利用する，
2) 訳語候補の順位付けにおいて，複数の情報源・推定尺度を併用する，
といった点に重点を置いた研究事例がいくつかみられる．
例えば，ウェブ上のテキストを利用する研究事例としては，
ウェブ上で，他言語への翻訳が専門用語に併記されているページを利用して，
訳語対応を推定するもの~\cite{Nagata01a,Cheng04a}がある．
特に，\cite{Cheng04a}では，
英語タームを検索質問として，ウェブ上の中国語ページを収集した
結果から中国語訳語候補を生成し，
中国語訳語候補と英語タームとの間の統計的相関，および，
文脈ベクトルの類似性を併用して，英語・中国語間の訳語対応を
推定する手法を提案している．
また，\cite{Cao02a}では，基本語対訳辞書中の訳語の組合せにより，
複合語の訳語候補を生成し，
ウェブから訳語候補を検索したページから文脈ベクトルを生成して，
訳語対応を推定する手法を提案している．
また，通常の報道記事をコンパラブルコーパスとして訳語対応を推定する
手法の研究においても，
英語・中国語間の翻字の情報と文脈ベクトルの類似性を併用して訳語対応を
推定するもの\cite{Shao04a,Huang04a}などがある．

これらの最近の研究の他に，
本論文の問題設定に比較的近い研究事例としては，
\cite{Fung04a,Munteanu04a}がある．
これらにおいては，まず，
同時期の報道記事をコンパラブルコーパスとして，
同一内容の記事組を抽出し，その記事組から対訳文を抽出する．
そして，その結果から，統計的機械翻訳モデルを用いて訳語対応を推定する\cite{Fung04a}，
あるいは，統計的機械翻訳モデルの性能により，
対訳文の質を評価する\cite{Munteanu04a}，
といったことが行なわれる．
本論文の問題設定と比較すると，
これらの研究事例の問題設定は，
同一内容の記事組を抽出した後，何らかの形で訳語の対応を推定するという
処理を行なう点が類似していると言える．
ただし，最も重要な違いとして，これらの研究事例においては，
対訳文を抽出する過程を経る必要があるのに対して，
本論文の手法においては，
記事対応を粗く推定するだけで，訳語対応の推定が可能である点が
挙げられる．
したがって，本論文の手法は，厳密な文対応付けが困難であるような
粗い関連記事群に対しても有効であるという点が特徴であると言える．

また，上で述べた本論文の問題設定とは独立な観点として，
訳語対応を推定する際に用いる情報という観点から，
関連研究，および，本論文の手法の間の関係を整理することができる．
まず，本論文では，
訳語対応を推定する際に用いる情報としては，
「関連記事組における訳語候補の共起および分割表」
(\ref{subsec:estm-cont}~節)を用いた場合，
および，
「文脈の類似性」(\ref{subsec:estm-cv}~節)を用いた場合の
比較を行なった．
一方，本論文の評価実験で用いなかった情報としては，その他には，
複合語の構成要素の訳語対応~\cite{Cao02a,Yoshimi04aj}，
読み等を利用した翻字の情報~\cite{Shao04a,Huang04a,Yoshimi04aj}等がある．
さらに，これらの複数の情報を併用することにより，
訳語対応推定の性能が改善することが期待できる~\cite{Hino04aj,Yoshimi04aj}．



************************ [./logs/V12N05-08/related_study] ************************
関連研究
\label{sec:related}

\cite{Ehara04aj}においては，
日本語形態素解析システム茶筌
を処理系として，
モンゴル語文の形態素解析を行なうための文法体系の構築を
試みている
．
茶筌の処理系は，基本的には，
活用語・非活用語の品詞体系，および，
活用語の活用語尾の語形変化の体系を定義する機能を持つ．
また，各形態素，および，二個もしくは三個程度の形態素の連接に対してコストを
定義することにより，形態素解析の結果得られる複数の解析結果を絞り込む機能
を持つ．
\cite{Ehara04aj}においては，
茶筌の処理系が持つ機能のうち，
活用語の活用語尾の語形変化の体系を定義する機能を利用することにより，
名詞・動詞の詳細な活用型・活用形を定義している．
また，語尾については，活用語とはせず，変化形をすべて別形態素として
登録している．
この方式と比較すると，
本稿のアプローチは，茶筌辞書のような明示的な文法体系を立てるのではなく，
できる限り抽象化したレベルで音韻論的・形態論的特性を整備し，
この制約を用いて語幹・語尾の接続制約・語形変化規則を記述するという
アプローチであると言える．
本稿のアプローチでは，形態素として辞書に登録されるのは，
(名詞・動詞の)語幹およびそれらの語幹に接続する語尾(の基本形)のみとなり，
語幹や語尾の変化形を別途登録することはしない．
そのかわりに，語幹・語尾が語形変化して生成される句については，
その全ての可能性を語形変化テーブルに登録することとなる．
ここで，日本語文の形態素解析と，モンゴル語における句の形態素解析を
比較すると，モンゴル語においては，句が空白により分かち書きされる点が
特徴的である．
したがって，モンゴル語の句の形態素解析においても，実用的には，
語幹に対して高々数個の語尾が連続して接続する可能性を考慮すれば十分である．
本稿では，モンゴル語におけるこの特性を考慮して，
句の候補をすべてテーブルに登録するアプローチを採用している．

\cite{Cucerzan02a}においては，
スペイン語およびルーマニア語について，
時制・人称・数の活用変化形の生成規則を人手で記述しておき，
コーパス中で実際に観測される不規則変化形との間の類似度を計算して，
各々の不規則変化形に対して，
最も近い規則変化形の時制・人称・数を割り当てるという方法により，
各言語での不規則変化形の形態素解析規則を獲得している．



************************ [./logs/V13N01-01/related_study] ************************
関連研究
\label{関連研究}

我々の手法の特徴は，言語資源を効果的に利用することにより，低出現回数の対訳表現を
抽出することができるという点にある．言語資源を利用する手法には，\cite{Melamed:1995}，
\cite{Al-Onaizan-Kevin:2002} がある．一方，低出現回数の対訳表現を抽出する手法には，
\cite{Moore:2003}，\cite{佐藤2002,佐藤2003}がある．

\cite{Melamed:1995} は，対訳辞書，品詞，語源情報，構文情報の4種類の情報によって，抽出
すべき対訳表現をフィルタリングしている．フィルタリングという点では我々の手法と
似ているが，異なる点は我々の手法は複数の単語列からなる対訳表現候補を抽出対象と
しているのに対し，Melamedの手法は，単語対応に限定している点である．単語対応の
場合は，その組み合わせ数は少なく，言語資源の利用の際にも計算量を考慮する必要は
ない．しかし，任意長の長さの表現を対象にする場合，計算量をできるだけ抑え，資源を
利用するような仕組みが必要となる．我々は，信頼性の高い対訳表現から段階的に抽出する
という漸進的な手法を活かし，処理の途中に対訳辞書利用や人手介入を行うことにより
任意長の対訳表現の抽出の際に起こりがちな計算量の問題を解決している．

\cite{Al-Onaizan-Kevin:2002}は，言語資源としてWebページのような大規模な生成側の
単言語テキストや，トランスリタレーション(音表記)情報を利用している．Al-Onaizanらが
対象にしている言語は，アラビア語と英語であり，両者のような異なる言語族の２言語を
抽出対象とする場合，対応の規則を抽出することが難しく既存の言語知識をいかに効率良く
利用するかが重要となる．この点では我々のアプローチと似ており，Al-Onaizanの手法は
我々の手法にも応用することができる．例えば，我々の手法での人手確認の代わりに
Web上での検索を利用することができる．また，カタカナ表記語はトランスリタレーション情報を
用いて，対応度を再評価する等が考えられる．

一方，\cite{Moore:2003}の手法は，3段階の学習モデルを用いることによって，対応度の精度を
高めていきながら対訳表現を抽出する手法であり，低出現回数の対訳表現も抽出することが
できる．ある語とその訳語は常に一対一の関係にあるという前提や先頭文字種情報などの
表層的な情報を学習モデルとして利用することにより，出現回数が1回の対訳表現でも
精度良く抽出することができる．この手法は，統計モデルと表層的な言語特徴情報のみを
利用し，辞書や形態素解析結果などの既存の言語知識を利用しないため，専門用語の抽出も
可能である．しかし，上述した前提や先頭文字種情報は専門用語の翻訳の特徴であり，
イディオムや慣用句などの抽出精度は下がる．我々の手法では形態素解析結果を利用するが，
対訳表現として適切でない単語列を除去するために利用するに過ぎないので，表\ref{対訳表現抽出例}の
「コモンハウス:the common house」等の専門用語も抽出することができる．

佐藤は，最大エントロピー法\cite{佐藤2002}やSVM\cite{佐藤2003}を利用して，少ない文書でも
高精度で抽出する方法を提案している．しかし，これらの手法は学習用の対訳文書が必要となる．
また，抽出対象を句単位と限定することにより，検索対象を絞っている．

最後に， \cite{山本2001}は，我々の手法と同様，統計的係り受け解析結果を用いて
漸進的な手法で対訳表現抽出を行う．しかし，彼らは文節を越えた構造的な対訳表現を
抽出することを目的としているのに対し，我々は候補とする連続単語列を文法的に意味のある範囲に
限定し抽出間違いを減らすことを目的とする．


************************ [./logs/V13N03-01/related_study] ************************
関連研究

これまで様々な用例ベース翻訳システムが提案されてきたが，それらは経験則
\pagebreak
に基づいて用例を選択しており，提案手法のような確率的な尺度に注意を払っ
ていない．

例えば，\cite{Richardson2001}はマニュアルドメインの用例ベース翻訳シス
テムを提案した．
彼らのシステムは，用例と入力文の間で一致する部分のサイズのみを用いて用
例を選択している．

\cite{Furuse1994,Imamura2002}は，一致サイズとコンテキストの類似度の両
方を用いて用例を選択している．
\cite{aramaki2003}は，それらに加え，さらにアライメントのもっともらしさ
を用いて用例を選択している．
これらの手法では，複数の尺度をどのような重みで考慮するか，という重み付
けの問題が存在する．



************************ [./logs/V13N03-02/related_study] ************************
関連研究との比較
自然言語処理においてSVMsを仮想事例とともに用いている研
究として，Yi らの研究 \cite{Yi2004} がある．
ここで彼らの研究との違いをまとめておく．違いは大きく
二つある．
対象タスクと，仮想事例の元となる事例の選び方である．彼らは
固有表現認識を対象とし，全ての事例から仮想事例を作っている．一方，我々
の研究では，文書分類を対象とし，サポートベクタとなる事例からのみ仮想事
例を作っている．サポートベクタ以外から仮想事例を作っても精度向上にはあ
まり影響せず\footnote{
Sch\"{o}lkopf は，
手書き数字の認識タスクにお
いて，全ての事例から仮想事例を作った場合は精度が向上しなかったと報告し
ている \cite[page 112]{Schoelkopf1997}．
SVM では，サポートベクタ以外の事例は最適超平面の位置を決めるのに影響し
ないので，サポートベクタ以外から仮想事例を作っても精度向上にあまり寄与
しないのは納得できることである．
}，
また事例数増加による学習時間増加のデメリットがあるので，本研究ではサポー
トベクタのみから仮想事例を作っている．
なお，対象タスクが異なるので，仮想事例の作り方が異なるのは言うまでもない．
彼らは，ある文中に現れた固有表現を，同じクラスを持つ別の固有表現で
置き換えて新しい文を作り，これを仮想事例としている．



************************ [./logs/V13N03-04/related_study] ************************
関連研究
\label{sec:related_work}

Utiyamaらは，GDAで意味情報・文章構造がタグ付けされた文書からスライドショー
を生成する手法を提案している\cite{Utiyama99}．GDAタグとは，文書に意味論
的構造や語用論的構造を与えるもので，人手で付与される．まず，共参照を示す
タグから文章構造をボトムアップに決定する．そして，重要なトピックを抽出し，
各トピックに対して関連する文を集め，それらを箇条書きにして一枚のスライド
を生成する．GDAタグを用いることにより，ある程度長い文章についても文章構
造を解析し，スライドを生成することができるが，GDAタグを付与するコストは
大きなものとなる．

安村らは，論文からプレゼンテーション資料の作成支援を行なっている
\cite{Yasumura03j}．まず，論文中の各セクションに対して，使用するスライド
の枚数を割り当て，そして，個々のスライドに対してレイアウトを決定し，論文
中から抽出した文や図表といったオブジェクトを配置している．しかし，この研
究ではTF*IDF法で重要文を抽出しており，文章構造の解析や文簡約は行なわれて
いない．また，入力は\TeX 形式の論文に限られており，本研究のように生テキ
ストからスライドを生成することができない．

次に，個別の処理に関連する研究をあげる．まず，談話構造解析の分野でよく知
られているものとして，Marcuらの研究がある
\cite{Marcu99b,Marcu00,Carlson01}．彼らは談話構造タグ付きのコーパスを作
成し，機械学習の手法を用いることにより談話構造解析を行なっている．彼らの
手法には精度の向上が見られるが，談話構造タグ付きコーパスを作成するにはコ
ストがかかってしまう．これに対して，我々の談話構造解析は一般的なヒューリ
スティックルールに基づいている．我々のシステムの確信度などはもともと比較
的少数の科学技術文章を対象に経験的に定めたものであるが，そのままの設定で
地震ドメインのテキストに対しても，スライドを生成するのに十分な精度を達成
しているといえる．従って，地震ドメインで談話構造タグ付きコーパスを作成し，
機械学習を行なう必要はないと考えている．

また，文末表現の整形で関連するものとして，\cite{Yamamoto05}の研究がある．
この研究では，体言止めや助詞止めといった文末表現に着目し，新聞記事の表現
を，新幹線車内や街頭での電光掲示板で流れるニュースで使われる表現に変換す
る手法を提案している．手法は我々と同じルールベースで，本研究で扱っている
ものよりも多くのパターンを利用しているが，誤り例も報告されており，我々の
扱ったパターンでも十分であると考えている．

Jingらは，自動要約の質を向上させるために，新聞記事とそこから作られた人間
による要約のペアから文簡約の手法を学習している\cite{Jing00}．本研究にお
いても，論文とプレゼンテーションスライドのペアから文の対応関係をとる研究
\cite{Hayama05}を利用して，このアイデアを適用し得ると考えられる．



************************ [./logs/V13N03-07/related_study] ************************
関連研究 \label{chap5}

本研究は，一つの用語から，それに関連する用語集合を収集するという問題を
扱っている．これを，特定分野の用語集合を収集する方法とみなせば，
その関連研究は{\bf 重要語抽出}となる．また，これを，用語間の関連性の推定
とみなせば，{\bf トピックワードグラフ生成}や
{\bf 特定の関係を持つ用語対の自動獲得}と関連する．


\subsection{重要語抽出}

重要語抽出は，与えられた文書(または文書集合)から，その文書の内容を代表
するような重要語を抽出・列挙する技術である．その最も重要な要素は，用語
の重要性を測る尺度であり，tf.idf，C-value
\shortcite{frantzi98cvalue_ncvalue}，FLR \shortcite{nakagawa03flr}や
Term Representativeness \shortcite{hisamitsu00representativeness}など
の尺度が提案されている．

重要語抽出の技術の主な応用は，情報検索のための索引語の抽出である．しか
し，ある特定の分野を代表するような文書集合を集め，この文書集合に重要語
抽出を適用すれば，そこで得られる重要語集合は，その分野の専門用語集合の
候補と考えることができる．このような見方においては，重要語抽出の研究と
本研究は，強く関連する．

重要語抽出技術を用いた専門用語抽出と，本研究の大きな違いは，入出力の違
いである．前者の入出力は，文書集合と専門用語集合(多数)であるのに対し，
後者の入出力は，専門用語と専門用語集合(少数)である．この違いは，出力
する用語の選択に用いる尺度の違いをもたらす．すなわち，前者は，特定の文
書集合における用語の重要度を測る尺度を使用するのに対し，後者は，2つの
用語間の関連度を測る尺度を使用する．

重要語抽出技術を用いた専門用語抽出の大きな問題点は，特定の分野を代表す
るような文書集合を作成することが難しいという点にある．本研究では，入力
を一つの専門用語(シードワード)に限定することによって，この問題を回避
している．しかしながら，その代償として，1回の収集ではそれほど多くの関
連用語(専門用語)を収集することはできない．この新たな問題は，収集され
た用語をシードワードとして，再帰的に関連用語を収集する方法によりある程
度解決できると考えられる．


\subsection{トピックワードグラフ生成}

検索システムDualNAVI \shortcite{niwa99dualnavi}におけるトピックワード
グラフは，検索文書集合を代表するトピックワード集合をユーザーに提示する
方法として提案されたもので，トピックワードを節点，2つのトピックワード
間の関連をリンクとするグラフである．このグラフの生成過程のうち，グラフ
のリンクの作成，すなわち，関連するトピックワードの決定は，ある用語に対
して関連する用語を決定するという側面において，本研究と強く関連する．

上記のリンク作成は，与えられたトピックワード集合の各要素に対して，
最も強く関連するトピックワードを決定することによって行なわれる．
このとき使われる関連性の尺度は，$\frac{a}{a+c}$に相当するような用語の共起
に基づいている．ただし，$a$や$c$を計算する対象は，検索された文書集合である．

このことから分かるように，トピックワード間の関連推定と本研究の大きな違
いは，前者が，ある特定の文書集合(検索された文書集合)における関連性の
推定問題を対象としているのに対し，後者は，文書集合に依存しない，より一
般的な(辞書的な)関連性の推定問題を対象としている点である．この違いは，
前者の目的が文書検索支援であるのに対し，
我々の最終目的は特定分野の用語集の自動編纂であるという違いから来ている．


\subsection{特定の関係を持つ用語対の自動獲得}

与えられたコーパスから，ある特定の関係を持つ語のペアを抽出することは，
上位・下位関係，類義関係などを対象として，比較的よく研究されてきている．
例えば，上位・下位関係の獲得では，上位・下位関係を表す特定の文型パター
ンを用いる方法
\cite{hearst92acquisition_hyponyms}や，HTML文書のリスト構造を利用する方法
\cite{shinzato05html}などが提案されている．
また，類義関係の獲得では，それぞれの語に対して，特定の文脈情報をコーパ
スから抽出し，それらをクラスタリングすることによって，類義語を発見する
方法などが提案されている\cite{hindle90noun_classification,lin98automatic_retrieval}．
これらの方法で得られる「関係」は，特定の文書に依存しない(辞書やシソーラ
スに記述すべきような)一般的な「関係」であり，そのような関係にある語の組
を求めるという側面において，本研究と共通の側面を持つ．ただし，これらの研
究が主に対象としているのは，一般語や固有名詞であり，専門用語ではない．ま
た，使用している技法も大きく異なる．



************************ [./logs/V13N03-08/related_study] ************************
関連研究

日本語などの音声言語が，音声だけでなく文字による表現を持つことの重要性
を考えれば，手話を音声言語に訳さず，手話言語のままテキストとして扱える
ことは，手話の使用者（手話研究者や学習者を含む）にとっても有用であると
考えられる．このため，従来から目的に応じていくつかの表記法が考案されて
きた．その多くは，音声言語における発音記号のように，手話の動作そのもの
を書き取り，再現するのに適した表記法である．

HamNoSys (Hamburg Sign Language Notation System)は国際音声記号のように，
特定の国の手話に依存しないことを目指した表記法であ
る\cite{Prillwitz2004}．手話単語を構成する手の形や位置，動き，掌の向き，
非手指要素といった個々の要素を約200種類の単純な図形記号（文字）で表し，
それらを一定の順序で一列に並べることによって一つの手話単語の動作を表記
する．例えば，図\ref{fig:bear}(a)に示すアメリカ手話で熊を表す手話単語は，
同図(b)のような記号列により表現される．直感的には分りにくいが，研究用途
での使用を想定しており，動作の詳細な記述が可能となっている．
\begin{figure}
  \centering
  \epsfxsize=10cm
  \epsfbox{bears_in_asl.eps}
  \caption{ASLの ``熊'' を表す手話単語の表記：(a) イラストによる描写
    \cite{Fant1994}，\\(b) HamNoSysによる表記\cite{Bentele1999}，(c)
    SignWritingによる表記\\\cite{Sutton2006}}
  \label{fig:bear}
\end{figure}

SignWritingはダンスの振り付け表記法 (DanceWriting) をもと
に，1974年Suttonによって考案された手話表記法であ
る\cite{Sutton2002}．HamNoSysと同様，基本となる記号（International
Movement Writing Alphabet, IMWA）を組み合わせて手話単語を表現するが，基
本記号を一列に並べるのではなく，図\ref{fig:bear}(c)のように，2次元的に
配置することにより，直感的に分りやすい表記になっている．基本記号は手の
形，動き，顔など8つカテゴリ，約450種類が定義されている．HamNoSysとは対
照的に，手紙や新聞，文学，教育など，主に日常生活で使用されることを想定
している．SignWritingを日本手話用に拡張する研究も行なわれてい
る\cite{Honna1990} ．

sIGNDEX\cite{Ichikawa2001,SILE2005}は手話単語をローマ字表記の日本語ラベ
ルで表し，同時表現や非手指要素を表す記号を付加して手話文を表記する．個々
の単語における詳しい手指動作についてはビデオ画像によって別途与えている
（sIGNDEX V.1 の動画語彙数は545語）．眉の上げ下げ(eBU, eBD)，目の開閉
（eYO, eYS），口角の動き（cLD,cLP）など，目に見える動作を現象的に捉え，
記号化することを基本としている．図\ref{fig:signdex}にsIGNDEXによる手話
表記例を示す．


以上は，手話の動作そのものを書き取ることを目的とした表記法であった．一
方，徳田・奥村（1998）\nocite{Tokuda1998}は，計算機上で手話を自然言語と
して処理することを目的とした表記法を提案している（図\ref{fig:tokuda}）．
手話単語には手話単語辞書に登録された日本語見出しを使用し，指文字表記や，
代名詞に対する働きかけを表す動作の表現（左手で代名詞，右手で動詞），単
語の繰り返しなどの表記を定義した．しかし，基本的に日本語対応手話を表記
対象としているため，非手指要素や語形変化を表記する方法については定義さ
れていない．

\begin{figure}[tb]
  \centering
  \begin{minipage}{.65\linewidth}
    \begin{screen}[4]
      \small
      \setlength{\baselineskip}{12pt}
\begin{verbatim}
    pT2dOCHIRA+@eBU+@eYO+hDN+mOS-DOCCHIkOOCHA+
    hDN+mOS-KOOCHAkOOHII+hDN+mOS-KOOHIIdOCHIRA+
    eS2+hDF+mOS-DOCCHI+@@eBU+@@eYO+eYB//
\end{verbatim}
    \end{screen}
  \end{minipage}
  \caption{sIGNDEXによる手話表記例．「コーヒーと紅茶，
    どちらがよい\\です
    か？」に対する手話表記．\cite{Ichikawa2001}から引用}
  \label{fig:signdex}

  \vspace*{2ex}

  \begin{minipage}{.65\linewidth}
    \small
  \begin{screen}[4]
\tt\hfil 今日/本/買う
  \end{screen}
  \end{minipage}
  \caption{文献\cite{Tokuda1998}から引用した手話表記例．「今日，\\
本を買った。」に対する手話表記}
  \label{fig:tokuda}

  \vspace*{2ex}

  \begin{minipage}{.65\linewidth}
  \begin{screen}[4]
    \small
    \setlength{\baselineskip}{12pt}
\begin{verbatim}
    [[[],[[主格,[[[[[imoto.t,[rm,yes],[],[]],
    [[が,助詞,jyosi_ga.t], [],[],[]]],_]]]],
    [奪格,[[[[[kyoto.t,[rm,yes],[],[]],
    [[から,助詞,jyosi_kara.t],[],[],[]]],_]]]],
    [対格,[[[[[tokyo.t,[lm,yes],[],[]],
    [[に,助詞,yubi_ni.t],[],[],[]]],
    _G454]]]]],[[iku.t,[終始可変,rm,lm],[],[]],
    [ます。],_]]]$
\end{verbatim}
  \end{screen}
  \end{minipage}
  \caption{文献(池田・岩田・黒川2003)から引用した手話表記例．\\
    「妹が京都から東京に行きました。」に対する手話表記}
  \label{fig:ikeda}
\end{figure}

池田・岩田・黒川(2003)は，中間型手話を対象とした日本語-手話翻訳システム
において，手話文の格フレーム（入力日本語文の格フレーム中の日本語形態素
を手話形態素に置き換えたもの）からトークファイルと呼ばれる手話動作記述
ファイルを生成する際の中間形式として，手話表記法を定義して用いている．
各形態素での手の位置情報が記述可能となっており，格関係が，名詞や動詞の
位置情報として記述される．表記例を図\ref{fig:ikeda}に示す．表記には，入
力日本語文中の機能語情報が残され，組み込まれている．テキストという形式
はとっているが，他の表記法のように手話を記号化して読み書きするためのも
のではなく，翻訳過程（図\ref{fig:sltext}）における中間表現（中間言語）
に相当するものと考えられる．このため，手話を書き取り，記録し，コーパス
を構築するような用途には適していない．一方，我々は，音声言語に対する文
字表現に相当するものとして手話テキストを捉え，手話文を読み書きすること
を念頭に置いた上で，計算機でも処理しやすい表記法を目指した．

前述のように手話には複数のチャネルを使って複数の形態素を組み合わせた表
現が見られる．しかし，池田・岩田・黒川(2003)ではこのような同時的な語順
に対する表記は定義されていない．同研究は，ほぼ日本語の語順に沿って表現
される中間型手話を目的言語としているため，手話と日本語との語順の違いに
ついては重視されていないのかもしれない．あるいは，表記中に残された日本
語情報から，手話の語順を決定することが可能かもしれない．しかし，手話へ
の翻訳過程を「言語的な変換（テキスト間の翻訳）」と「表現の変換」に分け
たとき，どのような単語をどのような語順で表出するかは前者の段階で決定さ
れるべき問題であり，そのためには手話表記法が語順を記述できる必要がある．
本論文で提案した表記法では，手形と動作による同時的表現は語形変化
（\ref{sec:inflection}節）として，左右の手による同時表現は単語の合成
（\ref{sec:compound}節）として記述可能である．

非手指要素が手指要素と同時的に表現される場合も多いが，徳田・奥村(1998)，
池田・岩田・黒川(2003)ともに，非手指要素の記述方法は定義していない．い
ずれも音声日本語を伴う手話を表記対象としており，そのような手話では日本
語の口話と手指による表現が互いに情報を補完し合うために，非手指要素の役
割が小さくなり，表記する必要性が低いと考えられる．しかし，音声日本語を
伴わない日本手話では，非手指要素が話題化，疑問などの文法標識となるなど，
文法的にも重要な役割も持ち，非手指要素なしでは正しく意味が伝わらないた
め，本表記法では，非手指文法標識（\ref{sec:nms}節）や句読点
（\ref{sec:punctuation}節）として記述できるようにした．

語形変化に関しては，既存の表記法でも一致動詞の方向を記述できるものはあ
るが，数の一致に伴う動作や手形の変化については，表記を定めたものは見あ
たらない．本論文では，手形変化パラメータ，位置の複数形，位置集合
（\ref{sec:inflection}節）を定義することにより，手話の言語的な構造に沿
う形で，記述できるようになった．


************************ [./logs/V14N01-01/related_study] ************************
関連研究
ここでは，主に時間計算量に重点を置いて関連研究を述べる．
日本語はもちろん英語でも依存構造解析 (dependency analysis) は研究されてい
る \cite{Lafferty1992,Collins1996,Eisner1996}．
これらの論文の解析アルゴリズムでは，$O(n^{3})$ の時間がかかる．
ここで $n$ は単語数である\footnote{
Nivre \shortcite{Nivre2003} は，projective dependency parsing の決定的
なアルゴリズムを提案している．
このアルゴリズムの時間計算量の上限は $O(n)$ である．
このアルゴリズムをスウェーデン語のテキストで評価している．}
    $^{,}$\footnote{
Ratnaparkhi は実際の処理時間が $O(n)$ となる句構造を返す英語のパーザにつ
いて述べている \cite{Ratnaparkhi1997}．
これは依存構造解析について述べたものではない．
このアルゴリズムでは，解析途中の各ノードに対して，
いくつかの手続きを行ない，その中で確率値の高い $K$ 個の導出 
(derivation) を残して解析を進める (幅優先探索)．
そのため，時間計算量の上限は $O(n^2)$ と考えられる．
}．


日本語の係り受け解析では，文中の二つの文節の係り受けの確率を使うことが
非常に多かった．
Haruno ら \shortcite{Haruno1998} は，決定木を用いて係り受けの確率を推
定した．
Fujio と Matsumoto \shortcite{Fujio1998} は Collins のモデル\cite{Collins1996} の
修正版を日本語の係り受け解析に適用した．
Haruno らと，Fujio と Matsumoto の両グループとも CYK 法を用
いている．これは $O(n^{3})$ の時間がかかる．ここで $n$ は文の長さ，つまり文
節数を表している．
Sekine ら \shortcite{Sekine2000Backward} は最大エントロピー法 (Maximum
Entropy Modeling; ME) を係り受けの確率の推定に使い，後方ビームサーチ 
(文末から文頭に向かうビームサーチ) で最
もよい解析結果を見つける．
このビームサーチのアルゴリズムは $O(n^{2})$ の時間がかかる．
Kudo と Matsumoto \shortcite{Kudo2000Japanese} らも同じ後方ビームサーチ
を ME ではなくサポートベクタマシン (SVMs) とともに用いている\footnote{
彼らは，係り先候補間の係りやすさの相対的な大小関係をモデル化する手法
も報告している \cite{Kudo2005}．係り受けのモデルは異なるが，
同じく後方ビームサーチを用いている．
}．

二つの文節間の係り受けの確率を使わない統計的手法も少ないながらある．
一つは Sekine の決定的有限状態変換器を用いる手法\cite{Sekine2000Japanese} である．
Sekine は，\TermHead{}の場所の 
97\% は文中の五つの候補でカバーされると報告している．
似た現象は Maruyama と Ogino \cite{Maruyama1992} も観測している．
これらの調査にもとづき，Sekine は，決定
的有限状態変換器を用いる効率のよい解析アルゴリズムを提案している．
このアルゴリズムは，考慮する係り先の文節数を制限することでしらみつぶし
に探索することを避け，$O(n)$ の時間計算量となっている．
しかしながら，彼のパーザは京大コーパスに対して 77.97\% の係り受け正解
率 (定義は第~\ref{subsec:results} 節で述べる) しか得られていない．
これは，89\% を超える現在の最高精度よりもかなり低い．

2文節間の係り受けの確率を用いない別の興味深い手法は，Kudo と Matsumoto \shortcite{Kudo2002} による 
Cascaded Chunking Model である．このモデル
は \cite{Abney1991,Ratnaparkhi1997} のアイデアにもとづく．
彼らはこのモデルと SVMs を用いて，89.29\% を得ている．
彼らの手法では，解析時に評価される係り関係の数は CYK 法や後方ビームサーチ
よりも相当少ないが，それでも時間計算量の上限は $O(n^2)$ である．

以上見たように，高い精度を保ちつつ線形時間の処理を保証して，
どのように日本語係り受け解析を行なうかは，
まだ解決されていない問題である．
以下に記述するアルゴリズムがこの問題に対する答えとなろう．



************************ [./logs/V14N02-01/related_study] ************************
関連研究


語の関連性を自動的に得る方法は，これまでにさまざまな研究が行われている．
コーパス中での語の共起情報をもとに語の関連度を測る指標として，様々なものが提案され用いられており
\cite{Church90,Wettler93,Croft99,Curran02-2}，それらは大きく2つに分けられる．
1つは単語ベクトルを用いたベクトル空間手法である．
これは，単語を多次元ベクトル空間の単語ベクトルで表現し，
それぞれの単語ベクトルを比較することで関連度を測る手法である．
ベクトル空間手法では，表\ref{CompareMethod}のようにベクトルの内積をもとにした計算指標が用いられている．
表\ref{CompareMethod}において，$x_i,y_i$はそれぞれ単語ベクトル$\vec{x},\vec{y}$の$i$番目の要素を表す．
なお，overlap係数はバイナリベクトルにしか用いることはできない．
単語ベクトルの要素の取り方は研究によって様々であり，
各文書への出現頻度を要素とするベクトルや
各単語との共起頻度を要素とするベクトルなどが考えられる．
ただし，独立な事象の確率は足し合わせることができないため，
内積を用いる関連度では，語の出現確率を単語ベクトルの要素とすることは不適切と考えられる．


もう1つはコーパス中での確率を用いる確率手法である．
この手法では，2語がコーパス中で共起する確率をもとに
関連度を算出している．確率手法で用いられている計算指標を表\ref{CompareMethod}に示す．
表\ref{CompareMethod}において，$p(w \cap w')$は語$w,w'$の共起確率を表し，
$p(w \cup w')$は語$w,w'$のどちらかが出現する確率を表す．
また$f$は\cite{Lin98a}で定義されている関数であり，$f(w,r,w')$は語$w,w'$が$r$の関係を持って
出現する頻度を，$f(*,r,w')$は語$w'$がいずれかの語と$r$の関係を持って出現する頻度を表す．
これらの計算指標は，ベクトル空間手法で用いられている指標を書き換えたものが多い．
また，単語同士の共起確率ではなく，
各単語が他の語と共起する確率の確率分布関数の類似性を
用いて関連度を算出する研究も数多く行われている\cite{Brown92,Baker98,Slonim00}．
確率分布関数を用いた類似度は，確率分布類似度(Distributional Similarity)と呼ばれる．
類似した名詞は共通した動詞と共起すると仮定し，動詞との共起分布の類似性
から関連度を算出している．

\begin{table}[b]
 \begin{center}
  \caption{類似度の計算指標}
    \label{CompareMethod}
  \begin{tabular}{|c|c|c|c|}
   \hline
    \multicolumn{2}{|c|}{ベクトル空間手法} & \multicolumn{2}{|c|}{確率手法} \\ \hline
   cosine &
       $\frac{\vec{x} \cdot \vec{y}}{\sqrt{|\vec{x}||\vec{y}|}}$ &
   相互情報量 & 
   $\log \left( \frac{p(w \cap w')}{p(w)p(w')}\right)$  \\ \hline

   dice & 
   $\frac{2(\vec{x} \cdot \vec{y})}{\sum(x_i+y_i)}$ &
   dice & 
   $\frac{2p(w \cap w')}{p(w \cup w')}$ \\ \hline

   Jaccard & 
   $\frac{\vec{x} \cdot \vec{y}}{\sum(x_i+y_i)}$ &
   Jaccard & 
   $\frac{p(w \cap w')}{p(w \cup w')}$  \\ \hline

   overlap & 
   $\frac{|\vec{x} \cap  \vec{y}|}{min(|\vec{x}|,|\vec{y}|)} $ &
   T検定 & 
   $\frac{p(w \cap w')-p(w')p(w)}{\sqrt{p(w')p(w)}}$ \\ \hline

       Lin$^{*1}$ & 
   $\frac{\sum(x_i+y_i)}{|\vec{x}| + |\vec{y}|}$ & 
       Lin98A$^{*2}$\footnotemark &
   $\log \left(\frac{f(w,r,w')f(*,r,*)}{f(*,r,w')f(w,r,*)}\right)$ \\   \hline   
\multicolumn{4}{l}{$^{*1}$ \cite{Lin98a}で提案されている手法．}\\
\multicolumn{4}{l}{$^{*2}$ \cite{Lin98a}で提案されている手法．}
   \end{tabular}
  \end{center}
\end{table}


語の関連度が得られれば，関連度に基づいて語をクラスタリングすることで関連語が得られる．
実際には，同じクラスタに分類された語同士を関連語や同義語であるとしている．
語のクラスタリングには分布クラスタリング(Distributional Clustering)が用いられることが多い．
分布クラスタリングとは，類似した名詞は共通した動詞と共起すると仮定し，
各語の動詞との確率分布の類似度に基づいて，
データを結合もしくは分割していくクラスタリング手法である\cite{Pereira93,HangLi98,Dhillon02}．

これらコーパスから関連度を自動的に算出する手法では，コーパス内に出現する
語しか扱えないという欠点がある．
そのため，広範囲の語をカバーするためには，広範囲の内容をカバーする
コーパスが必要となる．

近年では，より広範囲の語をカバーするためにWebをコーパスとして用いることが提案されている．
しかしWeb上の文書は莫大であり，直接収集し，解析するためには非常に大きな時間コストと
設備コストがかかる．そのため，Web全体での語の出現頻度や2語の共起頻度を獲得するためには
従来のコーパスを用いたシソーラス構築とは異なる工夫が必要である．
そのような工夫の一つとしてKilgarriffらは検索エンジンを用いた
手法を紹介している\cite{Kilgarriff03}．「語$w_a$」をクエリーとして検索エンジンを利用すると，
語$w_a$のWeb上でのヒット件数が得られる．検索エンジンは非常に多くのページを
クローリングしているため，このヒット件数を語$w_a$のWeb全体での出現頻度と近似できる．
同様にして，「語$w_a$ and 語$w_b$」をクエリーとすれば，
Web上での語$w_a$と語$w_b$の共起頻度を獲得することができる．

検索エンジンから獲得できる頻度情報を用いて関連度を算出する手法としては，次のようなものがある．
Heylighenは検索エンジンのヒット件数を用いた語の関連度の尺度により，語の分類や語の曖昧性解消，より優れた検索エンジンの開発
の可能性を示唆している\cite{Heylighen01}．
BaroniやTuerneyは，類義語を同定するために，検索エンジンを用いた語の関連性の尺度を提案している\cite{Baroni04,Turney01}．
Turneyはその結果を用いることでTOEFLのシソーラスの問題で平均的な学生よりもよい得点を挙げたことを報告している．
佐々木らは検索エンジンの上位ページとヒット件数を利用した専門用語集の自動構築を行っている\cite{Sasaki05}．
Szpektorは名詞ではなく動詞の関連度を検索エンジンを用いて定義している\cite{Szpektor04}．
これら検索エンジンを用いて関連度の計算を行っている研究では，条件付き確率や表\ref{CompareMethod}の確率手法で定義されているような
相互情報量，Jaccard係数が計算指標として用いられている．



************************ [./logs/V14N02-02/related_study] ************************
関連研究
\label{sec:related_work}

関連研究として，\cite{Fujii00}は，言語横断情報検索の目的のため
に要素合成法による訳語推定法を提案した．
本論文では，ここで提案されているスコア関数に対して，部分対応対訳辞書だけ
でなく，英辞郎自体も利用できるように拡張し（スコア関数`DP-CP'），他のスコ
ア関数との比較を行った．
その結果，スコア関数`DP-CP'は他のスコア関数と比べ，$Y_S$全体に対する評価
では最も再現率が高かったが，不正解訳語も多く生成されるため，訳語候補が1
つ以上生成される用語に対する評価では，精度は高くないことがわかった．
そして，F値に関しては，他のスコア関数とほとんど差がなかった．
\cite{Fujii00}の手法と，本論文で提案した手法の重要な違いの一つは，
\cite{Fujii00}においては，訳語推定対象の用語が属する分野の文書のみを含む
コーパスではなく，様々な専門分野にわたる65種類の日本の学会から出版された
技術論文を集めたものをコーパスとして利用していることである．
また，\cite{Fujii00}において，彼らは言語横断情報検索の性能のみを評価し，
訳語推定の性能評価はしていない．

\cite{Adama04}も，言語横断情報検索の性能を評価対象として，クエリー翻訳の
方法を提案している．
この研究では，コーパススコアはNTCIR-1\cite{Kando99-NTCIR1}，または，NTCIR-2\cite{Kando01-NTCIR2-JEIR}の言語横断情報検索タスクの検索課題文
書（論文等の技術文書）から求める．
コーパススコアには，\cite{Fujii00}で提案されたスコアと合わせて$\chi^2$検
定を用いたスコアを併用している．
しかしながら，2つのコーパススコアの併用は，言語横断情報検索の精度向上に
は貢献しなかったと報告されている．
また，カタカナ語に関しては翻字技術を適用している．

\cite{Baldwin04multi}も要素合成法による訳語推定手法を提案している．
コーパスに基づく8つの素性と辞書に基づく6つの素性とテンプレートに基づく2 
つの素性を立て，SVMを利用して訳語候補のスコア関数を学習している．
この論文でも，辞書に基づく素性でのみ，もしくは，コーパスに基づく素性での
みスコア関数を構成するよりも，両者を利用した方が精度が良いことが報告され
ている．
\cite{Baldwin04multi}の手法と，本論文で提案した手法の重要な違いは，
\cite{Baldwin04multi}においては，コーパスとして，
英語側ではReuters Corpusを，日本語側では毎日新聞を利用しているの
に対し，本論文では，専門分野コーパスを利用した場合と，サーチエンジンを通
してウェブ全体を利用した場合の比較を行っている．
また，\cite{Baldwin04multi}においては，訳語推定対象の用語を，英語2単語
または，日本語2形態素のものに限定している．

\cite{Cao02as}もまた，複合語に対する要素合成法による訳語推定法を提案した．
\cite{Cao02as}の手法では，用語の訳語候補は，用語の構成要素の訳語を結合す
ることによって構成的に生成され，サーチエンジンを通してウェブ全体を用いて
検証される．
本論文では，サーチエンジン通してウェブ全体を用いて訳語候補の検証をするス
コア関数を導入することによって，\cite{Cao02as}で提案されたアプローチの評
価を行い，ウェブから収集した専門分野コーパスを用いる方法と比較した．
その結果，訳語候補が一つ以上出力される場合においては，サーチエンジンを通
してウェブ全体を用いるよりも，ウェブから収集した専門分野コーパスを用いる
方が精度が良いことがわかった．
その一方で，再現率に優れるのは，サーチエンジンを通してウェブ全体を用いる
方法であった．
そこで，この2つの方法の長所を生かすために，まず，ウェブから収
集した専門分野コーパスを用いる方法で訳語推定を行い，訳語候補が一つも得ら
れなかった場合，サーチエンジン通してウェブ全体を用いて訳語推定を行う方法
を評価した．
その結果，本論文で評価したどのスコア関数よりも高いF値を達成できることが
わかった．
なお，\cite{Cao02as}においては，英語の用語に対して中国語の訳語を推定して
いるが，訳語推定対象の用語は英語2単語から構成されるものに限定されている．

\cite{Maeda00}は，言語横断情報検索のためのクエリー翻訳の方法を提案してい
る．
まず，要素合成法によりクエリーの訳語候補を生成する．
次に，ウェブ上の頻度が一定数以上の訳語候補に対して，それぞれの訳語候補の
スコアは，訳語候補の構成要素間の相互情報量を拡張した尺度で計算される．
最後に，スコアの閾値を越える訳語候補を（サーチエンジンの）OR演算子で結合
したものを，クエリーの翻訳結果とする．
評価は言語横断情報検索の性能に関して行っているので，訳語推定結果の比較は
できないが，\cite{Maeda00}らの手法は，本論文で言えば，コーパススコアのみ
のスコア関数を利用した手法に対応する．

\cite{Kimura04}も，言語横断情報検索のために，クエリー翻訳における訳語
曖昧性解消の方法を提案している．
準備として，あらかじめYahoo!の日英のウェブディレクトリのそれぞれのカテゴ
リにおいて，特徴語の抽出と重み付与をし，日英のカテゴリの対応付けをしてし
ておく．
検索をするときは，まず，クエリーに含まれる単語とカテゴリの特徴語を利用
して適合するカテゴリを決める．
次に，クエリーに含まれる各単語に対して，訳語を対訳辞書で調べる．
そして，適合カテゴリの特徴語となっている訳語のうち，特徴語の重みが最も大
きいものをその単語の訳語と決定する．


************************ [./logs/V14N03-04/related_study] ************************
関連研究
\label{sec:kanren}

評判情報・意見抽出の研究例としては，\citeA{Tateishi2001,Kobayashi2003,Kobayashi2004,Kobayashi2005}の研究がWeb上に大量に存在する言語データから特定の製品の評判情報を抽出している．彼らの方法は，評価表現を肯定的・否定的表現に分類してあらかじめ辞書として用意しておくという固定的な尺度を用いているのに対し，本研究では，アスペクトは状況によって評価が変化しうるという前提に立って取捨選択の様子をEBAに則って捉えるという点が異なっている．また，\citeA{Nasukawa2005}は種となる好不評表現を少数定義し，文章中でその種表現が存在する場所を特定することで好評・不評の認知結果を抽出している．彼らの研究では情報処理機器を対象としており，ユーザーから質問や苦情の形で挙がってきた言語データの中から開発サイドが対処すべき項目を抽出するという目的のため，認知結果が好評・不評に分類できるという前提に基づき，不評の認知結果を抽出するという立場に立っている．このように，評判情報・意見抽出の研究例の多くは評価表現を固定的に捉えているが，それらが選択に際して実際にどの程度寄与しているかについての分析は必ずしも十分とは言えない．一方，本研究の方法は選択のプロセスを捉えることになるため，取捨選択に寄与した認知結果をピンポイントで抽出することができる．

また，認知結果は必ずしも1語では表現できず，「待ち時間が長い」のように長い語となる点に注意する必要がある．これについて，\citeA{Kobayashi2005}は認知結果とその評価を〈対象，属性，属性値，評価〉の4つ組で抽出することを試み，例えば〈フィット（ホンダの車種名），走行性能，キビキビ，満足〉という組を抽出し，その上で，属性値と評価の区別は困難であるため，評価と属性値を合わせて評価値としている（図\ref{fig:4_2}）．これに対して本研究では，「疲れる」「暑い」のように，「身体が」等の属性を表す語は通常省略されて属性と属性値が1語になる場合や，「到着時間の見込みがたてやすい」のように属性と属性値の区別が曖昧な場合があり，両者を合わせて認知結果とすることが適切である．また，本研究では，評価は選択肢の取捨選択のトリガーとして捉えるので，特定の属性値に特定の評価を与えるという立場には立っていない．

\begin{figure}[t]
  \begin{center}
  \begin{picture}(300,75)(0,10)
	\put(10,70){\makebox(60,15)[l]{\shortciteA{Kobayashi2005}}}
	\put(200,70){\makebox(90,15){評価値（例：キビキビ＝満足）}}
	\put(245,65){\oval(90,10)[t]}
	\put(100,40){\framebox(40,20){対象}}
	\put(150,40){\framebox(40,20){属性}}
	\put(200,40){\framebox(40,20){属性値}}
	\put(250,40){\framebox(40,20){評価}}
	\put(195,35){\oval(90,10)[b]}
	\put(150,10){\makebox(90,15){認知結果（例：疲れる）}}
	\put(10,10){\makebox(60,15)[l]{本研究}}
  \end{picture}
  \end{center}
      \caption{\protect\shortciteA{Kobayashi2005}の方法と本研究の方法の違い}
  \label{fig:4_2}
\end{figure}

印象表現に関する研究例としては，楽曲に関する印象表現の研究\shortcite{Kumamoto2002}や，テレビ番組に関する印象表現の研究\shortcite{Hitachi2000}がある．さらに，\shortciteA{Kumamoto2004}は楽曲検索システムにおける程度語の研究も行っている．それに対し，本研究では表現収集を直接の目的とはしておらず，選択プロセスを捉えることに主眼を置き，それに必要な表現をシソーラスを利用しながら収集する立場をとる．

また，自由記述型アンケートの自由回答文から人間の心理状態を分析しようと試みる研究例もある．\citeA{Inui2004}は，道路に関する自由記述型アンケートの自由回答文を，回答の背後にある態度や回答意図の分類に焦点を当てて分析している．本研究の対象は態度や意図ではなく選択理由であるという違いはあるものの，言語データからヒトの心理状態を知ろうとする点が本研究と類似しており，また，表現に着目するという点でヒントを与えてくれている．

経路選択にどのような要因が影響を及ぼすかについての研究もいくつかなされている．\citeA{Nakamura2002}は駅周辺の危険・不快要因を研究している．\shortciteA{Fukuda2002}は交通手段選択行動を対象として各種項目の5段階の主観的評価を尋ねている．このように，経路選択行動には種々の要因が影響を及ぼす．しかし，いくつかの要因については個々に研究がなされているが，選択行動に関して包括的に要因を捉える研究は十分なされているとはいえない．そこで，本研究では被験者にことばで記述してもらう方法で選択のきっかけとなる要因を捉えることを試みる．


************************ [./logs/V14N03-10/related_study] ************************
関連研究

テキストから，「意見」「評価」「感情」など主観表現の態度を抽出，または分類する研究では，処理対象とする要素が様々である．論文によって要素の名称が異なるが，今節では説明のため代表的な要素の表記を統一する．対象へ向けられた意見，評価，感情などは「態度」とする．同様にして態度が向けられた対象は「対象」，主観を表明した主体は「主体」，「態度」における肯定的，否定的，中立などの属性を「極性」とする．

\subsection{主観の極性を判定する研究}

テキスト中に表明された主観を扱う研究の中のひとつの大きなグループは，極性を判定する研究である．Turney（Turney 2002）はWeb上の映画のレビューテキストから態度を抽出し，その極性を検索エンジンを用いて得られた「excellent」または「poor」との共起しやすさから判定し，さらに抽出された態度の極性の集合から，テキストの極性を判定している．極性を正しく判定するため，態度を単語ではなく前後の文脈を追加した句として抽出している．舘野（舘野2002）は，企業のサポートセンターによせられた「お客様の声」に含まれる態度に着目し，事前に行った構文構造解析から木構造を用いて，否定的な極性を含む文を抽出している．那須川ら（那須川，金山2004）は，デジタルカメラまたは映画について書き込まれているWeb上の掲示板から，極性が既に判定されている既知の態度をもとに，新たな態度を極性付きで抽出している．極性の判定には，極性が定義されている態度と新たな態度の間に，極性を反転させる「しかし」などの表現が出現するかどうかを利用している．立石ら（立石ら2004）は評判情報検索に，態度，対象，極性を組み合わせた辞書を用いている．たとえばコンピュータの分野において「小さい」が1,000表現中8回出現し，そのうち7回が肯定的なら，コンピュータが対象のとき，「小さい」という態度は肯定的な極性であると判定される．Kim and Hovy（Kim and Hovy 2004）は，態度，主体，対象，極性に着目し，抽出を行っている．主体ごとに極性の方向（肯定的か，否定的か，中立か）とその強さを計算することで，主体がどのような極性をもっているか判定している．

\subsection{主観の極性以外の側面も扱う研究}

小林ら（小林ら2005）は態度，対象，対象の属性を抽出している．対象の属性とは，対象の要素のことである．例えば「携帯電話」における対象の属性は「液晶画面」であり，大きな枠組みである対象と，下位要素である対象の属性を区別している． 

 Wiebeら（Wiebe et al. 2005）は，手作業でコーパスにタグ付けを行うことにより，態度の構成要素を定義している．主観的な発言や明示的である率直な態度に関しては，態度，主体，対象，極性に加え，主観の強さ，表現の強さ，主観に実態があるかどうか（仮定の話か，実際の話か），の各要素を定義している．表現による主観的な態度については，態度，主体，主観の強さ，極性を定義している．発話や記述の事実に関しては，発話や記述の部分とその主体，対象を定義している． 

 Liuら（Liu et al. 2003）は，常識知から出来事と感情の組み合わせを学習することで，文を感情カテゴリに分類している．例えば「自動車事故で恐怖を感じた」という事例から「自動車事故」は「恐怖」という組み合わせを辞書に登録することにより，「自動車事故にあった」という文を「恐怖」という感情カテゴリに分類している．田中ら（田中ら2004）はテキストの情緒を推定するため，日本語語彙体系をもとに作成した結合価パターンを用いている．この研究で情緒属性と呼ばれている要素には，「前提条件」「情緒主（主体）」「情緒対象（対象）」「原因」「情緒名」がある．「太郎がコンサートのチケットを入手した」という文から情緒主「太郎」は原因「獲得」から「獲得による喜び」という情緒を導き出している． 


大塚（大塚2004）は，道路計画に対する住民への自由記述アンケートテキストが要求か否かを判定している．要求の要素として要求動機，要求内容，要求意図が定義されている．要求意図が明示的に表明されていなくても，要求動機が出現することでそこに暗黙的な要求意図が存在すると示されている．例えば「歩道がせまい」という事実を要求動機ととらえることで，「歩道を広くして欲しい」という暗黙的な要求意図を導き出している．ただしテキストが要求か否かの判定に関しては，明示的要求のみを対象としている． 


これまでの研究において，我々が「理由」と呼ぶ理由，原因，根拠，動機などは，態度を導きだしたり，テキストや文，句などを分類するための手がかりとして扱われてきた．本研究では利用者にとって作品レビューが鑑賞する作品を選択するのに参考になるかどうかを判断する手がかりとして感情表現の「理由」に着目し，その特性や働きの分析を行った．


************************ [./logs/V14N04-04/related_study] ************************
関連研究

これまでに，語彙的選好を明示的に扱う構文解析手法がいくつか提案されてきた．
白井らは，PGLRの枠組みに基づく統計的構文解析手法を提案している
\cite{Shirai1998}．語彙的選好として，例えば$P（パイ|を，食べる）$のような確
率を新聞記事5年分から学習している．しかし，本研究で用いたような格フレー
ムは導入しておらず，用言の意味的曖昧性を区別せずに確率推定を行っている．
京都テキストコーパス中の比較的短い500文を用いて評価を行い，84.34\%の解析
精度であったと報告している．

藤尾らは，語の共起確率に基づく構文解析手法を提案している\cite{Fujio1999}．
2つの語が係り受けをもつ確率と距離確率の積で定義した確率モデルを用いてお
り，それらの確率はEDRコーパスから学習している．EDRコーパス1万文を用いて
評価を行い，86.89\%であったと報告している\footnote{文末から2つ目の文節も
評価に入れている．}．

阿辺川らは，同じ用言を係り先とする格要素間の従属関係と，格要素・用言間の
共起関係を利用した構文解析手法を提案している\cite{Abekawa2006}．これら2
つの関係を新聞記事30年分から収集し確率モデルを学習している．既存の構文解
析器の出力するn-bestの構文木候補に対して，確率モデルに基づくリランキング
を適用し，もっとも確率値の高い構文木を選択している．京都テキストコーパス
中の約9,000文を用いて評価を行い，既存の構文解析器よりも0.26\%高い91.21\%
    の精度を実現している\kern0pt$^{3}$．さらに，阿辺川らの被連体修飾詞の解
析\cite{Abekawa2005}を統合することによって，0.04\%高い91.25\%の精度を得て
いる．

一方，語彙情報を素性として用いている様々な機械学習手法が提案されている．
その中でもっとも良い精度を実現しているのは，工藤らが提案している統計的構
文解析手法である\cite{Kudo2002}．この手法は，SVMに基いてチャンキングを段
階的に適応していくモデルであり，京都テキストコーパスから学習している．同
コーパス（約40,000文）を用いて2分割交差検定により評価を行い，90.46\%の精度
    を実現している\kern0pt$^{3}$．しかし，数万文程度のタグ付きコーパスから
では，係り先候補間の語彙的選好を十分学習するのはほとんど困難であると思わ
れる．なお，本論文の実験で比較対象とした「CaboCha」は，本手法を実装した解
析器である．

\vspace{-0.5\baselineskip}

************************ [./logs/V14N05-04/related_study] ************************
考察と関連研究



一般に複数の解をアンサンブルすると，複数の解の平均よりも良い値が得られると考えられる．
本実験でも 18 個のデータセット中 17 個でアンサンブルの効果が得られているが，
データセット tr23 に関しては，本手法のエントロピーの値の方が高い．
これは解の分散の影響と考えられる．

実験で得られた各データセットに対する NMF による 20 個のクラスタリング結果の
エントロピーの分散と，\mbox{表\ref{tab:result}}における
NMF mean と weighted hypergarph との差（つまりアンサンブルによる改善の度合い）を
プロットした図を\mbox{図\ref{kou}}に示す．図の横軸が分散を示し，
縦軸がweighted hypergarph と NMF mean との差（改善の度合い）を示している．

\begin{figure}[tbp]
\begin{center}
\includegraphics{14-5ia4f3.eps}
\caption{解の分散とアンサンブルによる改善}\label{kou}
\end{center}
\end{figure}

\mbox{図\ref{kou}}をみると，分散が大きい2つ（cranmad と reviews）は，
アンサンブルによる改善の度合いも大きいことが分かる．
そして3番目に分散が大きなデータセットが tr23 である．
つまり分散の大きな解をアンサンブルすると，非常に良い結果を
得ることもあるが，逆に悪い結果を得ることもあり得ると考えられる．

データセット tr23 に対する NMF の結果を見ると，1つだけ非常にエントロピーの
低いクラスタリング結果が得られていた．この解を取り除いて，
19個のクラスタリング結果で本手法によるアンサンブルを試したところ，
NMF mean のエントロピーは 0.493，weighted hypergarph のエントロピーは 0.492 となり，
アンサンブルの効果が現れた．

また，ここでは NMF で複数個のクラスタリング結果を生成する際に，
個々のクラスタリング結果のクラスタ数は，最終的な
クラスタ数と一致させている．
しかしハイパーグラフの考え方を用いれば，
生成される個々のクラスタリング結果のクラスタ数は任意でかまわない．
実際に k-means では少ないクラスタ数に直接クラスタリングするよりも，
多数のクラスタに分割してから，目的のクラスタ数にまとめた方が
効果があることが経験的にわかっている．
論文\cite{fred02data}ではこのヒューリスティクスを利用して，
多数のクラスタに分割してから，アンサンブルを行っている．
本手法においても，そのような工夫を取り入れることも可能である．


本手法ではハイパーグラフの値として，1 に当たる部分を行列\( V \)の
値を用いることで，実数値に変換した．
この効果は実験で確認できている．
この工夫を更に進めると，0 に当たる部分にも行列\( V \)
の値を用いることで，実数値に変換することが考えられる．
この場合，ハイパーグラフは単純に各クラスタリング結果に
対応する行列\( V \)を結合させたものになる．
実際にこのようにして作ったハイパーグラフに対して，クラスタリングを
行ってみた．結果を表\ref{tab:vresult}に示す．
ここで hypergraph V が行列\( V \)を結合させてハイパーグラフを作成する手法を示す．

\begin{table}[tbp]
\input{04t3.txt}
\end{table}

通常のハイパーグラフを使うよりも結果は良好であるが，
1 に当たる部分だけを精密化する方が効果があることがわかる．
また 0 の値はそのままにしている方が，ハイパーグラフがスパースになり，
データ間の類似度が 0 であるケースが生じやすくなる．
そのためグラフスペクトル理論を用いたクラスタリング手法\cite{graph-minmax-cut}なども使えるように
なるために好ましい．

最後にアンサンブル学習\cite{breiman96bagging}との関連について述べる．
アンサンブル学習とアンサンブルクラスタリングの違いは，
クラスタにラベルがつくかどうかである．
アンサンブル学習ではデータにラベルが付くので，
そのラベルをもつデータがラベル付きのクラスタと見なせる．
アンサンブルクラスタリングの場合は，クラスタにラベルがついていない．
もしもクラスタにラベルをつけることができれば，
アンサンブル学習の手法を直接利用できるために，
さらなる改良や発展が可能である．
クラスタにラベルをつける処理は，
クラスタ数が 2 や 3 などの小さい場合はそれほど大きな問題ではないので，
今後はクラスタにラベルをつけるという戦略で，アンサンブルを行う手法を開発したい．



************************ [./logs/V14N05-05/related_study] ************************
関連研究

言語学，日本語教育学，自然言語処理の3つの分野における，
機能表現（特に，複合辞）の扱いについて述べる．

\subsection{言語学（日本語学）}

複合辞というとらえ方を初めて提唱したのは，
国立国語研究所の資料\shortcite{hukugouzi}によると，
永野\shortcite{Nagano1953}である．
永野は，
語源的・構造的にはさらにいくつかの語に分解できるが，
単なる部分の合成以上の「一まとまりの意味を持っているものと見てよい」
連語形式の助詞相当表現の存在を指摘し，これを複合助詞と呼んだ．
そして，
同様の基準で複合助動詞，複合感動詞，複合接続詞についても考え，
複合助詞とこれらを合わせて，複合辞と呼んだ．

森田ら\shortcite{Morita1989}は，複合辞に関する大量の用例を収集し，
数百の複合辞の意味と用法について詳細に分析している．
また，この研究を受け，
国立国語研究所は，代表的な複合辞を選定し，
それらの用例集を作成した\shortcite{hukugouzi}．

言語学においては，現在も，
複合辞の研究が活発に続けられている\shortcite{Fujita2006}．

\subsection{日本語教育学}

日本語教育学において，
複合辞は，文法項目として重要視されている．
例えば，日本語能力試験 1，2級の文法問題を解くためには，
さまざまな種類の複合辞について
正しく理解している必要がある\shortcite{nouryoku}．
そして，この理解を助けるために，
日本語学習者のための，日本語文法の辞典は，
複合辞を見出しに立て，
それらについて詳しく解説していることが多い
\shortcite{Makino1986,Makino1995,Jamasi1998}．

\subsection{自然言語処理}

自然言語処理において，
複合辞は，それらを一まとまりの意味の塊として扱う必要があることから，
特に，機械翻訳において重要視されてきた．
首藤ら\shortcite{Shudo1980a,Shudo1980b}は，
機械翻訳への入力とするために，
概念を表す内容語と機能的な付属語列からなる拡張文節
という考え方を導入し，
そこで機能語に相当する1単位として複合辞を扱っている．
現在，彼らは，日本語において，2500の機能表現を収集し，
それらを意味に基づいて分類している\shortcite{Shudo2004}．
しかしながら，
異形についての大規模な整理は行なわれておらず，
彼らの辞書は，五十音順以外に特別な構造を持っていないようである．

EDR日本語単語辞書\shortcite{EDR_2}には，
助詞相当語82表現，助動詞相当語49表現が登録されているが，
異形に関する情報は記載されていない．

兵藤ら\shortcite{Hyodo2000}は，2つの層を持つ日本語機能表現の辞書を提案している．
第一の層には，375の項目があり，
これらの項目から，第二の層において，
自動的に13,882の可能な出現形が生成される．
この辞書は，
表現のある部分に対して交換可能な文字列を列挙しているだけであり，
2つの異なる表現間の関係についての情報を何も提供しない．

これらの辞書が機能表現の異形を適切に扱っていないのに対して，
われわれの辞書は，
階層構造に基づいて機能表現の異形を整理しており，
2つの表現間の関係について，
「音韻的異形」や「表記のゆれ」といった情報を提供することができる．

日本語話し言葉コーパスにおいては，
助詞相当句29表現と助動詞相当句37表現が，
長単位の見出し語として扱われている\shortcite{csjMORPH}．
それらの表現は，丁寧形や異形態などの観点から，
前者は80，後者は92の表現に細分されている．

土屋ら\shortcite{Tsuchiya2006a}は，
複合辞の用例データベースを作成するにあたり，
「現代語複合辞用例集」\shortcite{hukugouzi}に
記載されている複合辞123項目（見出し語に相当）に対して異形を展開し，
細分した337小項目の表現を用例収集の対象としている．
彼らは，助詞の交替や文体などの観点から異形を分類し，
それぞれの小項目に対して8文字のIDを付与している．

これらの研究の機能表現リストは，小規模なものである．
一方，われわれの辞書は，
見出し語で341，
出現形で16,771の機能表現を分類整理している．





************************ [./logs/V14N05-07/related_study] ************************
関連研究
\label{sec:関連研究}

\cite{Uchimoto04aj,Uchimoto04}は，話し言葉コーパス\cite{CSJ}を対象コーパ
スとして，半自動で精度良く短単位・長単位の2種類の粒度の形態論的情報を付
与する枠組みを提案している．
この枠組みでは，なるべく少ない人的コストで話し言葉コーパス全体に2種類の
粒度の形態素情報を付与するため，最初に短単位の解析を行い，次に，短単位の
形態素情報を素性として，短単位をチャンキングすることによって長単位の形態
素情報を付与するという手順を採っている．
例えば，「という」という機能表現は，短単位列としては助詞「と」および動詞
「いう」の連体形の2短単位に分割され，長単位としては助詞「という」という1 
長単位にチャンキングされる．
短単位から長単位をチャンキングするための機械学習手法としては，最大エント
ロピー法(ME)とSVMを比較し，SVMがより優れていると報告している．
内元らの研究は，話し言葉コーパス全体を対象としているのに対して，本論文で
は，機能表現に焦点をあてて検討を行っている点で異なる．
そのため，内元らは話し言葉コーパス中の長単位全体に対する形態素解析精度の
評価は行っているが，機能表現に特化した評価は行っていない．一方，本論文で
は，既存の解析系における機能表現の取り扱い状況を整理した上で，機能表現に
特化した性能評価を行っている．
また，本論文では，対象となる機能表現のリストを事前に用意しているため，形
態素列のどの部分が機能表現として検出される可能性があるかという情報（チャ
ンク素性およびチャンク文脈素性）を利用して，チャンキングを行うことができ
る．
機械学習手法としては，CRFとSVMを比較し，SVMの方が検出性能が高いことを示
している．

\cite{shudo.coling80,shudo.NL88,shudo.NLC98,shudo.mwe2004}は，機能表現や
慣用表現を含む複数の形態素からなる定型的表現をできるだけ網羅的に収集し，
機能表現間に類似度を定義して，機能表現の言い換えや機械翻訳に利用すること
を提案している．
\cite{hyoudo.NLC98,hyoudo.NLP99,hyoudo.NLP00}と\cite{isaji.NLP04}は，日
本語の文構造の解析を容易にするため，通常よりかなり長い文節を単位として解
析を行うことを提案し，機能表現を含む大規模な長単位機能語辞書を作成してい
る．
しかし，これらの先行研究における日本語処理系においては，機能表現と同一の
形態素列が内容的に振る舞う可能性が考慮されていない．

\cite{knp-2.0}と\cite{TKudo02aj}は，機能表現を考慮して，係り受け解析を実現
している．
\cite{knp-2.0}では，接続詞として形態素解析辞書に登録されている
機能表現は，形態素解析時に検出される．次に，構文解析時に，解析規則に記述
された特定の形態素列が現れると，直前の文節の一部にまとめたり，直前の文節
からの係り受けのみを受けるように制約を加えて，機能表現を考慮した係り受け
解析を実現している．
\cite{TKudo02aj}では，形態素解析辞書に「助詞・格助詞・
連語」や「接続詞」として登録されている機能表現は，形態素解析時に検出され
る．また，「ざるを得ない」などの表現は直前の文節の一部としてまとめること
によって，機能表現を考慮した係り受け解析を実現している．
しかし，\ref{subsec:既存の解析系}~節で述べた通り，
これらの手法において考慮されている機能表現の数は，
我々の一連の研究において対象とした機能表現の数よりも少ない．
また，これらの研究では，機能表現検出が係り受け解析に
どれだけ効果的かという評価を行っていない．
一方，本論文では，評価対象を機能表現候補を含む文節に限定し，機能表現検出
が係り受け解析にどのような影響を与えるのかを調べ．機能表現検出が，係り受
け解析に効果的であることを示している．

\cite{Tsuchiya07aj}では，本論文の\ref{sec:chunker}~節の内容に相当する
機能表現のチャンキングについて述べられており，本論文では，
この結果をふまえて，機能表現検出の結果を考慮した日本語係り受け解析手法
（\ref{sec:係り受け解析}~節）を提案している．
\cite{Tsuchiya07aj}と本論文との差分は\ref{sec:係り受け解析}~節の内容に
相当するが，技術的な内容を本論文の記述範囲で完結させるために，
本論文では，\ref{sec:chunker}~節を設けて，
機能表現のチャンキングについても記述している．



************************ [./logs/V15N02-01/related_study] ************************
関連研究\label{sec:kanren}

ここでは，
本稿の基礎として，
クラス所属確率を推定する代表的な方法である Platt の方法および，Zadrozny らにより提案されたビニングによる方法と Isotonic 回帰による方法について述べる．
これらはいずれも 2 値分類を想定しているが，
Isotonic 回帰による方法においては，2 値分類を多値分類に対応させる方法についても述べる．
最後に，
Platt の 方法と Isotonic 回帰による方法について，多種類の分類器とデータセットによる実験を行って比較した Caruana らによる研究~\cite{Caruana04,Mizil05} について述べる．

\subsection{Platt の方法}

Platt~\cite{Platt99} は，分類器を SVM に限定し，
分類スコアを事例に対してクラスが予測された際の分離平面からの距離 $f$ として，
シグモイド関数 $P( f ) = 1/\{1+\exp(Af+B)\}$ により [0,1] 区間に変換される値 $P (f)$ をクラス所属確率の推定値として用いることを提案した．
ただし，パラメータ $A$ および $B$ は，あらかじめ最尤法により推定しておく必要がある．
シグモイド関数による方法の利点は，
分類スコアから直接，クラス所属確率の推定値を求めることができるため，
パラメータ $A$ および $B$ が推定されていれば，手続きが容易であることである．
Platt は，シグモイド関数の過学習を避けるために，out-of-sample モデルを用いて，
Reuters~\cite{Joachims98} を含む 5 種類のデータセットを用いて実験を行い，
この方法の有効性を示した．
データセットが Adult の場合における結果を図\ref{Platt}~\cite{Platt99} に示す．
図~\ref{Platt} において，$X$ 軸は分類スコア，$Y$ 軸はクラス所属確率を表し，$+$ 印は分類スコアを 0.1 の区間に分けた場合に対応するクラス所属確率の実測値，実線は推定値を表す．


\begin{figure}[b]
  \begin{center}
\includegraphics{15-2ia1f1.eps}
\caption{Platt の方法による推定値と実測値の例}
\label{Platt}
  \end{center}
\end{figure}


しかし，Bennett~\cite{Bennett00} は，Platt の方法は分類器がナイーブベイズの場合にうまくいかないことを Reuters 21,578 データセット\footnote{
	\texttt{http://www.daviddlewis.com/resources/testcollections/reuters21578/}
}により示した\footnote{
	Bennett~\cite{Bennett00} の実験では，特に，出現頻度が少ないクラス（例えば $Corn$ など）
	において信頼度曲線（3.2.1 節を参照のこと）による評価が悪かった．
}. 
また，Zadrozny ら~\cite{Zadrozny02} も，この方法がデータセットによっては適合しない場合があることを示し\footnote{
	Zadrozny ら~\cite{Zadrozny02} の実験では，Adult データセットと 
	TIC (The Insurance Company Benchmark) データセットにナイーブベイズ分類器を適用した場合は，
	スコアの変換がうまくいかなかった．
}，以下に述べる方法を提案した．

\subsection{ビニングによる方法}

Zadrozny らは，分類器としてナイーブベイズを想定し，ビニングによる方法（ヒストグラム法）を提案した~\cite{Zadrozny01a,Zadrozny01b}. 
ビニングによる方法は，未知の事例のクラス所属確率を直接推定せずに，
あらかじめ作成しておいた「ビン」を参照し，そのビンにある正解率を用いて間接的に推定を行う方法である．
ビニングによる方法における処理手順は次の通りである．

まず，訓練事例を分類スコアの値順に並べ，
各区間に属する事例数が等しくなるように区切ってビンを決める．
このとき，各ビンに属する事例の分類スコアから，そのビンに所属する事例における分類スコアの最大値と最小値を調査しておく．
ここまでの処理を図~\ref{bining} に示す．
図~\ref{bining} はナイーブベイズ分類器の例で，数値（斜体）は分類スコアを表す．


\begin{figure}[b]
  \begin{center}
\includegraphics{15-2ia1f2.eps}
\caption{ビンの作成例（訓練事例数が 12 でビンの数が 3 個の場合）}
\label{bining}
  \end{center}
\end{figure}


次に，各ビンごとに正解の事例を数えてそのビンに属す全事例数で割り，正解率を計算する（表~\ref{bining1} を参照のこと）．
最後に，未知の事例の分類スコアから該当するビンを見つけ，そのビンの正解率を未知の事例のクラス所属確率値とする．

実験は KDD'98 データセット\footnote{
	\texttt{http://kdd.ics.uci.edu/}
}を用いて行われ，
平均二乗誤差や平均対数損失による評価の結果，有効性が示された（ビンの数が 10 個の場合）．
ビニングによる方法は処理が単純であるという利点があるが，
最適なビンの個数をどのようにして決めればよいか（各ビンに含まれる事例数をいくつにするか）という問題がある．

なお，Zadrozny らは，この後に，誤分類に対するコストを考慮した方法として，ビニングによる方法を改良した「Probing」という方法を提案したが，
実験の結果，有効性を示さない場合も多かった\footnote{
	決定木，バギングされた決定木，SVM, ナイーブベイズ，ロジスティック回帰において，
	UCI machine learning repository や UCI KDD archive, 2004 KDD における
	計 15 種類のデータセットを用いて実験し，二乗誤差，クロスエントロピー，
	AUC (Area under the ROC curve) によりを評価を行った．
}~\cite{Zadrozny05}. 

\subsection{Isotonic 回帰による方法}

Zadrozny らは，ビニングによる方法の問題点を解決する方法として，
次には，分類スコアと正解率が単調非減少な関係にあるという観察に基づく Isotonic 回帰による方法\footnote{
	Chanらは，語の曖昧性解消タスクにおける EM アルゴリズムで，Isotonic 回帰による方法を
	用いてクラス所属確率の推定を行った~\cite{Chan06}.
}を提案した~\cite{Zadrozny02}. 
ここで，Isotonic 回帰問題とは，
実数の有限集合 $Y=\{y_{1}, y_{2}, \cdots, y_{n}\}$ が与えられたとき，
制約条件 $x_{1}\le \cdots \le x_{n}$ の下で目的関数 $\sum_{i = 1}^{n} w_i(x_i - y_i)^{2}$ を最小化する 2 次計画問題である~\cite{Kearsley96}. 
ただし，$w_i$ は正値重みを表す．

Isotonic 回帰問題の解法としては，PAV (pool-adjacent violators または pair-adjacent violators) アルゴリズム（以下では，PAV と略す）が最も代表的であり~\cite{Kearsley96,Ahuja01,Mizil05,Fawcett06}, Zadrozny らが提案した Isotonic 回帰による方法も PAV が適用されている．
ここで，PAV とは，単調非減少ではないブロックがある場合に，そのブロック内に存在する値のすべてをブロック内の値の平均値で置き換える処理を繰り返すことにより，全体の単調非減少性を保つ方法である．
例えば，
前述の目的関数において重みがすべて 1 のとき，
\{1, 3, 2, 4, 5, 7, 6, 8\} において，まず \{3, 2\} 
のブロックが単調非減少ではないために，ブロック内のすべての値を平均値 2.5 で置き換えて \{1, 2.5, 2.5, 4, 5, 7, 6, 8\} に修正する．
次に，\{7, 6\} のブロックが単調非減少ではないために，同様に平均値 6.5 で置き換えて \{1, 2.5, 2.5, 4, 5, 6.5, 6.5, 8\} に修正する方法である~\cite{Kearsley96}. 

PAV を用いた Isotonic 回帰による方法も，ビニングによる方法と同様に，最初に訓練事例を分類スコア順にソートする必要があるが，
事例をまとめて扱わずに，各事例に対して正解率（正例の場合は 1, 負例の場合は 0 となる）を付ける点が異なる（図~\ref{Isotonic} における開始時点の表を参照のこと）．
正解率が分類スコアと単調非減少な関係になるまで正解率の修正を繰り返し，最終的に定まった値を正解率とする（図~\ref{Isotonic} における終了時点の表を参照のこと）．
図~\ref{Isotonic} では 1 回修正された値が再度修正されることはなかったが，値の並び方によっては再修正される可能性が高く，一般的には何度も修正が繰り返される場合が多い~\cite{Kearsley96,Ahuja01,Mizil05,Fawcett06}. 


\begin{figure}[t]
  \begin{center}
\includegraphics{15-2ia1f3.eps}
\caption{Isotonic 回帰による方法における正解率の修正例（SVM を利用し事例数が 10 の場合）}
\label{Isotonic}
  \end{center}
\end{figure}

実験は，
ナイーブベイズ分類器と SVM において KDD'98 データセットなどを用い，
ビニングによる方法やシグモイド関数による方法と比較された（ビニングの数は 5 個から 50 個まで変えて行われた）．
平均二乗誤差による評価の結果，PAV による方法はビニングによる方法を常に上回ったが，
シグモイド関数による方法との差は少しであった．

Zadrozny らは，次に，多値分類においては，分類器は各々の予測クラスに対して分類スコアを 1 つずつ出力すると仮定し，多値分類における PAV の効果を調査した．すなわち，2 値分類において PAV により推定したクラス所属確率値を統合した場合と，PAV を用いずに推定した値を統合した場合との比較を行った~\cite{Zadrozny02}. 
Zadrozny らは，この実験の前に，あらかじめ，ナイーブベイズ分類器とブーステッドナイーブベイズにおいて 20 Newsgroups データセット\footnote{
	\texttt{http://people.csail.mit.edu/jrennie/20Newsgroups/}
}などを用いた実験を行って，2 値分類への分解法である all-pairs と one-against-all の間で精度の差がないことを確認し，実験ではすべて one-against-all を用いた．
2 値分類における推定値を統合する方法としては，
one-against-all に対応した正規化の方法の他に，
どちらの分解方法にも対応可能な最小 2 乗法による方法や対数損失を最小化するカップリングの方法が用いられたが，正規化の方法が最もよい結果を示した．


PAV の有効性については，まず，ナイーブベイズ分類器とブーステッドナイーブベイズによりデータセット Pendigit を用いた実験の結果，
分類器や統合する方法に関係なく，平均二乗誤差による評価では改善がみられたが，
エラー率による評価ではほとんど改善されなかった．
次に，ナイーブベイズ分類器によりデータセット 20 Newsgroups を用いた実験結果も，
多値分類への統合方法に関係なく，平均二乗誤差による評価では改善がみられたが，
エラー率による評価ではほとんど改善されなかった．
ここで，2 値分類における推定値の 3 種類の統合方法を比較すると，
ナイーブベイズ分類器による値を PAV により修正した値を正規化する方法がよかったが（平均二乗誤差により評価した場合），他の分類器や評価法においては差がなかった．

なお，Zadrozny らは，この後さらに提案した Probing とよばれるクラス所属確率の推定方法を多値分類へ拡張する場合には，ここで述べた統合方法を用いずに，
one-against-all により分解した各組において 2 値分類として推定した値をそのまま用いるという非常に単純な方法を示した~\cite{Zadrozny05}. 
ただし，この方法に対する評価実験は行っていない．

\subsection{方法の比較}

Caruana ら~\cite{Caruana04,Mizil05} は，
アンサンブル学習を含めた 10 種類の分類器（SVM, ニューラルネット，決定木，k 近傍法，
bagged trees, random forests, boosted trees, boosted stumps,  ナイーブベイズ分類器，
ロジスティック回帰）を，
8 種類のデータセット（UCI Repository から 4 種類，医療分野から 2 種類選んだデータセット，IndianPine92 データセット~\cite{Gualtieri99}, Stanford Linear Accelerator）に適用し，
Platt の方法と Isotonic 回帰による方法 (PAV) の比較を行った．
その結果，
Platt の方法はデータが少ないとき（約 1,000 サンプル未満）に効果的であり，
Isotonic 回帰による方法は過学習しない程度に十分なデータがあるときによかった．

Jones ら~\cite{Jones06} は，検索を成功させるために，ユーザが入力したクエリから新しくクエリを生成して置き換えるというタスクにおいて，置き換えられたクエリの正確さの程度を予測するために確信度スコアが必要であると考え，Isotonic 回帰による方法 (PAV) とシグモイド関数による方法についての簡単な比較実験を行った． 
その結果，
Isotonic 回帰による方法は過学習の問題があり，
平均二乗誤差および対数損失のいずれにおいてもシグモイド関数による方法の方が上回ったため，
彼らのタスクではシグモイド関数による方法が採用された．


************************ [./logs/V15N02-02/related_study] ************************
関連研究
\label{sec:関連研究}

同義語を自動的に計算する研究は，これまで数多く行われてきた．
その種類としては，カタカナと英語の対応，英語とその略語の対応，日本語とその
略語の対応などがある．
略語処理では，略語の近傍に括弧書きで略語の定義がされている場合の
研究がある~\cite{schwartz03}, \cite{pustejovsky01}. 
この手法は，略語の定義が略語の近傍でされているものについては有効であるが，
文書の中で必ずしも略語の定義がされているとは限らない．本論文で扱う文書では略語の定義は
されていないので，この手法は適用できない．
カタカナとアルファベット（英語）の対応では，
Knight らは，カタカナとアルファベット（英語）の対応を発音記号から対応付けしている~\cite{knight98}.
阿玉らは，カタカナのローマ字表記とアルファベットとの対応付けをしている~\cite{adama04}. 
Terada らは，英語における原型語とその略語の対応を
両者に含まれる文字及びその順序などの情報を使用することで同定している~\cite{terada04}. 
この研究も本論文と同じく，航空分野という特定分野を対象としているが，
対象とする言語が英語であり，略語をその原型語に復元するタスクを目的としている．

同義語の類似度の計算は，文脈情報から余弦を用いて計算するものが多い．
文脈情報として，語句の前後の局所的なものを用いるもの~\cite{terada04}, 文書全体から抽出して
用いるものがある~\cite{sakai05}. 
Ohtake らは，カタカナの変形を探すのに，エディット距離で候補を絞った後に，文脈情報を
用いているが，その際，カタカナが用いられている構文を解析して，動詞，名詞，助詞を使用してい
る~\cite{ohtake04}. 
Masuyama らは，カタカナ処理で WEB データから英語に対応するカタカナのエディット情報を取得している~\cite{masuyama05}. 
文脈情報を用いる場合には，全ての種類の語句を用いるのでなく，内容語を用いるものが多い．

計算量の削減及び精度の向上のために，文脈情報だけではなく，文字情報を用いて，
対応関係を絞り込む，または，決定する研究が多い．

本論文では，日本語を対象とし，漢字，ひらがな，カタカナ，アルファベット，およびそれらの
略語の類似度を同時に計算するために，
文字情報による絞り込みは行わず，文脈情報のみでどの程度の精度が得られるかを
実験した．
Terada らは，英語を対象として，略語とその原型語の対応を
文脈情報および文字情報を使用して行っているが~\cite{terada04}, 
略語とその原型語のみならず，その他の同義語においても文脈情報を使用する
ことにより，クエリに対する同義語が得られると考えた．
したがって，提案手法は，Terada らの手法を応用し，言語を日本語に適応し，対象を略語から同義語に
拡張し，文脈情報の使用に工夫を加えたものである．また，Terada らは，略語復元の
精度を向上させるために，略語の多いコーパスと
略語の少ないコーパスを使用しているが，提案手法では，同義語が同一のコーパスに
含まれている場合は，コーパスは 1 つでよいと
考え，1 種類のコーパスのみを使用した．
文脈情報のみを使用しているが，同義語の日本語の文字種
（漢字，ひらがな，カタカナ，
アルファベット）について，種類の組み合わせにより精度が異なるかを調べ，今後の精緻なシステム
構築の参考となるようにした．
さらに，文脈情報のみでは，十分な精度が得られない場合があるので，既知の
同義語を知識として使用することにより，精度の向上を図った．




************************ [./logs/V15N02-04/related_study] ************************
関連研究

乾らは，
語彙・構文的言い換えを，次の6つに分類した\shortcite{Inui2004}．
\begin{enumerate}
\item 節間の言い換え
\item 節内の言い換え
\item 内容語の複合表現の言い換え
\item 機能語／モダリティの言い換え
\item 内容語句の言い換え
\item 慣用表現の言い換え
\end{enumerate}
本研究は，機能表現の言い換えに焦点をあてているので，
上記の(4) 機能語／モダリティの言い換えと，
(1) 節間の言い換えの一部である「接続表現の言い換え」に
分類される．

自然言語処理において，
日本語機能表現の言い換えに関する研究は少ない．
飯田ら\shortcite{Iida2001}は，
機能表現の解説文や例文から，
279個の言い換え規則を人手で作成している．
土屋ら\shortcite{Tsuchiya2004}は，
機能表現を含む文とその機能表現を言い換えた文の対のデータを作成し，
そこから642個の言い換え規則を半自動的に生成している．
これらの研究で作成された言い換え規則は，
ある機能表現と別の機能表現が言い換え可能であることを示す
個別的なものである．
このような個別的な規則の集合を用いる手法では，
数多く存在する機能表現の異形を言い換えるために，
膨大な量の言い換え規則を作成しなければならない．

Tanabeら\shortcite{Tanabe2001}，
Shudoら\shortcite{Shudo2004}，
本田ら\shortcite{Honda2007}は，
「なければならない」や「てもよい」など，
助動詞型機能表現に対して約150の意味的等価クラスを定義し，
意味的等価クラス間における
論理的類似性規則と語用論的類似性規則に基いて
機能表現を言い換える手法を提案している．
彼らの研究が対象としている機能表現は，
助動詞型機能表現のみであり，
「にあたって」や「からすると」のような格助詞型機能表現や，
「にもかかわらず」や「や否や」のような接続助詞型機能表現などは
扱っていない．

これらの研究において提案されている機能表現言い換えシステムは，
言い換え先の機能表現の文体や難易度を制御できる機構を持っていない．
くわえて，
これらのシステムは，
体系的に機能表現の異形を扱っていないため，
機能表現$f$を異なる機能表現$f^\prime$に言い換える場合，
潜在的には$f^\prime$のすべての異形を
生成することができることは保証されていない．
一方，
われわれが提案する機能表現言い換えシステムは，
形態階層構造と意味階層構造を持つ機能表現辞書を用いることにより，
文体と難易度を制御しつつ，
機能表現を言い換えることができる．
そして，このシステムは，
与えられた機能表現の出現形に対して，
意味的に等価な機能表現のすべての出現形を列挙することができる．
このシステムの言い換え対象は，
表~\ref{tab:good}に示されるように，
助動詞型機能表現だけでなく，
すべての型の機能表現である．

伊佐治ら\shortcite{Isaji2005}は，
解析後に
機能表現を標準的な表現（代表表記）に言い換えることができる
日本語の文節構造解析システムibukiCを提案している．
例えば，
このシステムは，
「でしょう」を「だろう」に，
「からすると」を「からすれば」に，
「に違いない」を「にちがいない」に言い換える．
しかしながら，
この機構は十分であるとは言えず，
「にたいしまして」を「にたいして」に，
「なければならない」の異形である
「なけりゃならない」，「ねばならない」を
「なければならない」に言い換えない．
一方，われわれの言い換えシステムは，
形態階層構造と文体などの情報を利用することにより，
体系的に機能表現を代表表記に言い換えることができる．




************************ [./logs/V15N02-05/related_study] ************************
関連研究\label{sec_関連}

興味の分析や抽出を行う先行研究には，
多数の販売記録から対象者が興味を持つ商品の推薦を行うものや，
時系列分析を行って
単語の出現傾向から現在注目されている
イベントや単語を検出する研究などがある．
さらに最近では文書の書き手が個人
であるBlogなどを用いることで主観情報の分析や個人の興味，トレンド分析を行
う研究が存在する．

奥村らはWeb上の文書を時間情報が含まれたdocument streamで扱い，
burst検出手法に基づいて分析を行い注目されて
いる話題を検出する手法を提案している{\cite{fuziki}}．
このシステムではBlogなど
に対する書き込み時間を基に単語の出現間隔が短くなっている箇所に注目すべき
話題があるとして注目されている話題の検出を行っている．
このように
現在注目を集めている話題，及び過去に注目を集めていた話題を検出するのは
トレンド分析の1種であり，
興味を分析する研究の1つとして挙げられる．


また西原らは興味を持ってもらえるタイトルやアブストラクトの作成を目標に，興味
を引く研究発表のタイトルを作成する支援システムを提案している\cite{nishihara}．
システムは論文タイトルを入力にとり，興味を値として推定しこの値によっ
て入力されたタイトル群に順位を付与している．
興味の値として
タイトルに含まれる名詞の分かりやすさと，単語の組み合わせ
の斬新さという2つの尺度からタイトルの面白さを評価している．

興味自体の分析を行っている研究として福原らのKANSHINの開発が挙げられる
\cite{fukuhara2}．このシステムはBlog記事を大量に収集し時系列分析す
ることで社会の{「関心動向」（トレンド）}を把握することを
可能としている．
収集した記事から単語が出現する頻度の推移を分析し，
興味の発生を{「関心パタン」}として周期型，漸次増
加型，突発型，関心持続型，その他の5つに分類している．
さらに単語と気温の関係のような出現単語と
現実で起こる現象の関係についても議論を行っている
\cite{fukuhara2005acp}．

興味を持つ商品や楽曲の推薦を行う研究として協調フィルタリングを使用した研
究が挙げられる．
協調フィルタリングは複数のユーザが評価したデータが存在す
る中で，
対象者と嗜好等の類似度によって求めたいくつかの類似性の高いユーザ
の情報を選択し，使用者へ推薦する情報の選別に利用する．
個人の興味を対象としプ
ロファイルやユーザー間の類似度を利用して情報の選択を行う．Resnick
らはニュースの記事の集合から，対象者の好みに合う記事を推薦するシステムを
開発した\cite{Resnick}．個人がニュース記事に付与した5段階の評価値を利
用している．
Upendraらは楽曲のプレイリスト生成システムを考案してい
る\cite{Upendra}．これは多数の人が作成したプレイリストから対象者にプレイリスト
を提供するシステムである．


これらの研究では
単語の出現する頻度を時系列で解析することや，
出現した単語の斬新さ等の尺度を定義することによって
興味を捉えている．
これに対し本論文では
文書の内容自体と
大衆の興味が反映されているデータを基に興味の強さを推定した．
対象とする興味はトレンドのような時間で変動する興味ではなく，
語句自体が持つ時系列で変化しない興味の強さである．
このような興味を対象に，
大衆の興味が反映されているデータから興味の強弱を値として
推定する研究は従来行われていない．




************************ [./logs/V15N03-04/related_study] ************************
関連研究

我々の知る限り，評価文の自動収集を行ったという研究はこれまでに報告されて
いない．最も関連が深いのは，評価語や評価句の自動獲得に関する研究である．
これには主に2つのアプローチがあり，1つはシソーラスや国語辞典のような言語
資源を利用して評価語を獲得する手法である．Kampsらは，類義語／反義語は同一
／逆極性を持ちやすいという仮定にもとづき，WordNetを利用して評価語を獲得す
る手法を提案した\cite{Kamps04}．同様の考え方にもとづく手法はこれまでに多
数提案されている\cite{Hu04,Kim04,Takamura05}．一方で，Esuliらは語の評価
極性の判定を定義文の分類問題として解いている\cite{Esuli05,Esuli06a}．

評価語や評価句を獲得するためのもう1つのアプローチは，評価表現同士の共起
関係を利用する方法である．Turneyは，評価表現は同一ウィンドウ内に共起しや
すいとことに着目し，語句の評価極性を判定する手法を提案した
\cite{Turney02a,Turney02b}．共起頻度を求めるときに既存のコーパスを利用す
るのではなく，検索エンジンを利用してウェブという大規模コーパスでの頻度を
見積もり，データスパースネスの問題に対処している点が特徴である．
Hatzivassiloglouらは，コーパス中で2つの形容詞が{\it and}などの順接を表す
接続詞で結ばれていれば同一評価極性を持ちやすく，逆に{\it but}のような逆
接で結ばれていれば逆極性を持ちやすいという観察にもとづき，コーパスから評
価語を獲得する手法を提案した\cite{Hatzivassiloglou97}．この考え方は
Kanayamaらによってさらに拡張されている\cite{Kanayama06}．

語句の評価極性ではなく主観性に着目した研究報告も存在する．Wiebeは，人手
でタグ付けされたトレーニングデータ利用して，主観的形容詞を学習する手法を
提案している\cite{Wiebe00}．Riloffらは主観的名詞の獲得を行っている
\cite{Riloff03a}．Riloffらの手法では，主観的名詞とその抽出パターンが交互
に学習される．まずシステムには少数の主観的名詞が入力として与えられる．そ
して，それを利用して主観的名詞の抽出パターンを学習する，学習されたパター
ンで新たな主観的名詞を獲得する，という処理が繰り返される．これにより大量
の主観的名詞の獲得が行われる．\cite{Riloff03a}では名詞が対象とされていた
が，\cite{Riloff03b}では名詞以外の句も獲得対象となっている．同様のブート
ストラップ的な手法は\cite{Wiebe05}でも議論されている．

提案手法のように語彙統語パターンやレイアウト（箇条書きや表）パターンを用い
て知識獲得を行う手法は古くから研究されてきている．Hearstは，{\it such
as}のようなパターンに着目して，単語間の上位下位関係をコーパスから獲得す
る手法を提案した\cite{Hearst92}．Hearstの手法は英語を対象としているが，
日本語においても安藤らが同様の手法を試している\cite{Ando03}．一方，新里
らは，同一箇条書きに出現する単語は共通の上位語を持ちやすいという仮説にも
とづき，上位下位関係を獲得する手法を提案している\cite{Shinzato05}．これ
以外にも，全体部分関係にある単語対の獲得や，属性と属性値の獲得といったタ
スクにも，同様の手法が用いられている
\cite{Berland99,Chklovski04,Yoshinaga06}．我々の知る限り，評価情報処理に
同様の手法を適用したという報告はない．HuらやKimらの手法ではレイアウトパ
ターンが利用されているが，これらの研究では，レイアウトは特定サイトに固有
の手がかりとして議論されているため意味合いが異なる\cite{Hu05,Kim06}．



************************ [./logs/V15N05-02/related_study] ************************
関連研究
\label{sec:related_works}

\ref{sec:intro}節でも述べた通り，本研究と関連が深いタスクとして，2つの方
向がある．第1は，新規の言語対に対する対訳辞書を自動構築するという研究であ
り，第2は，既存の対訳辞書に登録されていない未知語に対する訳語を推定すると
いう研究である．

新規の言語対に対する対訳辞書を自動構築する研究は，さらに，大きく2つのアプ
ローチに分けることができる．
第1のアプローチは，既存の対訳辞書をまったく仮定せず，対象としている言語対
のコーパスから直接に対訳辞書を構築しようとするアプローチである．
第2のアプローチは，ある中間言語を導入して，その中間言語との間の対訳辞書を
利用することにより，新規言語対の対訳辞書を構築するというアプローチである．

第1のアプローチでは，基本的に，単語の周辺の文脈を何らかの方法で表現し，入
力言語と出力言語で類似した文脈に出現する語を訳語対とする．
例えば，\cite{fung95}は，英語と中国語を対象として，ある単語の直前と直後に
現れる単語の種類数を求め，その種類数によって単語の文脈的な特異性を表し，
良く似た特異性を備えた英語単語と中国語単語とを訳語対としてまとめるという
方法を提案している．
\cite{rapp95}は，英語とドイツ語を対象として，非常に基本的な単語の訳語対
（6個）とコンパラブルコーパスを用意しておき，これらの訳語との共起頻度に基づ
いて定義した単語間の類似度を用いて訳語対を求める方法を提案している．

新規対訳辞書を自動構築する第2のアプローチとしては，本研究と同様に，英語を
中間言語として用いる試みが幾つか報告されている．
\cite{日仏対訳辞書}は，英語を中間言語として利用して，和仏対訳辞書を作成す
る方法を提案している．この方法では，和英辞書と英仏辞書を利用して，日本語
単語に対するフランス語訳語候補を獲得し，仏英辞書と英和辞書を利用して，得
られたフランス語単語に対する日本語訳語候補を調べる（逆引きを行う）ことに
よって，訳語候補の絞り込みを行い，訳語推定精度を改善している．この方法で，
名詞を対象とした場合の精度は76\%，再現率は44\%である．
白井ら\cite{shirai01}は，田中らと同様の方法を用いて，英語を中間言語として
日本語と韓国語の対訳辞書を作成している．
Bondら\cite{bond01}も同様に，英語を中間言語として利用して，日本語とマレー
語の対訳辞書を作成している．
張ら\cite{日中対訳辞書}は，英語を中間言語として利用して，日中対訳辞書を
作成する方法を提案している．この方法では，和英辞書と英中辞書を利用して，
日本語単語に対する中国語訳語候補を獲得し，日本語と中国語の品詞情報と漢字
情報を利用して訳語候補の順位付けを行っている．この方法で，第1位に順位付
けられた訳語候補のみを出力した場合の精度は81.4\%である．
これらの先行研究は，対象となる言語対の辞書が全く存在しない状況を想定して
おり，入力言語—出力言語の対訳辞書から得られる情報を考慮することは行われて
いない．それに対して，本論文の手法では，対象となる言語対について小規模な
種辞書が存在する状況を想定しており，その種辞書から得られる情報をなるべく
有効に利用しようとしている点で，これらの先行研究とは異なる．

既存の対訳辞書には含まれていない語について訳語推定を行い，かつ，その推定
にあたっては既存の対訳辞書を最大限に利用しようするという2つの点において，
本論文で提案する種辞書の拡充というタスクと，未知語の訳語推定というタスク
は関連が深い．
例えば，\cite{tanaka02}は，対象となる言語対のコンパラブルコーパスを用意し，
周辺に共起する単語を文脈ベクトルとして表現し，文脈ベクトルの類似度を求め
て訳語を推定するという方法を提案している．ただし，推定対象は，複合名詞に
限られており，動詞や形容詞には対応していない．
\cite{kaji01}も，類似の方法を提案し，複合語と単純語の両方に対して評価を行っ
ている．ただし，\cite{kaji01}は，非常に大規模な既存の対訳辞書（50,000語）を
用いている点で，本論文とは問題設定が異なっていると考えられる．
本論文で提案している辞書の拡充というタスクにもっとも近い問題設定としては，
\cite{tanaka96,fung98,chiao02,gaussier04}がある．例えば，\cite{tanaka96}
は，英語と日本語のコンパラブルコーパスと小規模な対訳辞書を用意し，英語コー
パス上で観測された単語共起と，日本語コーパス上で観測された単語共起とを比
較して，単語共起として類似した振る舞いをしている単語を訳語として選択する
という方法を提案している．
本論文の提案手法は，入力言語から中間言語への対訳辞書と，中間言語から出力
言語への対訳辞書の情報をも利用することによって，より小さい種辞書で，より
再現率の高い訳語推定を行っている．
また，本論文では，得られた訳語を人手で判定して評価を行うだけでなく，実際
の言語横断情報検索システムに組み込んだ性能評価を行っている．



************************ [./logs/V15N05-04/related_study] ************************
関連研究 \label{related-work}

\subsection{ドメイン資源の関連研究}

上位下位関係に比べると，単語間のドメイン関係に関する研究は
少ない．
上位下位関係については，多くの言語資源（シソーラス）が構築され，また，
その構築方法に関する研究も活発である．
一方，ドメイン関係では，構築された言語資源も構築に関する方法論もわず
かである\footnote{
\citeA{Fellbaum:WordNet:1998}は，
WordNet等の語彙資源におけるドメイン情報の欠落を
Tennis Problemと呼んでいる．
}．

既存のドメイン資源として挙げられるのはHowNet
\cite{HowNet:Dong:Dong:2006}とWordNet \cite{Fellbaum:WordNet:1998}，
そしてLDOCE \cite{LDOCE:1987}である．
HowNetでは，ECONOMY，INDUSTRY，AGRICULTURE，
EDUCATION等，計32のドメインが想定されている．
WordNetではsynset間にドメインにあたる情報が定義されている．
例えば，\textit{forehand}，\textit{rally}，\textit{match}は
\textit{tennis}に関連づけられている．
人間向けの辞書としては，LDOCEがドメインにあた
る情報（subjectコード）を単語に付与している．
しかしながら，上記のようなドメイン資源は英語や中国語などのごく少数の言語
でしか利用できない．

多くの言語でドメイン情報が利用できるようになるために，効率的なドメイン資
源の構築方法が求められる\footnote{
日本語では\citeA{Yoshimoto:Kinoshita:Shimazu:1997}がドメイン資源を構築し
ているが，一般に公開されてはいない．
}．
しかし，従来のドメイン資源構築手法のほとんどは，LDOCEやWordNetといった，
高度に構造化された既存の語彙資源を利用しており，そのような高価な語彙資源
が存在しない言語に対しては適用できない．
例えば，\citeA{guthrie91subjectdependent}はLDOCEにあるドメイン情報を利用
して単語間のドメイン関係を得ている．
\citeA{Magnini:Cavaglia:2000}では，WordNetにあるドメイン情報を拡充するこ
とを目的に，上位のsynsetに手作業でドメイン情報を与え，その後，自動で下位
階層にドメイン情報を伝搬させている．
\citeA{Agirre:Ansa:Martinez:Hovy:2001}では，WordNetの各synsetに対し，Web
から集めた文書集合から，そのsynsetと同じドメインに属する語を抽出している．
Webから文書集合を集める際，効果的なクエリを生成するため，WordNetの意味情
報を活用している．
\citeA{Chang:Huang:Ker:Yang:2002}は，WordNetとFar East Dictionaryに定義さ
れているドメイン情報から「ドメインタグ」を定義し，それをWordNetに付与し
ている．
このように，既存の手法はWordNetやLDOCE等の語彙資源の存在を前提にしている
ため，そのような資源の無い言語には適用できない．

そこで，高度に構造化された語彙資源に頼らないドメイン資源構築手法が望まれ
る．
そのような手法の第一候補は，情報検索や専門用語抽出の分野で開発された重要
語抽出手法
\cite{Frantzi:Ananiadou:Tsujii:1998,Hisamitsu:Tsujii:2003,中川:森:湯本:2003}
であろう．
しかし，\S\ref{2issues}で述べた通り，本研究のように基本語を対象としたド
メインの場合，重要語抽出の元となる文書集合を正確に収集するのが非常に困難
である．
一方，本研究のドメイン資源構築手法は，文書集合もWordNetのような高価な語
彙資源も必要としない．
また，本研究では表\ref{domain-table}にある12ドメインを採用してドメイン資
源を構築したが，\S\ref{domain-construction-method}で述べたように，
本研究の基本語ドメイン辞書構築手法は特定のドメイン体系に依存しない．

以上を踏まえると，本研究のドメイン資源研究における貢献は次の2点である．

\begin{itemize}
 \item （一般に利用可能な）世界初の日本語ドメイン資源を構築した．
 \item 文書集合もWordNetのような高価な語彙資源も必要としないドメイン資源
       構築手法を開発した．
\end{itemize}


\subsection{文書分類の関連研究}

従来の文書分類手法は，機械学習等の統計的手法を用いたものがほとんどである．
例えば，$k$-最近隣法 \cite{yang99evaluation}，
決定木 \cite{lewis94comparison}，
ナイーブベイズ \cite{lewis98naive}，
決定リスト \cite{li99text}，
サポートベクトルマシン \cite{平:春野:2000}，
ブースティング \cite{schapire00boostexter}を用いたものがある．
これらのような統計的手法では，訓練データとして大量の文書集合をあらかじめ
用意しなくてはならない．
近年，少量の正解情報が付与されたデータと正解情報のない大量のデータから分
類器を構築する研究 \cite{Abney:2007}も行われているが，その技術は発展途上
と言える．

一方，本研究の分類手法では，基本語のみを対象に分類の手掛かり（
ドメイン情報）を整備しておくだけで済む．
上述の通り，本研究の基本語ドメイン辞書を作るには，高価な語彙資源も文書集
合も必要なく，Webへのアクセスさえ用意すればよい．
また，\S\ref{domain-construction-method}で述べた通り，構築の過程は全自動
ではなく手作業をわずかに要するが，その作業は軽微なものである．

本研究の分類手法は我々が構築した基本語ドメイン辞書に基づいているた
め，分類体系が表\ref{domain-table}にある12ドメインに固定されている．
しかし，\S\ref{domain-construction-method}で述べた通り，基本語ドメイン辞
書のドメイン体系はユーザが目的に応じて自由に選べるため，
我々が使用した12ドメインとは異なる分類体系を使用する場合，
その体系に合わせて基本語ドメイン辞書を作りなおせばよい．
そのコストは上述の通り軽微なものである．
また，文書分類が実際に用いられる現場では，分類体系が頻繁に変更されるとい
うことは考えにくいため，いったん基本語ドメイン辞書を構築すればそれで済む．

本研究の分類手法のもう一つの強みは，未知語への対応能力である．
\S\ref{bunrui-method}で述べた通り，本手法では，記事中に未知語を発見する
と，リアルタイムでその語のドメインを推定する．
そしてその正解率も77\%と実用に耐えうるものであった．
この能力は，ブログのような，新たな語が次々に生まれ出るようなジャンル
の文書を分類する際に非常に有効である．
一方，上に挙げた統計的手法で未知語に対応するには，訓練データの文書集合を
集め直す必要があり，大変手間がかかる．
しかもブログを対象とした場合，次々と未知語が現れるため，訓練データの更新
を短い周期で行う必要がある．

以上をまとめると，本研究の文書分類研究における貢献は次のようになる．

\begin{itemize}
 \item 機械学習（そして文書集合）を必要としない，未知語にも柔軟に対応で
       きる文書分類手法を開発した．
\end{itemize}



************************ [./logs/V15N05-05/related_study] ************************
関連研究

 直接照応解析に関係する先行研究で用いられた手法としては大きく，人手で作
 成した規則に基づく解析手法と，タグ付きコーパスを用いた学習手法に分けら
 れる．
  

  \subsection{規則ベースの手法}

  Zhouら\cite{Zhou2004}は，英文に対して，coreferenceを7種類に分類し，照
  応の種類ごとに規則を作成し直接照応の解析を行っている．各段階で必要とな
  る制約は基本的にデータから人手で作成している．Zhouらはこの手法により，
  MUC-6に対して73.9\%，MUC-7に対して66.5\%のF値という解析結果を得ている．
  
  村田ら\cite{Murata1996b}は，日本語を対象として，名詞の指示性を考慮した
  9個のルールを用いて名詞の同一性の解析を行っている．名詞句の指示性に関
  しては，人手で作成した86個の規則を適用することにより，すべての名詞を総
  称名詞，定名詞，不定名詞の3種類に分類している．童話や新聞記事を用いた
  実験を行い，結果として適合率79\%，再現率77\%を得ている．童話，新聞記事
  それぞれの精度，および，複合名詞の構成素が関係する照応をどこまで扱って
  いるかなどは不明である．


  \subsection{機械学習を用いた手法}

  機械学習を用いた同一指示解析手法はいくつかの手法が提案されている．これ
  らの手法の多くは，共参照解析の問題を，照応詞候補に対して，先行詞の候補
  となる名詞句の各々が先行詞となるか否かを判別する2値分類問題として扱って
  いる．分類器は対象の名詞句が先行詞かどうかという2値分類問題を解く．

  Soonら\cite{Soon2001}は，訓練時には，先行詞と照応詞の対を正例，先行詞
  と照応詞の間の各名詞句と照応詞の対を負例として学習した．照応問題を解く
  際には，照応詞から先行文脈に向かって，先行詞候補となる名詞句の各々につ
  いて，それが先行詞かどうかを分類していく．そして，分類器がいずれかの名
  詞句を先行詞として決定した時点で解析を終了する．分類器が，先行する名詞
  句をすべて先行詞でないと分類した場合は，対象としている照応詞は先行詞を
  持たないと判断する．Soonらの実験では，12個の素性を用い，決定木を用いて
  学習を行ない，MUC-6に対して62.6\%のF値，MUC-7に対して60.4\%のF 値と，
  規則ベースの手法と同程度の精度を得ている．
  
  Ngら\cite{Ng2002a}はSoonらの手法を2つの点において改良している．一つは
  素性集合を拡張し，語彙的な素性や意味的素性など，53個の素性に増やした．
  もう一つは先行詞同定の探索アルゴリズムの変更である．Soonらが照応詞に近
  い名詞句から順に先行詞かどうかを決定的に決めるのに対し，Ngらはすべての
  先行する名詞句を分類器にかけ，分類器が先行詞と決定した名詞句の中で，最
  も先行詞らしいと判定した名詞句を先行詞とする．NgらのモデルはSoonらのモ
  デルよりも先行詞同定の精度がよく，MUC-6に対して70.4\%，MUC-7に対して
  63.4\%のF 値を得ている．

  日本語における機械学習を用いた同一指示性解析に関する研究としては Iida ら
  \cite{Iida2003}の研究がある．Iida らは日本語では冠詞などの情報が無く，
  名詞句の指示性の推定がそれほど容易でないことから，まず名詞の指示性の判
  断を行った後に先行詞の同定を行うのではなく，まずある表現に対する最尤先
  行詞候補を決定した後先行詞候補の情報も用いて名詞の指示性の判断を行って
  いる．Iida らは分類器としてSVMを用い，語彙的な情報を用いた素性や統語的
  な情報を用いた素性，意味的な情報を用いた素性，名詞句間の距離情報を用い
  た素性計30あまりの素性を用いている．京大コーパスの報道90記事に対して名
  詞句同一指示関係のタグを付与し，10分割交叉検定を行った結果，F値として
  70.9\% を得ている．
    


 
************************ [./logs/V15N05-07/related_study] ************************
関連研究

多言語{\text}を入力する際に必要なキー入力は，以下のように分類できる．
この章では，関連研究をこの分類に沿って述べる．
\begin{enumerate} 
\item 本文の文字列に対応する入力
\item {\tes}に対する操作
\item {\tes}を切り替えるための操作
\end{enumerate} 

(1)は，本文の文字列に対応する入力である．
例えば，本文が``多言語''であれば，日本語のローマ字入力では，``tagengo''がこれに当たる．
1つの言語に対応するIMEは，入力方式やキーボード配列の違いによって，複数存在する場合もある
\footnote{文献によっては，1つの言語に1つのIMEを対応付け，
入力方式やキーボード配列の違いを別の名称で呼ぶ場合もあるが，本論文では，
入力方式やキーボード配列が違えば，IMEも異なるものとする．}．
例えば，日本語には前述のローマ字入力の他に「かな入力」もある．
日本語のかな入力のIMEでは，``多言語''に対応する(1)のキー列は``q:@yb@''であり，ローマ字入力の場合とは異なっている．
日本語に限らず，英語でも，QwertyやDvorakといったキーボード配列の違いによって，複数の{\tes}がある．

ただし，本論文は，入力方式やキーボード配列の違いについて論じることが目的ではないので，
以後，各言語について標準的と思われる入力方式やキーボード配列を1つに定め，
「日本語のIME」のように呼ぶものとする．

(2)の操作は，仮名漢字変換ソフトウェアに対する変換候補選択などの操作に相当する．
例えば，上記の``多言語''の例であれば，(1)だけでは，``多言語''のほかに``他言語''という選択肢も存在する．
このとき，両者から``多言語''を選び出すための入力が，(2)である．
予測入力を行う場合の，キー入力から候補を予測した変換候補を選択する操作も，
(2)に相当する．

(1)と(2)は，単言語で構成される文書を入力する際にも必要となるので，
ユーザーインターフェースや自然言語処理の分野で詳細な研究がなされており，\cite{entry}にまとめられている．

(3)の操作は，{\tes}を切り替える操作である．本論文では，以後この操作を「IME切り替え操作」と呼ぶ．
例えば，英語と日本語を切り替える際の``Alt+半角''などのキー操作が(3)に相当する．
IME切り替え操作は，主に，多言語{\text}の入力時に言語を切り替える際に発生する．

\secref{sec:introduction}で述べたように，このIME切り替え操作を，キー列からユーザーが入力している
言語を判別することで，自動化するのが本論文の主旨である．



(3)の操作量を直接扱った既存研究は少ない．
著者らが調査した範囲では，論文が公開されているものとしては，\cite{pinyininput}が該当するのみである．
この論文では，
ピンイン\footnote{ローマ字による中国語の発音表記のこと．中国語入力では，ピンインを入力して漢字に変換する
ピンイン漢字変換が一般的である．}と英語が混じったキー列を正しく変換するタスクが述べられている．
しかし，この機能はあくまで中国語入力の一環として述べられているにすぎず，
本論文で扱うように，言語の種類や数を変更することは考慮されていない．

論文の形でなく，フリーソフトウェアでは，{\em PUNTO switcher}\footnote{http://punto.ru/}
というロシア語圏のソフトウェアが，ロシア語と英語の間の切り替えの目的で
2001年より開発されている．当該ウェブサイトの情報によれば，このソフトウェアは150万件のダウンロードがあり，
一定の成功を収めていると思われる．
また，{\em Keyboard Ninja}\footnote{http://www.intelife.net/ninja/}というソフトウェアも作成されている．
Keyboard Ninjaは，ロシア語，英語，フランス語，ドイツ語，イタリア語，スペイン語，ウクライナ語の間での
切り替えを行うソフトウェアである．
これらのソフトウェアの用いているアルゴリズムや性能評価については，著者らの知る範囲では公開されていない．

一方，本研究は，{\text}の言語判別問題としてとらえることも可能である．
この{\text}の言語判別問題は，次のように分類することが可能である．

\begin{enumerate}
\item ある単言語の{\text}が与えられ，その{\text}の言語を判別する問題
\item 多言語の{\text}が与えられ，その中の部分が何語であるかを判別する問題
\end{enumerate}

(1)の問題については，OCRを多言語に対応させることを
主な目的とした，\cite{cavnar}の研究が基礎的である．
この論文では，ニュースグループへの投稿文書という長いテキストを対象に，
文字ベースのN-gramの頻度を用いて文書の言語を判別している．
また，\cite{sibun}では，やはりECIコーパスという長いテキストを対象に，
KL情報量を用いて文書の言語を判別している．
どちらの論文でも，言語によって判別精度に差があることと，
平均して90\%を超える高い精度が達成できることを報告している．

一方，本研究との関連がより深いものは，(2)の問題である．(2)は，(1)を拡張した問題になっているうえ，
多くの場合(1)より短い部分から言語を判別しなければならないため，(1)より難しい．
以下，代表的な(2)に関する研究を2つ挙げる．

\cite{indian}は，小さいサンプルを対象とした言語判別問題に機械学習を用い，高い判別精度で解けることを報告している．しかし，この研究は，一般の文書からのサンプルを対象に2言語の間の判別を行うことを目的とし，インドで使用されている言語や文字に特化した素性を用いて判別精度の向上を報告するものである．本研究は，入力中のキー列を対象に3言語以上の間の判別も扱い，言語の種類に特に制限は設けていない点で，目的も手法も異なる．


\cite{alex2005}は，ドイツ語中に混在する英語を判別する方法について論じている．
この論文では，{\text}中に混在する他の言語を発見するタスクを，
{\em foreign inclusion detection} (FID)と呼び，
音声合成 (Text to speech) の分野で研究されてきたことを紹介している\cite{tts2003}, \cite{tts2005}．
近年Alexは，FIDを構文解析の前処理として使用することで，構文解析の精度が向上させられることを示している\cite{alex2007}.
このFIDのように，他の処理の前処理として言語判別を使用する場合は，高い精度が求められるため，
対象言語について大規模なコーパスが入手可能であることが前提となる．
一方，本論文の目的では，対象言語の大規模なコーパスが手に入るとは限らないため，
FIDの手法をそのまま適用することは困難である．

以上の関連研究を踏まえて，次の\secref{sec:premodel}で，設計方針を立てる．



************************ [./logs/V16N01-02/related_study] ************************
関連研究
\label{節：関連研究}

文書の母語話者性の判別と関連の深い研究分野として，文書の記述言語を推定す
る言語識別がある．

Cavnar らは，出現頻度上位の文字列とその順位を言語および文書の特徴と考え
る言語識別を行っている\cite{cavnar}．
各言語 $L_i$ の学習データ文書中での 1〜5 の長さの文字列のうち出現頻度上
位 300 個の文字列とその順位を求めて，言語 $L_i$ における順位表を作成し
ておく．同様に，識別対象文書 $d$ に対しても順位表を作成する．$d$ の順位
表中の各文字列の順位と $L_i$ の順位表での順位との差の絶対値の和を
$d$ と $L_i$ の非類似度 $dissim(d,L_i)$ と考え，$dissim(d,L)$ が最小の
言語 $L$を $d$ の記述言語として識別する．これは，前節で述べた枠組みに
対して，$dissim(d,L_i)$ の逆数を尤度 $Lh(d,L_i)$ と考えたことに相当する．

また，前田らは，長さ2の文字列の出現頻度分布をユークリッド空間上の
ベクトル（頻度ベクトル）であると考え，識別対象文書 $d$ の頻度ベクトルと
各言語 $L_i$ の学習データ文書の頻度ベクトルとの余弦を，
$d$ と言語 $L_i$ の類似度 $sim(d,L_i)$ とする言語識別を行っている\cite{前田}．
これは，前節で述べた枠組みに対して，
$sim(d,L_i)$ を尤度 $Lh(d,L_i)$ と考えたことに相当する．

行野らは，長い文字列の頻度は統計的な揺らぎが大きいものの，言語を特定する
能力が高いと考え，1〜7 の長さの文字列を言語および文書の特徴と考える言語
識別の手法を提案している\cite{行野}．彼らの手法は，識別対象文書 $d$ に出
現する 1〜7 の長さの文字列集合と言語 $L_i$ の学習データに出現
する 1〜7 の長さの文
字列集合の積集合の大きさを $Lh(d,L_i)$ とする手法である．長い文字列を特徴
として使用した結果，類似言語間の識別や識別対象文書が極めて短い場合の識別
でも，Cavanr らの手法，前田らの手法に比べ高い識別精度を実現したと報告している．

Dunning は文字 $n$-gram モデルにより $P_{L_i}(d)$ を求め，
言語の事前確率を等確率($P(L_i)=P(L_j)$)と仮定して，ベイズ識別に
より，$d$ の属す言語の識別を行っている\cite{dunning}．
ただし，ゼロ頻度問題に対処するため，
加算スムージングによる $n$-gram 確率のスムージングを行っている．
$n=1\sim 5$ を試した結果，$n=2$ の場合，
つまり，bi-gram モデルの場合が最も識別精度が高かったと報告している．

Sibun らは，長さ $n$ の文字列の確率分布を言語および文書の特徴と考え（実
際には $n=1$ または $n=2$ を採用している），
確率分布間の相違尺度である KL-Divergence に基づいた言語識別手法を提案
している\cite{sibun}．
確率分布 $P$ と $Q$ の KL-Divergence（Kullback-Leibler 距離）
$D_{KL}(P||Q)$ は
\[
 D_{KL}(P||Q)=\sum_{x\in{\cal X}} P(x)\log \frac{P(x)}{Q(x)}
\]
で定義される\cite{確率的言語モデルテキスト}．Sibun らの手法は，
$D_{KL}(P_d||P_{L_i})$ が最小の $L_i$ を $d$ の記述言語として識別す
る手法である．ただし，
$P_d(\vec{a})$ は文書 $d$ における文字列 $\vec{a}$ の生起確率 ，
$P_{L_i}(\vec{a})$ は言語クラス $L_i$ における文字列 $\vec{a}$ の生起確
率である．
$P_d(\vec{a})$ は
\[
 P_d(\vec{a})=\gamma\cdot f_d(\vec{a})
 \qquad \text{ただし，}
 \gamma=\frac{1}
       {\sum_{\svec{a}\in A^n}f_d(\vec{a})}
\]
と推定されるので（$f_d(\vec{a})$ は $d$ での文字列 $\vec{a}$ の出現頻度，
$A$ は記号の全体集合，$A^n$ は可能な $n$ 長さの文字列
の全体の集合），言語 $L_i$ における文書 $d$ の生起確率 $P_{L_i}(d)$ を
\begin{equation}\label{式：文書の生起確率の大胆な近似}
 P_{L_i}(d)=\prod_{\svec{a}\in A^n} P_{L_i}(\vec{a})^{f_d(\vec{a})}
\end{equation}
と大胆に近似するならば，
\[
 D_{KL}(P_d||P_{L_i})
 = \sum_{\svec{a}\in A^n} P_d(\vec{a}) \log \frac{P_d(\vec{a})}
                                                   {P_{L_i}(\vec{a})}
 = -\gamma\cdot\log P_{L_i}(d)+
         \sum_{\svec{a}\in A^n} P_d(\vec{a}) \log P_d(\vec{a})
\]
となる．つまり，$D_{KL}(P_d||P_{L_i})$ が最小の $L_i$ を $d$ の記述
言語として識別する Sibun らの手法は，
式(\ref{式：文書の生起確率の大胆な近似})の近似を行った上で，
言語の事前確率を等確率と仮定して，ベイズ識別により，
$d$ の属す言語の識別を行うことと等価である．
なお，Sibun らもゼロ頻度問題に対処するために，$P_{L_i}(\vec{a})$ は
\begin{equation}\label{式：Sibun加算スムージング}
P_{L_i}(\vec{a})=\frac{f_{L_i}(\vec{a})+\delta}
                      {\sum_{\svec{a}\in A^n} \{f_{L_i}(\vec{a})+\delta\}}
\end{equation}
のように，加算スムージングによりスムージングしている
（$\delta$ は非負の定数）．

次に，本論文で扱う文書の母語話者性判別に関する従来研究について述べる．

Tomokiyo らは，長さ $n$ ($n=1,2,3$)の記号列（記号として
は，単語，品詞，および単語品詞混合の3種を試している）を言語および
文書の特徴と考え，文書（あるいは，文書を構成する単語をその品詞に置き換えた
もの，文書を構成する一部の単語を品詞に置き換えたもの）の生起確率を
式(\ref{式：文書の生起確率の大胆な近似})で近似し，ベイズ識別に基づく
母語話者／非母語話者クラスの判別を行っている\cite{Tomokiyo}．
しかし，彼らは，子
供用ニュース記事の音読による発話や観光などに関する自発的発話を，音声認
識器によりテキストにした文書，および，人手で書き起こした文書を対象として
いる．音読では読み間違いが非母語話者の大きな特徴であり，自発的発話では，
使用語彙が母語話者／非母語話者の間の大きな違いである．一方，我々は，論文
などのように十分推敲して作成されているフォーマルな文書を対象としており，
母語話者／非母語話者判別に有効な特徴量も異なってくるため，彼らの判別
実験結果と直接比較することはできない．

藤井らは，品詞 tri-gram モデルを言語モデルとし，ゼロ頻度問題に対処できる
Skew Divergence を用いて英文書の母語話者性の判別を行っている\cite{藤井}．
以下で定義される判別対象文書 $d$ と言語クラス $C$（$\in\{N,\NN\}$,
$N$: 母語話者言語クラス，$\NN$: 非母語話者言語クラス）との相違度
$ED(d\:;\:C)$ 
\[
   ED(d\:;\:C)=\sum_{\tuple{ab}\in H^2}
                  f_d(ab)D(P_d^\tuple{ab}\:||\:P_C^\tuple{ab})
\]
を求め，$ED(d\:;\:C)$ を最小にする $C(\in\{N,\NN\})$ を $d$ が属すクラ
スとして推定する．ただし，$H$ は品詞の全体集合，
$P_d^\tuple{ab}$ は文書 $d$ における条件を $ab$ とする品詞 tri-gram 
分布，$P_C^\tuple{ab}$ はクラス $C$ における条件を $ab$ とする品詞 tri-gram 
分布\footnote{
つまり，$P_d^\tuple{ab}(x)$ は文書 $d$ において，
品詞列 $ab$ の次に品詞 $x$ が生起する確率 $P_d(x|ab)$，
$P_C^\tuple{ab}(x)$ は言語クラス $C$ において，
品詞列 $ab$ の次に品詞 $x$ が生起する確率 $P_C(x|ab)$である．
}，$D$ は確率分布間の相違度である．確率分布間の相違度 $D$ として KL Divergence 
を用いた場合，$ED(d\:;\:C)$ を最小にする $C(\in\{N,\NN\})$ は，文書 $d$ 
の各単語をその品詞で置き換えた品詞列の生起確率を最大にする言語クラスであ
る．したがって藤井らの手法は，品詞 tri-gram モデルを言語モデル
とし，言語の事前確率を等確率と仮定して，Bayes 識別
に基づいて $d$ が属すクラスを判定する方法と本質的には同じである．
藤井らの手法の特徴は，分布間の相違度 $D$ として KL Divergenceを用
いるのではなく，以下で定義される Skew Divergence \cite{Lee} を用いている
点にある．
\[
 D_{skew}(p\:||\:q)
  =\sum_{x\in {\cal X}} p(x)\log\frac{p(x)}{\alpha\cdot q(x)+(1-\alpha)\cdot p(x)}
\]
Skew Divergence はゼロ頻度問題に弱い KL Divergence を改良したものである．
藤井らは，$\alpha$ を，分布 $q$（つまり，
$P_N^\tuple{ab}$，$P_{\NN}^\tuple{ab}$）の推定に用いた学習データのサイズに応じて，$ab$ 毎に
\begin{equation}\label{Skew:α}
\alpha(ab)=1-\exp\left( -\sqrt{\beta\cdot \min(f_N(ab),f_{\NN}(ab))} \right)
\end{equation}
と設定している．彼らは，Skew Divergence を用いることで，線
形補間を施した品詞 tri-gram 分布による KL Divergence を用いた手法
および多くの識別問題で高い精度を実現している Support Vector Machine を用
いた手法よりも有意に高い判別精度を実現できたと報告している．

英文書の母語話者性判別は，母語話者英語，非母語話者英語という類似した言語の
識別問題と捉えることもできる．
青木らは，文書を，それを構成する単語を品詞で置き換えた品詞列と見なし，
基本的には KL-Divergence を用いた Sibun らの言語識別手法に基づいて，
文書の母語話者性の判別を行っている\cite{青木}．
「長い文字列も言語特徴とすることで類似言語の識別精度が向上する」と
いう行野らの知見
からの予想通り，長い品詞列の頻度情報を利用した場合の判別精度が高く（
$n=6$ のときが最も精度が高い），藤井らの手法より高精度で母語話者性を判別
できたと報告している．

著者らは，青木らの主張と同じく，長い品詞列の頻度情報も利用すること
が文書の母語話者性判別に有効であると考えている．
藤井らの手法は言語モデルを $n>3$ の品詞 $n$-gram モデルとしてもそ
のまま適用できる．しかし，藤井らは，式 (\ref{Skew:α})を
\begin{itemize}
\item $\alpha$ は $f=\min(f_N(ab),f_{\NN}(ab))$ の単調増加関数，
\item $\lim_{f\rightarrow\infty} \alpha =1$，
      $\lim_{f\rightarrow 0} \alpha =0$
\end{itemize}
を満たす $\alpha$ の設定法の一例として用いたに過ぎず，$n$ を大きくした
場合に，式 (\ref{Skew:α}) で良いのかどうかは疑問である．さらに，
$n$ を大きくしたとき，式 (\ref{Skew:α})では高い精度が得られない
場合に，式 (\ref{Skew:α})に代えて，上記の性質を満たす
$f=\min(f_N(ab),f_{\NN}(ab))$ のどのような関数を $\alpha$ の
設定に用いればよいのかも明らかではない．
一方，青木らの手法は，統計的パターン認識の立場で見るならば，
近似式(\ref{式：文書の生起確率の大胆な近似})を仮定したベイズ識別による
判別法である．しかし，品詞 $n$-gram モデルに比べ，
式(\ref{式：文書の生起確率の大胆な近似})は，近似としては非常に粗い．

$n$が大きな品詞 $n$-gram モデルを言語モデルとして使用し，かつ，
ゼロ頻度問題およびスパースネスの問題を克服する新たな
母語話者性判別手法を次節で述べる．



************************ [./logs/V16N01-04/related_study] ************************
関連研究

リスト型質問応答については，米国における大規模検索実験プロジェクトである，TREC (Text REtrieval Conference) における，Qusetion Answering Track（以下，TREC QAと記述）で議論されている．

TREC QA でリスト型質問応答のタスクが始まったのは，2001年からである．
2001年\linebreak
\cite{TRECoverview01}と2002年\cite{TRECoverview02}のリスト型質問応答のタスクでは，正解の個数は質問文中に示されており，
システムは示された個数の解候補を出力し，その精度で評価された．
2003年\cite{TRECoverview03}では，リスト型質問応答はメインタスクに含まれる質問のうちの一種類になった．
2003年からは正解数が陽に示されることはなくなり，システムは
正解の個数を判定しなければならなくなった．
システムの評価はF値で行なわれる．
2004〜2006年\cite{TRECoverview04}\cite{TRECoverview05}\cite{TRECoverview06}のTREC QA 
のメインタスクの質問セットは，シリーズ型質問の集合になっている．
シリーズ型質問には，初めにそのシリーズの話題が示されており，
その次に何問かのfactoid質問，list質問があり，
最後にother質問がある．
factoid質問とlist質問では，どちらも事実を問う質問であり，要求される解の種類は
同じである．factoid質問とlist質問の違いは正解の数で，正解数が一つの質問はfactoid質問，正解数が複数の質問はlist質問という様に分けられている．
factoid質問ではシステムはただ一つの回答を出力し，list質問ではリスト形式で回答を出力する．
other質問は，質問文は与えられておらず，
そのシリーズの話題に関連することを出力することが要求される．ただし，factoid質問とlist質問で問われていないことのみを出力しなければならない．
各質問がfactoid, list, otherのどれであるかは質問文と共に与えられている．

list質問では，システムは与えられた質問の正解を過不足無く出力することが要求される．正解の数は明記されておらず，システム自身が判定する必要がある．
2006年の質問セットでは，全質問の正解の平均数は10個であり，最小のものは2個，最大のものでは50個ある．

factoid質問とlist質問は要求される回答数が違うだけであるので，参加したほとんどのチームは，そのチーム自身のfactoid質問に対するシステムと同じものを用い，出力する回答数のみを変えていた．以下に，具体例を紹介する．
F値のみではリスト型処理の善し悪しが分からないため，factoid質問に対する精度 (Accuracy) も併記する．

Harabagiu et al \cite{Harabagiu:AnswerMiningbyCombiningExtractionTechniqueswithAbductiveReasoning}は，一位の解候補と二位以下の各解候補の間の類似度を求め，
類似度に閾値を設けて回答選択をする手法を提案している．
閾値は一位の解候補と最下位の解候補の類似度を基に求められる．
類似度が閾値以上になる解候補のうち，最下位に順位づけされているものまで
を回答リストに加えている．
このシステムのlist型質問に対する精度は，F値で0.433であった．
また，このシステムの基になったfactoid質問に対するシステムの精度は，0.578であった．

Bos \cite{Bos:TheLaSapienzaQuestionAnsweringsystematTREC2006}は，質問文から正解の個数が推定できる場合には，
上位からその個数を回答とし，
それ以外の場合にはあらかじめ決められた個数の解候補を回答とする手法を用いていた．
このシステムのlist型質問に対する精度は，F値で0.127であった．
また，このシステムの基になったfactoid質問に対するシステムの精度は，0.15であった．

Burger \cite{Burger:MITREsQandaatTREC15}は，期待されるF値を求め，それを最大化するように
回答の個数を決める手法を提案している．
このシステムのlist型質問に対する精度は，F値で0.208であった．
また，このシステムの基になったfactoid質問に対するシステムの精度は，0.087であった．

また，国立情報学研究所主催の質問応答に関する一連の評価型ワークショップであるNTCIR QACにおいても同様にリスト型質問応答について議論されている．

NTCIR4 QAC2のsubtask1では，システムは与えられた質問に対して，順位付けされた5つの回答を出力することが求められる．
システムの精度にはMRR（Mean Reciprocal Rank．正解順位の逆数の各質問平均）が用いられる．
正解が複数存在する質問に対しては，システムはそのうちの一つを出力できれば良いとされている．

NTCIR4 QAC2のsubtask2（リスト型タスク）\cite{Fukumoto:QAC2Subtask12}でもTREC QAのlist質問と同様に，システムは与えられた質問の正解を過不足無く出力することが要求される．全質問の解の平均数は3.2個であり，最小のものは1個，最大のもので15個あり，TREC QAに比べると少なくなっている．各質問に対する正解の数が与えられていないこともTREC QAと同様であるが，
TREC QAでは正解数が1個のfactoid質問と2個以上のlist質問が分けられていたのに対し，NTCIR4 QAC2のリスト型タスクでは分けられていないという違いがある．
システムの精度には，修正F値（MF値）の全質問平均である，MMF値が用いられる．
修正F値の詳しい説明は，\ref{Chapter:exp-eval}節で述べる．

NTCIR4 QAC2に参加したシステムはTREC QAに参加したシステムと同様に，factoid質問に対するシステムを基にしており，各解候補に付けられたスコアの値を基に上位何件を回答するかの線引きを行なっている．
以下に具体例を説明する．MMF値のみでは順位付けの善し悪しが分からないため，
QAC2 subtask1に対する精度 (MRR) も併記する．

秋葉ら\cite{秋葉:質問応答における常識的な解の選択と期待効用に基づく回答群の決定}は期待効用最大化原理に基づく回答群選択手法を提案している．これは，リスト型質問応答の評価指標であるF値に着目し，その期待値を求め，期待値を最大化するように回答数を求める手法である．また，リスト内の解候補の重複を避けるために，複数の解候補が同じ内容を指していると判断される時には，スコアの高いものを残して削除するということをしている．このシステムの，QAC2 subtask2のテストセットに対する精度は，MMFで0.318であった．また，このシステムの
基になったfactoid質問に対するシステムの，QAC2 subtask1のテストセットに対する精度は，MRRで0.495であった．

福本ら\cite{Fukumoto:Rits-QA}は，スコアの差が最も開いているところよりも上位のものを回答とする手法を提案している．さらに，質問文の表層表現から解の個数を判別している（「誰と誰」なら二つ，など）．このシステムのQAC2 subtask2のテストセットに対する精度は，MMFで0.164であった．また，このシステムの
基になったfactoid質問に対するシステムの，QAC2 subtask1のテストセットに対する精度は，MRRで0.311であった．

村田ら\cite{Murata:JapaneseQAsystemUsingDecreasedAddingwithMultipleAnswers}は
最大スコアに対する比率に閾値を設けて回答を選択する手法を採用しており，
QAC2 subtask2のテストセットに対する精度は，MMFで0.321であった．
また，このシステムの
基になったfactoid質問に対するシステムの，QAC2 subtask1のテストセットに対する精度は，MRRで0.566であった．

高木ら\cite{Takaki:NTTDATA-QAatNTCIRQAC2}は
n番目の解候補のスコアとn+1番目の解候補のスコアの比率を求め，
それが閾値以上ならばn番目までの解候補を回答とするという手法を
用いている．このシステムのQAC2 subtask2のテストセットに対する精度は，MMFで0.229であった．また，このシステムの
基になったfactoid質問に対するシステムの，QAC2 subtask1のテストセットに対する精度は，MRRで0.335であった．

上記の各手法と本論文で提案する手法でとでは，
スコアの並びを見て動的に回答の数を変えるという点で類似している．
しかし，スコアが複数の混合分布から生成されると仮定することにより，
スコア分布のパラメタより解候補が適切に見つかっているかどうかを判定できるという付加機能を有する点において，我々の提案手法は新しい．
また，精度についても
他の単純な手法に対して比べて精度が高いという結果となった．




************************ [./logs/V16N03-01/related_study] ************************
関連研究\label{sec:bib}

本節では，既存の上位下位関係の自動獲得手法について説明する．上位下位関
係の獲得は，1990年代にHearstが語彙統語パターンを用いて新聞記事から上位
下位関係を獲得する手法を提案し\cite{hearst_1992}，以後各言語への応用が
みられた\cite{JImasumi,JAndo,Pantel_2006,Sumida_2006,Oishi_06}．その
後Webの発達に伴い，箇条書き表現などのWeb文書特有の手がかりを用いた獲得
手法が提案されてきた\cite{Shinzato_2004,Etzioni_2005}が，近年，具体物を
含む概念間の知識を密に記述したWikipediaに注目が集まってい
る
\cite{Ruiz-Casado_2005,Pasca_2006,Herbelot_2006,Suchanek_2007,Kazama_2007}
．以下では，まず新聞記事やWeb文書を対象とした上位下位関係の獲得手法につ
いて紹介し，その後Wikipediaに特化した上位下位関係の獲得手法について述べ
る．以下，各手法について提案手法との違いについて述べる．


\subsection{新聞記事・Web文書からの上位下位関係獲得}

まず，語彙統語パターンを利用した研究として，
\cite{hearst_1992,JImasumi,JAndo,Sumida_2006}があげられる．Hearstは英語
の新聞記事を対象に，“〈上位語〉such as 〈下位語〉”などのパ
ターンを用いて上位下位関係を獲得した\cite{hearst_1992}．安藤らはHearst
に倣い，日本語の新聞記事コーパスを構文解析した結果から，“〈下位語1〉
（や〈下位語2〉）*という〈上位語〉”などの同格・並列表現を含む語彙統語
パターンを用い上位下位関係を獲得した\cite{JAndo}．また今角は日本語の新
聞記事コーパスに対して“〈上位語〉「〈下位語〉」” のような括弧を用いた
パターンを適用している\cite{JImasumi}．この括弧を用いたパターンと名詞連
続パターンを利用して，SumidaらはWeb文書から上位下位関係を獲得した
\cite{Sumida_2006}．これらの手法では，信頼性の高いパターンを用いること
で比較的高い精度で上位下位関係を獲得できるが，そのようなパターンで文書
中に出現しない上位下位関係も数多く存在し，語彙統語パターンのみで大量の
上位下位関係を獲得するのは本質的に難しい．

そこで，語彙統語パターンにマッチしない上位下位関係を獲得するため，Web文
書に頻出する箇条書き表現の文書構造を用いる手法が，ShinzatoらやEtzioniら
によって提案されてい
る\cite{Shinzato_2004,Etzioni_2005}．ShinzatoらはWeb文書中に繰り返し出
現するHTMLタグに囲まれた語の群を1つの単語クラスと見なし，この単語クラス
に上位語を付与することで，上位下位関係を獲得する手法を提案し
た\cite{Shinzato_2004}．またEtzioniらは語彙統語パターンを用いて抽出した
上位下位関係をより広範な下位語に対応させるため，抽出した下位語を多く含
むリスト構造を用いて，未知の下位語に上位語を割り当てる手法を提案し
た\cite{Etzioni_2005}．これらの手法でリソースとして用いているWeb文書の
箇条書き表現は，上位下位関係の記述に限らず様々な用途に用いられるためノイズが多く，高い精度を保ったまま大量の上位下位関係を獲得する
ことは難しい．これらの手法では箇条書き表現を基本的に下位語候補を収集す
るためにのみ用いており，上位語候補は別途獲得する必要があるが，我々の手
法では上位語も含めて文書構造から獲得している点が異なる．

{本研究と同様に分類器を用いて上位下位関係候補の正誤を判断する手
  法としては，Web からタグ構造を手がかりに収集した見出し語（用語）とそ
  の説明文（見出し語を含む段落）の組を入力として，見出し語間の上位下位
  関係を判定する手法を大石らが提案している}\cite{Oishi_06}．{彼
  らが説明文中に含まれる単語を素性としているのに対し，我々は上位語候補／
  下位語候補自体に関する情報（例えば形態素）を主に素性として用いており，
  それぞれの手法の素性セットはほぼ独立である．また，彼らの手法の評価は
  コンピュータに関する用語のシソーラスを利用して人工的に作成したテスト
  セットでの識別性能評価に止まっており，見出し語集合から生成した上位下
  位関係候補の分類精度は評価できていない．さらに，彼らの手法では上
  位語候補／下位語候補は説明文が獲得できている用語に限定されるため，具
  体物を下位語とするような上位下位関係を大量に獲得することは難しいと考
  えられる．}

また，以上の新聞記事・Web文書を対象に上位下位関係を獲得する手法は，十分
な量の関係を獲得するために，大量の文書が扱えるストレージやそれを処理す
るための高速な計算機などの大規模な計算機資源が必要となる．{例え
  ば，}\cite{Sumida_2006}{の手法を用いた場合，約700~GBのHTML文書
  を処理して獲得できる上位下位関係の数は，約40万対であるが，我々の手法
  ではわずか2.2~GBのWikipedia文書から同程度の精度で約135万対の上位下位関
  係を獲得できている（詳しくは節}\ref{sec:exp_result}{の実験結果
    を参照のこと）．}


\subsection{Wikipediaからの上位下位関係獲得}

Wikipediaからの上位下位関係獲得についても新聞記事やWeb文書からの上位下
位関係獲得のときと同様に語彙統語パターンを用いる手法が開発されてい
る\cite{Ruiz-Casado_2005,Herbelot_2006,Toral_06,Kazama_2007}．これらの
手法では，Wikipediaの記事に概念の定義を記述する定義文が多く含まれること
に注目し定義文から上位下位関係を獲得している．図~\ref{fig:wiki}（b）では
「紅茶とは，摘み取った茶を乾燥させ，もみ込んで完全発酵させた茶葉。」と
いう定義文が含まれており，紅茶の上位語（の一つ）である茶葉を用いて紅茶が
説明されている．この文に対し“とは*〈上位語〉。”というパターンを適用
することで紅茶の上位語である茶葉を抽出することができる．Kazamaらは，英
語の固有表現抽出タスクのために，Wikipediaの記事の見出し語を下位語として
記事の冒頭の一文を定義文とみなし，その定義文中の特定の語彙統語パターン
にマッチする表現を上位語として獲得した\cite{Kazama_2007}．ま
たHerbelotらは，Wikipediaの記事の全文を意味解析し，定義文に対応する項構
造を認識することで，約88.5\%の精度で上位下位関係を獲得してい
る\cite{Herbelot_2006}．Ruiz-Casadoらは
WordNet~\cite{wordnet-book_1998}を利用して学習された上位下位関係からパ
ターンを学習・適用することで，69\%の精度で上位下位関係が獲得できたと
報告している\cite{Ruiz-Casado_2005}．これらの手法は，Wikipediaに頻出す
る語彙統語パターンに着目した上位下位関係獲得手法であり，前節で述べた上
位下位関係手法と同様に精度が高い一方で{Wikipediaの記事数と同程
度の数の下位語に関する上位下位関係しか}獲得できないという問題がある．

一方，SuchanekらはWikipediaの各記事の見出し語に対し，記事に付与されたカ
テゴリのラベルを上位語として上位下位関係を獲得する手法を提案してい
る\cite{Suchanek_2007}．彼らは，英語特有の経験則を用いてカテゴリを選別
し，外的知識としてWordNetを利用することで，約95\%と高精度で上位下位関係
を獲得している．提案手法では，WordNetなどの外的な言語資源を用いることな
く，機械学習のみで高精度の上位下位関係を大量に獲得することを目指す．ま
たKazamaらやSuchanekらの手法のように，下位語候補が記事の見出し語に制限
されないため，より網羅的な上位下位関係が獲得できると期待される．

また，本研究と同様にWikipediaの記事構造を用いた研究とし
て\cite{Watanabe_2008}が存在する．渡邉らは Wikipediaの記事構造か
らWikipediaのアンカーリンク間の関係を元に条件付確率場を学習し，そのモデ
ルを適用することでアンカーリンクから固有表現を抽出し
た\cite{Watanabe_2008}．本提案手法では記事構造から直接上位下位関係を獲
得するのに対し，渡邉らの手法では記事構造をアンカー間の関係が同じカテゴ
リか，関連語か，部分全体関係かどうかの判定に用いており，異なる手法とい
える．




************************ [./logs/V16N04-05/related_study] ************************
関連研究

近年，大学における活動状況を内外へ周知する目的でWebを利用した情報公開が進んでおり，Information Communication Technology (ICT) を活用した教育の一環として，シラバスの電子化と公開が多くの大学で取り組まれている．さらに，シラバスの公開から一歩すすんで，MIT をはじめとする主要大学にて， Open Course Ware (OCW) という，講義とその関連資料をインターネット上に公開する取り組みもはじまっている\citeC{Web_OCW,Web_JOCW}．また，一部の大学では，シラバスを可視化するシステムに関する研究・開発が行われている\cite{Article_Miyoshi_2006}．これらのシステムでは，講義間の関連性を可視化することによって，学生が自ら履修計画を立てたり，多様なコース中から最適なキャリアパスを自ら指向することを目指しているが，現段階では，比較的単純な可視化という段階にとどまっており，より深くシラバスを分析し構造化することが求められている．

シラバスの分類支援に関する研究\cite{Article_Miyazaki_2005}では，ユーザ支援のためには，分類の根拠となった素性（用語）を含めた形でユーザへ情報提示することが重要であると指摘されており，通常の文書分類において一般的に利用されているサポートベクターマシン(SVM)や確率モデルがユーザ支援には向かないことが議論されている．現時点では，分類の判定根拠を明確にする目的においては，SVMに比べて決定木\cite{Inproc_Lewis_1994}等の手法のほうが有望であると考えられるが，近年の研究では，SVMにおいても判定根拠の説明を行おうとする試みが行われている\cite{Inproc_Itabashi_2008}．尚，本システムでは，分類精度の観点からSVMを分類器として選択したが，分類の根拠を明示する目的において，可視化時に素性（用語）を提示する等の工夫を行っており，修正作業者からは，修正に有益な情報であるとの評価を得ている．しかしながら，より大規模の対象に対する分類・修正の場合は，\cite{Inproc_Itabashi_2008}と同様の手法の導入も検討する必要があろう．一方，システム上は，分類器を変更できるようモジュラリティを確保して設計を行っており，今後の研究の進展および技術発展等を検討しつつ，他の分類手法へ変更に柔軟に対応できるように実装を行っている．

また，本研究のようにあらかじめ課題を設定して分類しようとするトップダウン型ではなく，ボトムアップ型の分類手法として，文書クラスタリングによりシラバスを分類する研究も行われている\cite{Article_Nozawa_2005}．文書クラスタリングは，分析の切り口が定まっていない状況での分類や，新たなカテゴリの発見には有用であり，東京大学工学部においても，MIMA Searchシラバス分類システム\cite{Inproc_Mima_2006a,Web_MIMA_Search}を開発し，学生に向けたサービスを提供している．また，学生を主体としたボトムアップ型アプローチとして，フォークソノミーを用いた講義選択知識の抽出の試みも行われている\cite{Inproc_Nishijima_2008}．ただし，学生の観点からの分類が，学生への教育にとって必ずしも適当とは限らないこと（例えば，単位取得の容易さによる分類等）や，実際にサービスを行う際には，学生へのインセンティブ付与の問題や，不適切な内容の削除に必要なコスト等，検討すべき課題は多い．

一方，高等教育機関における，教育内容・方法等の改善のための組織的な取り組み（ファカルティディベロップメント，FD）の制度化が近年急速に進展しており，教員の教育力向上のために ICT を活用した FDの実施が求められている\cite{Techrep_NIME_2008}．この点からも，シラバスを構造化し全体を俯瞰することによって，講義内容の重複や抜けを発見し FD の実施に資することが期待されている．ただ，ICT活用教育の導入の際のデメリットとして，「システムやコンテンツを作成，維持するための人員が不足していること」が最も高いと報告されているように\cite{Techrep_NIME_2008}，シラバスを構造化するためには，通常，人手によりあらかじめシラバスを分析・分類・整理する必要があり，これは大きな人的コストと時間を必要とする．したがって，この作業を可能な限り自動化し，効率的な手法を開発することが非常に重要な課題となる．



************************ [./logs/V16N05-04/related_study] ************************
事態間関係知識獲得の関連研究
\label{sec:related_work}

\sec{introduction}でいくつか関連研究を取り上げたが，そこで紹介できなかっ
た関連研究を本節で紹介する．

初期の共起パターンを用いた事態間関係獲得の手法はChklovskiとPantel
~\cite{chklovski}のVerbOceanである．この手法は人手で作成した\textit{to
  Verb-X and then Verb-Y} のような少数の共起パターンを利用し
て，\textit{strength}（例えば \textit{taint --
  poison}）や \textit{happens-before}（例えば \textit{marry --
  divorce}）のような6種類の事態間関係を獲得し，約29,000の述語対
を65.6\%の精度で獲得した．

多くの事例と共起するパターンを用いて獲得した事態間関係から誤りを除くた
めに，Inui ら~\cite{inui:DS03} は因果関係を表わす接続表現「ため」と教師
付き分類学習を用いて，80\%の再現率と95\%以上の精度で因果関係
を4種類 \textit{cause}，\textit{precondition}，\textit{effect}，
\textit{means} に分類した．

他に，Torisawa~\cite{torisawa:NAACL}は接続パターン「Verb-X て Verb-Y」
を用いて獲得した述語対と，それぞれの述語と格の間の共起情報を組み合せて
時間的制約にある事態対を獲得した．ただし，この手法は時間的制約にある事
態対以外の関係を獲得するために応用できるのかは明かではない．

また，共起パターンを用いた~\cite{chklovski}手法を元に
Zanzottoら~\cite{zanzotto:06}は名詞化した動詞を利用して含意関係にある事
態対を獲得した．例えば，含意関係 \textit{X wins $\rightarrow$ X plays}
を \textit{player wins} のようなパターンから獲得した\footnote{動詞
  ``play'' を名詞化すると名詞 ``player'' となる．}．しかし，動詞を名詞
化するには様々な変形パターンを考えることができるが，この実験では限定的
な変形パターンのみを対象としている．



************************ [./logs/V17N01-02/related_study] ************************
関連研究
\label{sec:related-work}

言語の多様化にともなって，自動的に言語の識別を行うことの重要性が増してきている．
その一つの背景には，ウェブの急速な成長にともなう英語以外の文書の増加がある．
Internet World Stats による近年のウェブユーザ数の言語別集計の結果\footnote{http://www.internetworldstats.com/stats7.htm} によると，2008 年現在では，依然として英語を利用するユーザが最も多く，それに続いて，中国語，スペイン語，日本語，フランス語を利用するユーザが多い．
2000 年からの言語別ユーザ数の増加の割合は，アラビア語，ポルトガル語，中国語，フランス語，スペイン語が大きな伸びをみせている．
この調査結果は，用いられる言語の多様性が増していることを示しているといえる．
また，ウェブ上の文書の内の半数以上は英語以外の言語で書かれているものであると同時に，一つの文書の中で複数の言語が使われていることもある．
多種多様な情報元からの情報検索や質問応答，機械翻訳等，ウェブ上の膨大なデータを対象とした自動処理の実現においては，
文書の使用言語の自動推定だけでなく，文書中に出現する固有名詞等の外国語の語句の的確な解析も，自然言語処理の応用分野における精度向上に大きな影響を与える要因となりうる．


\subsection{統計情報を用いた言語識別に関する研究}
\label{sec:language-identification}

言語識別の研究は，文書を対象としたものに限らず，音声認識 \cite{matrouf98,berkling94} や，文書イメージを対象にしたものもある．
Sibun らは，文書イメージから抽出された文字の形状の統計的な分布を利用して言語識別を行った\cite{Sibun94}．
彼らは，アルファベットの文字の形状を，ベースライン，ボトム，トップ，X ハイトの情報を使って分類し，文書中の文字を Linear Discriminate Analysis (LDA) を用いて分類した．
2,000 から 3,000 文字を含む 23 の言語の文書イメージを用いて文書を構成する言語を識別する実験を行った結果，90\% 以上の精度を達成している．

Dunning は，ドイツ語の `\"u' やフランス語の `\^e' 等のアクセント記号を用いずに，5,000 バイトのトレーニングコーパスと 500 バイトのテスト用テキストを用いた言語識別の実験で 97\% の精度を実現している \cite{dunning94}．
Dunning らは 20 バイトのテスト用テキストでも 92\% の精度での言語識別を実現し，短い文書や数単語で構成される句であっても，言語識別が可能であることを示した．

Lins らは，文書中に含まれる複数の単語に対して各言語の辞書中での出現の有無を調べる手法で言語識別を行った~\cite{Lins04}．
Lins らは言語内で比較的種類が少ないとされている副詞，冠詞，接続詞，感嘆詞，数詞，前置詞，代名詞のみを辞書引きの対象とすることで，高速かつ汎用性の高い手法を提案している．
Lins らはこの手法を用いて，ポルトガル語，スペイン語，フランス語，英語の文書（約 1,000 単語で構成される 600 の文書）を対象とした評価実験を行い，ウェブの文書でも 80\% 以上，通常の文書では 90\% 以上の精度を達成している．

Martins らは，ウェブページに特化した言語識別手法を提案した\cite{Martins05}．
Martins らの手法は，$n$-gram（1 から 5）情報のプロファイル間の距離と，ウェブページ固有のヒューリスティクスを用いるものである．
12 の言語で各 500 ウェブページを用いた実験では，全ての言語で 84\% 以上の正解を出したが，スウェーデン語とデンマーク語等の北欧の言語の類似性が若干の精度の低下をもたらしたことを今後の課題として挙げている．

また，地名以外の固有名詞として人名に着目し統計情報を用いた所属国の推定を行う研究もある．
Nobesawa らは，言語識別の手法を人名に対して適用することで，人名用の言語識別のためのシンプルなシステムを提案し，人名を属する国で分類することが可能であることを示した\cite{nobesawa0512paclic}．
この手法は，人名文字列の長さや，人名の文字単位の $n$-gram の情報を活用したものであり，9 種類の言語圏の 12 の国に対して 90\% 以上の精度を実現することに成功している．
また，Nobesawa らは，英語の人名に対して SVM の分類器を用いた手法も提案している\cite{nobesawa0605ieee}．


\subsection{エリア推定に関する関連研究}
\label{sec:toponym-resolution}

地名のエリア推定の最終的な目標は，その地名が地球上のどの場所を示しているのかを判断することである．
文章中の地名のエリア推定タスクは，一般に， (1) 地名文字列の認識， (2) 地名文字列の国推定， (3) 地名文字列と場所との対応付け，の三段階の処理からなる．
(1) の地名文字列の認識は固有名詞の自動抽出タスクに相当する．
(2) および (3) は，地名文字列と地球上の場所との対応付けを行う処理である．
本稿ではこのうち，研究のあまり進んでいなかった (2) の所属国推定を目的とし，その実現手法を提案するものである．

(3) の地名文字列と場所との対応付けの研究では，あらかじめ対象ドメインや対象言語を制限することで，(2) の所属国の推定のステップを省略することが多い．
したがって，本稿が対象とする所属国の推定の研究は，この地名文字列と場所との対応付けの研究を助け，その精度の向上に寄与するものと考えている．

本節では，関連する研究として (3) の地名文字列と場所との対応付けを行っているものについて述べる．
これらは，対象の地名が辞書に登録されていることを前提として辞書引きによって場所の候補を探しだし，複数の場所が候補として挙がる等の曖昧性がある場合には文脈情報等を用いてその判別を行うアプローチが一般的である．

Hauptmann と Olligschlaeger は，音声データを対象とした場所の判別を行う手法を提案している~\cite{Hauptmann99,Olligschlaeger99}．
基本的には地名辞書に含まれる地名のみを対象としているが，同じ地名であっても複数の異なる場所を示す曖昧性がある場合には，同一の会話内に現れる他の地名の情報を活用することによって，その場所の相違を判断している．
Hauptmann らの手法では，200 のニュース記事に出現した 357 の地名のうち 269 の地名を正しく判別することができ，75\% の精度を達成している．
Hauptmann らは，正しく判別することができなかったものは，地名辞書に載っていなかったもの，曖昧性によるエラー，音声認識誤り等が原因であると報告している．

また，Smith らは地名の曖昧性の解消に焦点を当て，文書中の地名の出現頻度の重心を利用した手法を提案した \cite{Smith2001}．
これは，地名の出現頻度によって重みが付けられた地図上での重心を計算し，ある閾値よりも離れているものを枝刈りした上で重心を再度計算しなおすことで候補の曖昧性を解消しようとするものである．
大きな地名辞書を使うことによって，再現率を高く保てるようにした上で，F 値が 0.81 から 0.96 という結果を出している．
しかし，この手法はその重心の付近を示すだけであり，重心のみを使用しただけでは頑健性に欠ける場合があると結論づけている．

地名の曖昧性を解消するための手法として，Li らは，地名表現のパターンマッチングと重み付き地名の類似度グラフの探索，サーチエンジンを用いた地名辞書の補間の三つのアプローチを組み合わせる方法を提案した \cite{Li2002}．
地名表現のパターンマッチングでは，`city of' + ``地名''（``city of Vancouver'' 等）や ``地名1'' + `,' + ``地名2'' + `,' + ``地名3''（``Boston, Massachusetts, USA'' 等）といった地名の周辺の表現のパターンを利用している．
Li らは大きな地名辞書と地名表現のパターンマッチングを用いることで，93.8\% の精度を実現した．

Pouliquen らは，ヨーロッパのエリアに限定したマルチリンガルテキストを対象として，場所の判別の精度の向上を目指す手法を提案した~\cite{Pouliquen06}．
この手法では，``And'', ``To'', ``Be'' 等の瀕出する単語と同形異義語であるような地名を geo-stop list として抽出し，このような地名を候補から排除することで，精度の向上を図っている．
また，それ以外にも場所の重要度，人名との区別，地名同士の物理的な距離の情報等を用いて曖昧性の解消を行っている．
geo-stop list に登録されている地名を候補から排除することで再現率は低下するが，F 値で 0.77 という結果を示している．

Clough は，複数の地名辞書を用いた場所の判別手法を提案している \cite{Clough05}．
複数の地名辞書を優先順位を付けて検索し，stop-list を使って，各候補に対してスコアを与えている．
このスコアは，地名表現の周辺の出現パターン，オントロジにおける階層の深さ，地名辞書の優先順位，ユーザのプリファレンスによって計算される．
Clough はイギリス，フランス，ドイツ，スイスを対象とした実験で 89\% の精度を達成している．

Zong らは，ウェブページに対してそのページが記述しているエリアを判別する実験を行った~\cite{Zong05}．
アメリカに関する文書のみを対象とし，地名の周辺の出現パターンと地名同士の物理的な距離を利用することで，地名が 32 個以上 199 個以下だけ含まれるウェブページを対象に 760 の地名について実験を行い 88.9\% の精度を達成している．

これらの関連研究のほとんどは，文書中に出現する地名を対象としており，文脈の情報を用いて地名の場所の判別を行っている．
これらは，その地名がどんな文脈で出現し，同時に出現するその他の地名とどんな関連があるのかといった情報を積極的に利用する方法である．
これらの手法の大きな特徴として，地名の認識および場所の判別に地名辞書を利用していることが挙げられる．
地名を表記するときによく用いられるパターンや会話における局所性等の自然言語処理でよく用いられるヒューリスティクスだけではなく，都市の人口数，実際の二地点間の距離等の地理的な情報を活用しているものもある．
このような辞書ベースのアプローチは，特定のドメインを対象とした処理の場合には高い精度で場所の判別を行うことが可能である．
このように一般的な自然言語処理のヒューリスティクスが適用可能な情報元を用い，かつ，そのドメインに出現しうる地名が変化するスピードが遅く，辞書や地理的なデータの整備を十分に行える場合には，これらの手法は十分に適用可能である．

しかし，全世界のすべての地名を網羅した地名辞書を整備することは現実的でない上に，情報元の多様化のスピードがますます加速している現在では，より頑健性の高い柔軟な手法が必要と考えられる．
Rauch らは，知らない地名であっても人間はその所属地域をある程度推測可能であるという事実を背景として，表層的な統計情報をベースとしたシステムが有効であると主張している~\cite{Rauch03aconfidence-based}．
本稿は，Rauch らと同じ主張を共有し，具体的な実現手法を提案するものである．



************************ [./logs/V17N01-07/related_study] ************************
関連研究
\label{sec:relatedwork}

\cite{grimshaw:1990}は動詞と同様事態を指す名詞のことを
\emph{event nominal} （事態名詞）と呼び，
\emph{result nominal} （結果名詞）
・\emph{simple event nominal}（単純事態名詞）
・\emph{complex event nominal}（複雑事態名詞）
の3つに分類した．
結果名詞とは，「梅干」のように「梅を干してできたもの」という結果物を指す
名詞であり，単純事態名詞とは「運動会」のように意味役割を持たない名詞である．
「推薦」のように「誰が誰を誰に推薦した」という \emph{event structure}
（事態構造）を持つ複雑事態名詞\footnote{
	本稿で扱う事態性名詞はGrimshaw の複雑事態名詞に相当する．}
だけが項（必須格）を持つ\footnote{
	「報告」のように
	結果名詞「報告書・報告物」としての語義と複雑事態名詞
	「出張の報告をした」としての語義両方を持つものもある．
	また「試験」のように文脈に応じて結果名詞・単純事態名詞・複雑事態名詞の
	いずれも取りうる名詞もある．}．
このように，事態を指しかつ項を持つ名詞の存在は古くから知られていた．
近年この現象に対する自然言語処理的観点からの関心が高まり，
複数の言語でコーパスが整備されるに至った．
以下で，英語・中国語・日本語における事態性名詞の項構造解析の
関連研究について述べる．


\subsection{英語における事態性名詞の項構造解析}

Macleod らは1997年から動詞の名詞化に注目し，高いカバー率の情報抽出を
目的とした事態性名詞の辞書 NomLex の作成に着手した\cite{macleod:1997:RANLP}．
NomLex は2001年に完成・公開され，NomBank プロジェクトに引き継がれる．
NomBank は NomLex と同じく英語における動詞の名詞化に着目したコーパス
\cite{meyers:2004:NAACL-HLT,meyers:2004:LREC}
であり，
Penn Treebank \cite{marcus:1993:CL}に対し
PropBank ら \cite{palmer:2005:CL} の仕様に従って項構造が付与されている．
Meyers らは2007年 Penn Treebank II に対するアノテーションを終了し，
NomBank 1.0 を公開した．
2008年と2009年の CoNLL 共通タスクでは，NomBank コーパスを用いた事態性
名詞の項構造解析もタスクの一つとして行われた．

NomBank を用いた事態性名詞の項構造解析は \cite{jiang:2006:EMNLP}や 
\cite{liu:2007:ACL}がある．
Jiang らは NomBank に対し，最大エントロピー法を用いた教師あり学習による
項構造解析を行った．彼らは PropBank を用いた動詞に対する意味役割付与
(Semantic role labeling) において有効性が確認されている素性に加え，
名詞の語幹やクラスといった事態性名詞についての意味素性や，
支援動詞構文を認識するための述語との位置関係といった統語素性，
そして項構造を正しく認識するための項同士の依存関係に関する大域素性を用いた．
Liu らは Jiang らの用いた素性をベースに半教師あり学習手法の Alternating
Structure Optimization (ASO)~\cite{ando:2005:JMLR} を適用した．
ASO は解くべき問題に関連する補助問題を作成することで
経験リスク最小化を行う手法であり，彼らの研究では事態性名詞の項構造解析に
有効なさまざまな補助問題が提案されている．

自然言語処理の評価型ワークショップ CoNLL 2008, 2009 では，
PropBank と NomBank
を用いた述語と事態性名詞に対する項構造解析の共通タスクが行われた．
CoNLL 2009には20チームが参加するなど，
事態性名詞の項構造解析は活発に研究されている．



\subsection{英語以外の言語における事態性名詞の項構造解析}

英語以外の言語における事態性名詞の包括的な研究としては，Xue らによる
Chinese Nombank~\cite{xue:2006:LREC}がある．
これは英語以外における初めての大規模な事態性名詞のコーパスである．

このコーパスを用いた解析として
\cite{pradhan:2004:NAACL-HLT,xue:2006:HLT-NAACL}がある．
日本語のサ変名詞と同様，
中国語では動詞と動詞化された名詞は同じ表層形を持つため，
動詞化された名詞は対応する動詞と共通の項構造を持つと仮定すると，
動詞に関する資源を事態性名詞に流用することができる．
\cite{xue:2006:HLT-NAACL} では，
単純に動詞の事例を事態性名詞の事例に追加して実験したところ，
同じ事態を指す表現であっても，動詞として使われる場合と名詞として
使われる場合では語彙統語パターンが大きく違い，かえって性能が下がった，
と報告している．
我々は日本語を対象に，大量に自動獲得した動詞と格要素の共起を
用いて事態性名詞の項同定を行い，大きく正解率を向上させることに成功した．
また，事態性名詞に特徴的な語彙統語パターンを用いることでさらに
性能を改善させた．

一方，日本語においては\cite{kurohashi:2005}によって，京都テキスト
コーパス第4.0版の一部，約5,000文に事態性名詞を含む名詞間の関係タグが付与された．
\cite{sasano:2005:NLJ}は自動構築された
名詞の格フレーム辞書の評価として事態性名詞の項同定を行った．
彼らは事態性名詞のみについての結果を報告していないため，
直接比較することはできないが，
我々は事態性名詞の解析に焦点を当て，事態性判別の問題を解いた点，
動詞と格要素の共起の情報を用いた点，
および事態性名詞に特徴的な語彙統語パターン（支援動詞構文）を用いた点が異なる．



\subsection{NAIST テキストコーパス}


我々は事態性名詞の項構造解析の問題に対し，NAIST テキストコーパス~
\cite{iida:2007:NL} を用いた教師あり学習を行った．
このコーパスでは，文章中の各事態性名詞について事態性の有無を判別し，事
態性がある場合には項構造（必須格となるガ格・ヲ格・ニ格）の情報が付加
されている．たとえば図\ref{fig:naisttextcorpus}のような
記事に対して，

\begin{figure}[b]
\begin{center}
\includegraphics{17-1ia8f1.eps}
\end{center}
\caption{NAIST テキストコーパスの事態性名詞のアノテーション}
\label{fig:naisttextcorpus}
\end{figure}

\begin{itemize}
\item[] [ \textsc{rel}=管理（する），ガ=〈外界〉，ヲ=リスク ]
\item[] [ \textsc{rel}=調査（する），ガ=ＢＩＳ，ヲ=実態 ]
\end{itemize}
のような情報が付与されている．
項に関しては，「管理」のヲ格「リスク」や
「調査」のガ格「ＢＩＳ」のように，項が文内に出現している場合はそれが
形態素単位で指示される．
また，「調査」のヲ格「実態」のように，文外に出現する項でも記事内で特定
できる場合はその要素が指示される．さらに「管理」のガ格のように，必須格で，
かつ文内にも記事内にも出現していない場合は特別な〈外界〉タグが付与されている．

現在公開されている NAIST テキストコーパス1.4$\beta$~
\footnote{http://cl.naist.jp/nldata/corpus/} は京都テキストコーパス3.0
全体約4万文に対してタグ付与されており，京都テキストコーパス4.0と比較して
大規模な学習を行うことができるという利点がある．

\cite{taira:2008:EMNLP}は NAIST テキストコーパスを用いて
構造学習を述語項構造解析および事態性名詞の項同定に適用した
結果を報告しているが，彼らは事態性判別の問題を解いていない．また，
比較的項同定が難しいガ格について報告せず，ヲ格とニ格に対する
結果しか議論していない．



************************ [./logs/V17N01-08/related_study] ************************
関連研究\label{relatedwork}

 Barzilayら\cite{barzilay2005,barzilay2008}は局所的な一貫性の
 モデルとしてentity gridを提案している．この
 モデルはテキスト
 中で述べられている要素の遷移に着目している．これは，センタ
 リング理論\cite{grosz1995}で示されているように，一貫性のあるテキストで
 はその文中の要素の出現に規則性があるという考えに基づいている．

 Elsnerら\cite{elsner2008}は，entity gridモデルがテキストの一貫性において要素の遷移
 にのみ着目しているということに言及し，例えば参照表現や対象の要素がこれ
 までに既に述べられている要素かどうかなどといった他の要素をモデルに組み
 込んでいる．Filippovaら\cite{katja2007}はentity gridモデルに要素
 間の関係を考慮したモデルを提案し，このモデルをドイツ語の新聞記事に対し
 て適用した結果を報告している．

 大域的な一貫性について，Barzilayら\cite{barzilay2004}は隠れマルコフモデル(HMM)を採用したモデ
 ルを提案している．このモデルでは文章中の話題をHMMにおける隠れ状態と見な
 し，話題の一貫性を隠れ状態の遷移確率によって表現している．Soricutら\cite{soricut2006}や
 Elsnerら\cite{elsner2007}は局所的な一貫性と大域的な一貫性を同時に考慮するモデルを
 それぞれ提案している．これらのモデルは
 entity gridモデルとHMMを組み合わせたものである．

 日本語の文章に対する一貫性の評価手法には，板倉ら\cite{itakura2008}の提案する段落の一貫性
 指標がある．これは段落内で用いられている単語間の意味的な関係に基づいている．

 これらの手法では文書中の単語の出現に着目してテキスト一貫性を評価してお
 り，\ref{introduction}章で述べたように一貫性に影響すると考えられる，テ
 キスト結束性に関係する表層的な特徴は考慮されていない．



 
************************ [./logs/V17N04-01/related_study] ************************
関連研究
\label{sec-related-works}

能動学習は固有表現抽出タスクへの適応
\cite{shen-EtAl:2004:ACL,laws-schutze:2008:PAPERS}に限らず，様々な自然
言語処理タスクへの適応が研究されており，品詞タグ付け
\cite{argamonengelson99committeebased}，テキスト分類
\cite{Lewis94heterogeneousuncertainty}，構文解析
\cite{Hwa:ActiveLearning2000}，単語選択での曖昧性解消
\cite{banko_ACL_2001}など数多くの関連研究がある．いずれの場合も信頼度
や情報量といった何かしらの指標に基づいて学習効果の高いデータを選択する
ことが重要であり，その指標の算出やデータセレクションの単位は基本的に文，
もしくは一定の語数以上の単語列であった．今回我々が提案する能動学習では，
モデル出力の信頼度を指標とするが，その算出単位は文単位ではなく，タグ単
位である点が従来研究とは異なる．更にデータセレクションも文単位ではなく，
タグ単位でリジェクト／アクセプトを決定し，リジェクトタグのみを修正箇所
対象として絞っているため，更なる学習コストの削減に繋がった．なお，
\cite{tomanek-hahn:2009:ACLIJCNLP}では，本稿と同様にタグ単位の信頼度に
基づいた能動学習を英語の固有表現抽出タスクで評価しており，本稿と同程度
のコスト削減効果を報告している．今回，我々は更にタグ単位の信頼度を利用
して半自動で誤り修正を行うUpdateNERの提案および評価を実施した点が新し
い．

一方，特に機械学習の分野において，正解データだけでなく膨大な量のプレー
ンテキストを利用する半教師あり学習の研究も進められている．自然言語処理
タスクでは，語義曖昧性解消\cite{Yarowsky:WSD1995}，テキスト分類
\cite{Fujino:SemiSupervised2008}，チャンキング・固有表現抽出
\cite{suzuki-isozaki:2008:ACLMain}などへも適応されている．特に近年は，
Giga Word単位のプレーンテキストも入手可能になってきたため，このデータ
を正解データと組み合わせてモデル学習することにより従来技術の性能限界を
超える可能性が示唆されている．ただし，今回我々がターゲットとしているの
は，日々語彙や話題の変化が激しいブログなどのCGMドメインにおいて，モデ
ルを低コストで再学習するタスクであり，このような状況を反映するような
Giga Word単位のプレーンテキストを入手するのは困難であると考えられる．
そのため，膨大な量のプレーンテキストを利用する半教師あり学習をそのまま
適応することは現実的ではない．

プレーンテキストを利用するという点で半教師あり学習と類似する手法にブー
トストラップ学習の研究がある
\cite{Etzioni2005,pantel-pennacchiotti:2006:COLACL}．これは，少量のシー
ドを準備して，シードと同じカテゴリに属する新しいインスタンスをプレーン
テキストから自動獲得する学習法である．本稿のUpdateNERはシードを準備す
るだけで，データセレクションとその修正・抽出までを自動的に実行するブー
トストラップ学習とみなすことができる．しかし，従来のブートストラップは
新しいインスタンスを獲得して辞書（シソーラス）を構築することを目的とし
ているのに対し，本手法では，固有表現単体ではなく，固有表現を含むタグ列，
即ちコンテキスト全体を獲得している点が異なる．モデルの再学習のためには
固有表現辞書だけではなく，固有表現を含むコンテキストそのものが必要であ
る．UpdateNERではブートストラップ学習を適用して最終的には教師あり学習
の枠組みでモデル更新を実現するという点が新しい．この学習コストはシード
を準備する部分のみのため，能動学習と比較しても極めて低く抑えられるとい
う利点もあり，本手法は有効である．


************************ [./logs/V17N04-04/related_study] ************************
関連研究

意味役割を汎化することで意味役割付与の疎データ問題を解消する方法は，こ
れまでにいくつか提案されてきた．
\shortciteA{Moschitti2005}は各役割を主役割，付加詞，継続項，共参
照項の四つの荒いクラスに分類した後，それらのクラスに対してそれぞれ専用の
分類器でPropBankのARGタグを分類した．
\shortciteA{Baldewein2004}は，意味役割分類器を学習する際，ある役割の訓練
に，類似する他の役割の訓練例を再利用した．類似度の尺度としては，
FrameNetにおける階層関係，周辺的役割，EMアルゴリズムに基づいたクラスタが
利用された．
\shortciteA{gordon-swanson:2007:ACLMain}はPropBankの意味役割に対して，各
フレームの統語的類似度に基づいて役割の汎化を行う手法を提案した．

また，
異なるフレーム間の意味役割を繋ぐ懸け橋として，フレームに依存しないラベル
セットである主題役割も用いられてきた．
\shortciteA{Gildea2002}はFrameNetの意味役割を彼らが選定した18種類の主題
役割に人手で置き換えることで，役割の分類精度が向上することを示した．
\shortciteA{Shi2005ppt,Giuglea2006}は異なる意味コーパスによって定義され
た役割の共通の写像先として，VerbNetの主題役割を採用した．

意味役割の汎化指標に対する比較研究としては，
PropBank上でARGタグと主題役割の比較を行った，
\shortciteA{yi-loper-palmer:2007:main,loper2007clr,zapirain-agirre-marquez:2008:ACLMain}
の研究が挙げられる．
\shortciteA{yi-loper-palmer:2007:main,loper2007clr}は，
主題役割をPropBankのARGタグの代替とすることで，
{\it ARG2}の分類精度を向上出来た一方で，{\it ARG1}の精度は低下すると報告した．
また，\shortciteA{yi-loper-palmer:2007:main}は同時に，
{\it ARG2-5}は多種にわたる主題役割に写像されることを示した．
\shortciteA{zapirain-agirre-marquez:2008:ACLMain}は
最新の意味役割付与システムを用いて，PropBankのARGタグとVerbNetの主題役割
の二つのラベルセットを評価し，全体として，PropBankのARGタグのほうがより
頑健な汎化を達成すると結論付けた．
しかしながら，これら三つの研究は，意味役割付与全体の精度比較しか行っていな
いため，各汎化指標により得られる効果の正確な理解のためには，詳細な検
証による理由付けが必要である．

FrameNet，PropBankにおけるこれらの汎化ラベルは，
汎化ラベル自身を直接推定するモデルとして設計されたため，
コーパス中の意味役割を汎化ラベルで直接置き換える方法か，それに準じる方法
で用いられてきた．
しかし，この方法では，異なる汎化指標を一つの分類モデルの中で同時に用いる
ことが出来ない．役割の特徴を複数の観点から共有しようとする我々の目的のた
めには，これらの指標を自然に混合できる分類モデルの設計が必要となる．



************************ [./logs/V17N04-07/related_study] ************************
関連研究
\label{sect5_rwork}


第\ref{sect_intro}節で述べた通り，
人手によって任意の分野における未知語の情報を収集することはコストの面で現実的で
はない．このため，未知語に関する情報を自動獲得する研究が多く行われている．

まず，形態素解析など，自動単語分割を行うシステムにおいて
単語辞書に未知語を追加することを目的とした研究について述べる．

文献\cite{未知語の確率モデルと単語の出現頻度の期待値に基づくテキストからの語彙獲得}
では，
ある文の自動単語分割候補における$N$-bestの相対確率を，それぞれの候補において
出現する未知語の出現頻度の期待値として与える．その後，出現した未知語の中から
一定の閾値より大きい出現頻度の期待値を持つ未知語を獲得している．
また，単語分割の際には，未知語を構成する字種によって9種類の未知語タイプを定義し，
それぞれのタイプにおける単語長の分布を考慮した未知語モデルを用いることで，
未知語モデルの性能向上を図っている．

形態素解析のため，品詞を考慮して未知語を獲得する研究として，文献
\cite{Word.Extraction.from.Corpora.and.Its.Part-of-Speech.Estimation.Using.Distributional.Analysis}
では，コーパス中に出現する任意の部分文字列$\alpha$に注目し，
$\alpha$の前後の文字から，$\alpha$が未知語として出現する可能性の高い品詞
に属する確率を推定している．
その後，出現頻度が一定値以上かつ2文字以上の文字列$\alpha$を単語として抽出しておき，
形態素解析器にかけた結果に辞書未登録語が含まれている文字列$\alpha$を未知語として獲得し
ている．

日本語は分かち書きを行わない言語であるため，
自動単語分割器や形態素解析器において必須となる未知語の情報は正しい単語単位である．
このため，形態素解析器のための未知語獲得を行う研究では
未知語の読みには言及しないことが多い．
しかしながら，本研究では統計的仮名漢字変換の精度向上を目的としているため，
未知語の表記ならびにその読みに関する情報を同時に獲得することが望ましい．

文献\cite{自動未知語獲得による仮名漢字変換システムの精度向上}
では，仮名漢字変換を用いる際の入力とその変換結果から
未知語の獲得と言語モデルの更新を行う手法を提案している．
また，言語モデルの更新を繰り返すことで，
仮名漢字変換システムの精度が徐々に向上すると報告している．
ただし，ここで行われている実験はユーザによるシステムの利用を想定し
たシミュレーションであり，本論文で扱う自動獲得とは性質が異なる．

音声認識の分野においては，未知語を原因とする認識誤りの影響を抑制するため，
単語より小さい単位の語彙であるサブワードを擬似的な単語とし，
未知語をサブワードの連続として認識する手法が提案されている
\cite{単語N-gram言語モデルを用いた音声認識システムにおける未知語・冗長語の処理} \cite{Open.Vocabulary.Speech.Recognition.with.Flat.Hybrid.Models} \cite{New.Word.Acquisition.Using.Subword.Modeling}．
しかしながら，日本語の音声認識において
サブワードは基本的に仮名文字列から構成されるため，
サブワードをそのまま未知語獲得に用いても仮名漢字変換への寄与は低いと考えられる．


文献\cite{音声とテキストを用いた認識単語辞書の自動構築}では，
規則を用いてテキストから未知語の候補を抽出，
音声認識を用いて読みを自動的に獲得し，発音辞書に追加する手法が提案されている．
この手法は，テキストと音声から未知語と読みの情報を獲得する点で本研究と共通しているが，
未知語候補の抽出方法と獲得する情報の粒度が本研究と異なる．
本研究では，疑似確率的単語分割コーパスを用いることにより，一貫した単語単位で言語モデル
と発音辞書を作成する．また，音声認識結果から未知語の読みだけではなく文脈情報を獲得し，
統計的仮名漢字変換で利用する確率的言語モデル全体の性能向上を図っている．




************************ [./logs/V17N05-01/related_study] ************************
関連研究
\label{sec:関連研究}

乾ら\shortcite{Inui06}は「評価を記述するもの」や「賛否の表明」は意見の下位分類に含まれるとしている．本研究でも，意見の一部として評判が存在すると考える．意見は主に主観的言明全般を指しているが，物事に対する肯定や否定を表明する評判はその中の一部と考えるからである．

評判情報に関連する研究は，レビュー中の意見記述部分や評判情報記述部分を特定する問題を扱う研究と，記述されている評判が肯定的か否定的かの極性を判断する問題を扱う研究の二つに大きく分かれる．

評判情報記述部分を特定する問題に関連する研究の一つに，文書中のある程度まとまった範囲での意見性を判定するものと，文単位での意見性判定を行うものがある．それとは別に，評判情報記述部分を特定する問題に対するアプローチの中には，評判情報の構成要素を定義し，各構成要素組を抽出しようとするものがある．

文単位での意見性判定においてはYu el al. \shortcite{Yu03}やPang et~al. \shortcite{Pang04}が評価文書中の事実文と意見文を分け，意見文を抽出している．
峠ら\shortcite{Touge05}においても文単位で意見性の判定を行っている．この研究では，文中に現れる単語が意見文になりやすい単語であるか否かを学習しWeb掲示板から意見文の抽出を行っている．
さらに，日本語における評判情報抽出に関する先駆けの研究でもある立石ら\shortcite{Tateishi01}の研究は，あらかじめ用意された評価表現辞書を用いて，対象物と評価表現を含む一定の範囲を，意見として抽出している．

評判情報の構成要素を定義し，各構成要素組を抽出しようとする研究においては，村野ら\shortcite{Nomura03}が評価文の文型パターンを整理し，その構成要素を``対象'' ``比較対象'' ``評価'' ``項目'' ``様態''としている．また，各構成要素毎に辞書を用意することで抽出を行っている．
Kobayashi et~al. \shortcite{Kobayashi07}では評判情報を``Opinion holder''，``Subject''，``Part''，``Attribute''，``Evaluation''，``Condition''，``Support''からなるものとし，対象とその属性・評価の関係を抽出している．また，構成要素組の同定に着目した研究として，
    飯田ら (飯田\ 他\ 2005)\nocite{Iida05}は
評判情報における属性—属性値の組同定問題を，照応解析における照応詞—先行詞の組同定問題に類似した問題と捉え，トーナメント手法を用いて属性—属性値対を同定している．

次に，記述されている意見の極性を判断する研究について述べる．極性を判断する単位についても文書単位，文単位など様々な範囲を対象とした研究が行われている．
Turney \shortcite{Turney02}は文書中に含まれる評価表現の出現比率から評価文書全体の評価極性を求めている．Pang et~al. \shortcite{Pang02}も文書単位での極性判断を行っており，これには機械学習手法を用いている．

文単位の極性判断としてはYu el al. \shortcite{Yu03}が，Turney \shortcite{Turney02}と同様の手法で評価文を肯定，否定，中立に分類している．

上記の研究に関連して，評判情報に関するコーパスの必要性に着目した研究も行われている．日本語の評判情報コーパスに関係する研究としては，小林ら\shortcite{kobayashi06}が，あらかじめ辞書引きにより評価値候補を与えた上での意見タグ付きコーパスの作成を行っている．この研究では，評価値候補に対する注釈者の判断はある程度一致したが，関係や根拠に対する判断の揺れは無視できるものではないことを報告している．
また，Kaji et~al. \shortcite{Kaji06}では箇条書き，表，定型文などを用いてHTML文書から評価文コーパスの自動構築を行っている．

また，意見分析に関するコーパス研究においてはWiebe et~al. \shortcite{Wiebe02}がMPQAコーパスを作成している．これは，新聞記事に対して，主観的な表現とその意見主が注釈付けされた大規模なコーパスである．
Seki et~al. \shortcite{Seki07}は日本語，中国語，英語の新聞記事に対し，文単位で意見文かどうかを注釈付けしている．また，意見文に対してその極性や意見主を注釈付けしている．



既存研究のコーパスと本研究で作成するコーパスの違いについて述べる．MPQAコーパス\shortcite{Wiebe02}においては，主観表現に対して注釈付けを行っており，この点が本研究で作成するコーパスと関連がある．評判情報を正確に捉えるには対象の項目がどのような様態かを表している表層表現に対して主観表現であることに限らず注釈付けを行う必要があるが，MPQAコーパスでは対応していない．一方で，本研究で作成したコーパスでは客観的表現にも注釈付けを行っている．
次にSekiら\shortcite{Seki07}のコーパスとの比較だが，このコーパスでは文単位で意見性の有無を注釈付けしている．評判情報の構成要素の抽出を行うためには文単位ではなく構成要素単位での注釈付けが必要であるが，これには対応していない．一方で，本研究で作成しているコーパスでは対応している．

また，Kajiら\shortcite{Kaji06}のコーパスは文書源としてWebデータを用いている点では，本研究で作成したコーパスと同じだが，注釈付けの単位は文単位であり，評判情報の構成要素の単位で注釈付けはされていない．小林ら\shortcite{kobayashi06}のコーパスも文書源はWebデータである．しかし，同論文に公表されている情報によれば注釈付けされている評価値は一語単位である．また，評価値として主観的な表現のみを注釈付けしており，客観的な値は評価値として扱っておらず，評価値に付随する根拠として扱っている．本稿では主観的な値に加え，客観的な値も属性値の一部として扱っている．小林ら\shortcite{kobayashi06}では客観的な値には注釈付けがされているわけではないため，この点が異なる．さらに，様態を表す表現も全て評価値として扱われている上，評価対象間の階層構造の注釈付けは必要性を認めながらも扱っていない．
これに対して，本研究のコーパスでは様態を表す属性値と評価を分離している．さらに，評価対象間の関係についてはオントロジー情報として記述を行う．

加えて，本研究のコーパスでは，長い表現や客観的な表現に対しても注釈付けを行っている．製品の評価に対して一語単位でのみ注釈付けを行うと，「好き」「嫌い」「良い」「悪い」などの特定の表現のみに注釈付けが行われてしまう．しかし，レビュアーがレビューを記述する際には様々な表現を用いて肯定否定を明示している．また，製品の様態についても，レビュアーは様々な様態に着目してレビューを記述している．このため，一語単位でのみ注釈を行う場合，レビュアーが着目した製品の様態を誤って注釈付けしたり，注釈付けを落としてしまうことがあると考えられる．さらに，これらの長い表現は，表現の一部の語のみに注釈付けを行っても正しい注釈にならないため，長い表現を注釈付けする必要性があると考えた．長い表現に注釈付けを行う必要性がある例を図\ref{fig:注釈付け対象となる長い表層表現の例}に示す．

\begin{figure}[t]
\input{02fig01.txt}
\caption{注釈付け対象となる長い表層表現の例}
\label{fig:注釈付け対象となる長い表層表現の例}
\end{figure}

図\ref{fig:注釈付け対象となる長い表層表現の例}の例における下線部は一語だけに注釈付けを行うことはできないが，これらは製品の様態を表す値段や，評価であり評判情報の一部となる．


注釈付け支援に関する研究では，
    野口ら (野口\ 他\ 2008)\nocite{Noguchi08}が
セグメント間の関係を注釈付けするためのアノテーションツールSLATを作成している．
また，洪ら\shortcite{Kou05}は対話コーパス作成の際に，事例参照を注釈付け作業の支援として用いており，有効性を報告している．他にも，翻訳の分野においては翻訳メモリと呼ばれる原文と訳文のデータベースを用いた翻訳支援が行われている．これも，ある種の事例参照と言える．
本研究では，Kajiら\cite{Kaji06}や小林ら\cite{kobayashi06}のコーパスのように注釈を自動的に付与せずに，注釈事例の参照を用いた評判情報コーパスの作成を行う．
注釈者が注釈付けを行う際に，あらかじめ注釈付けがなされている状態で対象の文を見てもらうと，あらかじめ付与された注釈が判断の前提となってしまうことが予想される．
あらかじめ付与された注釈が注釈者の判断に与える影響に関する実験については今後の課題とし本稿では行わない．
本研究では注釈者が判断に迷う場合に，事例を参照しつつも独自の判断の下で注釈付けを行ってもらうというアプローチを採用した．

また，評判情報のモデル化についても，従来研究においては製品の様態と評価が混在するモデルがその多くであった．本研究では，様態に対する評価がレビュアーによって異なる場合を正確に表現するために，これを分離するモデルを提案する．

本研究では製品全体やその部分について以下の2種類を様態として扱う．

\begin{itemize}
\item
具体的な値\\
例：30~cm，2~G
\item
他の製品との比較から得られる値など主観的であっても，特にその表層表現のみからでは肯定的や否定的といった評価が一意に決まらないもの\\
例：速い，静か，明るい
\end{itemize}


一方，以下に示す例のように，肯定や否定といった極性が陽に記述されている部分を評価として扱う．

\begin{itemize}
\item
陽に記述されている極性表現\\
例：大満足である，いいです，ちょっと不満
\end{itemize}





************************ [./logs/V17N05-02/related_study] ************************
関連研究

(Gross 1986)は，フランス語の複合副詞(compound adverb)，複合動詞(compound verb)の種類が単独の副詞，動詞の，それぞれ，3.3倍，1.7倍程度存在することを指摘した．
また，(Jackendoff 1997)は，英語の日常使用者の持つMWEレキシコンは単語レキシコンと同数規模だと推定されること，(Sag et al. 2002)はWordNet1.7 (Fellbaum 1999)のエントリーの41\%がMWEであることを指摘した．
日本語でも$[\text{動詞}+\text{動詞}]$型の複合動詞が動詞の種類の44\%を占めることが(Uchiyama et al. 2003)で示されている．
この様に日常の自然言語には意外に多種類の複単語表現が使用されており，充実したMWEレキシコンを整備することが重要であることが認識されている．
本論文で述べるMWE辞書，JDMWEの基本見出し数は104,000表現であり，(Jackendoff 1997)の指摘した英語におけるMWEの分布も日本語における分布と大差ない事が推定される．

(Sag et al. 2002)は，さらに，英語のMWE全体を俯瞰し，語彙的に纏められる句(lexicalized phrase)を形態・構文的な柔軟性の度合いによって，固定表現(fixed expression), 半固定表現(semi-fixed expression), 構文的に柔軟な表現(syntactically flexible expression)に分け，慣習的に使われる句(institutionalized phrase)と合わせて，それぞれのNLPにおける取り扱い方を論じた．
具体的には複合名詞(compound nominal: CN), 固有名詞(proper name: PN), 動詞・不変化詞構文(verb-particle construction: VPC), 軽動詞構文(light verb construction: LVC), 分解可能イディオム(decomposable idiom), 分解不能イディオム(non-decomposable idiom)などの種類ごとに，単語的な扱い(words with spaces approach, holistic approach)と形態，構文，意味上の構成的な扱い(compositional approach)の是非について論じた．
(Sag et al. 2002)の指摘の本質は，MWE現象が広範に亘る事，MWEを単語として扱うだけでなく，多様な形態・構文的柔軟性に応じた取り扱いをしなければならないという事であり，その後のMWE研究に多くの示唆を与えた．
(Sag et al. 2002) の枠組みによる日本語MWEに関する考察には(Baldwin et al. 2003a)がある．
(Villavicencio 2004)は (Sag et al. 2002)の分類に基づき，英語イディオムと動詞・不変化詞構文を例に，従来の単語辞書をいくつかの表で拡張する形でMWEをデータ化する方法を論じた．
本論文のJDMWEも一般の単語辞書や構文解析機との併用を想定しており，内容的に(Villavicencio 2004)の要請の多くを満たしているが，対象とする表現がより広範である点，辞書としての独立性がより強い点，意味と細かな形態・構文的変化に関する情報は未記載であるが，各表現に対して内部修飾(internal modification)の可能性を記載している点，日本語特有の異表記に対応している点などに違いがある\footnote{形態・構文的変化形，例えば，活用，助詞の交替・挿入・脱落，受動態化や語順の入れ替えによる名詞化の可否等の情報記載については (安武 他 1997)で報告した．}．

NLP用のMWE辞書を作成したという報告には限られた形態の表現のみを対象とするものや採録表現数が比較的少ないものが多い．例えば，フランス語の22種の構文構造を持つ動詞型MWE，12,000個を辞書化した(Gross 1986)，13,000個の英語のイディオムを構文構造付きでデータベース化した(Kuiper et al. 2003)，ポルトガル語の10種の構文構造を持つ動詞型MWE，3,500個を辞書化した(Baptista et al. 2004), オランダ語の一般的MWE，5,000個に構文構造を与えて辞書化した(Gr\'egoire 2007)，フランス語の15種の構文構造を持つ副詞性MWE，6,800個を辞書化した(Laporte et al. 2008) などが見られる．
そのほか，英語とドイツ語のクランベリー表現をそれぞれ77個と444個収集した(Trawi\'nski et al. 2008)の報告がある\footnote{例えば, 「cranberry」の「cran」，「おだをあげる」の「おだ」の様な不明語（クランベリー語）を含む表現はクランベリー表現(cranberry expression)と呼ばれる．}．

日本語MWEのNLP向け辞書化に関する研究としては，古くは日本語の機能語性MWE, 2,500種を組み込んだ文節構造モデルを提唱した(首藤 他 1979; Shudo et al. 1980)や，約20,000個の概念語性MWE集を作成した(首藤 1989)がある．
また，機能語性MWEの異表記，派生表記を生成する階層的な手法を考案し，これによって16,771表現（341見出し）の辞書を編纂した(松吉 他 2007)の研究がある．
日本語イディオムに関しては，市販の数種の慣用句辞典から3,600個の慣用句を収集してNLPの立場から考察を加えた (佐藤2007)がある．
イディオム，準イディオムに対して形態的・構文的変化への制約や格要素等，修飾句への制約がどこまで意味の曖昧さ解消に利用できるかは今後の重要な課題である．
この点を考慮して辞書構築を試みる研究に(Hashimoto et al. 2006)があり，今後の成果が注目される．

本論文の日本語概念語性MWEを対象とするJDMWEは，収録表現の構文・意味機能が26種類にのぼり，上記の各辞書化研究に比べてより広範囲のMWEを対象としていること，特に，イディオム，決まり文句以外に準イディオム，準決まり文句と言える表現候補を多数収録していること，取り扱う構文構造の種類が多彩で，例えば動詞型MWEの場合，80種以上の依存パタンを持つ表現が収録されていること，異表記に対応していることなど，従来の研究に見られない特徴がある．

MWE候補をコーパスから自動抽出する研究が近年盛んであり，例えば，日本語，英語のコロケーション検出を統計的手法と一種のコスト評価で試みた(Kita et al. 1994)の研究，中国語複合名詞の抽出を統計的手法で試みた(Pantel et al. 2001)，既存の意味的タグ付けシステムを統計的手法で補強することによって英語のMWE候補抽出を試みた(Piao et al. 2005)，形態・構文的柔軟性の少なさを統計的に検出して英語の$[\text{動詞}+\text{名詞}]$型イディオム候補抽出を試みた(Fazly et al. 2006; Bannard 2007)の研究など数多い．
この種の研究では相互情報量(mutual information, pointwise mutual information)，$\chi^{2}$ (chi-squared)，対数尤度(log likelihood)，KL情報量(Kullback Leibler divergence)などが相関尺度(association measure)としてよく用いられるが，自動抽出における相関尺度とMWEとの適合性を比較検討した研究に(Pecina 2008; Hoang et al. 2009)がある．MWEとその要素語のコーパス中でのコンテクストの違いを検出してMWEを認定する研究に(Baldwin et al. 2003b)がある．また，最近は対訳コーパスを利用してMWE候補を抽出する試み，例えば，英語—ポルトガル語で行った(Caseli et al. 2009)，ドイツ語—英語で行った(Zarre{\ss} et al. 2009)などが見られる．
一定の概念が言語Aでは単語で表現され，言語Bでは複数単語の列で表現されるということはしばしば起こる．
このとき，言語Bの単語列はMWEである可能性がある．
この種の現象を対訳コーパスから検出しようというのがこれらの研究の基本的な考えである\footnote{本辞書でも英語への訳出を参考にして選定した表現が多数含まれている．}． 


コーパス中のMWEデータはスパースな場合が多く，統計的手法によるMWE捕捉では十分な再現率(recall)の達成が難しい．
また，基準となる表現集合も明確でないため，MWE自動抽出の再現率評価自体が難しいという問題がある．
人の利用を目的としたイディオム辞典類は古くから編纂されてきており，日本語に関しても慣用句を対象とした(白石（編）1977)，(宮地裕（編）1982)，(米川 他（編）2005)，故事ことわざ慣用句を対象とした（尾上（監修）1993; 田島 2002），四字熟語を対象とした(竹田 1990)，擬声語・擬態語慣用句を対象とした(白石（編）1992)等々，数多くの成果が出版されているが，これらには典型的表現しか収録されていない場合や，表現の機能，内部構造，異表記，変化形，用法に関する体系的な記述が見られない場合が多く，そのままではNLPにおける基準集合とはなりにくい．これらの問題点を緩和するのにJDMWEが役立つことが期待される．

NLPにおける言語資源の評価は，応用システムの性能向上にどれだけ貢献したかで行うのが現実的であるが，MWEを対象としてそこまで行った研究はまだ多くないようである．この種の研究には，日本語MWEの主に文字面の情報を使って市販日本語ワープロの仮名漢字変換初回正解率を向上させた(Koyama et al. 1998)の研究，日本語の機能語的MWEを検出して用いれば，より正しい係り受け解析が実現出来ることを示した(注連 他 2007)の研究などがある．その他の日本語MWE処理に関する近年の研究には，複合動詞の多義選択法を考察した(Uchiyama et al. 2003)，複合名詞の機械翻訳方式を考察した(Tanaka et al. 2003)などがある．



************************ [./logs/V18N01-01/related_study] ************************
関連研究

学習者のコロケーション習得において，誤用の傾向や特徴を分析した研究はある
が\cite{曹,滝沢,小森}，誤用と正用の関係を扱った研究は少ない．
\cite{James}は，コロケーションの違反を3種に区別しているが，誤用と正
用の関係の解明には至っていない．また，誤用と正用の間に類義関係が成り立つ
ことを指摘している研究はあるが\cite{鈴木}，類義関係とは言えない誤用と正
用の関係を形式的に扱った研究はない．

作文支援としてコロケーションを扱った研究では，誤りの検出という立場と作文
に必要な語彙知識を提供するという2つの立場から研究が行われている．前者は，
計算機による誤用検出・訂正システムの構築の観点から行われた誤用分析におい
て，規則性がある誤用の検出は容易であるが，コロケーションの誤りを含め，語
彙選択の誤りの検出は難しいと考えられている\cite{白井}．語彙選択の誤りは，
数が非常に多い内容語に関わるため，機能語の誤り以上に捉えがたい．後者は，
Data-Driven-Learningという考え方に基づき，学習者が自分の判断で適切な語を
選択できるように，大規模なコーパスからキーワードに対して共起する語や例文
を提示する作文支援システム\cite{Nishina}や，結び付きの強い単語，あるいは，
類語の共通点や差異を示すツール\cite{Kilgariff}が開発されている．共起す
る語の例文を調べる，あるいは，共起する語の特徴や類似する語の使い方の違い
を知るためには，これらのシステムは有用である．しかし，共起の結び付きが強
い語の中に自分が表現したい単語があるとは限らない．自分が表現したい事柄に
適した日本語の表現を思いつかない場合，それを自分で探すのは難しい．

これらの問題の解決策として，本研究では誤用と正用の関係から代替共起表現を
提示する手法を提案する．本研究は，単語の出現環境の類似関係として誤用と正
用の関係を捉え，これを定量化した点でこれまでの言語教育における誤用分析
と異なる．また，代替候補を提示する点で，誤用検出システムや検索ツール
とは異なるものである．




************************ [./logs/V18N02-02/related_study] ************************
関連研究
\label{sec:related-work}

類似文字列検索は，データベースやデータマイニングの分野で，盛んに研究が行われている．
その中で最も多い研究は，文字列の編集距離を距離尺度として用いるものである．
Gravanoら~\cite{Gravano:01}は，$n$-gram\footnote{データベースの分野では$q$-gramと呼ばれることが多い．}で文字列のインデックスを作り，オーバーラップの個数，位置，文字列のサイズなどで編集距離の制約を満たす解を絞り込む方法を提案した．
Kimら~\cite{Kim:05}は，$n$-gramが出現した場所をインデックスに効率よく格納するため，2 階層の$n$-gramインデックスを提案した．
Liら~\cite{Li:07}は，クエリの処理速度を向上させるため，可変長の$n$を用いた$n$-gramインデックスを用いた．
Leeら~\cite{Lee:07}は，ワイルドカードを含む$n$-gramでインデックスを作り，編集距離制約の類似文字列を効率よく検索する手法を考案した．
Xiaoら~\cite{Xiao:08}は，検索クエリとマッチングできなかった$n$-gramを活用する，Ed-Joinアルゴリズムを提案した．

文字列を$n$-gramなどで表現することなく，編集距離に基づく類似文字列検索を実現する方式も，いくつか提案されている．
Bocekら~\cite{Bocek:07}は，データベースに文字列を格納するときに，元の文字列に近い複数の隣接文字列を格納するアプローチ（隣接文字列生成）として，Fast Similarity Search (FastSS) を提案した．
Wangら~\cite{Wang:09}は，隣接文字列生成手法を改善するため，文字列を分割したり，接頭辞で枝刈りを行う方法を紹介した．
Huynhら~\cite{Huynh:06}は，圧縮された接尾辞配列上で類似文字列検索を行うアルゴリズムを提案した．
Liuら~\cite{Liu:08}は，文字列をトライに格納し，類似文字列検索を行う枠組みを提案した．
これまでに紹介した研究は，編集距離を類似度関数として採用した場合に特化している．

Chaudhuriら~\cite{Chaudhuri:06}は，編集距離とジャッカード係数に対する類似文字列検索に向けて，SSJoin演算を提案した．
このアルゴリズムは，検索クエリ文字列からシグニチャを作成し，シグニチャの特徴を含む全ての文字列を解候補として検索し，編集距離やジャッカード係数の制約を満たす文字列を選び出すものである．
Chaudhuriらは，関係データベースの等結合 (equi-join) を用いてSSJoin演算を実装する方法を示した．
本論文では，SSJoin演算を関係データベース上で実装していないが，これは第\ref{sec:evaluation}節のSignatureシステムと同等である．

Sarawagiら~\cite{Sarawagi:04}は，$\tau$オーバーラップ問題を解くアルゴリズムとして，MergeOptを提案した．
このアルゴリズムは，転置リストを$S$と$L$という 2 つのグループに分け，$S$で解の候補生成を行い，$L$で解の検証を行う．
提案手法と異なる点は，$S$で解の候補生成を行うときにヒープを用いる点，$L$で解の検証を行うときに，枝刈りを行わない点である．
Liら~\cite{Li:08}は，Sarawagiらの手法を改良し，SkipMergeとDivideSkipというアルゴリズムを提案した．
SkipMergeアルゴリズムは，全ての転置リストの先頭から順にSIDをヒープに挿入し，ヒープの先頭から同じSIDの要素を取り出したとき，取り出された個数が$\tau$を超えたら，そのSIDを解とするものである．
ただし，ヒープに転置リストからSIDを挿入するときに，$\tau$オーバーラップ問題の解となり得ない要素をスキップするメカニズムが組み込まれており，転置リスト中の全てのSIDをヒープに挿入しなくても，$\tau$オーバーラップ問題が解けるように工夫されている．
DivideSkipアルゴリズムは，MergeOptアルゴリズムと同様，転置リストを$S$と$L$という 2 つのグループに分け，SkipMergeアルゴリズムを$S$に適用して解の候補生成を行い，$L$で解の検証を行うものである．
しかしながら，DivideSkipアルゴリズムでは解の枝刈り方法については，述べられていない．
これらの手法と提案手法を解析的に比較するのは難しいが，第\ref{sec:evaluation}節では，SkipMergeとDivideSkipアルゴリズムによる類似文字列検索の性能を測定し，提案手法の方が高速に検索できることを実験的に示した．

続いて，提案手法とMergeOpt，SkipMerge，DivideSkipを空間計算量に関して比較する．
ここに挙げた全てのアルゴリズムは，転置リスト上で二分探索を行うため，特殊な工夫をしない限り，転置リストの内容を主記憶に読み込む必要がある．
最悪の場合を考えると，どのアルゴリズムも与えられたクエリ文字列に対して，最もサイズの大きい転置リストを主記憶に読み込む必要が生じる\footnote{細かい議論になるが，SkipMergeとDivideSkipでは複数の転置リスト上で並行して二分探索を行うため，特殊な工夫を施さない限り，複数の転置リストを同時に主記憶に読み込む必要が生じる．}．
これに加え，各アルゴリズムとも解文字列の候補を主記憶上に保持しておく必要がある．
MergeOpt，SkipMerge，DivideSkipアルゴリズムは，解候補をヒープに格納するアルゴリズムであり，ヒープに格納される解候補の数は，クエリに対する転置リストの数（すなわち，クエリの特徴集合$X$の要素数$|X|$）を超えない．
これに対し，提案手法は，いったん解候補の列挙を行うため，おおよそ$\mbox{（転置リストの平均要素数）} \times (|X| - \tau + 1)$程度の解候補を主記憶に保持することになる．
したがって，提案手法はMergeOpt，SkipMerge，DivideSkipアルゴリズムよりも空間計算量が大きくなる．
表\ref{tbl:stats}には，提案手法の各データセットにおける解候補数（\# 候補）が示されている．
これによると，途中で保持した解候補数は数百〜数千程度のオーダーであり，提案手法の空間計算量は実用上は問題にならないと考えられる．

最後に，類似文字列検索に近いタスクとして，類似文字列照合との関係を説明する．
このタスクでは，与えられた 2 つの文字列の表記が近いかどうかを精密に検出するため，文字列の類似度関数を改良したり~\cite{Winkler:99,Cohen:03}，機械学習で獲得するアプローチ~\cite{Bergsma:07,Davis:07,Tsuruoka:07,Aramaki:08}が取られる．
これらの研究成果を用いると，2 つの文字列の類似性を高精度に判別できるが，判別する 2 つの文字列があらかじめ与えられることを前提としている．
このような精細な類似度で類似文字列検索を行いたい場合は，適当な類似度関数に対して緩い閾値$\alpha$を用い，提案手法で類似文字列の候補を獲得してから，類似文字列照合を適用すればよい．




************************ [./logs/V18N02-04/related_study] ************************
関連研究

自動単語分割の問題をある文字の次に単語境界があるかの予測として定式化することは
かなり
以前から行われている\cite{日本語における単語の造語モデルとその評価,品詞・区切り情報を含む拡張文字の連鎖確率を用いた日本語形態素解析,文字クラスモデルによる日本語単語分割}．これらの研究では，文字に単語境界情報を付与して
予測単位としている．文字間に単語境界があるかを識別学習により決定することも提案されて
いる\cite{日本語単語分割を題材としたサポートベクタマシンの能動学習の実験的研究}．この
研究の主眼は能動学習の調査・分析である．辞書の利用に関する記述はなく，また分野適応に
ついても述べられていない．

統計的手法による日本語の文の自動単語分割に関する初期の研究は，丸山ら
\cite{確率的形態素解析}による単語$n$-gramモデルを用いる方法がある．また，永田
\cite{統計的言語モデルとN-best探索を用いた日本語形態素解析法}による品詞$n$-gramモデ
ルによる形態素解析\footnote{ここでは，自動単語分割に加えてそれぞれの単語の品詞を同時
に推定する処理を形態素解析と呼んでいる．}もある．森ら
\cite{形態素クラスタリングによる形態素解析精度の向上}は，すべての品詞を語彙化した形態
素$n$-gramモデルを用いることによる精度向上を報告し，さらに単語辞書の参照を可能にする
方法を提案し，それによる精度向上を報告している．内元ら
\cite{最大エントロピーモデルに基づく形態素解析.--未知語の問題の解決策--}は，最大エン
トロピーモデルを用いる形態素解析において単語辞書を参照する方法を提案している．このよ
うに，単語分割基準に沿った単語辞書を参照する方法はすでにあるが，複合語や単語列を参照
し精度向上を実現した自動単語分割器の報告は，我々の知る限りない．

なお，提案手法は，形態素解析にも拡張可能である．提案する自動単語分割器は音声認識や仮
名漢字変換の言語モデル作成に用いることを想定しているので，品詞を推定する必要はないと
考えている．品詞も推定すべきか，どの程度の粒度の品詞体系にすべきか，などの問題は後続
する自然言語処理と準備すべきデータの作成コストを含む全体の問題であり，本論文の議論を
超える．

複合語や単語列を参照し精度向上を図る取り組みは，完全に単語に分割された文に加えて，そ
れ以外の断片的な情報を用いて精度向上を図る取り組みの一つである．坪井ら
\cite{日本語単語分割の分野適応のための部分的アノテーションを用いた条件付き確率場の学習}
は，学習コーパスの文の単語境界情報が部分的であるような不完全なアノテーションからも条
件付き確率場による自動単語分割器や形態素解析器を学習できる枠組みを提案している．本論
文で提案する自動単語分割器は点予測を用いているので，単語分割に関してはこの問題は解決
されているといえる．したがって，提案する自動単語分割器は，単語境界情報が部分的に付与
されたコーパスと複合語や単語列のすべてを同時に参照することができる．

自動単語分割は，中国語においても提案されている
\cite{A.Stochastic.Finite-State.Word-Segmentation.Algorithm.for.Chinese}．自動単語分
割器は，空白で区切られる単位が大きい韓国語やフィンランド語などにおいても有用で，言語
モデルの作成
\cite{Korean.large.vocabulary.continuous.speech.recognition.with.morpheme-based.recognition.units,Unlimited.vocabulary.speech.recognition.with.morph.language.models.applied.to.Finnish}
に用いられている．提案手法は，これらの言語に対する言語処理においても有用である．



************************ [./logs/V18N03-02/related_study] ************************
関連研究
\label{sec:work}

本節ではまず\ref{sec:intro}節でブートストラッピング手法として挙げた
Self-trainingおよびCo-trainingを用いた語義曖昧性解消の
先行研究を概観する．
また，アンサンブル学習に基づく語義曖昧性解消には
教師あり学習のアンサンブル，教師なし学習のアンサンブル，
半教師あり学習のアンサンブルに基づいた手法が提案されており，
これら先行研究を併せて概説する．


\subsection{ブートストラッピングに基づく研究}
\label{sec:work1}

Self-trainingに基づいた語義曖昧性解消の先駆けとしては
Yarowskyの研究\cite{Yarowsky95}が挙げられる．
Yarowskyは「語義はその語の連語より定まる(one sense per collocation)」
「語義はその語を含む談話より定まる(one sense per discourse)」という
二つのヒューリスティックに基づき，
ラベルなしデータに反復的にラベル付けするアルゴリズムを提案した．
この手法は二つの観点からラベル付けをするため，
Co-trainingの一種であると見ることもできる．
また，このヒューリスティックに基づいたYarowskyのアルゴリズムは
Abney \cite{Abney04}により，目的関数の最適化問題として定式化されている．

Co-trainingを用いた語義曖昧性解消の早期の例としては
新納の報告\cite{Shinnou01a}がある．
新納はCo-trainingを適用するにあたり，二組の素性集合の独立性を高めるため，
ラベル付きデータに追加するラベルなしデータを
素性間の共起性に基づいて選択する手法を提案した．
結果，日本語の語義曖昧性解消において通常のCo-trainingよりも
性能が向上したと報告した．

MihalceaはCo-trainingとSelf-trainingの両方を語義曖昧性解消に適用し，
\ref{sec:intro}節にて述べたパラメータの影響について調査した\cite{Mihalcea04a}．
この報告ではパラメータの自動での最適化はできず，
最適な設定と自動による設定に大きな差があったと報告している．
また，Mihalceaは同じ報告の中でスムージングしたCo-trainingおよび
Self-trainingを提案した．
これは手順の反復のたびに生成される各分類器の多数決より語義判定し
ブートストラッピングするという方式であり，
ブートストラッピングとアンサンブル学習の組合せの一種と見ることができる．
この方式は通常のブートストラッピングよりも性能が向上したと報告された．

以上の手法は\ref{sec:intro}節で述べたようなパラメータを
タスク（データセット）に合わせ調整しなければならないという大きな課題がある．

NiuらはZhuらの提案したラベル伝播手法\cite{Zhu02}に基づいた
半教師あり手法による語義曖昧性解消について調査した\cite{Niu05}．
ラベル伝播は事例を節点とする連結グラフを考え，重み付きの辺を通して
ラベルありの事例からラベルなしの事例へとラベル情報を反復的に伝播させる．
そして伝播の収束結果よりラベルを推定する．
この手法はSenseval-3 \cite{Mihalcea04b} English lexical sampleタスクの
データセットに適用した結果，従来の教師あり学習と比較して著しい成果は
得られなかったとしている．

PhamらはCo-trainingとSmoothed Co-training \cite{Mihalcea04a}に加え，
Spectral Graph Transduction (SGT) \cite{Joachims03}およびSGTとCo-trainingを
組合せたCo-trainingの語義曖昧性解消への適用を調査した\cite{Pham05}．
Transductionとは訓練データから分類器を生成せず，
直接テストデータにラベル付けする推論方法である\cite{Vapnik98}．
SGTは$k$近傍法のTransductive版であるとされる．
SGTは$k$近傍法の応用であるため，$k$がパラメータとなり，
かつ$k$は性能に与える影響が大きいと報告されている．
よってPhamらの調査した手法全てにはパラメータ設定の問題が
存在していることになる．

手法のアンサンブルを含まないブートストラッピングによる語義曖昧性解消の
研究の最後として小町らの報告\cite{Komachi10}を挙げる．
小町らはブートストラッピング手法の一つであるEspresso \cite{Pantel06}に対し
グラフ理論に基づいて意味ドリフト\cite{Curran07}の解析を行った．
意味ドリフトは，語義曖昧性解消の観点から考えると，
どのような語義の語も持つ素性をジェネリックパターンと考え，
ジェネリックパターンを持つ（信頼性が低いとされるべき）ラベルなしデータに対し
手順の反復過程においてラベルが与えられることにより，
反復終了後に生成される分類器の性能が低下してしまう現象と解釈できる．
この問題への対処のため小町らは二つのリンク解析的関連度算出法を適用した．
この手法は意味ドリフトに頑健かつパラメータ数が一つで
さらにその調整が比較的容易という利点を持つ．
Senseval-3 English lexical sampleタスクのデータセットに手法を適用した
実験の結果，小町らの手法は類似したグラフ理論的手法である
HyperLex \cite{Veronis04}やPageRank \cite{Agirre06}と比較して
高い性能が得られたと報告している．


\subsection{アンサンブル学習に基づく研究}
\label{sec:work2}

教師あり学習のアンサンブルに基づく研究としては，
AdaBoostを用いた\cite{Escudero00}，素性として用いる文脈の大きさを変えて
複数のNaive Bayes分類器をアンサンブルした\cite{Pedersen00}，
六種の分類器の組合せによる\cite{Florian02}，
二段階の分類器の出力の選択に基づいた\cite{Klein02}，
複数のNaive Bayes分類器の出力の比較に基づいた\cite{Wang04}が挙げられる．
ここではWangらの手法をより詳しくみる．
WangらはまずPedersonと同様に素性として用いる文脈の大きさ，
つまり目標の多義語前後$k$語以内の語を素性として用いるとして，
$k$を変えることで複数のNaive Bayes分類器を作成する．
次にラベル付きデータを各分類器にて分類する．
各要素がこの各分類器による分類結果であるベクトルをdecision trajectoryと呼ぶ．
最後に各ラベル付きデータから得たdecision trajectoryの集合を訓練データとし，
これらと入力から得たdecision trajectoryの類似度に基づいて入力の語義を判定する．
Wangらの手法は中国語の語義曖昧性解消実験の結果，
Pedersonの手法などと比較して最も良い結果が得られたと報告した．

教師なし手法のアンサンブルの例としてはBrodyらの研究\cite{Brody06}が挙げられる．
Brodyらは過去に語義曖昧性解消において有効と報告された教師なし手法である
Extended Gloss Overlap \cite{Banerjee03}，
Distributional and WordNet Similarity \cite{McCarthy04}，
Lexical Chains \cite{Galley03}および
Structural Semantic Interconnections \cite{Navigli05}を組合せた手法を提案した．
この手法は組合せに用いた各手法と比較し，
より良い結果が得られたと報告されている．

最後に本稿で提案する手法に最も関連の深いブートストラッピング手法の
アンサンブルを行ったLeらの研究\cite{Le08}について述べる．
Leらは我々と同様に従来のブートストラッピングによる語義曖昧性解消の問題点に
対する解決法を提案した．
Leらが解決法を提案した問題点は，(1)ラベル付きデータのラベル毎のデータ数の偏り，
(2)ラベル付きデータに追加するラベルなしデータ決定の基準，
(3)手順の反復の停止条件および最終分類器の作成法の三つである．
ここで問題(2)は\ref{sec:intro}節にて述べたパラメータ$G$の決定法，
問題(3)はパラメータ$R$の決定法とも換言できる．
Leらはこれらの解決のため，追加するラベルなしデータのリサイズ，
複数のデータ追加条件の閾値の設定および対応する複数の追加データ集合の設定，
訓練データを用いた追加データの評価および手順反復停止条件の設定，
そして追加データと教師あり学習手法別の各分類器のいくつかの組合せ法を提案した．
ここで追加データの評価と手順反復停止条件の設定の手法は，
Zhouらが提案したTri-training法\cite{Zhou05}で用いられた
手法を参考に設定している．
Tri-trainingはCo-trainingを発展させた手法であり，
Co-trainingと異なりパラメータ設定を不要とする特徴がある．
実験はSenseval-2 \cite{Edmonds01}およびSenseval-3のEnglish lexical sample
タスクのデータセットを用いて行われ，従来の教師あり手法と比較し
最大で1.51ポイントの精度向上が見られたとLeらは報告した．



\subsection{本研究の位置付け}

\ref{sec:work1}節と\ref{sec:work2}節を踏まえた上での
本研究の位置付けは以下の通りである．
まず，小町らの手法はパラメータ設定が容易という利点があるが，
他の教師あり手法と組合せるのが困難なのが問題点である．
高性能な教師あり手法を用いず，さらに性能を向上させるのは難しい．
また，Leらの手法の難点として手順の反復停止条件の設定が挙げられる．
これは，教師あり学習を用い追加データを訓練データとして分類器を作成し
オリジナルのラベル付きデータを分類して得られるエラー率，
および追加データの総数に基づき設定される．
具体的には次の式を用い追加データの評価値$q$を求める．
\begin{equation}
q=m(1-2\eta)^2
\label{eq0}
\end{equation}
ここで，$m$は追加データの総数，$\eta$はエラー率を示す．
この$q$が前回の値よりも小さければ反復は停止する．
しかし反復の停止にこの条件が用いられる具体的根拠は示されていない．
単に反復を自動的に停止するためと述べられているだけである．
このためこの停止条件が最適であるかどうか疑問が残る．

そこで本研究の立場だが，まず本稿ではこの停止条件の追究はしない方針とする．
しかし，\ref{sec:intro}節にて目標としたように
ブートストラッピングにおけるパラメータの削減は達成を目指す．
そこで本研究では手順の反復回数（パラメータ$R$）を一回に留めるという方針を採る．
この方針には次の利点が考えられる．

\begin{itemize}
\item 手順の回数が固定され計算時間の予測が立てやすい．
\item ラベル付きデータへのラベルなしデータの追加が一度のみとなるため，
追加されたデータに対し分析，考察を加えやすい．
\item 反復1回目の精度を向上させることで，
手法を複数反復できるように拡張したとき更なる精度向上が見込める．
\end{itemize}

以上の検討に基づき，本研究では反復を伴わず，かつ教師あり学習手法を
併用した高性能なブートストラッピング的手法を確立する．
また反復回数を一回にすることは，
反復回数以外のパラメータを削減することにもつながる．
詳しくは\ref{sec:method}節にて述べる




************************ [./logs/V19N01-01/related_study] ************************
関連研究 \label{sec:related-word}

大量文書からの上位下位関係の獲得手法はこれまでに数多く提案されてきた．
これらは
言語表現パターンを用いるもの
\cite{hearst92,ando04}，
クラスタリングに基づくもの
\cite{pantel04,etzioni05}，
HTML文書の構造を利用するもの
\cite{shinzato04}，
Wikipediaの構造を利用するもの
\cite{隅田:吉永:鳥澤:2009,oh09,Yamada:EMNLP:2009}
に大きく分類することができる．

上位下位関係を構成する概念の詳細さの問題に取り組んだ研究は我々の
知る限りHovyらの研究 \cite{hovy09}のみである．
Hovyらは，Doubly-Anchored Patternと呼ばれる語彙統語パ
ターンを用いたbootstrap手法によって，``people / Shakespeare'' といった
上位下位関係に中間語writersを挿入する手法を提案した．
しかし彼らの手法では，あらかじめ決めた ``animals'' と ``people'' という2種類のルートコンセプトのみ
を対象としている．
一方，本提案手法では，処理対象に制限はなく，あらゆる上位概念を扱うことができる．


本提案手法ではWikipediaを知識獲得源として利用しているが，
Wikipediaからの知識獲得研究は近年活発化している
\cite{kazama07,ponzetto07,suchanek07,nastase08,
隅田:吉永:鳥澤:2009,oh09,Yamada:EMNLP:2009}．
Wikipediaからの知識獲得という文脈における本研究の新規性は，
Wikipediaの百科事典としての性質を利用することで，
上位下位関係としてだけではなく，\attval{対象}{属性}{属性値}関係としても
解釈可能な知識を獲得する手法を開発した点にある．
一般的に，\attval{対象}{属性}{属性値}関係における属性と属性値のペアは，上位下位関係と解釈できないものも多数存在する．
提案手法により獲得できる\attval{対象}{属性}{属性値}関係は，その属性と属性値が上位下位関係を持つものに限定しているが，
\attval{対象}{属性}{属性値}関係を大量かつ高精度に獲得している．



************************ [./logs/V19N02-01/related_study] ************************
関連研究 \label{sec:prev}

\subsection{複合語分割}  \label{sec:prev_comp}

これまでにも，ラベルなしテキストを用いた複合語分割手法はいくつか提案され
ている．それらはいずれも，複合語の構成語の頻度をラベルなしテキストから推
定し，その頻度情報に基づいて分割候補を順位付けするものとなっている
\cite{Koehn03,Ando03,Schiller05,Nakazawa05,Holz08}．とりわけ本研究と関連
が深いのは\cite{Nakazawa05}であり，彼らもまた片仮名複合名詞を対象として
いる．しかし，こうした単語頻度に基づく手法は，対訳資源を用いた手法と比較
して，十分な分割精度が得られないという問題が指摘されている
\cite{Koehn03,Nakazawa05}．実際，我々の実験においても，これら単語頻度に
基づく手法と提案手法との比較を行ったが，提案手法の方が大幅に高い分割精度
を実現可能であることを確認した．

一方，Alfonsecaら\citeyear{AlfonsecaCICLing08}は，ラベルなしテキストでは
なくクエリログを複合語分割に利用することを提案している
\footnote{彼らはウェブテキストのアンカーテキストを用いることも提案してい
るが，精度の向上は実現されていない．}．しかし彼らの実験報告によると，ク
エリログを用いなかった場合の精度が90.45\%であるのに対して，クエリログを
用いた場合の精度は90.55\%であり，その改善幅は極めて小さい．一方，本研究
の実験（\ref{sec:exp}節）では，提案手法の導入によって精度は83.4\%から
87.6\%へと大きく向上し，なおかつ，その差は統計的に有意であることが確認さ
れた．また，クエリログは一部の組織以外では入手が困難であるのに対し，提案
手法に必要なラベルなしテキストは容易に入手することが可能である．

HolzとBiemann \citeyear{Holz08}は独語の複合語に対する分割手法と言い換え
手法を提案しており，本研究との関連性が高い．しかし，彼らが提案しているア
ルゴリズムは，複合語の分割と言い換えをパイプライン的に行うものであるため，
提案手法とは異なり，言い換えに関する情報は分割時に用いられない．


\subsection{その他の関連研究}

片仮名複合名詞の分割処理は単語分割の部分問題であると考えることができる．
そのため，既存の単語分割器を用いて片仮名複合名詞の分割処理を行うことも可
能であるが，実際問題として，それでは十分な分割精度を得ることは難しい
（\ref{sec:exp}節の実験結果を参照）．この原因として，既存の単語分割器は辞
書に強く依存した設計となっており，未知語が多い片仮名語の解析に失敗しやす
いことが挙げられる．これに関する議論は\cite{Nakazawa05}が詳しい．単語分
割の視点から見た本研究は，片仮名複合名詞という特に解析が困難な言語表現に
焦点をあてた試みであると言える．

\ref{sec:trans}節において我々は，片仮名複合名詞の分割のために逆翻字を利
用する手法を提案する．提案手法は，技術的な観点から見ると，ウェブから片仮
名語の逆翻字を自動抽出する既存手法\cite{Brill01,Cao07,Oh08,Wu09}と関連が
深い．しかしながら，そうした関連研究は翻字辞書や翻字生成システムを構築す
ることを目的としており，自動抽出した逆翻字を複合語の分割処理に利用する試
みは本研究が初めてである．




************************ [./logs/V19N02-02/related_study] ************************
関連研究

先行研究においては，文書/文レベルの全ての単語を素性とした分布類似度を用いたアプローチ (distributional approach) が提案されている \cite{pantel2009web}．
これらの手法は大域的情報を用いた手法とみなすことができるが，単語の素性空間は非常に多次元かつ疎な空間であり，データ量が増えた場合においてもこの問題を完全に解消することはできない．
我々の手法はトピック情報という中間的な単位に落とし込むことでこれらの問題を解消する．


我々が用いたトピックモデルは一種の確率的クラスタリングモデルであるので，
エンティティ獲得にクラスタリング情報を用いた先行研究として
Pa\c{s}caらの研究を挙げて比較する \cite{pasca2008weakly}．
Pa\c{s}caらはエンティティの獲得だけでなく，
周辺文脈をクラスタリングし，その中からクラスを代表するにふさわしい
単語を選択してクラス名として定義する．
さらに検索クエリログを用いて，当該クラス内のエンティティと共に用いられるクエリを当該クラスの属性であるとする，「クラス，エンティティ，属性」の3つ組を取得する手法を提案している．
Pa\c{s}caらの手法ではクラスタリングを用いているものの，クラスタリング対象範囲は周辺の文脈にとどまる．
これに対し我々の手法は文書全体からトピックを推定する点で，より広域な情報を取り入れることができる．
また提案手法は語彙獲得の目的に特化させるため，
Pa\c{s}caらで用いられていたクラス名を，
エンティティの候補が対象クラスに属するか否かを判定するための属性の1つ（``is-a''の属性）として扱う．
属性をクラス名のみに絞った方が適合率は高くなると考えられるが，
局所的な文脈中にはクラス名が存在しない場合も多い．
例えば書籍の場合，書籍タイトルの前後に「本」や「書籍」といったクラスを表わす単語が共起することは少ない．
このため，他の属性と併用することで，より高精度かつ網羅的なエンティティ獲得が可能となる．

トピックモデルを用いた関連研究として，selectional preferences をモデル化するために，LDAを拡張した生成モデルを利用したRitterらの研究が挙げられる \cite{ritter2010}．
Ritter らの手法は我々の手法に最も近いものと言えるが，生成モデルであるか識別モデルであるかの違いがあり，局所的文脈素性と柔軟に統合できるという点で我々のモデルには優位性がある．


\ref{sec:negative}節で述べた負例生成は，正例とラベルなしデータのみが存在する場合においての主要な問題と捉えられている \cite{liu2002partially,li2010negative}．
しかし先行研究における手法はある程度の規模の正例データを想定しており，
非常に少量な正例データについては有効に機能しないと考えられる．
これに対し，本稿では少量の正例データからでも適切に負例を生成可能な手法を提案した．
一方McIntosh は，複数クラスの語彙獲得タスクにおいて獲得されたエンティティが，シードエンティティよりもそれ以降のイテレーションに得られたエンティティ集合に近い場合に負例であると自動的に判定し，さらに負例のクラスタリングと拡張を行うことで，適切な負例集合を得る手法を提案している \cite{mcintosh2010unsupervised}．
またKiso らは単語の共起関係をグラフ上で表現し，
HITSスコアの高い単語が正例に該当しない場合はそれらをストップリストとして用いることで，セマンティックドリフトを抑える手法を提案している \cite{kiso2011hits}．
McIntoshやKisoらの手法が，セマンティックドリフトを生じやすい単語を直接的に負例として捉えることを主眼としているのに対し，我々はセマンティックドリフトが生じる先のトピックに制約を設ける目的で負例を捉えるという点で異なる．
特にMcIntoshの手法では，セマンティックドリフトを抑える効果の高い負例を抽出できる可能性が高い反面，本来の正例が負例になってしまう偽負例を生じる可能性がある．
本稿ではセマンティックドリフトを生じやすい単語，言いかえると正例・負例両方の多義性が存在する単語の場合，\ref{sec:tpc_sel}節のトピック情報による多義性解消を併用することで，負例として当該単語が用いられている事例では，正例としても負例としても用いないという判断を行っている．
一方正例として当該単語が用いられている事例では，正例学習データとして用いることで，学習データを可能な限り増やしていくというブートストラップ法の観点に見合った手法となっている．


本稿ではリソースとして文書集合を用いたが，一方でクエリログを用いたエンティティ獲得の研究も進められている．
小町らはクエリログ中に共起する単語をエンティティ及び属性とみなし，ブートストラップ法に基づくエンティティ獲得法の提案を行っている \cite{小町08}．
クエリログを使った他の手法としては，他にもSekineらの研究 \cite{sekine2007acquiring} やPa\c{s}caらの研究 \cite{pasca2008weakly} が挙げられる．
しかし，クエリログ単独ではトピックのような大域的な文脈を考慮することができず，また，非公開で一般的に入手が困難なリソースであるという現実的な側面もある．
我々はこれらの観点から文書をリソースとして用いることとした．




************************ [./logs/V19N03-02/related_study] ************************
関連研究 \label{Sec:関連研究}

領域適応は，学習に使用する情報により，supervised，semi-supervised，unsupervisedの
三種に分けられる．まずsupervisedの領域適応は，訓練事例として
少量のターゲットドメインだけでなく大量のソースドメインのデータを加えて
学習を行うもので，
訓練事例としてソースデータまたは少量のターゲットデータだけを利用する場合よりも，分類器を改良することを目指す．
次のsemi-supervisedの領域適応は，ラベルつきのソースデータに加え，
ラベルなしのターゲットデータを利用し，
訓練事例としてソースデータだけを利用する場合よりも，分類器を改良することを目指す．
また，最後のunsupervisedの領域適応は，ラベルつきのソースデータで学習後，ターゲットデータで実行する．
本研究で扱うのは，supervisedの領域適応である．

領域適応の研究は様々な分野で研究が行われており，
ここではその一部を紹介する．まず，\cite{article2}は，EMアルゴリズムによる
語義の事前確率推定によりWSDの領域適応を行っている．
\cite{article3}も，EMアルゴリズムによる事前確率推定を行っているが，
これは能動学習により事例をターゲットドメインから加えるsupervisedの領域適応である．
Count-mergingにより重要文に重みをつけることで，性能を向上させている．

また，
\cite{article4}はシーケンスラベリングを例にsupervisedの領域適応を行っている．
素性空間の次元を「ソースデータの素性空間」「ターゲットデータの素性空間」
「ソースデータとターゲットデータ共通の素性空間」に相当する三倍にし，モデルを三倍に拡張して実験を行うというもので，
様々なsupervisedの領域適応に併用できる手法である．利点として，上記の併用可能性に加え，
実装が簡単で処理が速いこと，マルチドメインに拡張が簡単（素性空間の次元をドメイン数+1倍にすればよい）
であることが挙げられる．

さらに，\cite{article12}は\cite{article4}をsemi-supervisedのために拡張した．
この手法がなぜ有効なのかはまだ解き明かされていないが，拡張前の利点を引き継いでいるだけでなく，
ラベルなしのターゲットデータを利用することでよりよい性能が得られる．

\cite{article5}は，semi-supervisedのWSDの領域適応を行った．大量のラベルなしのソースデータに，
ラベルなしのターゲットデータを加えて行列を作り，特異値分解 (SVD) により素性圧縮をして分類器を学習する手法である．
また\cite{article6}は，大量のラベルなしのソースデータの代わりに，少量のラベルつきのソースデータを使用して，
同様の手法でsupervisedの領域適応を行っている．

\cite{article7}は領域適応を行う際，事例の重み付けにより性能が向上することを示した．
この手法は様々なsupervisedまたはsemi-supervisedの領域適応との併用が可能である．
また，領域適応に悪影響を及ぼすソースデータを特定して削除することも試みているが，
ソースデータの削除は事例の重み付けを
行わなければ有効であるが，事例の重み付けを行った場合には有効ではないと結論づけている．

\cite{article14}はターゲットデータとソースデータの周辺確率を似せるようにカーネル空間を学習した後，条件確率が
ターゲットデータに似ているソースデータの事例をクラスタリングベースの事例選択を用いて選び，その事例を利用して
領域適応を行っている．


\cite{article15}はWeb上からランダムに取得したラベルなしデータを利用して，より高いレベルの素性を作成するために
スパースコーディングを利用したself-taught learningを提案している．これはunsupervisedの領域適応の一種である．

\cite{article16}はco-trainingにおいて領域適応を行ったco-adaptationの研究である．boostingによる線形補完により
領域適応を行い，両方の分類器においてエラー率が低下したことを報告している．

また\cite{article17}はsemi-supervisedの領域適応である．この研究では，ソースデータ中とターゲットデータ中の単語の類似度を
計算するために，pivot feature（ソースデータとターゲットデータの
両方でよく出てくる単語）の周りの単語の重みを計算する．この重みの行列にSVDを適用して新しい素性空間を作り，
オリジナルの素性に新しい素性を加えて使用するという手法をとっている．

本稿に最も近い研究は，\cite{article20}である．この研究では，多様なドメインからなる文書を構文解析する際，最も良いモデルは
異なるという問題に注目している．彼らは様々な混合モデルによる構文解析の正解率を回帰分析で予測し，それぞれのターゲットデータに対して，
最も高い正解率を出すと予測されたモデルを利用して構文解析を行っている．本研究との最も大きな違いは，対象のタスクが構文解析ではなく
語彙曖昧性解消である点である．そのため，本論文ではケースという単位ごとに最適な領域適応を行う．
また，彼らは複数のソースドメインから抽出した用例を混合して訓練事例とした領域適応を想定して
いるが，我々は想定していない．
本研究では決定木学習を用いることで，どのような性質が最適な領域適応の決定に影響を与えるのかについて考察する．

本稿では，ソースデータとターゲットデータの性質をもとに領域適応に用いる手法を自動選択する手法について述べる．
これに関連した研究として \cite{article10}や \cite{article11}
がある．\cite{article10}は，構文解析において，分野間距離をはかり，より適切なコーパスを利用して
領域適応を行えるようにした．また，\cite{article11}は，構文解析において，
自動的にタグ付けされたコーパスを用いて，ソースデータとターゲットデータの類似度から性能を予測できることを示した．
これらの研究では，領域間の距離からソースデータとして利用できるコーパスを選択するという立場をとっているが，
本研究では領域間の距離などの性質から，手法を選択するという立場をとる．




************************ [./logs/V19N04-01/related_study] ************************
関連研究
\label{sec:kanren_kenkyu}

\subsection{Wikipediaからis-a関係を抽出する研究}

Ponzetto and Strube \citeyear{Ponzetto}は，英語Wikipedia のカテゴリ間のリンク
からis-a 関係とnot-is-a 関係を抽出する手法を提案している．桜井ら
\citeyear{Sakurai}は，Ponzetto and Strubeの手法の一部を利用した手法に独自の
手法を加え，日本語Wikipedia に対し，カテゴリ階層からis-a 関係のオントロ
ジーを抽出する手法を提案している．玉川ら\citeyear{Tamagawa}は桜井らの手法に
加え，カテゴリ名とInfoboxテンプレートを文字列照合する手法によりさらに多
くのカテゴリ間のis-a関係を抽出している．また，記事中から「分類」や「種類」
といった語を含む節見出しと箇条書きの対をis-a関係として抽出している．

これらの手法はis-a関係のリンクの抽出に文字列照合を用いるため，適合率は高
いが再現率が低い．一方提案手法では，意味属性分類や固有名詞抽出などを用い
てnot-is-a関係を判定することにより，文字列照合では抽出できないis-a関係を
抽出できた．

次に隅田らの研究\cite{Sumida}及びその成果が利用されている鳥式改\cite{Torisawa}と比較を行う．隅田らは，Wikipedia の記事中の箇条書き構造を利用してis-a 関係の単語対を獲得する研究を行った．彼らは初めに，節見出しとその下位の節見出し，節見出しとその下位の箇条書きをis-a 関係の単語対の候補とし，SVM による分類器でフィルタリングを行ってis-a 関係の単語対を獲得している．これを2007年3月の日本語Wikipediaに適用した結果，135万対の上位下位語対を精度90\%で獲得できたとしている．これに対し本手法では，（隅田らが抽出対象としたWikipediaの記事構造ではなく）Wikipediaのカテゴリ階層から抽出を行い，カテゴリ間においては 95.3\%の精度（再現率96.6\%）で 3.4万件，カテゴリ‐記事で精度96.2\%（再現率95.6\%）で42万件をオントロジー化することに成功した．両手法は抽出対象が異なるため直接の比較はできないが，隅田らが論文で報告している\footnote{隅田らの論文の図5より，精度を90\%以上にすると再現率は65\%以下になる．また再現率を90\%以上にした場合の精度は70\%以下になる．}ように隅田らの手法で精度，再現率を共に95\%以上にするのは不可能であり，Wikipediaからの上位下位関係抽出性能としては我々の提案手法に優位性がある．

さらに，隅田らの手法で獲得した上位下位関係は局所的であり，これを階層化することでオントロジー化する（もしくは既存のオントロジーに連結する）ためには多くの手作業によるクリーニングを要する\cite{Kuroda}だけでなく，場合によって上位下位関係を詳細化する\footnote{例えば「作品←七人の侍」という上位下位関係に対して「作品←映画←七人の侍」のように中間概念を設定することを詳細化と呼んでいる．}必要がある\cite{Yamada}．一方，本手法では最初から階層化されたオントロジー構築を目指し，そのための手法を高精度で実現する手法を提案した．以上の比較から，本提案手法は隅田らの手法に対して一定の有用性を持つと考える．

\subsection{既存のオントロジーのカテゴリにWikipediaのカテゴリを結合する研究}

Suchanek et al. \cite{Suchanek}はYAGO において英語Wikipediaのカテゴリを英語WordNetのクラス(synset)の下位クラスとして統合することにより，高精度なオントロジー構築を試みている．YAGOは英語WordNetに英語Wikipediaを統合する手法だが，カテゴリ名が複数形であれば概念を表すカテゴリになりやすい，というような英語依存の手法を利用しているためそのままでは日本語Wikipediaに適用できない．
そのため
小林ら\citeyear{Kobayashi}は，YAGOとは異なる手法で，
日本語語彙大系とWikipediaを統合する手法を提案している．
彼らは語彙大系の意味属性に対してWikipediaのカテゴリ名と，そのカテゴリの下位記事の定義文からとれる上位語が語彙大系のインスタンスに文字列照合した場合，カテゴリ‐記事の対を語彙大系の1つ下位に接続している．小林ら\citeyear{Kobayashi2}は，is-a関係の記事\footnote{カテゴリとis-a関係にある記事を抽出するのに，小林ら\citeyear{Kobayashi}の手法を用いている．}の割合が閾値以上のカテゴリを上位概念カテゴリとみなし，上位概念カテゴリと全ての下位記事をis-a関係として抽出している．
これらの手法では，Wikipediaのカテゴリ階層の情報が失われてしまう．

そこで我々は，小林ら\citeyear{Kobayashi}の手法と桜井ら\citeyear{Sakurai}の手法を組み合わせ，語彙大系の下位にWikipediaから抽出した部分的な階層構造を接続した\cite{Shibaki}．この手法は，Wikipediaのカテゴリ階層の情報をオントロジーに組み込めている
点で上記の2手法と異なる．しかし上位階層に既存のオントロジーを用いているため，Wikipediaの上位のカテゴリ階層がオントロジーに組み込めないという問題がある．またこれらのように既存のオントロジーにWikipediaを接続する手法では，is-a関係のリンクの抽出や既存のオントロジーの接続に文字列照合を用いるため，適合率は高いが再現率が低い．

本手法では最上位カテゴリ（意味属性）を独自に設定し，機械学習による分類器でカテゴリと記事を意味属性に分類することで，既存のオントロジーのインスタンスに文字列照合しないWikipediaのカテゴリと記事もオントロジーに組み込めた．\\


\subsection{既存のオントロジーのカテゴリにWikipediaの記事を分類する研究}

Wikipedia中の単語を関根らの拡張固有表現階層のカテゴリに分類する研究に，
渡邉ら\citeyear{Watanabe}，杉原ら\citeyear{Sugihara}，藤井ら\citeyear{Fujii}の
研究がある．
渡邉らは，
CRFを用いて，Wikipediaの記事中の箇条書き構造になっている単語を関根の拡張固有表現階層のカテゴリに分類している．
杉原ら\citeyear{Sugihara}は，Wikipediaの記事の見出し語を関根の拡張固有表現階層のカテゴリに分類する手法を提案している．記事のカテゴリ情報を利用して学習を行い，one-vs-rest法で記事の固有表現クラスを一意に決定する．ここでカテゴリ情報として，Wikipediaのカテゴリ階層構造の最上位のカテゴリである「主要カテゴリ」ページから対象ページまでの最短パス上にあるカテゴリ名を素性として用いている．
藤井らは，固有名詞表現抽出のための素性作成を目的とし，杉原らと同じ手法でWikipediaの記事の見出し語を関根の拡張固有表現階層のカテゴリに分類している．ただし，杉原らの設定した素性に加え，記事の第一文の形態素も用いている．

これらの手法と提案手法における記事の意味属性分類を比較した結果，提案手法のほうが高精度な記事分類ができることがわかった．提案手法では，記事に付与されたカテゴリの意味属性を素性に用いたり，定義文からとれる上位語や語彙大系を用いて素性の単語を抽象化したり，is-a関係の記事を持ちやすいカテゴリ（上位概念カテゴリ）を判定したりすることで，高い適合率と再現率が実現できたためだと考えられる．


\subsection{Wikipediaからオントロジーを構築するその他の研究}

Bizer et al. \citeyear{Bizer}は Wikipediaの記事中にあるInfobox，カテゴリなどの半構造化された情報からRDFトリプルを抽出し，DBpediaとして公開している．DBpediaは他のオントロジーであるYAGOなどと関連づけられている．桜井ら\citeyear{Sakurai}や玉川ら\citeyear{Tamagawa}もInfoboxを用いてInfoboxトリプル（インスタンス‐プロパティ‐プロパティの値）を抽出する研究を行っている．

中山ら\citeyear{Nakayama}は，Wikipedia中の記事間のリンク構造を解析することで
単語の意味関係を抽出する手法を提案している．中山らは記事間のリンク数や間接的にリンクしている場合のリンクの距離などを用いて記事から重要文を抽出し，重要文を
構文解析することで単語対とその意味関係を抽出している．

提案手法では，これらの関連研究で用いたInfoboxや記事間のリンク関係は利用せず，
カテゴリ間やカテゴリ‐記事間のリンクのみを利用してオントロジーを構築した．
しかしこれらの知識を用いることでオントロジーを拡張したり精度を向上させたりできる可能性がある．



************************ [./logs/V19N04-02/related_study] ************************
関連研究

自動的に文章の評価を行うためのモデルを得る先行研究には様々なアプローチが存在する．
一つは，評価者による文章のスコアをラベル，文章上の素性を事例として，
教師付き学習により単一のスコア推定モデルを求めるものである\cite{BursteinEtAl1998,BursteinWolska2003,Elliot2003,Ellis1966,Ellis1994,LandauerLahamFoltz2003a,LandauerLahamFoltz2003b,FoltzLahamLandauer1999,AttaliPowers2008,AttaliBurstein2006}．
もう一方は，模範と考えられる文章上の素性値を基準として，その基準との距離を用いてスコア推定モデルを求めるものである\cite{IshiokaKameda2006,IshiokaKameda2003,Ishioka2008b}．

e-rater \cite{AttaliBurstein2006}は，12の固定的な素性\footnote{
	変数選択を行うことがなく，常に12の素性を説明変数とする．}
を説明変数，評価者によるスコアを従属変数として重回帰分析を行い，得られた回帰式をスコア推定モデルとする．しかし，この手法では，説明変数として用いられる特徴量が何の変量であるかが抽象的であり，評価者の評価基準の違いを十分に表現できないと考えられる．例えば，e-raterでは「総ワード数に対する語の使用法についてのエラーの割合」や「総ワード数に対する文法エラーの割合」といった特徴量が説明変数として扱われるが，「語の使用法」や「文法エラー」が具体的に示す言語現象が不明瞭である．したがって，評価基準の個人差をモデル式の回帰係数の差として示すことはできても，その差が何を意味するかについての具体性が乏しく，評価者にとって明確な差として捉えづらいと考えられる．また，モデルが重回帰分析であることから多重共線性が問題となり，扱う特徴量を詳細化する上では，各特徴量の独立性が厳密に保たれる必要がある．この点で，提案手法は，多重共線性の影響が少ない回帰的手法 (SVR) を用いており，素性間の関連性を殆ど考慮することなく，多種の詳細な言語現象についての素性を用いて，評価基準をモデル化することができる．

Jess \cite{IshiokaKameda2006}は，あらかじめ三種の観点（修辞，論理構成，内容）に沿って模範となる文章（新聞の社説やコラム）における種々の素性値の分布を獲得し，理想的な分布とする．評価対象文章の各素性値が模範文章における素性値分布の四分位数範囲の1.5倍を超える場合，外れ値とみなしてそれぞれについて評点を減ずる．しかしこの手法では，模範文章の選択の妥当性について，評価が行われる背景（試験の目的等）毎に検証が必要である．その検証自体も，実用的に難しいと考えられる．また，評価基準をある基準に固定することが前提とされているため，評価者の基準の個人差を明示する提案手法とは目的が異なる手法であるといえる．

なお，これらの関連研究に関しては石岡によるサーベイ\cite{Ishioka2008a}が詳しい．

提案手法は，多種の詳細な言語現象についての変量を教師付き学習の素性として用いることで，「詳細な言語的要素に視座を置いた，評価基準の個人差の明示」を実現するスコア推定手法である．




************************ [./logs/V19N05-01/related_study] ************************
関連研究と本研究の位置づけ

\addtext{オノマトペは，感覚と強く関連する言葉であることから，心理学や認知科学など幅広い分野で研究対象とされている．例えば，映像などの視覚や音などの聴覚から感じる印象をオノマトペを用いて調査し，人が感じる印象とオノマトペとの関係性を抽出したり}
\addtext{{\cite{Article_10,Article_11,Article_12,Article_13}}，味覚の官能評価においてその評価項目としてオノマトペが利用され，オノマトペと食品の硬さを示}
\addtext{す応力との関連性を評価したりしている{\cite{Article_14,Article_15}}．}

本稿で対象とする言語処理の分野におけるオノマトペに関する関連研究として，オノマトペ辞書の構築\cite{Article_01,Article_02}やオノマトペの自動分類手法\cite{Article_03}などが提案されている．

前者では，大規模なWebの情報を利用し，主に用例や動詞による同義表現などが調べられるオノマトペ辞書を自動で構築している．高い精度で用例を抽出できているが，新たなオノマトペが日々創出され続け，用法も変化していくため，辞書も構築し続けなければならないという問題はある．また，本稿で対象にしているオノマトペが表現する印象を扱うことはできない．

後者では，10種類の意味を動詞で表現し，292語のオノマトペがどの意味であるかを自動分類している．Webの情報から算出した共起頻度と子音と母音の出現頻度を用い，クラスタリングすることで自動分類を実現している．しかし，オノマトペ中のモーラの並びなどオノマトペの構造に深く着目した処理とはなっていない．また，定義している意味や分類するオノマトペの数が少ないという問題がある．

本稿では，未知のオノマトペが表現する印象は，類似したオノマトペが表現する印象と似ているという考えに基づき，類似度計算により類似したオノマトペを特定し，そのオノマトペが表現する印象を推定する．しかし，前述したように，オノマトペを構成する文字が少し異なるだけでまったく異なる印象を与えることもオノマトペの特徴である．そこで，既に出版されているオノマトペ辞書\cite{Book_01}に掲載されているオノマトペ1,058語を基に，オノマトペを構成する文字列とその構造を解析し，オノマトペが表現している印象を自動推定できる手法を提案する．具体的には，オノマトペ中のモーラの並びとモーラを構成する各音素が表現する印象をベクトル化した音象徴という2種類の特徴を利用することにより，先の問題を解決できる類似度計算を提案する．これにより，大規模な辞書を作成・利用することなく，新たに創出される未知のオノマトペの印象も推定することができる．また，推定する印象は48種類の形容詞で定義し，より詳細にオノマトペを理解できるようにしている．




************************ [./logs/V19N05-02/related_study] ************************
関連研究
\label{sec-related-work}

日本語学習者の助詞誤り検出・訂正は従来より研究されてきた．{\kern-0.5zw}近年では，
\shortciteA{suzuki-toutanova:2006:COLACL}が，最大エントロピー法 (ME) 
による分類器を用いて，助詞（主に格助詞）が欠落した文からの復元を行って
いる．この入力文は形態素・構文解析済みであり，基本的に誤り箇所が既に分
かっているとき，挿入操作だけで修正を行う．
\shortciteA{Ohki:ParticleError2011j}は，形態素・構文解析済みの入力文
（誤りを含む）に対して，周辺の形態素や係り先を素性として，SVMで助詞の
誤用検出する方法を提案している．ここでは，助詞の欠落も対象としている．
検出を行うのみで修正までは行わない．

英語の前置詞・冠詞誤り訂正では，\shortciteA{HAN10.821}が，前置詞周辺単
語や構文解析の主辞などを素性としたME分類器を用いて，前置詞の誤り訂正を
行った．\shortciteA{gamon:2010:NAACLHLT}は前置詞と冠詞誤りを対象に，ME
分類器による誤り検出，決定木による誤り訂正を行った．また，
\shortciteA{rozovskaya-roth:2010:EMNLP}は平均化パーセプトロンに基づく
分類器で前置詞の誤り訂正を行っている．これらの研究は，いずれも誤りの
種類を助詞や前置詞・冠詞に限定することで，分類器による誤り訂正を可能と
している．

一方，\shortciteA{mizumoto-EtAl:2011:IJCNLP-2011}は，誤りを助詞に限定
せず，すべての誤りを対象とした自動訂正法を提案した．ここでは，対訳文に
相当する学習者作文と日本人による修正文のペアを大量にSNSから収集し，句
に基づく統計翻訳の仕組みを利用して訂正を行う．誤りを含む入力の形態素解
析は行わず，文字単位で翻訳を行う．本稿で使用した系列変換は，基本的には
統計翻訳と同等な手法である．そのため，誤りの種類を助詞に限定する必要が
なく，他の誤りにも拡張できる．しかし，本稿の方式はあらかじめ学習者作文
が単語に分割されていることを前提としている．誤りを含む文を形態素解析，
構文解析した場合の精度は，一般的には日本語母語話者が記述した文の解析精
度より落ちると考えられるため，単語分割法も併せて検討する
必要がある\shortcite{Fujino:ErrorMorphAnalysis2012j}．

母語話者の記述したテキスト（日本語修正文相当）のモデル化という観点で上
記研究を俯瞰すると，
\shortciteA{suzuki-toutanova:2006:COLACL,Ohki:ParticleError2011j,HAN10.821,rozovskaya-roth:2010:EMNLP} 
はn-gram二値素性として利用している．
\shortciteA{gamon:2010:NAACLHLT,mizumoto-EtAl:2011:IJCNLP-2011}は，
n-gram確率という形でモデル化している．本稿では，識別モデルの枠組みで両
者を併用し，マッピング素性を含んで全体最適化を行うことにより，再現率を
向上することができた．

学習者作文の利用という観点で俯瞰すると，いずれの研究も，学習者の誤り傾
向をモデルとして組み込むことにより，母語話者の記述したテキストのみを用
いて誤り訂正を行う場合に比べ，訂正精度が向上したと報告している
\shortcite{HAN10.821,gamon:2010:NAACLHLT,rozovskaya-roth:2010:EMNLP,Kasahara:CaseParticleCorrection2012j} 
．本稿の方式は，マッピング素性という形で学習者の誤り傾向をモデル化して
おり，従来研究の成果を取り込んでいる．

学習者作文を模した擬似誤り文に関しては，
\shortciteA{rozovskaya-roth:2010:NAACLHLT}が提案を行っている．そこで
は，学習者の実誤りと同じ分布を持つ擬似誤り文を追加することにより，精度
が向上したと報告している．ただ，データ（論文では学習者の母語別）によっ
て最適な擬似誤り生成方法が異なっており，擬似誤り生成を制御する必要があ
る．本稿では，擬似誤りと実誤りのずれをドメイン適応技術を用いて修正する
ことで安定した精度向上ができた．

さまざまな種類の誤りの同時訂正は，
\shortciteA{dahlmeier-ng:2012:EMNLP-CoNLL}も行い，前置詞・冠詞誤りだけ
でなく，スペルミス，句読点，名詞の数の誤りも含めて訂正を行っている．誤
りの種別ごとに分類器やルールを用いて訂正仮説を生成し，山登り的に書き換
えを繰り返すことで 1 文中の複数の誤りを訂正する．彼らは，複数の仮説を保
持することで，山登り時に局所解に陥る可能性を軽減しているが，本稿の方式
はすべての仮説をフレーズラティスに持ち，Viterbiアルゴリズムで最適な組
み合わせを探索しているので，モデル上は最適な訂正結果であることが保証さ
れている．

本タスクは，訂正すべき助詞に比べ訂正不要な助詞が圧倒的に多く，安易な再
現率の向上は誤り訂正精度（相対向上数）の改善に直結しないと述べた．これ
はデータ不平衡問題 (Imbalanced Data Problem) と呼ばれ，機械学習を実タ
スクに適用するときの主要な問題の一つと認識されている（たとえば，サーベ
イ論文\shortcite{He:Imbalanced2009}を参照）．この問題の解決方法には，
少数派と多数派のデータを増減させることで平衡させる方法（サンプリング法）
や，少数派の分類誤り（本タスクの場合，訂正誤り）と多数派の分類誤りに異
なるコストを与えて学習する方法（ベイズリスク最小法）など，さまざまなも
のが提案されており，本タスクに適用できるか検討する必要がある．なお，本
稿で提案した疑似誤り文は，実誤りの分布を変えないようにデータを増やすの
が目的であるので，少数派データを増やすover-sampling 法とは異なる位置づ
けである．



************************ [./logs/V19N05-04/related_study] ************************
関連研究

これまで，インフルエンザの流行予測は，政府主導のトップダウンな集計が中
心であったが（2.1節），現在では，ICT技術を用いた大規模な調査方法が検討さ
れている（2.2節）．近年では，検索クエリやソーシャルメディアなどウェブの情
報を利用した研究が行われている（2.3節）．


\subsection{現在行われている調査方法}

インフルエンザの流行に対しては，事前の十分なワクチン準備が必須の対応と
なるため，世界各国で共通の課題として対策が行われてきた．このため，多く
の国で，自国の感染症調査のための機関が組織されている．例えば，アメリカ
ではCenters for Disease Control and Prevention
(CDC)\footnote{http://www.cdc.gov/}，EU は EU Influenza Surveillance
Scheme
(EISS)\footnote{http://ecdc.europa.eu/en/activities/surveillance/EISN/Pages/index.aspx}
を運営している．本邦では，国立感染症研究所 (Infection Disease Surveillance Center; IDSC) 
の感染症情報センター\footnote{http://idsc.nih.go.jp/}がこの役目を担っている．これらの機
関は主としてウイルスの遺伝子解析，国民の抗体保有状況や協力体制にある医
療機関からの報告に頼って集計を行っている．例えば，IDSCは全国約5,000の診
療所から情報を集め，集計結果を発表している．ただし，この集計には時間が
かかるため，1週間前の流行状況が発表される．この遅延のため，発表時にはす
でに流行入りしている可能性があり，より迅速な情報収集への期待が高まって
いる．



\subsection{新しい調査方法}

より早くインフルエンザの流行を捉えるため様々な手法が提案されてい
る．Espino et al. \cite{Espino2003}は電話トリアージ・アドバイス（電話を通じて
医療アドバイスを行う公共サービス）に注目し，一日の電話コールの回数が
インフルエンザの流行と相関していることを明らかにした．後の追試でもその
有効性は確かめられている\cite{Yih2009}．Magruder \citeyear{Magruder2003}は，
インフルエンザ市販薬の販売量 (over-the-counter drug sales) に注目した．た
だし，本邦ではインフルエンザ薬の購入には処方が必要となっているように，
この手法は万国で有効な手段ではない．このため，近年，薬事法など国ごとの
制度に依存しない手法としてウェブが注目されている．


\subsubsection{ログ・ベースの手法}

近年，もっとも注目されている手法はGoogleのウェブ検索クエリを用いた手
法\cite{Ginsberg2009}である．彼らはインフルエンザ流行と相関のある検索ク
エリ（相関係数の高い上位50語）を調査し，それらをモニタリングすることで，
インフルエンザの予測を行っている．彼らの予測は，アメリカのCDC報告との相
関係数0.97（最小値0.92；最大値0.99）という高い精度を報告している．同様の
アイデアにもとづいた研究は他にも報告されている．例え
ば，Polgreen et al. はYahoo! のクエリを用いて同精度の予測を行っ
た\cite{Polgreen2009}．Hulth et al. はスイス国内のウェブ検索会社のクエリを用い
た\cite{Hulth2009}．Johnson et al. は健康情報に関するウェブサイト「HealthLink」
の閲覧のアクセスログを用いた\cite{Johnson2004}．

これらは，それぞれ異なった情報源を用いているものの，患者の行動を直接調
査するという観点からは同様のアプローチであるとみなせる．このアプローチ
は，多くの情報を即時的に収集することができるが，これを実現可能
なのは，サービス・プロバイダのみという問題が残る．例えば，ウェブ検索クエ
リをリアルタイムに利用できる機関はGoogle，Yahoo! やMicrosoftといったいく
つかのサービス・プロバイダに限定されてしまう．


\subsubsection{ソーシャルメディアベースの手法}

前述したように，ログ・ベースの手法はサービス・プロバイダに依存してしまうため，より利用が容易であるソーシャルメディアを情報源とした手法も2010年から開始されている．その中でも特にTwitterは，1.2億ユーザが参加しており，550万ツイートが毎日やりとりされている（2011年3月現在）．このデータ量と即時性はクエリログに匹敵するため，感染症の把握について多くの研究が行われてき
た\cite{Lampos2010,Culotta2010,Paul2011,Aramaki2011,Tanida2011}．
これらの研究は，表\ref{t_relwork}のように，単語選択，発言の分類などの観点から分類できる．

\begin{table}[b]
\caption{ソーシャルメディア・ベースの疾患サーベイランス研究の分類}
\label{t_relwork}
\input{04table01.txt}
\end{table}

まず，この単語選択の問題に対しては，経験則で「風邪」や「インフルエ
ンザ」などのキーワードとなる単語を選択し，その頻度を集計することが考え
られ，これまでの多くの先行研究はこの手法をとってい
る\cite{Culotta2010,Aramaki2011}．
これに対し，最近では統計的に単語の選択を行った手法が模索されている．
例えば，
L1正規化を用いて単語の次元を圧縮する方法\cite{Lampos2010}，
疾患をある種のトピックとみたてトピックモデルを用いる方法\cite{Paul2011}や，
素性選択の手法を適応する研究\cite{Tanida2011}が試みられた．
本研究では，これらの最新の単語選択手法を用いず，経験的に語を決めている
が，単語選択と組み合わせることで，さらに精度を高めることが可能であり，
今後の課題としたい．

次に，いかに罹患した発言を集計するかという問題（分類問題）がある．この
問題に対しては，高橋ら \citeyear{Takahashi2011}が Boostingを用いた文章分
類，Aramaki et~al. \citeyear{Aramaki2011}がSVMによる分類手法を提案している．
本研究では後者を用いた．

以上の2つの単語選択問題と分類問題が，これまでの研究で扱われてきた主な問
題であった．これに加え，本研究では，1節にて指摘したように，そもそもウェ
ブから得られる情報は不正確である可能性に注目し，これを補正するために感
染症モデルを用いる．



************************ [./logs/V20N01-01/related_study] ************************
関連研究
\label{sec:relation}

本論文と関連する，レビューを処理対象とする研究課題を時
系列順に追って概観する．関連研究としてまず取り組まれた
課題は，レビューをその内容に従って肯定か否定かに分類す
る課題である\cite{turney,pang,mullen}．しかし，文書単位
（レビュー単位）で分類するだけでは「何がどう良いか？」
がわからない点が問題とされた．
そこで，この問題を解決するために，文書内で具体的に評価
されている事柄（すなわち，評価視点）を特定する研究が行
われるようになった\cite{hu,liu,kobayashi2,jakob}．しか
し，前節でも述べたように，これら手法では評価視点が多量
に出力されることから，出力される評価視点群を何らかの方
法で構造化することに関心が集まるようになった．


具体的な構造化の手法として，これまで，評価視点群をグルー
ピングする手法やランキングする手法が提案されている
\cite{zhai,yu}．
グルーピングする手法では，「buttery power」と「buttery
  life」といった同一あるいは類似の評価視点を同じグルー
プに所属させて合わせてユーザに提示することで，ユーザの
可読性を向上させることを狙う．このような手法として，例
えば，Zhaiら\cite{zhai}は，事前に人手で定義した評価視点
クラスに各評価視点を自動分類する手法を提案している．よ
り具体的には，Nigamら\cite{nigam2000a}によって提案され
た半教師あり学習手法に，「同じ語を含む評価視点は同じ評
  価視点クラスになりやすい」といった情報を制約として取
り込む手法を提案している．
この手法のように評価視点を分類して構造化する場合，あら
かじめ定義した通りの構造化ができる利点がある一方で，想
定していない評価視点のクラスが必要になった場合の対応に
かかる負荷が高いという欠点がある．


次に，評価視点をランキングする手法では，事前に定めたあ
る観点に従って各評価視点にスコアを付け，スコアの大きさ
に基いて評価視点を順番に並べてユーザに提示することを考
える．これによって，ユーザは観点に合致する評価視点のみ
を選択的にすばやく確認することが可能になる．
このような研究として，Yuら\cite{yu}の研究がある．Yuらは，
まず，レビューには評価対象に対する総合的な評価点
(rating) が書き手によって与えられていることを前提とす
る．そして，このようなレビューの中で，ユーザから多く言
及されており，かつ総合的な評価点の決定にもっとも影響を
与える評価視点が重要な評価視点であると仮定し，そのよう
な評価視点をランキングする手法を提案している．論文中で
は重要な評価視点の例として評価対象が「iPhone 3GS」であ
る場合，「moisture sensor」という評価視点よりも
「battery」や「speed」などの評価視点がより重要であると
述べている．

評価視点をランキングするという点で見た場合，本研究は上
述のYuらの研究と似ている．
しかし，Yuらの手法では，レビューに付与された総合的な評
価点を説明付ける評価視点に関心がある．そのため，評価対
象ごとに独立に評価視点のランキングをおこない，評価対象
間を比較することは念頭にない．
一方，本研究では，複数の評価対象に対して，評価対象間の
違いを明確に表すことができる評価視点に関心があり，Yuら
とは目的が異なっている．




************************ [./logs/V20N02-01/related_study] ************************
関連研究
\label{sc:relatedwork}

\subsection{情報信憑性判断支援における調停要約の位置づけ}

利用者の情報信憑性判断を支援する技術には幾つかのアプローチが考えられる．
まず，利用者が着目する話題や言明に関連するWeb文書に対して，対立の構図や根拠関係などを多角的に俯瞰することを支援する技術がある．
Akamine et al. \cite{Akamine2010,Akamine2009}は，利用者が入力した分析対象トピックに関連するWebページに対して，主要・対立表現を俯瞰的に提示するシステムWISDOMを開発している．
Murakami et al. \citeyear{Murakami2010}は，Web上に存在するさまざまなテキスト情報について，それらの間に暗に示されている同意，対立，弱い対立，根拠などの意味的関係を解析する言論マップの生成課題を論じている．
藤井\citeyear{Fujii2008}は，Web上の主観情報を集約し，賛否両論が対立する構図を論点に基づいて可視化している．
Akamine et al. \citeyear{Akamine2010,Akamine2009}やMurakami et al. \citeyear{Murakami2010}や藤井\citeyear{Fujii2008}の手法では，対立関係にある記述を網羅的に提示することに焦点があり，提示された対立関係の読み解き方に関しては対象としていない．
対立関係の把握が容易になるような要約を利用者に提示できれば，着目言明に関連する話題や言明群の全体像が把握しやすくなると考えられる．

山本ら\cite{Yamamoto2010}は，分析対象となるWeb情報とその関連情報をデータ対で表現し，データ対間のサポート関係を分析することでWeb情報の信憑性を評価する汎用的なモデルを提案している．
これは，対象データ対をサポートする関係にあるデータ対が多く存在するほど対象データ対の信憑性が高まる，という仮説に基づいている．
しかしながら，Web上には，ある特殊な条件や状況下でのみ真実となるような内容に言及している記述も存在しており，そのような記述は，おそらく一般論を述べているであろう多数の記述からサポートされるとは限らない．
調停要約は，一見すると矛盾するような情報が，ある条件や状況の下では成立する場合があることを利用者に示すことを目的としており，我々は，そういった特殊な条件や状況があることを示すことも利用者の信憑性判断を支援する上で必要であると考えている．

Finn et al. \cite{Finn2001}は，Web上の新聞記事を対象として，コラム等の主観的な記事と事実を伝える客観的な記事に分類する研究を行っている．
また，松本ら\cite{Matsumoto2009}は，文末表現を用いて，Webページが主観と客観のどちらの情報を中心として構成されているかを推定する研究を行っている．
好悪といった主観に依存する言明間の対立関係の場合，互いの内容は両立することができるため，主観的であるか否かの情報は対立関係の読み解き方に役立つと考えられる．
しかしながら，ディーゼル車の例のように客観的な内容の対立関係においても疑似対立となる場合があり，客観的な内容の疑似対立となる場合の読み解き方を調停要約は対象としている．

他にも，利用者が着目する言明に対するWeb上の意見の変遷と意見が変わった要因を提示する河合らの研究\cite{Kawai2011}，Webページのレイアウト情報を利用して情報発信者の名称を抽出するMiyazaki et al.の研究\cite{Miyazaki2009}，Webページの情報発信構成の考え方に基づいて情報発信者の同定を行う加藤らの研究\cite{Kato2010}等がある．
河合ら\cite{Kawai2011}やMiyazaki et al. \citeyear{Miyazaki2009}や加藤ら\cite{Kato2010}の研究は，発信された情報の内容ではなく，「いつ」「誰が」発信したかといった面から利用者の判断を支援するアプローチを取っている．
したがって，調停要約の元文書の情報発信者を提示することで，さらに利用者への支援が容易になると考えられる．


\subsection{従来の要約手法との比較}

調停要約は，複数文書を対象とした抜粋型の報知的要約の一つである．
その中でも，橋本ら\cite{Hashimoto2001}の研究のような，要約対象文書群から「まとめ文章」を取り出すことにより要約する手法に属する．
橋本ら\cite{Hashimoto2001}は，新聞記事を対象にしており，複数文書の記述内容に齟齬があることは想定せずに，複数記事の内容をそのまままとめることが目的である．
一方で調停要約では，まず，様々な立場の人物や組織が互いに対立した主張をしているようにみえる記述を含む文書集合を要約対象にしている点が異なる．
さらに，得られる要約の中に，疑似対立とその読み解き方が含められるようにすることで，情報信憑性の判断に寄与することを目的としている．

利用者の知りたい事柄に焦点を当てて要約する手法としては，Tombros and Sandersoni \citeyear{Tombros1998}の提案するQuery-biased summarizationがある．
これは文書検索結果に対する要約を行なう場合に，利用者が文書検索に用いたキーワードの重要度を高くして重要文抽出を行なうものである．
また，利用者が質問文を与えた場合にそれを考慮した要約を提示する研究もあり，平尾ら\cite{Hirao2001}の質問が問うている事物の種類の情報を用いる手法や，Mori et al. \cite{Mori2005}の質問文により焦点が与えられた場合にQAエンジンを用いて要約を行う手法などが提案されている．
従来の調停要約生成手法においても，利用者から与えられた着目言明に基づいて要約を生成しており，Query-biased summarizationの一種であるといえる．
平尾ら\cite{Hirao2001}やMori et al. \citeyear{Mori2005}の手法では，処理の直前に1回だけ利用者の興味が入力され，それに対する要約を提示した時点で処理は完結する．
一方，提案する対話型調停要約生成手法では，着目言明に基づいて提示された文書群に対して利用者が対立の焦点となる2文を明示することで，さらに利用者が信憑性を判断したい対立点に焦点を当てた調停要約を提示することができる．

酒井ら\cite{Sakai2006}は，利用者の要約要求を反映した要約を生成するために，利用者とのインタラクションを導入した複数文書要約システムを提案している．
酒井ら\cite{Sakai2006}のシステムでは，システムが要約対象文書集合から自動的に抽出したキーワードの中から，利用者が要約要求と関連するキーワードを選択するという方法で利用者とのインタラクションを実現しているが，我々のシステムでは，提示された文書集合に対して利用者が対立関係にある任意の2文を直接選択することを想定している．
キーワードではなく文によるインタラクションを行う理由として，キーワードとの関連性だけでは適切な調停要約の生成が不十分となることがあげられる．
例えば，「ディーゼル車は黒煙を出す」ことに関する事例や根拠のみが書かれた記述は，「ディーゼル車」，「黒煙」，「出す」といったキーワードとの関連性が高くなると考えられる．
しかしながら，そのような記述は，利用者が「ディーゼル車は黒煙を出す」という言明の信憑性を判断する材料として不十分である．
利用者が正しい判断をできるようにするためには，対立関係にある「ディーゼル車は黒煙を出さない」ことに関する事例や根拠，黒煙を出す場合と出さない場合とが両立できる状況も示す必要がある．
したがって，調停要約の生成には，命題レベルでの対立関係を扱う必要があり，利用者が文書集合中の任意の2文を直接選択することで，利用者が焦点とする対立点を明確化することとした．

システムが提示したテキストに利用者が直接操作を加えることで，直観的かつ簡単に利用者が必要とする情報を要求するという対話的な要約生成手法としては，村田ら\cite{Murata2007}の手法がある．
村田ら\cite{Murata2007}は，Scatter/Gather法\cite{Cutting1992}を要約提示の観点から捉え直す事により，提示した要約文章そのものに対し利用者が操作を行ない，それによって利用者の興味を反映した新たな要約を提示する手法を提案している．
提案手法も同様の考え方に基づいており，提示された文章群の中で信憑性を判断したい対立関係にある2文を利用者がマウス操作等により明確化するという操作を行うことで，利用者が焦点とする対立関係を反映した調停要約を生成する．


\subsection{質問応答システムとの比較}

利用者が入力したクエリに対して簡潔な文章を出力するという枠組みは，Non-Factoid型の質問応答システム\cite{Fukumoto2007}と類似している．
質問応答として捉えると，着目言明を入力としてその真偽を問うYes/No型の質問応答となることが考えられるが，調停要約の場合には，単純にYes/Noで回答できる質問ではないということを気付かせる文章を出力するという点で質問応答の考え方とは異なっている．
したがって，質問応答システムにおいてYesとNoの両方の解が得られるような場合に調停要約を提示するといった利用が考えられるが，本論文では，質問応答システムとの連携は今後の課題として，単純にYes/Noでは回答できない質問が入力されることを前提としている．



************************ [./logs/V20N02-02/related_study] ************************
関連研究と本研究の特徴
\label{Second}

文中の語を他の表現に変換に関する研究は数多くなされており，平易な表現への変換技術そのものとしての研究\cite{Article_02}やWeb検索への利用を目的とした複数パターンの変換の生成\cite{Article_13}，利用者の言語能力に配慮した平易化\cite{Article_01,Article_12,Article_16}，会話への利用\cite{Article_15}といった形で報告が成されている．これらの研究においても，語の表現を変換するためのアプローチとして\ref{First}章で述べたような1：1の変換処理および語を文によって表現する1：{$N$}の変換処理が挙げられている．

1：1の変換については，例えば\cite{Article_01}では児童向け新聞の記事と一般の新聞記事との間でベクトル空間モデルによるマッチングを取り，同一内容の記事の対から1語対1語の変換対を作成している．また\cite{Article_13}ではWebを用いて入力された文字列中の語の変換候補を生成している．変換対象となる語（名詞，形容詞，動詞，カタカナ語）を入力から取り除いた文字列を用いてWeb検索を行うことで，変換対象の語があった場所に入る他の語を取得することが出来る．対して\cite{Article_02}の報告では，国語辞典の定義文を変換に用いる1：$N$の変換処理が報告されている．定義文を変換に適した形の文に整形するルールを策定し，日本語として違和感の無い変換を行うことを目指している．

これらの変換処理はそれぞれ，1：1の変換処理および1：$N$の変換処理を単独で行っているが，本稿で提案する手法はこの双方を組み合わせることでより人間の思考に沿った変換処理を提案できると考える．人間がある語の変換を行う際には，まず別の1語に言い換えることができないかを考える．これは変換の対象となる語の同義語や類義語によって行うことが可能である．しかし同義語や類義語を持たない語も数多く存在することを考えると，この1：1の変換では不十分である．また私たちの行う会話では，1つの語の変換に文を用いる場合も多々考えられる．これは分かりにくい語が出現した場合にその語の「意味を説明する」ことで語の変換を行っている．例えば「明言」という語ならば，同じ意味を持つ一語を探すよりも「はっきりと言い切る」という文による変換が自然である．1つの語に対して文，つまり$N$個の語による変換という機能が無ければ，人間の会話に近い自然な変換はできない．

\cite{Article_12}や\cite{Article_16}では，本稿と同じく1：1の変換と1：$N$の変換の組み合わせについて述べられている．例えば\cite{Article_12}では対象となる文章を自治体のWebページに固定し，人手による変換対の作成によって語の変換を実現している．変換対はシソーラスや国語辞典の定義文を人の目で参照して作成しており，よってある語を変換するための対は1語である場合もあれば短い文の場合もある． \cite{Article_16}では文化遺産に関する説明文を平易化することを目的として，そのための変換パターンの解析を行っている．この中では専門用語に対して文章による変換で補足を行うパターンや，外来語を同じ意味の日本語へ変換するといった手法により説明文を平易化できると報告している．これらの手法では変換対や変換のためのパターンが人手により作成されるため高精度を期待できるが，それに伴う労力も非常に大きい．また，変換対象を固定しているため作成した変換対やパターンの汎用性に欠けると考えられる．本稿の提案手法では変換のための語や文を既存の辞書資源から自動的に選択するため，労力や汎用性の点で優位性があると考える．

国外でも語の変換に着目した評価型ワークショップ\cite{Article_22,Article_23}が開催され，\cite{Article_24,Article_25,Article_26}といった研究が報告されている．\cite{Article_22}では文章中の英単語1語を別の語で変換するというタスクが設定されており，例えば\cite{Article_24}では変換のための語を得るために$N-gram$，語の出現頻度，Webヒット件数，さらには変換前の文章を他言語に翻訳した後，再度英語に翻訳するなどの様々な手法を組み合わせることで語にポイント付けを行い，変換を実現している．\cite{Article_23}では\cite{Article_22}で示された1語の変換に際してより平易な語を選択するというタスクになっている．例えば\cite{Article_25}では\cite{Article_24}のポイント付けを基礎とし，さらに\cite{Article_27,Article_28}で定義された心理言語学的モデル，例えば語の具体性やイメージアビリティといった側面でスコア付けを行ったデータを用いて平易性の判断を行っている．\cite{Article_26}では語の平易性の判断材料として様々なコーパス内における出現頻度や語の長さを用いている．
これらのタスクにおいても変換の処理は1：1のものが大半であり，英文による変換は行われていない．また，\cite{Article_23}のタスクでは人手で用意された変換の候補となる語に対して平易性のランク付けをすることで変換を行っており，変換の候補となる語の選出は行っていない．候補の選出処理は\cite{Article_24}によって報告されているが，この手法は\cite{Article_22}における総括でも述べられている通り，変換に必要なリソースや処理過程が非常に複雑なものとなっている．前述したとおり，本稿の提案手法では1：1および1：$N$の変換手法を組み合わせることで人間の自然な変換を実現する点，語概念連想を用いることで変換のための語や文を既存の辞書資源から自動的に選択できる点でこれらの研究と比べて優位性があると考える．



************************ [./logs/V20N02-04/related_study] ************************
関連研究 \label{Sec:関連研究}

これまでに数多くの文書分類に関する研究がなされており，これらの中でも，Bayesの手法はよく用いられている\cite{持橋}．

\citeA{井筒}はhtmlファイルの自動分類でNBを使用し，ルールベースの手法や判別分析に比べて，プログラム開発者にかかる負担の低さとスケーラビリティの高さを指摘している．
\citeA{Andrew}はNBを適用してテキスト分類を行う際に使用する事象モデルとして，多項モデルと多変量ベルヌーイモデルの違いを述べ，分類結果から多項モデルの優位性を示している．
\citeA{Lewis}は，文書分類問題において単語と句，単語と句のクラスタ化の有無，全ての索引語を用いるか一部を用いるかの違いについて，分類精度の比較を行った．
\citeA{花井}は，NBが前提とする単語間の独立が成り立たないとし，依存性の強い 2 単語の組が同時に生起する確率を NB に適用することによって分類精度の向上を図った．Church (2000) は，単語の重み付け方法として IDF 値のかわりに``Adaptation''という概念を用い，文書に含まれていないが内容に関連している単語を``Neighbor''として定義して，
テキスト内の特徴的な単語の抽出を行った．
さらに\citeA{持橋06}は，NBにおけるクラス$c$を未知として拡張したDM (Dirichlet Mixtures) やInfinite DMを提案し，
    高村, Roth (2007)\nocite{高村}は，予測的尤度を用いて超変数を設定することなく，加算スムージングのパラメータを求めた．

NBを発展させた研究として，補集合を用いて学習を行うCNBが有名である\cite{Rennie}．
CNBは，「クラスに属する文書」ではなく「クラスに属さない文書」，つまり「補集合」を用いることによりNBの欠点を緩和した手法である．

本研究では，多項モデルを用いたNBとCNBに注目し，その分類における特徴を考慮して，Bayesのアプローチを用いた新しい分類手法NNBを提案する．




************************ [./logs/V20N02-08/related_study] ************************
関連研究\label{related}

日本語における述語項構造のタグ付きコーパスとして代表的なものには，京都大学テキストコーパス~\cite{kawahara:2002:jnlp}とNAISTテキストコーパス~\cite{iida:2007:law}がある．
これら二つのコーパスを中心にして日本語述語項構造解析の研究は進められてきた．
また，CoNLL Shared Task 2009~\cite{hajivc:2009:conll} は多言語を対象にした意味役割付与のワークショップであり，
日本語述語項構造解析もタスクの一つとして取り組まれた．

本研究で用いたデータはNAISTテキストコーパスである．
NAISTテキストコーパスは，毎日新聞から抽出された2,929記事，38,384文に対して，述語項構造及び照応・共参照のタグが付与されている．
NAISTテキストコーパスでは，ガ格，ヲ格，ニ格の3種類の格のみを表層格レベルで扱っているが，
格交替を考慮するなど，京都大学テキストコーパスとは異なるタグ付け基準を採用して述語項構造を付与している．

NAISTテキストコーパスが付与するのは述語項構造と照応の情報だけであるが，
京都大学テキストコーパスのテキストに対してアノテーションされているため，
人手で整備された形態素や係り受けの情報を利用することができる．
本研究でもこれらの情報については京都大学テキストコーパスのものを利用する．

本研究で利用するNAISTテキストコーパスにおける日本語述語項構造解析の先行研究として代表的なものは二つある．
一つは平らによるSVM分類器と決定リストの併用による述語項構造解析~\cite{taira:2008:emnlp}で，
彼らの研究では動詞だけで無く，事態性名詞についても述語項構造の解析を行っている．
もう一つは，最大エントロピー法に大規模データから構築した言語モデルを組み合わせることで，
平らを上回る性能を達成した今村らの研究である~\cite{imamura:2009:acl}．
今村らの研究では，述語項構造の解析対象を述語に限定しており事態性名詞は扱っていない．
また，述語項が同一文節内にある場合は無視できる程に少ないため，
直接係り受け，文内ゼロ照応，文間ゼロ照応の3種類のみの解析を行っている．
本研究の解析対象は今村らの研究よりも一段狭く，述語と項が同一文内にある場合に限定される．
これは文内の全体最適化を行うためだが，その従来研究手法との差異を以下で説明する．

平らと今村らの手法は，ガ，ヲ，ニという3種類の格毎に別々の分類器を構築して解析するものであった（図\ref{models}左を参照）．
ゆえに，彼らの手法では格の間にある依存関係を無視したまま解析を行っていた．
しかし，この格間の依存関係を無視することは，しばしば矛盾した解析結果を出力する危険性を孕むことになる．
例えば，図\ref{models}にある左のモデルでは，ガ格と二格の両方に，同じ名詞句``NP2''を出力しているが，
一般に同じ名詞句が一つの述語の二つ以上の格を占めることは起こりにくく，このような事例は矛盾していることが多い．
格間の依存関係を無視したモデルでは，このような事例が起こり得るのである．

\begin{figure}[b]
\begin{center}
\includegraphics{20-2ia8f2.eps}
\end{center}
\caption{本研究の提案手法と従来手法との相違}
        \label{models}
\end{figure}

本研究で提案するMarkov Logicモデルは，三つの格を同時に取扱い，格間の依存関係を考慮しながら，最適な状態を見つけ出すことができる手法である．
その結果として，先に示したような矛盾を排除できるのである（図\ref{models}右）．
さらに，今村らのモデルとは対照的に，本研究の提案手法は大規模データを利用しない．
今村らは大規模データを元に構築した言語モデルを利用することで，述語と項の間における選択選好性を考慮している．
一方，本研究では，文内での全体最適化により，大規模データを利用することなく解析している．

Markov Logicモデルによる述語項構造解析の先行研究はCoNLL Shared Task 2008，2009にある．
また英語では特に詳しい報告がMeza-Ruizらによってなされている~\cite{meza:2009:naacl}．
彼らの手法では，述語項構造解析を，述語同定，項同定，語義曖昧性解消，そして意味役割付与の四つの部分問題に分割しており，
これらの部分問題を同時に解くモデルを提案している．
本研究では彼らの手法を元にして日本語述語項構造解析器を構築する．
実験結果を比較するため，平ら及び今村らの実験設定に合わせ，本研究では述語項構造解析問題のうち，項同定及び，意味役割付与のみを対象とする．
ただし，日本語では格（ガ，ヲ，ニ）の同定が意味役割付与に代わることになる．

Markov Logicを利用しない述語項構造解析の同時推定手法も，CoNLL Shared Taskの参加者を中心に様々なモデルが提案されている．
例えば，\cite{toutanova:2008:cl}と\cite{watanabe:2010:acl}は，それぞれCoNLL Shared Task 2005と2009のデータを利用して同時推定モデルを提案している．
ただし，彼らの手法は\emph{述語毎}の推定モデルであるのに対し，本研究やMeza-RuizらのMarkov Logicモデルが\emph{文毎}の推定を行うモデルであるのは大きな相違点である．
NAISTテキストコーパスは，出現する述語の格フレーム辞書が整備されていない上に，格交替を考慮した述語項構造アノテーションを持ったデータである．
特に日本語では主節と従属節とで格要素を共有する形式，ゼロ代名詞の出現が多く，
このようなデータに対する述語項構造解析は，一つの述語を考慮するだけでは，その項の充足が決定できないため，
文毎での解析により文全体での最適化手法が望ましいと考えられる．
さらに，この最適化手法は直接係り受けの述語項関係と比較して，述語と項の統語的関係が弱い文内ゼロ照応の場合にこそ，高い効果を発揮すると期待できる．



************************ [./logs/V20N03-02/related_study] ************************
関連研究

テキストマイニングにおいて，クラスタリング，分類，情報抽出の研究は多数存在する(Berry 2004, 2008)．

文書の数学的表現としては，ベクトル空間法(Vector Space Model) が広く用いられている．多次元空間のベクトルを特徴量に用いるというもので，その歴史は古く (Salton 1975) で提案されている．クラスタリングに用いられる文書‐単語行列は，その自然な拡張である．

単語の表層文字列を素性として扱うような多次元空間では，個々の単語の出現頻度が低く，疎性 (Sparseness) の問題を引き起こす．この問題に対処するために，類義語処理の研究が多数存在しており，次元圧縮としてのLSI法 (Deerwester 1990)，トピックモデルとしてのpLSI法 (Hofmann 1999)，ベイズ推定を用いたLatent 
Dirchlet Allocation法 (Blei 2003) が代表的である．

クラスタリングに特異値分解や主成分分析を用いる場合の心得は，(Kobayashi 2004) に詳しい．

LSI法の数学的な基礎付けとなる特異値分解は，反復法による数値計算で行われる．大規模な疎行例のための実装には，(Dongarra 1978) や (Anderson 1999) がある．

分類問題に対する機械学習の方法としては，線形判別分析 (Fisher 1936)，サポートベクターマシン (Cortes 1995) が代表的である．本研究では，統計処理言語Rによる実装 (Karatzoglou 2004) でのサポートベクターマシンを用いる．抽出器の作成には，多クラス分類器 (Crammer 2000; Karatzoglou 2006) を用いて，2つの手法を比較する．1つは，サポートベクターマシンの事後確率を計算する方法 (Platt 2000; Lin 2007; Karatzoglou 2006) で，閾値を超える事後確率を持つクラスが存在する場合に抽出する (Manning 2008)．もう1つは，1-クラス分類 (Sch\"{o}lkopf 1999; Tax 1999; Karatzoglou 2006) である．

言語処理学会2012年度全国大会では，災害時における言語情報処理というテーマセッションで11件の発表があった．そこには，効率的な情報抽出という観点から， 
(Neubig 2012)や(岡崎 
2012)の研究がある．本研究は，クラスタリングによって分類カテゴリの決定をするところから始めるという点で，特定の種別の情報を抽出するこれらの研究とは異なる．

東日本大震災時にSNSが果たした役割については (小林 2011), (立入 2011)が刊行されている． (片瀬 2012), (遠藤 2012)でも指摘されているように，今後の震災時にSNSが取材情報源として担う役割は大きいものと思われる．なお，当時のメディアについては，(片瀬 2012), (稲泉 2012), (遠藤 2012), (福田 2012), (徳田 2011)に詳しい．




************************ [./logs/V20N03-05/related_study] ************************
関連研究\label{sec:Related}

\subsection{デマ分析}

Web情報の信憑性を判断する研究はこれまでにも多く行われてきているが，
近年はTwitterを対象としたものも多い．
Twitter上に流れるデマを見つけ出す研究はその典型である．
Castilloら\cite{Castillo_2011}は，手動で信憑性のラベル付けをしたデータを学習させ，ツイートやアカウントの特徴を用いてTwitter上の情報の信憑性を自動で判別する分類器を訓練し，高い性能を得た．彼らは流行しているトピックをニュースとその他に分類し，ニュースが正しいか間違っているかを分類している．

また，ソーシャルネットワークの関係をグラフ化し，解析する研究も盛んである．
Twitterのグラフ関係を利用したマイニングを行っているものとして，\addspan{Gupta}らの研究\cite{Gupta_2012}が挙げられる．Guptaらは，Castilloらと同様の分類器を用いた手法と，グラフ上の最適化の組み合わせにより，
Twitter上で観測されたイベントの信憑性の推定を行った．彼らは信憑性を，イベントが起こった確からしさとして扱っている．アカウント・ツイート・イベントに分類器の結果に基づく信憑性の初期値を与え，それらをノードとするネットワークを構成し，リンク関係を用いて信憑性の値を更新していくことで，最終的にイベントの真偽を判定する．

これらの研究は，いずれもデマかそうでないかという真偽判定の問題を扱っている．
しかしながら，情報の真偽を判断する事はそもそも難しく，真偽が存在しない情報に対応できないという問題点がある．我々はそのような立場ではなく，価値判断を行いたい情報の周辺を整理することで，真偽情報の受け手側の行動決定を支援することを狙っている．


\subsection{感情分析}

本研究は，返信ツイートによって，投稿者の「同意」「反論」「疑問」などの態度が表明されると仮定しているため，評判分析，感情分析に関する研究とも関わりが深い．
感情分析に関する研究の中で近年特に盛んなのは，極性分類と呼ばれるタスクである．
極性分類では，単語や文を単位としてポジティブもしくはネガティブに分類するが，本研究では，ツイートを単位とした極性分類が必要になる．

Speriosuら\cite{Speriosu_2011}はグラフ上での値の更新を利用したツイートの感情分類を提案している．
グラフを構成するノードとしてはユーザやツイートの他に，単語Nグラムや分類器の学習に使用した感情表現などを加えている．
ツイート及び分類器の学習に用いた素性をシードとして各ラベルの確率重みを初期値とし，それらのラベル（ポジティブとネガティブの 2 値）をグラフを通して拡散させることで，ネットワーク構造を利用しない手法を上回る性能を得ている．
なお，ユーザのフォロー関係も利用しているが，その有効性については示せていない．


\subsection{情報のグルーピングと構造の可視化}

このアプローチの代表的なものとして，WISDOM \cite{Akamine_2009}や言論マップ\cite{水野_2011}が挙げられる．
これらの研究は，あるトピックに関連する言明を整理して表示することで，ユーザの信憑性判断支援を行う．
WISDOMでは，評価極性（良い／悪い）に基づき意見を分類する．
評価極性による意見の分類を行う研究は，Turneyの研究\cite{Turney_2002}以降，他にも数多く存在する．
極性としては良い／悪い，あるいは正の感情／負の感情が用いられる．
元々は製品に対するレビューなどを大雑把に分類するのが目的であったが，近年では，評価対象をより明確にした極性分類が盛んである．
例えば，「Windows7はVistaよりもずっと良い」という文から，Windows7に対する良い／正の感情とVistaに対する悪い／負の感情を区別して抽出したいという要求がある．
Jiangらは，評価対象との構文関係に関するルールを素性に用いた上で，リツイートなどの関連ツイートを利用することにより，評価対象を明確にした極性分類の性能を向上させた\cite{Jiang_2011}．

水野ら~\cite{水野_2011}の言論マップでは，与えられたクエリと検索対象文の間の意味的関係，
すなわち同意するか反対するかに基づいて意見を分類している．
言論マップでは，良い・悪いという絶対的な評価極性で分類するのではなく，
クエリ文と任意の文が与えられたときに，その関係を文間関係認識で推定している．
この文間関係認識は，近年盛んに行われている含意関係認識\cite{Dagan_2005}を対立や根拠などの関係に拡張したもので，根拠関係などのより広範な分類を扱うことにより，言論構造の把握を目指している．

本論文に最も近いものはHassanらの研究\cite{Hassan_2012}である．
彼らは，ディベートサイトにおける各議論トピックへの参加者のグループ分けを行った．
まずユーザ間のポストをpositiveかnegativeかに分類し，
同じ意見を持つ者同士はpositiveなやり取りが多く，違う意見を持つ者同士はnegativeなやり取りが多いという
仮定に基づき，最終的に同じ意見を持つ者同士をまとめたグループを出力した．
これらの先行研究は，いずれも内容に基づくポジネガ分析が主体である．
しかしながら，字数が制限されくだけた文体が多いツイートに対し，これらの技術をそのまま適用するのは容易ではない．
本研究では，Twitter特有のグラフ関係を利用することで，内容ベースの手法だけでは難しい言論の整理を行う．



************************ [./logs/V20N03-07/related_study] ************************
関連研究\label{sec:reference}

本論文では，災害時のマイクロブログ上での流言について分析を行う．
そこで本章では，まず，流言に関するこれまでの定義について述べた後，
災害や流言について扱ったソーシャルメディアに関する研究について述べる．


\subsection{流言の定義と流言の伝達}

本節では，実社会における流言の先行研究について述べる．

流言の分類としては，ナップによる第2次世界大戦時の流言の分類があ
る\cite{ナップ1944}．ナップは，流言を「恐怖流言（不安や恐れの投影）」
「願望流言（願望の投影）」「分裂流言（憎しみや反感の投影）」の3つに分類
している．また，これらの流言がどのように流通するかは，例えば不景気，
災害など，社会状況に依存すると述べている．

また，社会状況だけでなく，流言の伝達に影響する要素として，流言の内
容，特に，{\bf 曖昧さ}，{\bf 重要さ}，{\bf 不安}という3つの要因が知られてい
る\cite{Book_Kawakami}．オルポートとポストマンは，流言の流布量につい
て，$R \sim i \times a$のように定式化し，「流言の流布量 (R) は，重要さ
 (i) と曖昧さ (a) の積に比例する」と述べている\cite{Book_dema}．

このように，流言に関しては古くから研究が行われてきたが，主に口伝えでの
流言の伝達を対象としてきた．
本論文では，口伝えより，より迅速に，また，広範囲に広まりうるネットワーク上での
流言を扱った点が新しい．


\subsection{災害，流言とソーシャルメディア}

本節では，災害を扱ったTwitterをはじめとするソーシャルメディアの
先行研究について概観する．

  災害時のソーシャルメディアの利用方法について分析した研究として
  は，まずLonguevilleらやQuら，Backら，Cohnら，Viewegらの研究があ
  る\cite{Inproc_Longueville,Inproc_Qu2009,Inproc_Qu,Article_Back,Article_Cohn,Inproc_Vieweg}．
  Longuevilleらは，2009年にフランスで発生した森林火災に関し
  て，Twitterに発信されたツイートの分析を行ってい
  る\cite{Inproc_Longueville}．この研究においては，ツイートの発信者の分
  類や，ツイートで引用されたURLの参照内容に関する分析などを行ってい
  る．Quらは四川大地震および青海地震において中国のオンラインフォーラ
  ム (BBS) がどのように利用されたのかを分析してい
  る\cite{Inproc_Qu2009,Inproc_Qu}．
  また，BackらやCohnらは，9.11時のブログの書き込み内容を分析し，人々の感情の変
  化を分析している\cite{Article_Back,Article_Cohn}．
Viewegら\cite{Inproc_Vieweg}は，
  2009年のオクラホマの火事 (Oklahoma Grassfires) やレッドリバーでの洪水
   (Red River Floods) におけるTwitterの利用方法を調査している．これらの
  研究では発信された内容を分類し，情報の発信の方法（情報発信か返信か）
  や，その位置関係について議論しているが，情報が流言かどうかといった観
  点からの分析は行われていない．


  流言については，災害時に限らず，多くのソーシャルメディア上の研究がある．
Qazvinianらは，マイクロブログ
   (Twitter) における特定の流言に関する情報を網羅的に取得することを目的
  とし，流言に関連するツイートを識別する手法を提案してい
  る\cite{Inproc_Qazvinian}．Mendozaらは，2010年のチリ地震におけ
  るTwitterユーザの行動について分析を行っている\cite{Inproc_Mendoza}．
  この研究では，正しい情報と流言に関するツイートを，「支持」「否定」
  「疑問」「不明」に分類し，支持ツイート，否定ツイートの数について，正
  しい情報と流言との違いを分析している．分析結果として，正しい情報を否
  定するツイートは少ないが (0.3\%)，流言を否定するツイートは約50\%に上
  ることを示している．




************************ [./logs/V20N04-01/related_study] ************************
関連研究

\subsection{辞書を用いた言い換え研究}

2つの異なる表現が同義か否かを判別する研究のひとつとして，述部を対象にした言い換え研究がある．藤田・降幡・乾・松本 (2004) は，語彙概念構造（Lexical Conceptual Structure; Jackendoff 1992; 竹内，乾，藤田 2006）を用いて，「株価の変動が為替に\underline{影響を与えた}」のような述部が機能動詞構造で構成されている文を，「株価の変動が為替に\underline{影響した}」といった単純な述部に変換する言い換えを行っている．同様に，鍜治・黒橋 (2004) は，「名詞＋格助詞＋動詞」の構造をもつ述部を対象に，「非難を浴びる」と言った迂言表現や，「貯金をためる」と言った重複表現の認識と言い換えを，国語辞典からの定義文を手掛かりに行っている．松吉・佐藤 (2008) は，階層構造化された日本語の機能表現辞書 (松吉，佐藤，宇津呂 2007) をもとに，「やる\underline{しか／ない}」の機能表現にあたる「しか／ない」を，「やら\underline{ざる／を／得ない}」という別の表現に自動で言い換える方法を提案している．

述部を対象とした言い換えの研究を用いて，複数の言い換え表現をあらかじめ生成することで，本稿が目的とする同義述部のまとめ上げが可能である．しかし，語彙概念辞書などの特殊な言語リソースを用いて言い換えを生成する場合，リソースの規模が十分でなければ，ブログなどの幅広い表現を扱う際にカバレッジが問題となる．


\subsection{コーパスからの分布類似度計算}

2つの異なる表現の意味が似ているか否かを判定する研究に，大量のコーパスを用いた分布類似度の研究がある(Curran 2004; Dagan, Lee, and Pereira 1999; Lee 1999; Lin 1998)．分布類似度とは，文脈が似ている単語は意味も似ているという分布仮説 (Firth 1957) に基づき，対象の単語の周辺に現れる単語（文脈）を素性として計算される単語の類似度である．

Szpektor and Dagan (2008) は，``X takes a nap''と``X sleeps''の関係のように，述部と1つの変数を単位として分布類似度計算を行い，述部を対象に含意ルールの獲得を行った．柴田・黒橋 (2010) は，「景気が\underline{冷え込む}」の「冷え込む」と「景気が\underline{悪化する}」の「悪化する」のように組み合わさる項によって同義になる表現をも考慮し，大規模コーパスから項と述部（e.g., 景気が‐悪化）を単位にした分布類似度ベクトルを用いて同義語獲得を行った．

大規模コーパスから周辺単語を用いて単語の意味類似度を測る分布類似度計算は，WordNetなどの特定の言語リソースを用いる手法に比べてバリエーションに富んだ表現を獲得することが可能である．しかし，分布類似度計算には柴田・黒橋 (2010)で述べられているように，2つの問題がある．1つ目は，反義関係にある単語の類似度が高くなってしまう問題である．「泳ぎが\underline{得意だ}」と「泳ぎが\underline{苦手だ}」のように，反義関係の単語は同一の文脈で現れることができ，結果として類似度が高くなる．2つ目は，時間経過を表す述部同士の類似度が高くなる問題である．たとえば，「（小鼻の脇などの狭い場所には）ブラシを使って粉を取って，（粉を）つけます」の「粉を取る」と「粉をつける」のような時間経過の関係にある述部の場合，下記のように類似した文脈で出現しやすい．

\enumsentence{
「粉を取る」と「粉をつける」の文脈の例
\begin{gather*}
 \left.\begin{array}{l}
	\text{\textbf{ブラシを使う}}\\
	\text{\textbf{パフを使う}}\\
	\text{水で洗う}\\
	\cdots
 \end{array}\right\}
 \text{粉を取る}
 \left\{\begin{array}{l}
	\text{袋に入れる}\\
	\text{\textbf{肌に乗せる}}\\
	\text{粉をつける}\\
	\cdots
 \end{array}\right. \\
 \left.\begin{array}{l}
	\text{\textbf{ブラシを使う}}\\
	\text{\textbf{パフを使う}}\\
	\text{形を整える}\\
	\cdots
 \end{array}\right\}
\text{粉をつける}
 \left\{\begin{array}{l}
 \text{卵に通す}\\
 \text{\textbf{肌に乗せる}}\\
 \text{粉を落とす}\\
	\cdots
 \end{array}\right.
\end{gather*}
}
2つの文があった場合，双方とも「ブラシを使う」や「パフを使う」という「項‐述部」を共有しているため，「粉を取る」と「粉をつける」という時間経過を表す述部同士の類似度が高くなってしまう．

Yih and Qazvinian (2012) は，WikipediaとWebスニペットを用いて計算した分布類似度や，WordNetなどのシソーラスで計算された類似度を統合することで，語の関連度を計算している．しかし，複数の類似度の平均値をとっているだけであり，それぞれの類似度に重みづけがされていない．また，類似度のみを手掛かりとしているため，反義表現と同義表現の識別は困難である． 



\subsection{教師あり学習を用いた同義判定}

教師あり学習として同義表現の識別や獲得を行っている研究としてHashimoto, Torisawa, De Saeger, Kazama, and Kurohashi (2011)がある．Hashimoto et al. (2011)では，Webコーパスから定義文を自動で抽出し，同じコンセプトを表している定義文ペアから大量の言い換え表現を獲得している．例えば，``Osteoporosis（骨粗鬆症）''というコンセプトを定義している文のペアから，``makes bones fragile（骨がもろくなる）''と``increases the risk of bone fracture（骨折リスクを高める）''といった言い換え表現を獲得している．しかし，Hashimoto et al. (2011)では，言い換え表現の獲得に定義文を用いているため，獲得される表現は必ず何らかのコンセプトを説明している表現（もしくはその一部）になる．そのため，対象の同義表現によって説明されるコンセプトが存在しない場合は，定義文からそれら同義表現を獲得することが不可能である．例えば，「食パン‐が‐出来上がった」と「食パン‐が‐焼けた」のような表現で定義されるコンセプトは想像が難しいため，定義文にも出現しづらい表現であると考えられる．本稿が目的とする意見集約などのマイニングにおけるまとめ上げを行うためには，定義文に出てこない表現（すなわち，それらの表現によって説明されるコンセプトが存在しない場合）に対するカバレッジを補う必要がある．つまり，定義文という制約を加えずにブログなどの多様な表現を含む幅広い言語リソースを用いて，高い精度で同義表現の識別をする必要がある．

Hagiwara (2008)は，分布類似度の素性と文中の単語ペアの統語構造を組み合わせて，教師あり学習の識別問題として，分布類似度単体よりも高精度に同義識別を行った．しかし，Hagiwara (2008)の手法では，コーパスからの言語情報のみしか用いておらず，分布類似度が不得意とする反義単語と同義単語の識別の有効性については述べられていない．Turney (2008)は，同義語 (synonym)・反義語 (antonym)・関連語 (association)という3つの異なる意味関係を表す単語ペアを対象に，コーパスの周辺単語情報を素性とした識別学習を行った．Turney (2008)の手法は，あらゆる意味関係もひとつのアルゴリズムで分類できるという点で有益だが，彼が述べているように，同義を認識するタスクに特化した場合，複数のアルゴリズムや言語情報を組み合わせた手法 (Turney, Littman, Bigham, and Shnayder, 2003) に対して精度が劣ってしまう．

Weisman, Berant, Szpektor, and Dagan (2012)は，``snore（いびきをかく）''と``sleep（寝る）''といった含意関係（snoreはsleepを含意する）にある動詞ペアを対象に，文，文書，文書全体それぞれにおける動詞ペアの共起情報を用いて含意関係の認識を行った．含意関係を認識するうえで必要な情報を言語学的に分析し，動詞のクラスや，副詞を素性とした分布類似度など新しい言語情報を入れることで，既存の手法に比べて高精度に含意関係の認識を行った．しかし，Weisman et al. (2012)は英語の動詞を対象としており，素性も英語に特化したものがある．例えば，``cover up''のようなphrasal verbs （句動詞）に対して，``up''などのparticleと共起しやすいかを手掛かりに，動詞の意味の一般性を計測しており，英語のような句動詞をもたない日本語で同様の事を行うのは困難である．また，日本語のように動詞以外の単語が述部に現れたり，複数の文末表現と組み合わさって述部を構成する言語を対象にする場合には，それらの意味を表現する素性を工夫する必要がある．



************************ [./logs/V20N05-02/related_study] ************************
関連研究
\label{sec:related}

\subsection{コーパスアノテーション}
\label{subsec:anno}

\modified{一般に言語の生産過程の産物であるアノテーションなしのテキストコーパスか
らは，言語の受容過程について直接的に調査することは困難である．
言語の受容過程の調査には，生産されたテキストを受容する過程を記号化する必要がある．
テキストコーパスに対し作業者が内容を理解して記号を付与するアノテーションは，
工学研究者のベンチマークデータ作成だけでなく，人の言語の受容過程を記録する一研究
手法としても利用可能である．}

\modified{コーパスアノテーション作業には二つの基準を決める必要がある．
一つはアノテーションをどのような形式で表現するかという形式的な基準である．
アノテーション対象が文字間なのか文字列範囲なのか，対象に対しシングルラベルを付与
するのかマルチラベルを付与するのか，対象間の関係が推移的なのか対称的なのか，大局
的な構造として木をなすのか有向非循環グラフをなすのかなどを決定し，抽象化する必要
がある．
抽象化された形式は，インラインで記述するのかスタンドオフで記述するのかなどを基準
として定める．
この形式的な基準は，研究者間の相互利用性を高めたり，構造学習器を実現するための必
要な抽象表現の仕様を決定するために利用される．関係する研究者があらかじめ議論をし
て標準仕様をコミュニティ駆動で策定したり，最初に策定された類似のアノテーショ
ンの形式をそのまま事実上の標準にしたりなど，標準化機関以外による何らかの標準化が行われることが多い．}

\modified{もう一つはコーパスに出現する言語表現をどのような記号に割り当てるかとい
う値割り当てについての基準である．アノテーションにおいては，個々の事例についてど
の形式に割り当てるのかという基準が必要であり，一般に言語テストなどを作業者に行っ
てもらいその判断に基づき記号に写像する基準が策定される．しかし，アノテーション作
業の当初から完全で健全な基準を作成することは困難であり，基準の策定と
アノテーション作業を何度も繰り返しながら基準を更新する．}

\modified{Pustejovsky \cite{Pustejovsky-2012}は，基準の策定方法を含めたアノテー
ション作業に二種類のサイクルがあることを示している．一つは MAMA サイク
ルで図\ref{fig:cycle}の左のような
サイクル\footnote{図は ``Model and
Guideline''-``Annotate''-``Evaluate''-``Revise'' からなり ``MAERサイクル''と呼ぶ
べきであるが，引用元の表現 ``MAMA サイクル'' をそのまま本稿でも採用する．}である．もう一つはMATTERサイクル（Model-Annotate-Train-Test-Evaluate-Revise サイクル）で
図\ref{fig:cycle}の右のようなサイクルである．工学研究のように構造学習器を作成す
ることを目的とする場合には MATTER サイクルを用いることが多いが，MATTER サイクル
で構造学習器が構成できないアノテーション初期においては MAMAサイクルを用いること
が多い．言語研究で現象そのものを観察する場合においては MAMAサイクルのみで閉じて
アノテーションを行う傾向がある． }

\begin{figure}[t]
\begin{center}
\includegraphics{20-5ia2f1.eps}
\end{center}
\caption{MAMAサイクルと MATTER サイクル}
\label{fig:cycle}
\vspace{-0.5\Cvs}
\end{figure}

\modified{このようなアノテーションの基準とサイクルを考えた場合に，アノテーション
基準の妥当性はどのように評価されるべきだろうか．形式的な基準においては利用者系に
より評価されるべきであり，当該基準を利用するコミュニティの規模などにより定
量的に評価され，相互利用における障害の有無などにより定性的に評価されるだろう．
後者の値割り当てとしての基準においては，構造学習器の構成を目的として研究を実施す
るのであれば，未知事例を含めた構造学習器の性能により評価されるだろう．
一方，言語研究を目的とする場合には，アノテーション作業を行う指針である基準の妥当
性は，成果物のアノテーションそのものによって評価されるべきである．
アノテーション単体としての評価は一致率などの定量的な指標を提示することが可能であ
るが，言語研究のためのアノテーションにおいては，必ずしも一致率などを目的関数とし
て最適化を行っているわけではない．このようなアノテーション基準の妥当性を評価する
ためには，MAMAサイクルの外側の言語研究者によって評論として行われるべきである．
近年，均衡コーパスが整備され，コミュニティ駆動によりアノテーション対象の標準化が
行われてきた．各機関で様々なレベルの言語情報のアノテーションが進められている．
このような状況を鑑みると，MAMA サイクルの外側の言語研究者による評論の代わりに，他のアノテーションとの重ね合わせによる齟齬検出結果から，アノテーションそのものの妥当性評価が検証される可能性がある．
}


\subsection{\modified{コーパスアノテーション基準の標準化}}
\label{subsec:standard}

\modified{コーパスアノテーションの基準について，形式的な基準については標準化機関
などが共有すべき規格を提案している．
例えば，国際標準化機構(International Organization for Standardization: ISO) の標準化技術委員会(Technical Committee) TC 37 は ``Terminology and other language and content
resources'' と題し，言語資源に関するさまざまな標準化を提案している．
そのなかに分科会(Subcommittee) が五つ設定されているが，
TC 37/SC 4 が言語資源管理(Language resource management; LRM)に関する国際規格の規定を行っている．
TC 37/SC 4 は作業部会を六つ（表 \ref{table:tc37sc4}）設定
しており，さまざまな形式・出自の一次言語
データに対するアノテーションや XML に代表される汎用マークアップ言語に基づくアノ
テーションの表現形式についての仕様記述言語を設計している．
例えば，公開されている規格として，語彙表の規格 Lexical Markup Framework (LMF: ISO-24613:2008)，
素性構造表現 Feature Structure Representation (FSR: ISO-24610-1:2006)，単語分かち書き（ISO-24615-1:2010が一般，
ISO-24615-2:2011が日中韓言語），統語論アノテーション Syntactic
Annotation Framework (SynAF: ISO-24615:2010) があ
る．
意味論的アノテーション規格は作業部会 TC 37/SC 4/WG 2 を中心にさまざまな
Semantic Annotation Framework (SemAF) が提案されている．
時間情報表現関連については，英語で策定された TimeML \cite{TimeML} をもとに
TimeML 開発者と作業部会 TC 37/SC 4/WG 2が連携をとりながら SemAF-Time
(ISO-24617-1:2012) TimeML を提案した．次の\ref{subsec:time}節では，時間情報表現関連のアノテーションの研究動向を示す．}

\begin{table}[t]
\caption{TC 37/SC 4 の作業部会}
\label{table:tc37sc4}
\input{02table01.txt}
\end{table}


\subsection{時間情報表現に関する研究動向}
\label{subsec:time}

\modified{時間情報表現は哲学・言語学・人工知能研究・言語処理など複数分野の研
究者により研究されてきた．}

\modified{以下では言語処理関連の代表的な研究を俯瞰する．
テキスト中の時間情報表現を分析する研究は大きく分けて時間情報表現抽出，時間情報正規化，時間的順序関係解析の三つのタスクに分類される．
一つ目の時間情報表現抽出は，固有表現・数値表現抽出の部分問題として解かれてきた．
二つ目の時間情報正規化は書き換え系により解かれることが多い．
三つ目のタスクである時間的順序関係解析は，事象の時間軸上への対応付けと言い換える
ことができる．}

表 \ref{tbl:previous_work} に英語\modified{と}日本語を対象とした時間情報表現に関連する研究を示す．

\begin{table}[b]
\caption{関連研究}
\label{tbl:previous_work}
\input{02table02.txt}
\end{table}

英語においては，評価型国際会議 MUC-6 \cite{MUC6} の一タスク固有表現抽出の中に時
間情報表現の抽出が含まれている．MUC-6 で定義されている時間情報表現タグ \timex\ は
日付表現({\tt @type="DATE"}) と 時刻表現 ({\tt @type="TIME"}) からなる．アノテー
ション対象は絶対的な日付・時刻を表す表現にのみ限定され，``last year'' などといった相対的な日付・時刻表現は含まれていない．
この MUC-6 のアノテーション基準 \timex\ に対し，Setzer は時間情報表現の正規化に関するアノテーション基準を提案している\cite{Setzer-2001}．
評価型国際会議 TERN \cite{TERN} では，時間情報表現検出に特化したタスクを設定して
いる．TERN で定義された時間情報表現情報タグ \timexii\ は，相対的な日付・時刻表現，
時間表現や頻度集合表現が検出対象として追加されている．
時間情報表現の正規化情報を記述する ISO-8601 形式を拡張した \value\ 属性などが設
計され，こちらも自動解析対象となっている．

その後，Pustejovsky らによりアノテーション基準 TimeML \cite{TimeML}が提案されている．
その中では，TERN で用いられている \timexii\ を拡張した \timexiii\ が提案され，さら
に時間情報表現と事象表現の時間的順序関係を関連づけるための情報 \tlink\ が付加される．これらの情報は人手でアノテーションすることを目的に設計され，
TimeBank \cite{TimeBank}や Aquaint TimeML Corpusなどの人手によるタグつきコーパスの整備が行われた．

これらのコーパスに基づく時間情報表現の自動解析\cite{Boguraev-2005,Mani-2006}が試みられたが，タグの情報に不整合があったり，付与されている時間的順序関係ラベルに偏りがあったりなど扱いにくいものであった\cite{Boguraev-2006}．
2007 年に開かれた SemEval 2007 の一タスク TempEval \cite{TempEval} では，時間的順序関係のラベルを簡略化し，人手で見直したデータによる時間的順序関係同定のタスクが行われた．
このタスクでは，時間情報表現に対する正規化情報 \value\ 属性などが\modified{データにあらかじ
め}付与されており，事象表現の時間的順序関係同定に利用できる\modified{設定になっ
ている}．

\modified{時間情報表現の自動解析に関する研究は英語中心に行われていたが，やが
て言語横断的な研究が進められ，前の\ref{subsec:standard}節に示し
たような国際標準化がすすめられた．その成果物として，アノテーション形式の共有可能な基準としてISO-TimeML が策定された．その作業と並行して，評価型会議}
TempEval-2 \cite{TempEval2} \modified{が実施され}，英語だけでなく，イタリア語，スペイン語，中国語，
韓国語に関しても同様なデータを利用したタスクが設定された．
2013年に開かれる SemEval-2013 のサブタスク TempEval-3 \cite{TempEval3}では，デー
タの規模を大きくした英語，スペイン語が対象となっている．

\modified{海外においては，哲学者・言語学者・人工知能
研究者・言語処理研究者が共有可能な言語資源を作成するという大義のもと，分野横
断的に研究が進められている．さらに多言語に拡張すべく言語横断的に研究が進められて
いる．このような状況のもと個々の研究について境界を明確に示すことは難しい．}

次に日本語の時間情報表現に関する研究を示す．
\modified{日本語において，時間情報表現抽出はアノテーションのみならず，評価型会議による解析
手法の検討が行われている．}
IREX \cite{IREX}の 一タスクとして，固有表現抽出タスクが設定さ
れた．IREX の時間情報では，日付・時刻表現を対象にし，相対的な表現が定義に含まれている．
関根らは拡張固有表現体系\cite{Sekine-2002}を提案し，辞書／オントロジやコー
パスの作成などを行っており，BCCWJ にも同じ体系の拡張固有表現タグが付与されている
\cite{Hashimoto-2010}．
\modified{時間情報表現正規化については，}
小西らが  TimeML に基づく \timexiii\ 相当のタ
グを BCCWJ の一部に付与し，時間情報表現の正規化を行っている\cite{小西-2013}． 
\modified{しかしながら，日本語の時間情報表現と事象表現をひもづける時間的順序関係に関する研究は，著者らが知る限りない．}

\modified{最後に，時間的順序関係アノテーションの目的について言及する．
工学研究者は(1)時間情報を解析する構造学習器の構成やベンチマークデータの
整備を目的としている．
一方，言語研究者は，(2)事象表現の時間構造を表現する形式意味論としての記述
体系の精緻化を目的としている．
これらに対し，本研究は(3)受容者としてのアノテーション作業者という要素を考慮し，
認知意味論的な分析を目的とする．(3) の目的のために，被験者実験的な設定のアノテーションを実施する．}


\subsection{アノテーション対象としての BCCWJ}
\label{subsec:bccwj}

\modified{本節ではアノテーション対象である BCCWJ について述べる．}

\modified{約1億語規模の書き言葉均衡コーパスである BCCWJ は2006--2010年に整備され，
2011年に国立国語研究所（以下「国語研」と略す）から一般公開された．
サンプリングの手法から生産サブコーパス・図書館サブコーパス・特定目的サブコーパス
の三つに大きく分かれる．生産サブコーパスは 2001--2005年に出版された書籍(PB)・雑誌
(PM)・新聞(PN)により構成され，生産実態に基づいてランダムサンプリングされている．
図書館サブコーパスは1986--2005年に出版された書籍(LB)により構成され，流通実態に基
づいてランダムサンプリングされている．特定目的サブコーパスは図書館サブコーパスで
十分に集まりにくい，白書(OW)・Yahoo!知恵袋(OC)・Yahoo!ブログ(OY)・国会会議録(OM) など様々なレジスタのテキストが収録されている．}

\modified{BCCWJ にはコアデータと呼ばれる約110万語からなる部分集合が設定さ
れている．コアデータには人手により国語研規程の短単位・長単位単語境界，UniDic 品詞体系
に基づく形態論情報，文節境界などが付与されている．コアデータは生産サブコーパスか
ら書籍(PB)・雑誌(PM)・新聞(PN)が，特定目的サブコーパスから白書(OW)・Yahoo!知恵袋
(OC)・Yahoo!ブログ(OY)が収録されている．表\ref{table:priority}に各レジスタのサン
プルについての統計を示す．}
\modified{このコアデータに対し，国内の様々な研究機関により，係り受け情報・述語項構造・節境界・モダリティ情報・フレームネット知識など重畳的にアノテーションが行われている．
しかしながら，100万語規模のコアデータ全てに対してアノテーションを実施することは
困難である．そこで，コアデータの各サンプルに対してアノテーションの優先
順位をつけ，約5--6万短単位ごとの部分集合（表\ref{table:priority}・2列目）を規定している．
アノテーションに従事する研究者は，それぞれの目的や能力に応じ，この優先順位に従っ
てアノテーションを実施する．これにより，優先順位の高いサンプルについてはより多種
の言語情報アノテーションが行われることになる．}

\begin{table}[b]
\caption{BCCWJ コアデータと部分集合}
\label{table:priority}
\input{02table03.txt}
\end{table}

\modified{各サンプルには書誌情報として様々なメタデータが付与されているが，本研究に重要なメ
タデータとして文書作成日時相当の情報がある．コアデータに収録されている 6 種類のレジスタのうち，新聞(PN)デー
タのみが日単位の文書作成日時の情報が収録されており，他のレジスタは年単位の文書作成日時の情報にとどまっている．}

\modified{本研究では新聞(PN)データの部分集合 A （54ファイル\footnote{BCCWJ において1ファイ
ル中に複数の記事が収録されているために記事数ではない．}，2,541文，56,518短単位）を対象にアノテー
ションを行う．アノテーション作業対象を上記範囲に限定した理由は，BCCWJ のコアデー
タにおいて新聞データのみが文書作成日時を日単位まで保持していること，生産実態に基づいて適切にサンプリングされてお
り通常の報道記事のみならずレシピやコラムが含まれていること，作業者が一人月でアノテーションを終えることが可能な分量であることなどがある．}




************************ [./logs/V21N01-01/related_study] ************************
関連研究

ここでは，
述語項構造解析の先行研究における，
位置関係と項へのなりやすさの優先順序の扱いについて紹介する．
先行研究と提案手法の概要を
\tblref{tbl:rwork}にまとめた．

\subsection{決定的な解析を行う方法}

\subsubsection{優先順序を統計的に求める方法}

\begin{savenotes}
\begin{table}[b]
\caption{先行研究と提案手法の概要}
\label{tbl:rwork}
\input{01table01.tex}
\end{table}
\end{savenotes}

\newcite{Kawahara:2004:JNLP}は，
解析を
ゼロ代名詞検出と先行詞同定の2段階に分け，
統計的に求めた優先順序を先行詞同定の際に用いた．
彼らの手法では，まず，
格フレーム辞書に基づく格解析によって，
ゼロ代名詞の検出を行う．
そして，
項が存在すると判断された場合は，
あらかじめ求めておいた
優先順序に従って候補を探索し，
候補と格フレーム用例の類似度が閾値以上かつ
分類器でも正例と分類される候補を先行詞として同定する．
分類器は項の位置関係に関わらず，
共通のものを作成した．
素性には，
格フレームとの類似度や品詞などを用いた．


彼らは，
従属節，主節，埋め込み文などといった文・文章中の構造をもとに，
項の位置関係（彼らは「位置カテゴリ」と呼んだ）を
20種類に分類した．

彼らは，位置カテゴリごとに，先行詞の取りやすさを
\begin{equation}
\frac{先行詞がその位置カテゴリにある回数}{その位置カテゴリにある先行詞候補の数}	
\label{a}
\end{equation}
でスコア化した．
そして，
位置カテゴリごとに，
京都大学テキストコーパス\cite{Kawahara:2002:LREC}から
スコアを算出し，
得られたスコアを
降順にソートしてそれぞれの格について
優先順序を得た．


\subsubsection{文内候補を優先的に探索する方法}
\label{iida-bact}

\newcite{Iida:2007:TALIP}は，
先行詞候補とゼロ代名詞の統語的関係をパターン化するために，
木を分類するブースティングアルゴリズムBACT \cite{Kudo:2004:IPSJ}を用いた．
BACTは木構造データを入力とし，
全ての部分木の中から分類に寄与する部分木に対して大きな重みをつける．
彼らは，
先行詞候補とゼロ代名詞間の係り受け木や，
関係を表す素性を，
根ノードに子としてつなげてBACTの入力とした．



文間先行詞の同定には係り受け関係を利用できないため，
彼らは
先行詞の同定モデルを文内と文間に分け，文内候補を優先的に探索する以下の方法をとった．

\begin{enumerate}
\item 最尤先行詞同定モデル$M_{10}$で，文内最尤先行詞$C_1^*$を求める
\item 照応性判定モデル$M_{11}$で，
$C_1^*$の先行詞らしさのスコア$p_1$を求める．
あらかじめ定めておいた閾値$\theta_{\rm intra}$に対して，$p_1 \geq \theta_{\rm intra}$であれば，$C_1^*$を先行詞として決定する．
そうでなければ(\ref{iida-inter})に進む．
\item 最尤先行詞同定モデル$M_{20}$で，文間最尤先行詞$C_2^*$を求める \label{iida-inter}
\item 照応性判定モデル$M_{21}$で，
$C_2^*$の先行詞らしさのスコア$p_2$を求める．
あらかじめ定めておいた閾値$\theta_{\rm inter}$に対して，$p_2 \geq \theta_{\rm inter}$であれば，$C_2^*$を先行詞として決定する．
そうでなければ，先行詞なしとする．
\end{enumerate}
$M_{10} \cdots M_{21}$はそれぞれBACTを使って学習・分類し，
パラメータ$\theta_{\rm intra}$と$\theta_{\rm inter}$は，開発データを用いて最適なものを求める．
この手法では，文内の最尤先行詞同定や照応性判定には文間の候補の情報は参照せずに，決定的に解析している．


\subsubsection{優先順位を経験的に決める方法}

\newcite{Taira:2008:EMNLP}は，
決定リストを用いて
全ての格の解析を同時に行う方法を提案した．
決定リストは規則の集合に適用順位を付けたものであり，
機械学習の結果を人が分析しやすいという特長がある．
彼らは
項の位置関係やヴォイス・機能語に加えて，
単語の出現形・日本語語彙大系\cite{goitaikei}から得られる意味カテゴリ・品詞
のいずれか1つを加えたものを組として扱い，
それぞれの組を1つの素性とした．
そして，
述語ごとに
Support Vector Machineの学習
で素性の重みを得て，
素性を重みでソートしたものを決定リストとした．
すなわち，1つの素性を1つの決定リストのルールとして扱った．



彼らは項の単位を単語とし，
項の位置関係を係り受け関係に基づいて次の7種類に定義している．
なお，fwとbwは追加的な種類で，その他の種類と兼ねることができる．

\begin{itemize}
\item Incoming Connection Type (ic): 項を含む文節が述語を含む文節に係っている\\
日米\underline{交渉}$_{ガ:進展}$が \underline{進展}した
\item Outgoing Connection Type (oc): 述語を含む文節が項を含む文節に係っている\\
衝動\underline{買い}した 新刊\underline{本}$_{ガ:買い}$
\item Within the Same Phrase Type (sc): 項が述語と同じ文節内にある\\
\underline{日}米\underline{交渉}$_{ガ:日}$が
\item Connection into Other Case role Types (ga\_c, wo\_c, ni\_c): 項を含む文節が述語を含む文節に，他の格の項を介して係っている\\
\underline{トム}$_{ヲ:説得, ga\_c}$への 友人$_{ガ:説得}$による\underline{説得}
\item Non-connection Type (nc): 項が述語とは異なる文にある

\item Forward Type (fw): 文章内にて，項が述語の前方にある
\item Backward Type (bw): 文章内にて，項が述語の後方にある
\end{itemize}


実際の解析は，各述語について次の手順で行った．

\begin{enumerate}
\item ic, oc, ga\_c, wo\_c, ni\_cについて，決定リストを用いて項を決定する \label{firststep}
\item (\ref{firststep})で決まらなかった格について，scの決定リストを用いて項を決定する
\item 対象の述語が項を持つ確率が50\%以上であれば(\ref{laststep})に進む
\item nc, fw, bwに関する決定リストを用いて項を決定する \label{laststep}
\end{enumerate}

この手法は経験的に，優先順序を\\
\hspace{2zw}ic, oc, ga\_c, wo\_c, ni\_c $>$ sc $>>$ nc, fw, bw\\
のように定めたといえる．
ic, oc, ga\_c, wo\_c, ni\_c間での，探索の優先関係はない．

この方法は，
格と項の位置関係を考慮しつつ，
項になりやすいものから決めていくのが特徴である．
ただし，
着目している候補と述語の情報のみを用いて項らしいかどうかを判断していくため，
必ずしも全ての候補を参照してから最終的な出力を決定するわけではなく，
候補間でどれが項らしいかの相対的な判断は行われない．


\subsubsection{述語と係り受け関係にある候補を優先的に項であるとみなす方法}

\newcite{Sasano:IPSJ:2011}は，
解析対象述語の格フレーム候補それぞれに対して，
次の手順で
格フレームと談話要素の対応付け候補を生成した．

\begin{itemize}
\item 解析対象述語と直接係り受け関係にある談話要素を，選ばれた格フレームの格スロットと対応付ける．
談話要素が係助詞をともなって出現した場合や，被連体修飾節に出現した場合など，
複数の格スロットとの対応付けが考えられる場合は，考えうるすべての対応付けを生成する．
\item 
上記の処理で生成された対応付け候補に対し，対応付けられなかったガ格・ヲ格・ニ格と，
解析対象述語と係り受け関係にない談話要素の対応付けを行う．
\end{itemize}

そして，
対数線形モデルにて最も確率的評価が高い対応付けを解析結果として出力した．
素性には，意味クラスや固有表現情報の他に，
出現格と出現位置に関する85個の2値素性も用いた．


この手法では，
格ごとに独立に解析を行なっているのではなく，
同時に解析を行なう．
しかし，
述語と係り受け関係にある候補を優先的に項であるとみなすため，
係り受け関係にある候補と，係り受け関係にない候補または他の文にある候補との比較は行えない．


\subsection{優先順序を素性として表現する方法}

位置関係と項へのなりやすさの関係を
優先順序として利用し決定的な解析を行うのではなく，
素性として利用した研究もある．

\subsubsection{最大エントロピー法を用いる方法}

\newcite{Imamura:2009:ACL}は，
最大エントロピー法に基づく識別的モデルを用いた．
彼らは，位置関係ごとにモデルを分けるのではなく，
素性として，述語と候補の位置関係，係り受け関係を用いた．
そして候補集合に，項を持たないことを示す特別な名詞句NULLを加え，
その中から最尤候補を同定するというモデル化を行った．
なお，候補数削減のため，文間項候補は
述語を含む文の直前の文に出現したものと，
これまでの解析ですでに項として同定されたものに限定している．

この方法では格ごとにモデルは1つだけ学習すればよい．
ただし，この手法では，候補間の関係を素性として用いることはできない．


\subsubsection{Markov Logicを用いる方法}

\newcite{Yoshikawa:2013:JNLP}は，
Markov Logicを利用して，
文内の複数の述語の	
項構造解析を同時に行う手法を提案した．
Markov Logicは
一階述語論理とMarkov Networksを組み合わせたもので，
一階述語論理式の矛盾をある程度のペナルティの上で認めることができる
統計的関係学習の枠組みである．
彼らは
項同定・項候補削減・格ラベル付与を同時に行うモデルを提案した．

彼らは，文間の項候補を加えるのは計算量の問題から困難だとしている．
素性（観測述語）は
述語と候補の係り受け関係などを用いた．



************************ [./logs/V21N01-04/related_study] ************************
関連研究


自然言語処理における領域適応は，帰納学習手法を利用する全てのタスクで生じる問題であるために，
その研究は多岐にわたる．
利用手法をおおまかに分類すると，ターゲット領域のラベル付きデータを利用するかしないかで分類できる．
利用する場合を教師付き領域適応手法，利用しない場合を教師なし領域適応手法と呼ぶ．
本稿における手法は教師付き領域適応手法の範疇に入るので，
ここでは提案手法に関連する教師付き領域適応手法の従来研究を述べる．

教師付き領域適応手法においては，一般に，ターゲット領域の知識は使えるだけ使えばよいはずなので，
ポイントはソース領域の知識の利用方法にある．
ソース領域とターゲット領域間の距離が離れすぎている場合，
ソース領域の知識を使いすぎると分類器の精度が悪化する現象がおこる．
これは負の転移\cite{rosenstein2005transfer}と呼ばれている．
負の転移を避けるには，本質的に，ソース領域とターゲット領域間の距離を測り，
その距離を利用してソース領域の知識の利用を制御する形となる．

Asch は品詞タグ付けをタスクとして領域間の類似性を測り，
その類似度から領域適応を行った際に精度がどの程度悪くなるかを予測できることを示した\cite{vanasch}．
張本は構文解析をタスクとしてターゲット領域を変化させたときの精度低下の要因を調査し，
そこから新たな領域間の類似性の尺度を提案している\cite{harimoto}．
Plank は構文解析をタスクとして領域間の類似性を測ることで，
ターゲット領域を解析するのに最も適したソース領域を選んでいる\cite{plank}．
Ponomareva \cite{ponomareva}や Remus \cite{rem2012}
は感情極性分類をタスクとして領域間の類似度を学習中のパラメータに利用した．
これらの研究はタスク毎に類似性を測るが，WSD がタスクの場合，
領域間の類似性は WSD の対象単語に依存していると考えられる．
古宮は対象単語毎に領域間の距離を含めた性質
\footnote{これら性質を全て含めて，領域間の類似性と呼べる．}によって適用する
学習手法を変化させている\cite{komiya3,komiya2,komiya-nlp2012}．

上記した古宮の一連の研究は広い意味でアンサンブル学習の一種である．
そこでアンサンブルされる各要素となる学習手法をみると
ソース領域のデータとターゲット領域のデータへの各重みが異なるだけである．
つまり領域適応においてはソース領域のデータとターゲット領域のデータへの各重みを調整して，
学習手法を適用するというアプローチが有力である．
Jiang \cite{jiang2007instance} は\( P_S(c|\boldsymbol{x}) \)と\( P_T(c|\boldsymbol{x}) \)との差が極端に大きいデータを
``misleading'' データとして訓練データから取り除いて学習することを試みた．
これは ``misleading'' データの重みを 0 にした学習と見なせるため，
この手法も重み付けの手法と見なせる．
本稿で利用する共変量シフト下での学習もこの範疇の手法といえる．

素性空間拡張法\cite{daume0}も重み付け手法である．
ただしデータではなくデータ中の素性に重みをつける．
そこではソース領域の訓練データのベクトル\( \boldsymbol{x_s} \)を
\( (\boldsymbol{x_s},\boldsymbol{x_s},\boldsymbol{0}) \)と連結した3倍の長さのベクトルに直し，
ターゲット領域の訓練データのベクトル\( \boldsymbol{x_t} \)を
\( (\boldsymbol{0},\boldsymbol{x_t},\boldsymbol{x_t}) \)と連結した3倍の長さのベクトルに直す．
ここで\( \boldsymbol{0} \)は\( \boldsymbol{x_s} \)や\(\boldsymbol{x_t}\)と同じ次元数であり，
しかもすべての次元の値が 0 であるようなベクトルである．

この3倍にしたベクトルを用いて，通常の分類問題として解く．
この手法は非常に簡易でありながら，効果が高い手法として知られている．
この拡張手法はソース領域とターゲット領域に共通している特徴が重なることで，
結果として共通している特徴の重みがつくことで領域適応に効果が出ると考えられる．

また領域適応の問題を共変量シフト下の学習を用いて解決する研究としては，
Jiang の研究\cite{jiang2007instance}と齋木の研究\cite{saiki-2008-03-27}がある．
Jiang は確率密度比を手動で調整し，モデルにはロジステック回帰を用いている．
また齋木は\( P(\boldsymbol{x}) \)を unigram でモデル化することで確率密度比を推定し，
モデルには最大エントロピー法のモデルを用いている．
ただしどちらの研究もタスクは WSD ではない．

また共変量シフト下では\( P_S(c|\boldsymbol{x}) = P_T(c|\boldsymbol{x}) \)を仮定するが，
\( P_S(\boldsymbol{x}|c) = P_T(\boldsymbol{x}|c) \)を仮定するアプローチもある．
この場合，ベイズの定理から
\begin{align*}
\arg \max_{c \in C} P_T (c|\boldsymbol{x}) & = \arg \max_{c \in C} P_T(c) P_T(\boldsymbol{x}|c) \\
                                   & = \arg \max_{c \in C} P_T(c) P_S(\boldsymbol{x}|c) 
\end{align*}
となるので領域適応の問題は\( P_T(c) \)の推定に帰着できる．
実際，Chan らは\( P_S (\boldsymbol{x}|c) \)と\( P_T (\boldsymbol{x}|c)\)の違いの影響は
非常に小さいと考え，\( P_S (\boldsymbol{x}|c) = P_T (\boldsymbol{x}|c)\)を仮定し，
\( P_T (c)\)を EM アルゴリズムで推定することで WSD の領域適応を
行っている\cite{chan2005word,chan2006estimating}．
更に新納らは\( P_S(\boldsymbol{x}|c) = P_T(\boldsymbol{x}|c) \)の仮定があったとしても，
コーパスのスパース性から単純に\( P_T(\boldsymbol{x}|c) \)を\( P_S(\boldsymbol{x}|c) \)で
置き換えることはできないと考え，\( P_T (c)\)の推定の問題と\( P_T(\boldsymbol{x}|c) \)
の推定の問題を個別に対処することを提案している\cite{shinnou-gengo-13}．




************************ [./logs/V21N02-02/related_study] ************************
関連研究

本節では，本稿に関連する各種研究について説明する．



\subsection{会議録を対象とした研究}

会議録を対象とした研究としては，以前より国会会議録を対象とした研究が行われてきた．川端ら\cite{kwbt}や山本ら\cite{ymmt}は特徴的な表層表現を手掛かりに国会会議録を対象とした自動要約を行っている．平田ら\cite{hrt}は，発言者の出身地域とオノマトペの使用頻度についての分析を行っている．また，国会会議録検索システムというシステムが公開されており，国会の会議録を自由に検索・閲覧することができる．これに対し，地方議会会議録のもつ会議録検索システムは，市町村ごとに様式が異なっているため，複数の市町村の会議録を対象に研究を行おうとした場合にそのまま利用することは難しい．そこで，地方議会会議録を収集して統一された書式に整形する必要がある．

これに関連し，木村ら\cite{kim1}や乙武ら\cite{ottk}は北海道内の各市町村を対象に地方議会会議録の自動収集に向けた公開パタンの分析を行っている．51種類の収集パタンによる自動収集プログラムを用いて約94\%の自治体から会議録の収集に成功している．この成果を参考にしつつ，我々は，各自治体が会議録を公開している形式を分析し，全国規模の会議録の収集を行った．


\subsection{コーパス構築に関する研究}

Web文書を対象としコーパスを構築する研究では，以下の研究が存在する．関口ら\cite{skgc}はWeb文書を収集し，HTMLタグや日本語文章の書法を用い，質の面での改善を行うことでWebコーパスを作成した．橋本ら\cite{hsmt}は，ブログを対象とした自然言語処理の高精度化への寄与を目的とし，81名の大学生に4つのテーマで執筆させた249記事のブログに，文境界，形態素，係り受け，格・省略・照応，固有表現，評価表現に関する注釈付けを行った．Ptaszynski et al.\cite{ptas}は，
日本語のブログを自動収集して構築した，3.5億文からなるコーパスYACISに対して自動的に感情情報を付与した．また，飯田ら\cite{iid}は新聞記事を対象とし，述語項構造・共参照タグを付与する基準について報告し，事態性名詞のタグ付与において，具体物のタグ付与と項のタグ付与を独立に行うことで作業品質を向上させている．しかしながら，本研究でコーパス構築の対象としたデータは地方議会会議録であり，これらのコーパス構築の手法とは対象とするデータが異なる．


\subsection{主観的な情報の注釈付けに関する研究}

本研究は政治的課題に対する賛否と積極性に関する注釈付けを行っており，主観的な注釈付けの一つである．主観的な注釈付けとしては，以下の研究がある．Weibe et al.\cite{wb}は，意見などのprivate stateをニュース記事の句に対して注釈付けを行っている．松吉ら\cite{mtys}は，書き手が表明する真偽判断，価値判断などの事象に対する総合的な情報を表すタグの体系を提案し，これに基づくコーパスを基礎とした解析システムを提案した．また，評判情報に関する研究では，小林ら\cite{kbys}は主観的評価の構成要素を「根拠」「評価」「態度」の3つの要素に分類したうえでの注釈付きコーパスの作成を行っている．宮崎ら\cite{myzk}は，Web文書を対象に，製品の様態と評価とを分離した評判情報のモデルを提案し，評判情報コーパス構築の際の注釈者間の注釈揺れを削減する方法を論じている．大城ら\cite{osr}は，施策や事業に対する賛否の意見を，構造的に捉えるための注釈付けタグセットを提案し，その有効性を確認した．

我々の提案する注釈付けは，意見や評判情報の注釈付けと同様に文中のある部分に対して極性を付与するという点で共通しているが，極性に加えて程度を表す積極性の情報を注釈付けしている点でこれらの研究と異なる．積極性の情報を注釈付けすることの有用性については，次節で説明する．


\subsection{ボートマッチに関する研究}

ボートマッチは選挙に関するインターネットサービスの一種で，有権者と立候補者，または有権者と政党の考え方の一致度を測定することができるシステムである．上神ら\cite{uekm,uekm2,kgm}はコンピュータによりコーディングを自動化する手法を提案しマニフェストの分析の自動化を行い，それを用いてボートマッチシステム「投票ぴったん」を作成した．また，毎日新聞の「えらぼーと」{\kern-0.5zw}\footnote{http://vote.mainichi.jp/} などが公開されている．

    木村ら (木村 他 2011) は意思決定の際に用いられる決定木を用い，
「決定木において同じ経路を選択する相手は同じ考え方をする相手とみなすことができる」という仮説のもとに，利用者の政治的興味や関心を同定するための質問生成手法を提案している．

我々の場合，賛否に加え積極性についても考慮し注釈付けを行うため，既存のボートマッチシステムでは比較を行うのが難しいある施策や事業に対し同意見の議員を積極性という尺度を用いて分類することが可能となる．それにより，「昨年度は○○などの事業に取り組んできた」と発言した議員と，「○○などの事業を行うのもやむを得ない」と発言した議員のように，どちらも賛成の意思を示しているが積極性の度合いが異なる場合に，我々の提案する注釈付け手法を用いれば，前者の議員がより積極的に賛成であると注釈付けることが可能である．これにより，当該の施策や事業を実現してほしい利用者に対し，その意見により近い前者の議員を提示することが可能となる． 



************************ [./logs/V21N02-04/related_study] ************************
関連研究

日本語の述語項構造および照応関係タグ付きコーパスとしては，京都大学テキストコーパス\cite{KTC}とNAISTテキストコーパス\cite{NTC}があり，述語項構造解析や照応解析の研究に利用されている\cite{笹野2008b,imamura-saito-izumi:2009:Short,iida-poesio:2011:ACL-HLT2011}．
これらのコーパスは1995年の毎日新聞に述語項構造および照応関係を付与したコーパスである．
新聞記事は内容が報道と社説に限られており，文体も統一されているため，新聞記事以外の意味関係解析への適応には不向きである．

様々なジャンルからなる日本語コーパスとしては現代日本語書き言葉均衡コーパス (BCCWJ)\footnote{http://www.ninjal.ac.jp/corpus\_center/bccwj/}がある．
このコーパスは書籍，雑誌などの出版物やインターネット上のテキストなどからなるコーパスである．
このコーパスでは，書籍などについては幅広いジャンルのテキストから構築されているが，インターネット上のテキストは掲示板やブログなどに限定されている．
このためインターネット上に多数存在する企業ページや通販ページなどはコーパスには含まれない．
また，BCCWJに意味関係を付与する研究も行われている．
一つ目は\cite{JapaneseFrameNet}によるBCCWJに日本語FrameNetで定義された意味フレーム情報，意味役割，述語項構造を記述する試みである．
この研究ではBCCWJのコアデータに含まれる用言と事態性名詞に対して項構造の記述を行っている．
しかしFrameNetではゼロ代名詞の有無は述語項構造に含まれるものの，先行詞が同一文内にない場合にはその照応先の情報を付与していない．
また，照応関係の情報も付与されておらず，文をまたぐ意味関係の情報は付与されていない．
二つ目は\cite{小町2012bccwj}による，述語項構造と照応関係のアノテーションである．
この研究では，NAISTテキストコーパスと同様の基準で述語項構造と照応関係をタグ付けしている．
述語項構造についてはNAISTテキストコーパスと同様にガ格，ヲ格，ニ格など限られた格にしか付与されていない．
しかし，NAISTテキストコーパスでは付与されている橋渡し照応などの関係は付与されていない．


日本語以外で複数のジャンルに渡って意味関係を扱ったコーパスとしては，OntoNote \cite{hovy-EtAl:2006:HLT-NAACL06-Short}やZ-corpus \cite{Z-corpus}，LMC (Live Memories Corpus) \cite{LMC}などがある．
OntoNoteは英語，中国語，アラビア語の新聞記事，放送原稿，Webページなどからなるコーパスで，コーパスに含まれる一部のテキストは複数言語による対訳コーパスとなっている．
構文木，述語項構造，語義，オントロジー，共参照，固有表現などが付与されている．

Z-corpusはスペイン語の法律書，教科書，百科事典記事に対しゼロ照応の情報を付与したコーパスである．
ゼロ照応のみを扱っており，前方照応や述語項構造の情報は付与されていない．
スペイン語ではゼロ照応は主語のみに発生するため，述語項構造の情報とは独立にゼロ照応の情報を記述できるためである．

LMCはイタリア語のWikipediaとblogに照応関係のタグ付けをしたコーパスである．
照応関係としてゼロ照応も扱っているが，述語項構造は扱っていない．
イタリア語もゼロ照応は主語のみに発生するので，このコーパスではゼロ照応の起こった用言を照応詞としてタグ付けしている．



************************ [./logs/V21N02-05/related_study] ************************
関連研究
\label{sec:related}

言語学の分野においては，英語や日本語を対象として，
否定という言語現象に関して多くの研究や解説書が存在する．
そこには，否定の焦点についての説明や理論を述べる
文献\cite{Cambridge,kato2010,neg2007}も存在する．
日本語においては，否定文の解釈にとりたて詞が強く関わる．
それゆえ，否定との共起関係\cite{neg2007,toritate2009}や，
とりたて詞のスコープの広さ\cite{Numata1986,Mogi1999,Numata2009,Kobayashi2009}
といった観点から，とりたて詞が関わる否定文の研究が行われている．

自然言語処理の分野では，これまでに，
否定のスコープを対象としたアノテーションコーパスが
いくつか構築されている．
BioScope \cite{VeronikaVince2008}は，
生医学分野における英語文章を対象に，
``not''や``without''などの否定の手がかり語句と
そのスコープをアノテーションしたコーパスである．
Moranteらは，このコーパスを利用して，
教師あり機械学習手法を用いた，否定のスコープ検出システムを
提案している\cite{Morante2008}．
Liらは，BioScopeを対象として，
浅い意味解析を取り入れた，否定のスコープ検出システムを
提案している\cite{Li2010}．
*SEM 2012\footnote{
http://ixa2.si.ehu.es/starsem/}では，
Shared taskの1つとして，否定のスコープを検出するタスクが設定されており，
Conan Doyleの小説を対象とした，
否定のスコープアノテーションコーパスが提供されている\footnote{
http://www.clips.ua.ac.be/sem2012-st-neg/}．
日本語に関しては，
川添らが，
日本語の新聞を対象として否定のスコープのアノテーションを
進めている\cite{Kawazoe2011}．

否定のスコープを対象とした研究に比べ，
否定の焦点を対象とした研究はまだ少ない．
Blancoらは，
PropBank \cite{Olga2005}を基盤データとし，
そこにラベル付けされた述語と項の間の関係を利用して，
否定の焦点をアノテーションする方法を提案し，
アノテーションコーパスを構築した\cite{EduardoMoldo2011b}．
彼らは，次の手順で否定の焦点をアノテーションする．
\begin{enumerate}
\item ``not''などの否定の語句に付与されるMNEGラベルを含む文を抽出する
\item MNEGラベルと直接関係する述語を対象とする
\item 対象の述語に関係する項（A0, A1, A2, TMP, LOCなど）の中から
  否定の焦点を選択\footnote{
否定の焦点がスコープ全体である場合は，便宜上，MNEGラベルを選択する．}し，
  その項のラベルを「焦点」としてコーパスに記述する
\end{enumerate}
このコーパスを利用して，Blancoらは，
機械学習手法やヒューリスティックを用いて否定の焦点を検出するシステムを
提案している\cite{EduardoMoldo2011b,EduardoMoldo2011}．
*SEM 2012では，
Shared taskの1つとして，このコーパスを利用して，
否定の焦点を検出するタスクが設定された\footnote{
http://www.clips.ua.ac.be/sem2012-st-neg/}．
Rosenbergらは，4つのヒューリスティック規則を組み合わせる手法を用いて，
否定の焦点を検出するシステムを提案している\cite{Rosenberg2012}．
日本語に関しては，松吉らが，
拡張モダリティの1項目として否定の焦点を扱っている\cite{matuyosi2010}．
しかしながら，主要な項目ではないとして，
彼らのコーパスにおいて実際にアノテーションされた事例の数は非常に少ない．



************************ [./logs/V21N02-08/related_study] ************************
関連研究
\label{sec:related_work}

述語項構造を解析したコーパスとしては，
日本語文章に対するものに，
京都大学テキストコーパス(KTC), 
NAISTテキストコーパス(NTC), 
GDAタグ付与コーパス(GDA), 
KTC準拠のアノテーションをブログ記事に対して行った解析済みブログコーパス (Kyoto University and NTT Blog Corpus: KNBC), 
日本語書き言葉均衡コーパス(BCCWJ)に対してNTC準拠のアノテーションを
行ったコーパス(BCCWJ-PAS)などがある．
英語を対象としたコーパスとしては，FrameNet, PropBank, NomBank, OntoNotesなどが主要なものとして挙げられる．
特に，NTC, FrameNet, PropBank, NomBankなどは，比較的多くの文章事例を含むことから，
これまでに，様々な解析器の学習データとして用いられてきた\cite{marquez2008srl,Yoshikawa2011,Iida2011,taira2008japanese}．

\begin{table}[b]
\caption{述語項構造コーパスの比較}
\label{tbl:corpora}
\input{ca12table01.txt}
\end{table}

表~\ref{tbl:corpora}に，各コーパスの特徴を示した．
コーパス間の主な仕様の差としては，文書ドメイン，述語-項関係を表すラベル，格フレーム辞書の有無，
文外の項に関する取り扱いの有無などが挙げられる．

コーパスの文書ドメインは，従来，新聞記事を中心に整備されてきたが，
係り受け解析等のその他の技術同様，教師あり学習によって開発された
述語項構造解析器の精度が学習データの文書ドメインに依存するという結果\cite{Carreras:2005:ICS:1706543.1706571}から，
近年は複数文書ドメインへのアノテーションが進みつつある（BCCWJ-PAS, KNBC, OntoNotesなど）．

述語-項関係ラベルとしては，文中の統語的なマーカーを関係ラベルに利用した表層格，
項のより意味的な側面を取り扱った意味役割ラベル等のバリエーションがある
\footnote{
表~\ref{tbl:corpora}の表記では，述語-項の意味的な関係を規定するラベル全般を「意味役割」ラベルと表記したが，
中でも格文法理論から派生し，少数のラベルを用いて述語横断的な項の統語／意味的な性質を
表現する主題役割(thematic roles)については，特に区別して「主題役割」と表記した．}．
既存のコーパスでは，英語のコーパスが意味役割を中心としたアノテーションを行った
のに対して，日本語では表層格を中心としたアノテーションが一般的である．
この違いが現れた理由としては，言語による性質の違いと，
それまでに作成された他のコーパスとの情報の差分の違い，という2点が挙げられる．
日本語においては，項の省略が頻繁に起こるという性質のほか，
副助詞「は」「も」等が使用されている場合や，連体修飾の関係にある場合など，KTCの文節単位の係り受け情報だけからでは表層的な格関係
自体が自明でない場合があるため，
述語とその項となる句の位置関係や表層的な格関係を明らかにすることが第一の目標とされた．
一方で，英語の場合，項の省略のほとんどはto不定詞や関係節，疑問詞などの統語的な性質に基づいた
移動によって説明でき，また，この移動には，
句構造にもとづいて統語的アノテーションを行ったPenn Treebank
コーパス~\cite{marcus1993building,marcus1994penn}
においてtraceというラベルを用いて述語項構造相当のアノテーションがなされており，
実際の項の位置や，移動前における統語関係が既に明らかにされていたことから，
述語が表現する概念におけるそれぞれの項の意味的な役割を表現するラベルを
アノテートすることが次の段階の目標となったと考えられる．

日本語の述語項構造アノテーションの主要なコーパスであるKTCとNTCでは，
日本語の統語上の格関係マーカーである格助詞を関係ラベルとして利用している．
KTCでは，述語が現れた時，その述語が伴っている助動詞・補助動詞等を含めた形（述部出現形）に対して
用いる格助詞を利用して，項にラベルを付与する．
\eenumsentence{
\item $[太郎_{ガ}$]が[本$_{ヲ}$]を\underline{買う}。
\item $[この本_{ヲ}$]は[太郎$_{ニ}$]に\underline{買ってほしい}。
}
上の例では，下線部が述語表現，[ ]括弧で囲まれた部分が項，その内部の下付き文字が格関係ラベルを表す．
以降，特に断りのない限りは，例文での項構造はこのように表す．
一方で，NTCでは，述語の原形に対して用いる格助詞を使って
ラベルを付与する．
\eenumsentence{
\item $[太郎_{ガ}$]が[本$_{ヲ}$]を\underline{買う}。
\item $[この本_{ヲ}$]は[太郎$_{ガ}$]に\underline{買っ}てほしい。
}
この方法は，使役・受身・願望など，格の交替が起こる表現の間で格のラベルを正規化することで，
表層格に主題役割のようなより意味機能的な側面を持たせることを試みたものと捉えることができる．
ただし，\ref{sec:ntc-case-ktc-case}節でも述べる通り，この二つについては
他方には含まれない情報をそれぞれ持っており，
どちらの方式がより適切かはアプリケーションによっても異なるため，一概に優劣を決めることは出来ない．
述部出現形アノテーションにおける格交替の情報を補う研究として，
自動的に収集された出現形の格フレームの間で格ラベルの交替がどのように起こるかを自動的に対応付ける研究\cite{sasano2013}も試みられている．

英語に対する主要なコーパスでは，述語と項の間のより詳細な意味関係をとらえる
「意味役割ラベル」が用いられる．これは，例えば，
同じ意味機能を持った項が異なる統語関係として表れる統語的交替と呼ばれる
現象に対して，それぞれの項に一貫した意味的役割を割り当てたり，
Agent, Theme, Goalなどの主題役割(thematic roles)のように，
項の述語横断的な意味機能を扱いたい場合に有用である．
また，日本語でのアノテーションではあまり取り扱いのない，必須格と周辺格の区別についても扱っている．

ただし，意味役割によるアノテーションスキーマでは
項の役割を表すラベルの数が数十から数千という規模になり，
意味の類似するラベルも多種存在するのが一般的であることから，
ガイドラインにおいて類似ラベルの取り扱いを明確に区別したり，
あるいは述語の語義ごとに格フレーム情報をあらかじめ作成し，
各語義で項として取り得るラベルの選択肢を厳密に定めることによって
曖昧な選択肢が生じないように工夫を行う必要がある．

既に構築が完了している
日本語のコーパスでは，唯一GDAが主題役割を取り扱っているが，アノテーション対象が文外のゼロ照応関係にある項に絞られており，
述語項構造に見られる現象を網羅しているとは言い難い．
NTCの表層格アノテーションでは，
述語-項関係を述語が原形の場合の格関係に正規化するため，格助詞と述語とその語義の三つ組を考えれば，
この三つ組は各述語の各語義に固有の意味役割を考えるPropBankやFrameNetとおよそ同等の意味表現となる．
ただし，主題役割のような述語横断的な意味機能については考慮できない．

一方で，近年では，日本語に対する新たな意味役割アノテーションの試みも進みつつある．
現状では一致率や規模の問題から言語処理研究への実用レベルには至っていないものの，
小規模な日本語文章への主題役割の試験的な付与例として，
林部ら~\cite{hayashibe2012}やMatsubayashi et al.~\cite{MATSUBAYASHI12.941}の研究が挙げられる．
林部らの研究では，作業者間一致率がF値で67\%前後と低く，実用に至っていない．
Matsubayashi et al.の研究では，あらかじめ述語ごと，語義ごとの格フレームを用意するため
必須格に対する一致率は$91\%$と高いが\footnote{Matsubayashi et al.の研究では，文外の項に対するアノテーションを行っていない点に注意されたい．}，アノテーションに必要となるフレーム辞書のサイズが未だ小さく，規模を拡充する必要がある．
開発過程にある意味役割付与コーパスとして，BCCWJに対し，
動詞項構造シソーラスを用いた意味役割アノテーションを行う研究~\cite{takeuchi2013}や，
同じくBCCWJに対し，FrameNetと同様の理論的枠組を利用して意味フレームの
アノテーションを行う研究\cite{ohara2013}などが進んでいる．

英語のコーパスでは，それぞれの述語が取り得る格を列挙した格フレーム辞書と呼ばれる資源を構築するのが一般的な手法である．
格フレーム辞書は，大規模な生コーパスの観察によりアノテーション作業に先立って構築される．
アノテータは格フレーム辞書を参照しながら項構造の付与
を行うことによりアノテーションの揺れを抑えることができるため，
高い作業者間一致率を得ることができる．
日本語の場合，英語に比べて項の省略が多く，また，英語のコーパスでは行っていない文をまたいだ項のアノテーションを行っているなど，
アノテータが確認しなければならない領域が相対的に広いため，英語の場合との一致率の単純な比較は出来ないが，
PropBankの項アノテーションに関する一致率は周辺格を含める場合でkappa値で0.91，
含めない場合で0.93と極めて高い\cite{palmer2005pba}．
また，含意関係認識タスクのためにFrameNet準拠のコーパスアノテーションを行った研究では，意味役割の
付与に関する
一致率が$91\%$であったとしている\cite{burchardt2008fate}．
これに対して，明示的な格フレーム辞書を持たないNTCでは，一致率が$83\%$前後と相対的に低い．
KTCでは，ガイドラインを安定化させた段階での格関係アノテーションの作業者間一致率を$85\%$と報告している\cite{河原大輔2002関係}．
NTCの仕様に準拠する形でBCCWJに対するアノテーションを行った研究では，アノテータが既存の格フレーム辞書を参照しながら
作業を行うことによって作業者間一致率に一定の改善を得ることが出来たとしている\cite{komachi2011}．

日本語コーパスの初期のアノテーションにおいて，英語コーパスであらかじめ整備された格フレームが
用いられなかった理由としては次の2点が挙げられる．
第一に，英語のコーパスで行われた意味役割を用いたアノテーションでは，
項のラベルとして統語機能的なラベルを用いず，
純粋に項の持つ意味そのものを表現するラベルを用いたため，
それぞれの述語が取る項の数やその意味役割を明示的に記述する必要があったのに対し，
日本語の場合は格助詞を関係ラベルとして採用することで，ラベルセットが少数のラベルで規定されるので，
明示的に述語ごとのラベルセットを列挙する必然性がなかったことが挙げられる．
このため，初期のアノテーション作業として，格フレームを記述するためのコストとのバランスを考慮して，
格フレームを用意せずに作業が進められたことはきわめて自然なことであった．
第二に，日本語では項の省略
が頻繁に起こるため，
統語的な文構造の制約が強い英語の場合に比べて格フレームの分析が難解となっていることが挙げられる．
日本語の述語に対して表層格の格フレーム情報を与える既存の言語資源としては
NTT語彙大系・構文体系の辞書\cite{nttlexicon}や計算機用日本語基本動詞辞書IPAL \cite{ipal}, 
竹内らの動詞語彙概念辞書\cite{takeuchi2005}, 
京都大学格フレーム\cite{kawahara2006case}などがあるが，いずれも異なった格フレームを
与えており，またNTC開発における実際のアノテーション作業時には
既存の格フレーム辞書では被覆されない格が出現するなどの問題があった．
このため，日本語においては精緻な格フレーム辞書を構築する手段についても研究課題の一つとなっている．


英語を対象としたコーパスにおいては，一般に，文をまたいだ項についての取り扱いがない．
これは，日本語が項の省略を頻繁に伴うのに対して，英語における項の省略が
比較的少ないことに由来する．しかし，英語の文章においても，イベント間の照応関係や推論的解釈により，
同一文中には現れないが暗黙的に定まっている項があると解釈される場合もあるため，
近年は，この問題を解消するための試みも研究されている\cite{laparra2013impar,frankpredicate,Silberer:2012:CIR:2387636.2387638}．

また，多くのコーパスでは，名詞についてもその項構造が考慮されている．
NTCでは，名詞のうち一般の述語で表されているような状態やイベントを表現するもの
~\cite{noun2008}（本論文中では，これをイベント性名詞と呼ぶ）について，他の述語と同様に項構造を割り当てている．
KTCやNomBankでは，イベント性名詞に加えて，ある名詞の意味解釈をするにあたって
その名詞の意味の中に取り込まれていない
別の何らかの概念との関係が必須であるもの，いわゆる非飽和名詞~\cite{nishiyama2003}についての項(\ref{enum:ktc-no-a})や，所有の関係，修飾の関係など，二つの名詞間に何らかの関係が成り立つ場合もラベル付与を行っている(\ref{enum:ktc-no-b})(\ref{enum:ktc-no-c})．
\eenumsentence{
\item $[米国_{ノ}$]の\underline{大統領}（KTC．「大統領」は非飽和名詞．「ノ格」のラベル付与）\label{enum:ktc-no-a}
\item $[花子_{ノ？}$]の\underline{眼鏡}（KTC．非飽和名詞以外の関係．「ノ？格」のラベル付与）\label{enum:ktc-no-b}
\item the [vice$_{ARG3}$] [\underline{president}$_{ARG0}$] of [North America operations$_{ARG2}$] (NomBank)\label{enum:ktc-no-c}
}



************************ [./logs/V21N03-07/related_study] ************************
関連研究
\label{114736_18Jun13}

日本語でのゼロ照応解析は文章内ゼロ照応を中心に行われてきた．

ゼロ照応解析の研究では，ゼロ代名詞は既知のものとして照応先の同定のみを行っているものがある．
\cite{iida-inui-matsumoto:2006:COLACL}はゼロ代名詞と照応先候補の統語的位置関係を素性として利用することでゼロ照応解析を行った．この研究では，外界照応を，それに対応するゼロ代名詞に照応性がないと判断する形で扱っている．この研究では，表\ref{文章内照応，外界ゼロ照応，ゼロ照応なしの分類とその例}における(a)文章内ゼロ照応と(b)外界ゼロ照応を区別して扱っているが，(c)ゼロ照応なしについては扱っていないといえる．
\cite{磯崎秀樹:2006-07-15}は，ランキング学習\footnote{当該論文中では優先度学習と呼ばれている．}を利用することで，ゼロ代名詞の照応先同定を行っている．この研究で扱うゼロ代名詞は文章内に照応先があるものに限定しており，表\ref{文章内照応，外界ゼロ照応，ゼロ照応なしの分類とその例}における(a)文章内ゼロ照応の場合のみを扱っているといえる．


ゼロ照応解析は述語項構造解析の一部として解かれることも多い．
述語項構造解析を格ごとに独立して扱っている研究としては\cite{imamura-saito-izumi:2009:Short,hayashibe-komachi-matsumoto:2011:IJCNLP-2011}がある．
\cite{imamura-saito-izumi:2009:Short}は言語モデルの情報などを素性とした最大エントロピーモデルによるゼロ照応解析を含めた述語項構造解析モデルを提案している．このモデルでは各格の照応先の候補として，NULLという特別な照応先を仮定しており，解析器がこのNULLを選択した場合には，「項が存在しない」または「外界ゼロ照応」としており，これらを同一に扱っている．
\cite{hayashibe-komachi-matsumoto:2011:IJCNLP-2011}は述語と項の共起情報などを素性としたトーナメントモデルにより述語項構造解析の一部としてゼロ照応解析を行っている．この研究でも外界ゼロ照応と項が存在しないことを区別して扱っておらず，また解析対象はガ格のみとしている．
用言ごとに全ての格に対して統合的に述語項構造解析を行う研究としては\cite{sasano-kawahara-kurohashi:2008:PAPERS,sasano-kurohashi:2011:IJCNLP-2011}がある．
\cite{sasano-kawahara-kurohashi:2008:PAPERS}はWebから自動的に構築された格フレームを利用し，述語項構造解析の一部としてゼロ照応解析を行う確率的モデルを提案した．
\cite{sasano-kurohashi:2011:IJCNLP-2011}は格フレームから得られた情報や照応先の出現位置などを素性として対数線形モデルを学習することで，識別モデルによるゼロ照応解析を行った．これらの研究では外界ゼロ照応は扱っておらず，外界ゼロ照応の場合にはゼロ代名詞自体が出現しないものとして扱っている．
これらの研究では表\ref{文章内照応，外界ゼロ照応，ゼロ照応なしの分類とその例}における(b)外界ゼロ照応と(c)ゼロ照応なしを区別せず扱っているといえる．

外界ゼロ照応を扱った研究としては\cite{山本和英:1999,平2013}がある．
\cite{山本和英:1999}では対話文に対するゼロ代名詞の照応先の決定木による自動分類を行っている．
この研究では，ゼロ代名詞は既知として与えられており，その照応先を5種類に分類された外界照応，および文章内照応（具体的な照応先の推定までは行わない）の計6種類から選択している．また，話題は旅行対話に限定されている．
この研究では，機能語および用言の語彙情報がゼロ照応における素性として有効であるとしている．機能語，特に待遇表現は著者・読者に関する外界ゼロ照応解析で有効であると考えられ，本研究でも機能語の情報を素性として利用する．一方，用言の語彙情報は文章内ゼロ照応において有効であるとしているが，これは話題を限定しているためであると考えられ，本研究の対象である多様な話題を含むコーパスに対しては有効に働かないと考えられる．本研究では，格フレームにおける頻度情報などとして用言の情報を汎化することで，用言の情報を扱うこととする．また，この研究ではゼロ代名詞を既知としているため，ゼロ代名詞検出において外界ゼロ照応を扱うことの影響については議論されていない．
\cite{平2013}では新聞記事に対する述語項構造解析の一部として外界ゼロ照応も含めたゼロ照応解析を扱っている．新聞記事コーパスでは外界ゼロ照応自体の出現頻度が非常に低いと報告しており，外界ゼロ照応の精度（F値）はガ格で0.31，ヲ格で0.75，ニ格で0.55と非常に低いものとなっている．
また，これらの研究では文章中に出現する著者・読者（本論文における著者・読者表現）と外界の著者・読者との関係については扱っていない．

日本語以外では，中国語，ポルトガル語，スペイン語などでゼロ照応解析の研究が行われている．
中国語においてはゼロ照応解析は独立したタスクとして取り組まれることが多い．
\cite{kong-zhou:2010:EMNLP}ではゼロ代名詞検出，照応性判定，照応先同定の3つのサブタスクにおいて構文木を利用したツリーカーネルによる手法が提案されている．
ポルトガル語，スペイン語では述語項構造解析の一部ではなく，照応解析の一部としてゼロ照応解析に取り組まれることが多い．
これらの言語では主格にあたる語のみが省略されるが，照応解析の前処理として省略された主格を検出し，照応先が文章内にあるかを分類する研究が行われている\cite{poesio2010creating,rello2012elliphant}．


英語においてはゼロ照応解析に近いタスクとして意味役割付与の研究が行われている\cite{gerber-chai:2010:ACL,ruppenhofer-EtAl:2010:SemEval}．
\cite{gerber-chai:2010:ACL}では頻度の高い10種類の動作性名詞に対して，直接係り受けにないものも項として扱い意味役割付与を行ったデータを作成している．また，共起頻度の情報などを利用して自動的に意味役割付与を行っている．
\cite{ruppenhofer-EtAl:2010:SemEval}では意味役割付与タスクの一部として省略された項を扱っている．また，省略された項については，照応先が特定されるDefinite Null Instanceと照応先が不特定なIndefinete Null Instanceを区別して扱っている．



************************ [./logs/V21N05-02/related_study] ************************
関連研究


自然言語処理における領域適応は，帰納学習手法を利用する全てのタスクで生じる問題であるために，
その研究は多岐にわたる．
利用手法をおおまかに分類すると，ターゲット領域のラベル付きデータを利用するかしないかで分類できる．
利用する場合を教師付き領域適応手法，利用しない場合を教師なし領域適応手法と呼ぶ．
提案手法は教師なし領域適応手法の範疇に入るので，
ここでは教師なし領域適応手法を中心に関連研究を述べる．

領域適応の問題は，一般の教師付き学習手法における訓練事例のスパース性の問題だと
捉えることもできる．そのためターゲット領域のデータにラベルを付与しないという条件では，
半教師付き学習\cite{chapelle2006semi}が教師なし領域適応手法として使えることは明らかである．
ただし半教師付き学習では大量のラベルなしデータを必要とする．
半教師付き学習を WSD に利用する場合，対象単語毎に用例を集める必要があり，
しかもターゲット領域のコーパスは新規であることが多いため，
対象単語毎の用例を大量に集めることは困難である．
このため WSD の領域適応の場合，半教師付き学習を利用しようとすれば，
Transductive 学習\cite{joachims1999transductive}に近い形となるが，
ソース領域とターゲット領域が異なる領域適応の形に 
Transductive 学習が利用できるかどうかは明らかではない．

WSD の領域適応をタスクとした教師なし領域適応の研究としては，
論文\cite{shinnou-gengo-13}の研究がある．そこでの基本的なアイデアは WSD で使うシソーラスを
ターゲット領域のコーパスから構築することであるが，
WSD で使うシソーラスが分野依存になっているかどうかは明らかではない\cite{shinnou-jws5}
\footnote{この論文\cite{shinnou-gengo-13}は本論文と同じタスクに対して，一部同じデータを用いた
実験結果を示しているため，考察において提案手法との比較を行う．}．
また Chan はターゲット領域上の語義分布を EM アルゴリズムで推定している
\cite{chan2005word,chan2006estimating}．
これも教師なし領域適応手法であるが，本論文で扱う領域適応では
語義分布の違いは顕著ではなく，効果が期待できない．

本論文は，WSD の領域適応では共変量シフトの仮定が成立していると考え，
共変量シフト下の学習を利用する．共変量シフト下の学習を領域適応に応用した研究としては
Jiang の研究\cite{jiang2007instance}と齋木の研究\cite{saiki-2008-03-27}がある．
Jiang は確率密度比を手動で調整し，モデルにはロジステック回帰を用いている．
また齋木は$P_S({\bm x})$と$P_T({\bm x})$を unigram でモデル化することで確率密度比を推定し，
モデルには最大エントロピー法を用いている．
ただしどちらの研究もタスクは WSD ではない．しかもターゲット領域の
ラベル付きデータを利用しているために，教師なし領域適応手法でもない．
また新納は WSD の領域適応に共変量シフト下の学習を用いているが\cite{shinnou-gengo-14}，
そこではDaum{\'e} が提案した素性空間拡張法 (Feature Augmentation)\cite{daume0}を
組み合わせて利用しているために，これも教師なし領域適応手法ではない．

一方，共変量シフト下の学習は，事例への重み付き学習の一種である．
Jiang は識別精度を悪化させるようなデータを
Misleading データとして訓練データから取り除いて学習することを試みた\cite{jiang2007instance}．
これは Misleading データの重みを 0 にした学習と見なせるため，この手法も重み付き学習手法と見なせる．
吉田はソース領域内の訓練データ${\bm x}$がターゲット領域から見て外れ値と見なせた場合，
${\bm x}$をMisleading と判定し，それらを訓練データから取り除いて学習している\cite{yoshida}．
これは WSD の教師なし領域適応手法であるが，
Misleading データの検出は困難であり，精度の改善には至っていない．
また WSD の領域適応をタスクとした古宮の手法\cite{komiya-nenji2013}も重み付き学習と見なせる．
そこでは複数のソース領域のコーパスを用意し，そこから訓練事例をランダムに選択し，
選択された訓練データセットの中で，ターゲット領域のテストデータを識別するのに
最も適した訓練データセットを選ぶ．これは全ソース領域のコーパスの訓練データから
選択された訓練データの重みを 1，それ以外を重み 0 としていることを意味する．
ただし複数のソース領域のコーパスから対象単語のラベル付き訓練データを集めるのは
実際は困難である．また古宮は上記の研究以外にも 
WSD の領域適応の研究\cite{komiya3,komiya2,komiya-nlp2012}を行っているが，
これらは教師付き学習手法となっている．



************************ [./logs/V21N06-05/related_study] ************************
関連研究 

 Levin \citeyear{Levin1993}は態の交替現象に着目し，3,000以上の英語の動詞を共有
 する意味構成と構文的な振る舞いに基づくクラスに分類した．コーパスを用いた
 英語の動詞の自動分類に関する研究においても態の交替を手掛かりとして利用し
 ている研究が数多く存在している
 \cite{Lapata2004CL,Walde2008,Joanis2008,Li2008,Sun2009,Sun2013}．また，
 態の交替に関連してコーパスから得られる用例の分布類似度を利用した研究とし
 てBaroniらの研究\cite{Baroni2010}がある．Baroniらはコーパスに基づく意味
 論の研究において，使役・能動交替を起こす動詞と起こさない動詞の分類にコー
 パスから得られる用例の分布類似度が有用であることを示している．

 日本語における受身形・使役形と能動形の格の変換を扱った研究として
 は，Baldwinらの研究\cite{Baldwin2000}，近藤らの研究\cite{Kondo2001}，村
 田らの研究\cite{Murata2002,Murata2008}が挙げられる．Baldwinら
 \cite{Baldwin2000}は日本語における受身・使役と能動形の交替を含む動詞交替
 の種類と頻度の定量的な分析を行っている．具体的には人手で記述した格フレー
 ム辞書である日本語語彙大系\cite{NTT}の結合価辞書を解析し，格スロット間の
 選択制約を比較して動詞交替の検出を行っている．しかし，Baldwinらの研究の
 目的は動詞交替の定量的な分析であり，受身形・使役形の能動形への変換は行っ
 ていない．

 近藤ら\citeyear{Kondo2001}は単文の言い換えの1タイプとして，受身形から能動形
 の格の変換，および，使役形から能動形の格の変換を扱っており，動詞のタイプ
 や格パターンなどをもとに作成したそれぞれ7種類，6種類の交替パターンを用い
 て格の変換を行っている．動詞のタイプとしては，「比較動詞」，「授受動
 詞」，「対称動詞」，「一般動詞」の4種類を定義しており，IPAL基本動詞辞書
 \cite{IPAL}をもとに1,564エントリからなる動詞辞書（VDIC辞書）を作成し使用し
 ている．また，村田ら\cite{Murata2002,Murata2008}は京都大学テキストコーパ
 ス\cite{TAG}の社説を除く約2万文において，受身形・使役形で出現した述語に
 係る格助詞を対象に，述語を能動形に変換した場合の格を付与した学習データを
 作成し，SVM\cite{SVM}を用いた機械学習により受身形・使役形と能動形の格を
 変換する手法を提案している．学習に使用する素性には，関係する動詞や体言，
 格助詞の出現形や品詞情報などといった情報に加え，IPAL基本動詞辞書やVDIC辞
 書から得られる情報を使用している．

 このように日本語文における格交替に関する研究では，人手で整備された大規模
 な語彙的リソースや人手で作成した大規模な学習データが利用されてきた．しか
 しながら，文(\ref{EX::FRIEND})と(\ref{EX::PARTY})のように述語と表層格が
 一致していても能動形における格が異なる場合があることからも分かるように，
 格の対応は述語ごと，用法ごとに異なっており，網羅的な対応付けに関する知識
 を人手で記述することは現実的ではないと言える．そこで本研究では格交替に関
 する大規模な語彙知識の自動獲得に取り組む．

 また，NAISTテキストコーパス\cite{Iida2007}では能動形の表層格情報が付与さ
 れていることから，NAISTコーパスを対象とした述語項構造解析やゼロ照応解析
 に関する研究
 \cite{Taira2008s,Iida2009,Imamura2009s,Yoshikawa2010,Hayashibe2014}は，
 受身・使役形で出現した述語の解析を行う際には格交替の解析も行っているとみ
 なすことができる．しかし，これらの研究では素性の1つとして態に関する情報
 を考慮しているものの，格交替に関する語彙知識は使用していない．このため，
 能動形以外の態で出現した述語に対しては相対的に低い解析精度である可能性が
 高く\footnote{実際に，Iidaらからゼロ照応解析システム\cite{Iida2009}の提
 供を受け，NAISTテキストコーパスの社説を除く記事に適用した場合の精度
 （F値）は，\pagebreak 能動形で出現した述語に対してはガ格，ヲ格，ニ格で，それぞれ
 0.358，0.110， 0.014であったのに対し，受身・使役形で出現した述語に対して
 は0.113，0.034，0.021であった．ニ格に対しては後者の方が僅かに高い精度で
 あったものの，全体の80\%以上を占めるガ格に対しては後者の方が大幅に低い精
 度であり，全体として能動形以外の態で出現した述語に対しては低い解析精度で
 あった．}，本研究で獲得を行う格交替に関する知識は有用な情報になると考え
 られる．


 
************************ [./logs/V22N01-01/related_study] ************************
関連研究
\label{sec-related-work}

近年の日本語の述語項構造解析は，教師あり学習をベースにしている．これは，
英語の意味役割付与の考え方を参考にし，日本語の問題に当てはめたものであ
る．英語の意味役割付与も，近年は意味役割として，述語とその項（格要素ご
との名詞句）に関する情報を付与しており，述語項構造解析と非常に似たタ
スクとなっている．

\subsection{英語の意味役割付与}

英語の意味役割付与は，\citeA{Gildea:PredArgs2002}が教師あり学習を
用いた方式を提案して以来，コーパスが整備されてきた．国際ワークショップ
CoNLL-2004, 2005で行われた共有タスク
\cite{carreras-marquez:2004:CONLL,carreras-marquez:2005:CoNLL}では，
PropBank \cite{Palmer:PropBank2005}を元にした評価が行われた．
PropBank は，文に対して，述語とその項を注釈付けしたコーパスで，文自体は，
Penn Treebank（元記事はWall Street Journal）から取られているため，ここ
で行われた評価も新聞記事に対するものである（このあたりの経緯は，
\citeA{Marquez:SRLSurvay2008}が整理している）．

OntoNotes \cite{hovy-EtAl:2006:HLT-NAACL06-Short} は，ニュース記事，
ニュース放送，放送における対話など，複数のジャンルを含んだコーパスであ
る．付与された情報には，意味役割も含んでいるが，現在は共参照解析のデー
タとして使用されるに留まり\cite{conll2012-shared-task}，対話解析へ
の適用はこれから期待されるところである．

意味役割付与は，タスク指向対話の意味理解にも利用される場合がある
\cite{Tur:UnderstandingSRL2005,coppola-moschitti-riccardi:2009:NAACLHLT09-Short}
．\citeA{Tur:UnderstandingSRL2005}は，電話のコールセンタにおけるユー
ザとオペレータとの対話において，述語と項の対を素性としたコールタイプ分
類器を構築している．ここで，述語・項の対は，ユーザ発話をPropBank ベース
の意味役割付与器で解析することで得ている．彼らの実験は，素性として用い
る場合は新聞記事用の意味役割付与器でも効果があることを示したが，本稿で
は，対話における述語項構造解析自体の精度向上を狙っている．
\citeA{coppola-moschitti-riccardi:2009:NAACLHLT09-Short}は，同じく
コールセンタ対話に対して，
FrameNet \cite{RuppenhoferEtAl2006:ExtTeoryAndPractice}に準拠する意
味役割付与を行っている．彼らは，コールセンタ対話を解析するため，分野依
存の意味フレームをFrameNetに追加して，スロット（フレーム要素）の穴埋め
を行っている．コールセンタ対話のように，意味役割が非常に限定される場合
は，フレーム追加で対応できるが，タスクを限定しない雑談対話の場合は，分
野依存フレームの追加は困難である．

なお，述語だけでなく，事態性名詞（例えば，動詞`\textit{decide}'に対する
事態性名詞`\textit{decision}'）に対する意味役割付与の研究もある
\cite{jiang-ng:2006:EMNLP,Gerber:NomPredArgs2012,laparra-rigau:2013:ACL2013}
．事態性名詞の場合，英語でも格要素を省略して表現することがあるため（た
とえば，``\textit{the decision}''の対象格は省略されている），日本語の
ゼロ代名詞と同様の問題を解決する必要がある．


\subsection{日本語の述語項構造解析}

日本語では，奈良先端大が，述語項構造と照応データを新聞記事に付与した
NAISTテキストコーパス\footnote{http://cl.naist.jp/nldata/corpus/}
を公開している\cite{iida-EtAl:2007:LAW,Iida:NAISTCorpus2010j}．
NAIST テキストコーパスは，毎日新聞の記事に対して，日本語で必須格と言わ
れているガヲニ格の名詞句を，各述語に付与したものである．名詞句は，述語
能動形の格に対して付与されている．また，名詞句は述語と同じ文内に限らず，
ゼロ代名詞化されている場合は，先行詞までさかのぼって付与されている．

述語項構造解析も，上記コーパスを利用したものが多く提案されている
\cite{Komachi:PredArgs2007,taira-fujita-nagata:2008:EMNLP,imamura-saito-izumi:2009:Short,Yoshikawa:PredArgs2013j,Hayashibe:PredArgs2014j}．日本語の場合，ゼロ代名詞が存在するため，述語項構造解析時に，
文をまたがるゼロ代名詞照応も解釈する
場合がある（たとえば
\cite{taira-fujita-nagata:2008:EMNLP,imamura-saito-izumi:2009:Short,Hayashibe:PredArgs2014j}）．

新聞記事以外を対象とした述語項構造解析研究には，以下のものがある．
\citeA{Hangyo:ZeroAnaphra2014j}は，ブログなどを含むWebテキストを対
象に，特に一人称・二人称表現に焦点を当てた照応解析法を提案している．彼
らは同時に述語項構造解析も行っており，本稿のタスクと類似している．彼ら
はWebテキストを解析するにあたり，外界照応（記事内に項の実体が存在しない）
を著書（一人称），読者（二人称），その他の人，その他に分けるという拡張
を行っている．本稿でも，NAISTテキストコーパス（バージョン1.5）の分類に
従い，外界照応を一人称，二人称，その他に分け，項の推定を行う．また，
\citeA{Taira:PredArgs2014j} は，ビジネスメールを対象とした述語項構
造解析を試みている．彼らは新聞記事用の述語項構造解析器をそのままビジネ
スメール解析に適用したが，一人称・二人称外界照応は，ほとんど解析できな
かったと報告している．

英語，日本語いずれも，現状の意味役割付与，述語項構造解析は新聞記事のよ
うな正書法に則って記述されたテキストやWebテキスト，メールを対象としてい
る．非常に限定されたタスクを扱うコールセンタ対話の例はあるが，タスクを
限定しない雑談対話を解析した際の精度や問題点については不明である．



************************ [./logs/V22N01-02/related_study] ************************
関連研究

二文書以上の文書間において，文書を跨いだ文同士の関係に踏み込んだ研究は新聞記事を対象としたものが多い \cite{Radev2000,宮部:2005,宮部:2006,難波:2005}．Radev は新聞記事間に観察できる文間関係を「同等 (Equivalence)」「反対 (Contradiction)」など24種類に分類する Cross-Document Structure Theory を提案した \cite{Radev2000}．これら文間関係のうち，宮部らは「同等」「推移」関係の特定に \cite{宮部:2005,宮部:2006}，難波らは「推移」「更新」関係の特定に特化した自動推定手法を提案している \cite{難波:2005}．これらの各研究では「同等」「推移」「更新」関係を特定するために文同士が類似しているなどの特徴を利用している．これらの研究と我々の研究を比較すると，まず，これらの研究が扱う新聞記事間における文対応と我々が扱う往信-返信文書間における文対応は異なった傾向を持っている．すなわち，新聞記事では同じ事象に対して複数の書き手が記事を作成したり，事象の経過により状況が異なったりすることで文書間や文書を跨いだ文間に対応が発生するのに対し，往信-返信文書ではコミュニケーションという目的を達成するために文対応が発生するという違いがある．また，我々が対象としている往信-返信文書対における文対応では，先に見た通り類似しない文同士にも文対応が存在することもあり，対応する文同士が類似していることを前提にせずに推定を行う必要がある．

新聞記事以外では，地方自治体間の条例を対象とした研究 \cite{竹中要一:2012-09-30}，料理レシピと対応するレビュー文書を対象とした研究 \cite{Druck2012} がある．竹中・若尾は地方自治体間で異なる条例を条文単位で比較する条文対応表を作成するために，条文間の対応を自動で推定する手法を提案している \cite{竹中要一:2012-09-30}．また Druck \& Pang は，レシピに対応するレビュー文書に含まれる作り方や材料に対する改善提案文の抽出を目的とし，その最終過程で提案文をレシピの手順と対応付ける手法を提案している \cite{Druck2012}．ただし，推定するべき対応が類似していることを前提としている（すなわち，竹中・若尾の場合は同一の事柄に関する条例を対応付ける手法であり，Druck \& Pang の場合はレシピ手順とレビュー文を対応付ける手法である）ため，これらの手法も対応する文の間に同じ単語や表現が出現していることを前提としている．

対話を対象とした研究には，Boyer らによる対話における発話対応関係の分析がある \cite{Boyer2009}．彼女らは，対話における隣接対 (adjacency pair) 構造を隠れマルコフモデル (Hidden Markov Model; HMM) を用いてモデル化している．ただし，彼女らの分析では，隣接対の場合は多くが位置的に隣接している可能性が高いことを前提としている\footnote{隣接対の多くは位置的に隣接しているが，位置的に隣接していない場合もある．例えば，挿入連鎖（隣接対の間に別の隣接対が挿入されるような構造）の場合は，位置的には離れた隣接対が観察される．}．これに対し，我々の研究の対象である文書対における文対応ではこういった傾向を利用できないという違いがあるため，単純に彼女らの分析手法を我々が対象としている文対応に適用することはできない．

我々の研究と最も近い研究として，Qu \& Liu の質問応答ウェブサイトにおける文依存関係（sentence dependency; 質問に対する回答，回答に対する解決報告など）を推定する研究がある \cite{Zhonghua2012}．彼らは条件付確率場 (Conditional Random Fields; CRF) \cite{Lafferty2001} による分類器を利用することで，隠れマルコフモデルよりも高い性能で文依存関係を特定できたとしている．ただし，彼らの対象としているウェブサイトは図 \ref{fig:ex-dependency-c} に示すように対話に近い形で問題解決を図るという特徴を持っているため，我々の対象とする文書対とは若干の違いがある．そこで，本論文では最初に彼らの手法に変形を加えることで，我々の対象である往信-返信文書間の文対応推定が実現できることを示す．次に，彼らの手法の中心である文種類推定モデルと文対応推定モデルを発展させた文種類・文対応を同時に推定する統合モデルを提案し，文対応推定が更に高い性能で実現できることを示す．

\begin{figure}[t]
\begin{center}
\includegraphics{22-1ia2f2.eps}
\end{center}
\caption{Qu \& Liu が扱う文依存関係の例}
\label{fig:ex-dependency-c}
\end{figure}




************************ [./logs/V22N02-01/related_study] ************************
関連研究 \label{chp:related_work}
\vspace{-0.5\Cvs}

テキスト中の特定の語句や表現を抽出する処理を固有表現抽出や用語抽出とい
う．固有表現は，主に人名・地名・組織名などの固有名詞や時間・年齢などの
数値表現を指し，固有表現抽出では，これらの固有表現を抽出の対象とするこ
とが多い．一方，用語抽出では特定の分野の専門用語などを対象とする．しか
し，対象とする語句を抽出するというタスク自体に違いはないため，本稿では，
特に両者を区別せず，抽出対象として指定される語句を固有表現と呼ぶ．


\subsection{医療用語抽出研究}

日本語の医療文書を対象に医療用語の抽出や探索を行った研究として，
\pagebreak
\cite{inoue2001iryo,kinami2008kango,uesugi2007n-gram}が
ある．井上ら\cite{inoue2001iryo} は，文章の記述形式に定型性のある医療論
文抄録を対象に，パターンマッチングに基づく方法を用いて，病名（「論文が
取り扱っている主病名」）や診療対象症例（「診断，治療の対象とした患者，
症例」）などの事実情報を抽出した．たとえば，病名は「〜症」「〜炎」「〜
腫」などの接尾辞の字種的特徴を手がかりに用い，診療対象症例に対しては，
「対象は〜」「〜を対象とし」のような対象症例と共起しやすい文字列を手が
かりに用いて抽出している．なお，井上らの報告によると，論文抄録中に含ま
れる病名や診療対象症例の出現回数は平均1回強である．アノテーションを行っ
た医療論文抄録を用いた抽出実験では，病名や診療対象症例に関して，90\%前
後の適合率，80\%から100\%近い再現率という高い精度を得ている．しかし，本
稿で対象とする病歴要約では論文抄録と出現の傾向が異なり，同一文書中に様々
な病名が出現することが多いため，井上らのパターンマッチングに基づく手法
では抽出精度に限界があると考えられる．

木浪ら\cite{kinami2008kango} は，専門用語による研究情報検索への応用を目
的として，再現率の向上を優先した看護学用語の抽出手法を提案した．抽出対
象とされた専門用語は，解剖学用語（「血小板」，「破骨細胞」など）や看護
行為（「止血」，「酸素吸入」など）を含み，本稿で対象とする症状・診断名
よりも広い領域の用語である．特定の品詞を持つ語が連続した場合にそれらの
語を連接して抽出するなど，連接ルールに基づくシステムにより専門用語を抽
出した．システムによる抽出を行うフェーズと，抽出されなかった用語や誤っ
て抽出された用語を人手で分析してルールの修正・追加を行うフェーズを繰り
返し，再現率が向上し，再現率が低下しない範囲で適合率が向上するようなルー
ル集合を導出している．実験では，専門家によるアノテーションを行った看護
学文献を用いて評価し，再現率約80\%という抽出結果を得ている．この手法で
は，適用する専門用語の領域が異なる場合，再び導出手順を踏んでルールを導
出し直す必要がある．しかし，抽出ルール修正の過程が人手による判断に依存
しており，再度ルールを導出する際の人的コストが大きい．また，適合率が
約40\%と低く，適合率を改善するにはルール導出の基準自体も修正する必要が
生じる．

上杉\cite{uesugi2007n-gram}は，医療用語抽出の前処理として，医療辞書なし
で医療コーパス中の用語間の分割位置を探索する研究を行った．文字列X，Yの
出現確率に対し，XYが同時に出現する確率が十分に低ければX，Y間を分割でき
るとの考えに基づき，コーパスから求めた文字列の出現確率と相互情報量を使
用して分割位置を決定している．症例報告論文を用いた実験では，約740語に対
して60\%の分割精度\footnote{この文献で63.4\%と報告されている分割位置探
  索の成功事例の中には，分割された単語の内部でさらに不適切な位置で分割
  されたものが含まれている．意味のある区切りで分割された事例のみ考慮す
  ると，分割精度は約60\%であった．}であり，分割に成功した事例の中には，
複合語が1語と認識される場合と複合語の内部でさらに分割される場合がほぼ同
等の割合で存在した．残りの40\%には，助詞が付加される，英字やカタカナ列
の途中で分割されるなどの誤りがあるため，自然言語文に対する分割精度とし
て十分であるとはいえない．なお，医療用語抽出に応用するには，分割された
各単語が医療用語か否かを判定する基準が別途必要となる．


\subsection{医療言語処理ワークショップとNTCIR-10 MedNLP}

近年，医療文書を対象とした共通タスクを設定し，研究コミュニティでのデー
タ共有や，データ処理技術の向上を目的とする参加型ワークショップが開催さ
れている．英語の医療文書を処理の対象としたタスクとしては，2011年およ
び2012年にNISTが主催するTRECにおいてMedical Records trackが設定さ
れ，2006年および2008年から2012年に渡ってi2b2 NLPチャレンジが開催された．
i2b2 NLPチャレンジでは，診療記録からの情報抽出
技術の評価を目的とした共通タスクが実施され，
匿名化のための個人情報\cite{uzuner2007evaluating}，患者の喫煙状態\cite{uzuner2008identifying}，
医薬品の使用状況\cite{uzuner2010extracting}や医療上のコンセプト\cite{uzuner20112010}の抽出が行われた．
また，日本語の医療文書を使用したタスクとして，
2013年にはNIIが主催するNTCIRにおいてMedNLPタスク\cite{morita2013overview}が設定され，
患者の個人情報や診療情報を対象に情報抽出技術の評価が行われた．

NTCIR-10 MedNLPタスクでは，医師により書かれた架空の患者の病歴要約からな
る日本語のデータセット（MedNLPテストコレクション）が使用された．データ
から患者の年齢，日時などの個人情報を抽出する「匿名化タスク」と，患者の
症状や医師の診断などの診療情報（症状・診断名）を抽出する「症状と診断タ
スク」などが設定された．症状・診断名には，症状の罹患の肯定，否定などを
表すモダリティ属性が定義されており，モダリティ属性の分類もタスクの一部となっ
ている．なお，両方のタスクとも，それぞれ個人情報，診療情報を固有表現と
した固有表現抽出とみなせる．タスク参加者のシステムは，ルールに基づく手
法よりも機械学習に基づく手法が多く，特に，学習アルゴリズムとしてCRF
(Conditional Random Fields) \cite{lafferty2001conditional} の代表的な
モデルであるlinear-chain CRFを用いたシステムが高い性能を発揮した．
また，成績上位のシステムでは，文中の各単語が辞書中の語とマッチしたか否を
表す情報（辞書素性）が共通して用いられており，語彙資源の利用が精度向上
に寄与したことがわかる．一方，匿名化タスクではルールに基づく手法も有効
であり，最高性能を達成したのはルールベースのシステムであった．

Miuraら\cite{miura2013incorporating} は，固有表現抽出タスクを文字単位の
系列ラベリング\footnote{文をトークン（文字や単語）の列とみなし，
各トークンに対して固有表現か否かなどを表すラベルを推定していく方法を指す．}
として定式化してlinear-chain CRFを適用し，
症状と診断タスクで最も高い精度を達成した．固有表現の抽出を行った後，
抽出した固有表現のモダリティ属性を決定するという2段階の方法を使用している．
MEDIS標準マスターおよびICH国際医薬用語集\footnote{https://www.pmrj.jp/jmo/php/indexj.php}
を語彙資源に用いて辞書素性を与えている．

Laquerreら\cite{laquerre2013necla}，Imaichiら
\cite{imaichi2013comparison} は，ともに単語単位の系列ラベリングとして
linear-chain CRFを適用し，症状と診断タスクでそれぞれ2番目，3番目の精度を達成している．
Laquerreらは，ライフサイエンス辞書と
UMLS Metathesaurusを利用し，辞書素性を導入している．また，事前知識に基
づくヒューリスティック素性として，「ない」「疑い」などモダリティ属性判別の
手がかりとなる表現を素性としている．Imaichiらは，Wikipediaから収集した
病名，器官名などの用語集に基づく辞書素性を導入している．



************************ [./logs/V22N02-02/related_study] ************************
関連研究
\label{section_work}

\ref{section_intro}節で述べたとおり，我々の提案する
レシピ用語タグ付与コーパス
は，レシピテキストが単語に分割されていることを
前提としている．本節では，まずレシピテキストに対する自動単語分割の現状について述べる．
次に，系列ラベリングによるレシピ用語の自動認識手法として用いる，
一般的な固有表現認識手法について説明する．
最後に，レシピ用語の自動認識結果の応用について述べる．


\subsection{レシピテキストに対する自動単語分割}

本論文で提案する
レシピ用語タグ付与コーパス
は，各文
のレシピ用語の箇所
が適切に
単語に分割されている
ことを前提としている．したがって，コーパス作成に際しては，
    自動単語分割 \mbox{(森, Neubig, 坪井 2011)} や
形態素解析\cite{形態素解析システム「茶筌」,Conditional.Random.Fields.を用いた日本語形態素解析,日本語形態素解析システムJUMAN使用説明書.version.3.2}など
を前処理として行い，レシピ用語の箇所のみを人手で修正することが必要となる．
    \nocite{点予測による単語分割}
自動単語分割器や形態素解析器をレシピテキストに適用する際に問題となるのは，分野の特殊性
に起因する解析精度の低下である．実際，文献\cite{自然言語処理における分野適応}では，
『現代日本語書き言葉均衡コーパス』
\cite{Balanced.corpus.of.contemporary.written.Japanese2}
から学習した自動単語分割器によるレシピに対する単語分割精度が96.70\%で
あり，学習コーパスと同じ分野のテストコーパスに対する精度(99.32\%)よりも大きく低下する
ことを報告している．この文献ではさらに，10時間の分野適応作業を行い，精度が97.05\%に向
上したことを報告している．

本論文で詳述する
レシピ用語タグ付与コーパス
の構築に際しては，
レシピ用語
となる箇
所の単語境界付与も行うことになる\footnote{後述するIOB2タグは単語に付与されるため，適切な単語境界情報が前提となる．}．
この作業を実際に行う際には，まず前処理としてレシピテキストに対する
自動単語分割を行い，その後人手でレシピ用語となる箇所を確認しながら
タグ付与を行っている．
しかしながら，
レシピ用語
とならない箇所への単語境界
情報付与はアノテーションコストの増加を避けるため行っていない．したがって，自動単語分割
の学習コーパスとしては，
文の一部（レシピ用語となる箇所）にのみ信頼できる単語境界情報が
付与されており，レシピ用語以外の箇所においては信頼性の低い単語境界情報を持つ
部分的単語分割コーパスとみなすことができる．
部分的単語分割コーパスも学習コーパスとすることが可能な自動単語分割器
    (森 他 2011) を用いる場合は，我々のコーパスにより，自動単語分割の精度も向上すると考えられる．


\subsection{固有表現認識}
\label{rw_ner}

一般分野の固有表現タグ付与コーパスとして，新聞等に人名や組織名などのタグを付与したコーパスがす
でに構築されている
\cite{Message.Understanding.Conference.-.6:.A.Brief.History,IREX:.IR.and.IE.Evaluation.Project.in.Japanese}
．\ref{section_intro}~節で述べたように，
本論文で述べる固有表現は単語列であり
，コーパスに対するアノテーションでは，以下の例が示すように
IOB2方式
\cite{Representing.Text.Chunks}
を用いて各単語にタグが付与される．
\begin{quote}
  ９９/Dat-B \ 年/Dat-I \ ３/Dat-I \ 月/Dat-I カルロス/Per-B \ ゴーン/Per-I \ 氏/O が
  日産/Org-B \ の/O \ 社長/O \ に/O \ 就任/O
\end{quote}
ここで，Datは日付，Perは人名，Orgは組織名を意味し，それぞれに最初の単語であることを意
味するB (Begin)や同一種の固有表現の継続を意味するI (Intermediate)が付与されている．さ
らに，O (Other)はいずれの固有表現でもないことを意味する．本論文では，各単語に付与され
るタグをIOB2タグと呼ぶ．また，単語列に与えられる固有表現クラスを固有表現タグ（上の例では
Dat やPerなど）と呼ぶこととする．したがって，IOB2タグの種類数は固有表現タグの2倍より1多
い．これは本論文で取り扱うレシピ用語に関しても同様であり，それぞれをIOB2タグ・レシピ
用語タグと記述する．

自動固有表現認識は，系列ラベリングの問題として解かれることが多い
\cite{A.Maximum.Entropy.Approach.to.Named.Entity.Recognition,Conditional.Random.Fields:.Probabilistic.Models.for.Segmenting.and.Labeling.Sequence.Data,Introduction.to.the.CoNLL-2003.Shared.Task:.Language-Independent.Named.Entity.Recognition}
．一般分野の固有表現認識に対しては，1万文程度の学習コーパスが利用可能な状況では，80\%〜90\%の精度が得られると報
告されている．

レシピの自然言語処理においては，これら一般的分野の固有表現タグセットは有用ではない．出
現する人はほぼ調理者のみであり，人名や組織名は出現することはない．人工物のほとんどは，
食材と道具であり，これらを区別する必要がある．数量表現としては，継続時間と割合を含む量
の表現が重要である．
さらに，一般分野における固有表現タグセットとの重要な差異として，
調理者の行動や食材の挙動・変化を示す用言を区別・認識する必要
\cite{Structural.Analysis.of.Cooking.Preparation.Steps.in.Japanese}
が挙げられる．
このような分析から，我々は
レシピ用語のタグセット
を新たに設計した．
レシピ用語の定義については，
次節以降で詳述する．ただし，多くの固有表現抽出の研究を踏襲し，
レシピ用語は互いに重複しないこととし，
レシピ用語の自動認識
の課題に対しては，一般的分野の固有表現認識と同様の手法を用いることが可能とな
るようにした．


\subsection{レシピ用語の自動認識の応用}

レシピを対象とした自然言語処理の研究は多岐にわたる．ここでは，我々のコーパスが貢献で
きるであろう取り組みに限定して述べる．

山本ら\cite{食材調理法の習得順に関する一検討}は，大量のレシピに対して食材と調理動作の
対を抽出し，調理動作の習得を考慮したレシピ推薦を提案している．この論文では，レシピテキ
ストを形態素解析し，動詞を調理動作とし，予め用意した食材リストにマッチする名詞を食材と
している．食材に対しては，複合語が考慮されており，直前が名詞の場合にはこれを連結する．
この論文での食材と調理動作の表現の認識は非常に素朴であり，未知語の食材名に対応すること
ができないことや，
食材が主語となる動詞（レシピテキストに頻出）を調理動作と誤認する
などの問題点が指摘される．

Hamadaら\cite{Structural.Analysis.of.Cooking.Preparation.Steps.in.Japanese}は，レシピ
を木構造に自動変換することを提案している．変換処理の第一段階として，食材や調理動作の認
識を行っている．しかしながら，認識手法は予め作成された辞書との照合であり，頑健性に乏しい．

以上の先行研究では，いずれも，食材や調理動作等をあらかじめリストとして用意することで
問題が生じていると考えられる．
我々の提案するレシピ用語タグ付与コーパス
，およびそれを学習データとして構築されるレシピ用語の自動認識器
\footnote{http://plata.ar.media.kyoto-u.ac.jp/mori/research/topics/NER/ にて公開・配布している．}
は，その問題を解決
しようとするものである．
加えて，
レシピ用語の自動認識
には，これを実際に行っている調理
映像とのマッチングなどの興味深い応用がある
\footnote{調理映像とのマッチングのような応用においては，
レシピ用語の自動認識だけでなく，
レシピ用語同士の関係を自動認識する技術も必要となるが，本論文において
は議論の対象としない．}
\cite{料理映像の構造解析による調理手順との対
応付け}．映像処理の観点からは，調理は制御された比較的狭い空間で行われるので，カメラな
どの機材の設置が容易であり，作業者が1人であるため重要な事態はほぼ1箇所で進行し，比較的
扱いやすいという利点がある．実際，映像処理の分野では，実際に調理を行っている映像を収録
しアノテーションを行っている
\cite{調理行動モデル化のための調理観測映像へのアノテーション}
．あるレシピの
レシピ用語の自動認識
結果と当該レシピを実施している映像の認識結果とを合わせる
ことで，映像中の食材や動作の名称の推定や，テキスト中の単語列に対応する映像中の領域の推
定（図\ref{figure_0001}参照）を含む自然言語処理以外の分野にも波及する研究課題を実施する
題材となる
．さらに，本論文で詳述するコーパス作成に関する知見は，レ
シピ以外の分野の手順文章においても，映像との統合的処理や新たな機能を持つ検索などの実現
の参考になると考えられる．

\begin{figure}[t]
  \begin{center}
\includegraphics{22-2ia2f1.eps}
  \end{center}
  \caption{レシピテキストと調理映像のマッチング例}
  \label{figure_0001}
\end{figure}



************************ [./logs/V22N04-01/related_study] ************************
本課題の意義について

本研究では特定の分野の関連語・周辺語または説明文書を入力としたときの
\pagebreak
検索用語の予測・提示を行う検索支援を想定している．まず，説明文書による支援の意義は，たとえばThe 5th NTCIR Workshop Meeting on 
Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Accessのようなワークショップ型共同研究\cite{Ma}における長い文書を検索課題\footnote{検索課題例：AOLとタイムワーナー合併の影響に関する記事を探したい．AOL・タイムワーナー合併がインターネットとエンターテインメントというメディア産業に与える影響に関する意見を適合とする．AOL・タイムワーナー合併の展開についての記述は部分的に適合とする．総額と所有権転換の仕組みに関する情報は不適合とする．}としたタスクからも類推できる．
つまり，たとえばユーザが関連語・周辺語もはっきりわからないときはその支援要求を文書の形で伝える（入力する）ニーズはあると考える．また一方，当然のことではあるが，本研究では，少数キーワード（関連語・周辺語）による検索用語の予測も期待している．実際，表~\ref{tab:keyword-prec}は，DBNについて，各学習データセットを用いた場合の，表~\ref{tab:keyword}に示す 3 関連語・周辺語（$+1$ ノイズ語）\footnote{これらのキーワードは予備実験も含め一切精査せずに著者らの知識に頼って手動収集のデータから関連語・周辺語・ノイズ語としてふさわしいものを主観で選んでいる．しかし当然なことではあるが，これらのキーワードはすべて誰にも知られている用語である保証はない（また，本実験の目的からしてそう保証する必要もない）．}による全検索用語の平均予測精度を示している\footnote{当然のことではあるが，予測精度は用いるキーワードに大きく依存する．試しに10検索用語のうち6検索用語の関連語・周辺語を意識的に関連性の弱いものを選んで実験すると平均精度が8割程度までに下がった．}．実験はまだ小規模ではあるが，この結果は
提案手法が少数キーワードによる支援も可能であることを示唆していると思われる．

\begin{table}[t]
\caption{予測に用いるキーワード}
\label{tab:keyword}
\input{01table13.txt}
\end{table}
\begin{table}[t]
\caption{DBNの少数キーワードによる予測精度（223次元の特徴ベクトルを用いた場合）}
\label{tab:keyword-prec}
\input{01table14.txt}
\end{table}



************************ [./logs/V22N05-02/related_study] ************************
関連研究
\label{sec:relatedworks}

自然言語処理の研究は，Twitterを始めとしたソーシャルメディアの解析におい
て 2 つの主要なタスクに取り組んできたと言える：(1) ひとつは実在する自
然言語処理の技術をノイジーなテキストに適応させることで，(2) もうひと
つは，そこから知識や統計量を抽出することである．

前者としては品詞タグ付けの精度改善~\cite{gimpel-EtAl:2011:ACL-HLT2011}
や固有表現認識~\cite{plank-EtAl:2014:Coling}のタスクを始めとして，崩れ
た単語の正規化などが行われてきた
\cite{han-baldwin:2011:ACL-HLT2011,chrupala:2014:P14-2}．

\begin{table}[t]
\caption{Twitterを用いた関連研究}
\label{relwork}
\input{02table23.txt}
\end{table}

後者としては，イベント抽出やユーザ行動分析など様々なアプリケーションが提案されてきた（表\ref{relwork}）．
なかでも，疾患，特に即時的な把握が必要される感染症の流行検出は，主要なTwitter利用法の 1 つとして多くの研究がある．


感染症の流行は，毎年，百万人を越える患者を出しており，重要な国家的課題
となっている\cite{国立感染症研究所2006}．
中でも，インフルエンザは事前に適切なワクチンを準備することにより，重篤
な状態を避けることが可能なため，感染状態の把握は各国における重要なミッ
ションとなっている\cite{Ferguson2005}．

この把握は\textbf{ インフルエンザ・サーベイランス}と呼ばれ，膨大なコス
トをかけて調査・集計が行われてきた．インフルエンザ以外でも， West Nile
ウィルス検出 \cite{sugumaran2012real}など感染症の把握にTwitterなどのソーシャルメディ
アを利用する試みは多い．

本邦においてもインフルエンザが流行したことによって総死亡者数は，毎年１
万人を超えており\cite{大日2003}，国立感染症研究所を中心にインフルエンザ・
サーベイランスが実施されている
\footnote{https://hasseidoko.mhlw.go.jp/Hasseidoko/Levelmap/flu/index.html}．

しかし，これらの従来型の集計方式は，集計に時間がかかり，また，過疎部に
おける収集が困難だという問題が指摘されてき
た\footnote{http://sankei.jp.msn.com/life/news/110112/bdy11011222430044-n1.htm}．
このような背景のもと，近年，ソーシャルメディアを用いた感染症サーベイラ
ンスが，現行の調査法と比べて大規模かつ，即時的な収集を可能にするとして，
数多く提案されている
\cite{Lampos2010,culotta2010detecting,Paul2011,aramaki-maskawa-morita:2011:EMNLP,Tanida2011}．

しかしながら，実際にTwitterからインフルエンザに関する情報を収集するのは
容易ではない．例えば，ニュースや有名人の罹患に関するリツイートなど，多
くのノイズが混入する．Aramakiら\cite{aramaki-maskawa-morita:2011:EMNLP}によると，「インフルエン
  ザ」に関するツイートの半数は，本人の罹患に関するものではないと報告さ
れている．

これを解決するための 1 つの方法は，キーワードのセットを適切に選ぶ方法が
考えられる．例えば，「インフルエンザ」だけでなく「高熱」や「休む」など
のキーワードを加えることで，より確かに罹患者を抽出できると考えられる．
そこで，インフルエンザの流行と相関の高いキーワード群を，L1正規化を用い
た単語の次元圧縮によって得る方法\cite{Lampos2010}，疾患をある種のトピッ
クとみたてトピックモデルを用いる方法\cite{Paul2011}や，素性選択を
適応する手法\cite{Tanida2011}などが提案されている．

一方で，キーワードを固定して，疾患・症状がポジティブな発言のみを分類す
るというアプローチもある．高橋ら\cite{Takahashi2011}の Boostingを用いた
文書分類，Aramakiら\cite{aramaki-maskawa-morita:2011:EMNLP}がSVMによる分類手法を提案している．
本研究も後者をアプローチに属するが，モダリティの解析や，主体の解析いう
2 つの自然言語処理の問題を導入することで，精度を高めることに成功した．

以降，この 2 つの自然言語処理研究について関連研究をまとめる．


\subsection{主体解析の関連研究}

本研究で扱った主体解析とは，疾患に関係のある名詞の項を判別する意味解析
だと考えることができる．
関連する研究としては，PropBank~\cite{PropBank2004}は動詞の意味役割を大
規模にアノテートした初めてのコーパスであり，NomBank~\cite{NomBank2004}
は，それと似た規則で名詞の項にラベルが付与されている．
例えばNomBankでは，``There have been no customer
\underline{\mbox{complaints}} about that issue.''において，\textsc{arg0} とし
て，``customer''がアノテートされ，\textsc{arg1}として``issue''がアノテー
トされる．さらに，このアノテーションが扱う範囲を広げる研究もある
\cite{Gerber:2010}．

日本語においても，京都大学テキストコーパス4.0\footnote{http://nlp.ist.i.kyoto-u.ac.jp/} やNAISTテキストコーパス
\cite{iida2010}において，事態性名詞の項が付与されるなど近いアノテーションが試みられている．
Komachiら\cite{komachi2007}は，対象となる名詞に事態性があるか否かの事態性判別と，その後の項同定を別タスクとして扱い，解析精度を報告している．また，「娘の風邪」などの名詞句内の関係を解析する研究~\cite{sasano2009}も関連がある．
発言内で疾患・症状の主体が省略されていることも多いため，省略・照応解析~\cite{sasano2008}とも関連がある．

本研究で扱う課題も，基本的には，ある疾患に関する表現に関する項 (\textsc{arg0}) の同定を行なうタスクとみなすことができる．

しかし，疾患に関する表現，例えば，「寒気」，などは意味としては事態性を
もった概念であるが，文法的には，事態性があると認められず，単純な事態性
の名詞の項同定として考えることはできない．
つまり，意味的な疾患概念が，文法的な事態に対応づけられない場合がある．

しかも，今までの事態性名詞の研究は主に新聞を元にしたコーパスで行われており，
Twitter上への適応が困難だという技術的問題もある．これらの理由から，我々は疾患を保有する主体の推定を目的とし，主体推定器のためにラベルを付与
することを試みた．


\subsection{モダリティ解析の関連研究}


先行研究\cite{aramaki-maskawa-morita:2011:EMNLP}では，モダリティに関しての事例を集めたコーパ
スを作成することでモダリティ情報を利用しているが，本研究では，既存のリ
ソースやツールを活用することで，コーパス作成の手間を省き，一般的なモダリティ解析
が疾患のモダリティ解析にも貢献することを示した．


日本語モダリティに関するリソースとしては，文献
\cite{matsuyoshi2010}が態度表明， 時制，仮想，態度， 真偽判断，価値判断，
焦点などについて詳細に事象アノテーションを行っている．焦点を除いた6種の
項目を拡張モダリティと呼び，情報抽出や含意認識といった自然言語処理のタ
スクへの応用に向けて研究が行われている．
本研究は，意味IDとして，これを素性化したが，モダリティ間の類似関係など，
さらに緻密な素性化が可能であり，今後の課題としたい．

また， このような研究は日本語だけでなく英語に関しても活発であり，文献
\cite{sauri2012you}がモダリティを用いて， 事実性の度合いを判断する研究
を行っている．
また，特にモダリティの一部である否定(Negation)
や推量 (Speculation)については，情報抽出の実用化のために重要であり，
専門のワークショップ [NeSp-NLP 2010] が開
催されるなど盛んに研究されてきている．

本研究にこれらの知見を導入することで，さらなる精度向上が期待される．
 




************************ [./logs/V22N05-04/related_study] ************************
関連研究

ゼロ照応問題に対して解析の手がかりとするための情報は，
これまでにも様々考えられてきた．具体的に推定モデルに組み込まれた例としては，
一般的な統語係り受けパス情報の他に，
(1) 各述語がどのような語を項として取りやすいかという
選択選好の情報として，名詞，格助詞，述語の共起に関する統計値を用いる
手法~\cite{iida2006exploiting,iida2011cross,imamura2009discriminative,sasano2008fully,sasano2011discriminative}や，
(2) 語が提題化された場合など，文章中のそれぞれの位置における，特定の語の顕現性を表すスコアを用いる手法\cite{sasano2008fully,sasano2011discriminative,imamura2009discriminative,iida2011cross}などがある．

また，複数述語間の項の共有に関する情報として，(3) 支援動詞辞書を用いる手法~\cite{komachi2006noun}や，
    (4) スクリプト的な知識を学習する手法 (飯田, 徳永 2010; 大内, 進藤, Kevin, 松本 2015)\nocite{iida2010jnlp}\nocite{ouchi2015nl}，
(5) それぞれの述語の格スロットに出現する項の類似度を用いる手法~\cite{hayashibe2011japanese}，
(6) 直前の述語に対する項構造の解析結果を直後に出現する述語の解析に利用する手法\cite{imamura2009discriminative,hayashibe2014position}などが存在する．

そのほか，技術資料としては示されていないものの，述語項構造解析器ChaPASの0.74版\cite{chapas2013}や
KNP\cite{knp2013}は，(7) 項構造解析の前段の処理として並列構造解析を行っている．

しかし一方で，そもそもゼロ照応問題にどのような現象がどの程度あらわれるのか，
あるいは，特定の解析モデルが焦点をあてている課題について，どの程度の割合を解くことが出来たか
といった定量的な分析はこれまでになされておらず，今後具体的にどのような種類の
問題を中心に取り組めばよいか不明瞭な状態となっている．


